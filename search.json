[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT303-1",
    "section": "",
    "text": "This book is developed for the course STAT303-1 (Data Science with Python-1)."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html",
    "href": "Introduction to Python and Jupyter Notebooks.html",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "",
    "text": "This chapter is a very brief introduction to python and Jupyter notebooks. We only discuss the content relevant for applying python to analyze data."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#installation",
    "href": "Introduction to Python and Jupyter Notebooks.html#installation",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "1.1 Installation",
    "text": "1.1 Installation\nAnaconda: If you are new to python, we recommend downloading the Anaconda installer and following the instructions for installation. Once installed, we’ll use the Jupyter Notebook interface to write code.\nQuarto: We’ll use Quarto to publish the .ipynb file containing text, python code, and the output. Download and install Quarto from here."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#jupyter-notebook",
    "href": "Introduction to Python and Jupyter Notebooks.html#jupyter-notebook",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "1.2 Jupyter notebook",
    "text": "1.2 Jupyter notebook\n\n1.2.1 Introduction\nJupyter notebook is an interactive platform, where you can write code and text, and make visualizations. You can access Jupyter notebook from the Anaconda Navigator, or directly open the Jupyter Notebook application itself. It should automatically open up in your default browser. The figure below shows a Jupyter Notebook opened with Google Chrome. This page is called the landing page of the notebook.\n\n\n\n\n\nTo create a new notebook, click on the New button and select the Python 3 option. You should see a blank notebook as in the figure below.\n\n\n\n\n\n\n\n1.2.2 Writing and executing code\nCode cell: By default, a cell is of type Code, i.e., for typing code, as seen as the default choice in the dropdown menu below the Widgets tab. Try typing a line of python code (say, 2+3) in an empty code cell and execute it by pressing Shift+Enter. This should execute the code, and create an new code cell. Pressing Ctlr+Enter for Windows (or Cmd+Enter for Mac) will execute the code without creating a new cell.\nCommenting code in a code cell: Comments should be made while writing the code to explain the purpose of the code or a brief explanation of the tasks being performed by the code. A comment can be added in a code cell by preceding it with a # sign. For example, see the comment in the code below.\nWriting comments will help other users understand your code. It is also useful for the coder to keep track of the tasks being performed by their code.\n\n#This code adds 3 and 5\n3+5\n\n8\n\n\nMarkdown cell: Although a comment can be written in a code cell, a code cell cannot be used for writing headings/sub-headings, and is not appropriate for writing lengthy chunks of text. In such cases, change the cell type to Markdown from the dropdown menu below the Widgets tab. Use any markdown cheat sheet found online, for example, this one to format text in the markdown cells.\nGive a name to te notebook by clicking on the text, which says ‘Untitled’.\n\n\n1.2.3 Saving and loading notebooks\nSave the notebook by clicking on File, and selecting Save as, or clicking on the Save and Checkpoint icon (below the File tab). Your notebook will be saved as a file with an exptension ipynb. This file will contain all the code as well as the outputs, and can be loaded and edited by a Jupyter user. To load an existing Jupyter notebook, navigate to the folder of the notebook on the landing page, and then click on the file to open it.\n\n\n1.2.4 Rendering notebook as HTML\nWe’ll use Quarto to print the **.ipynb* file as HTML. Check the procedure for rendering a notebook as HTML here. You have several options to format the file.\nYou will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#in-class-exercise",
    "href": "Introduction to Python and Jupyter Notebooks.html#in-class-exercise",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "In-class exercise",
    "text": "In-class exercise\n\nCreate a new notebook.\nSave the file as In_class_exercise1.\nGive a heading to the file - First HTML file.\nPrint Today is day 1 of class.\nCompute and print the number of hours of this course in the quarter (that will be 10 weeks x 2 classes per week x 1.33 hours per class).\n\nThe HTML file should look like the picture below."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#python-language-basics",
    "href": "Introduction to Python and Jupyter Notebooks.html#python-language-basics",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "1.3 Python language basics",
    "text": "1.3 Python language basics\n\n1.3.1 Object Oriented Programming\nPython is an object-oriented programming language. In layman terms, it means that every number, string, data structure, function, class, module, etc., exists in the python interpreter as a python object. An object may have attributes and methods associated with it. For example, let us define a variable that stores an integer:\n\nvar = 2\n\nThe variable var is an object that has attributed and methods associated with it. For example a couple of its attributes are real and imag, which store the real and imaginary parts respectively, of the object var:\n\nprint(\"Real part of 'var': \",var.real)\nprint(\"Real part of 'var': \",var.imag)\n\nReal part of 'var':  2\nReal part of 'var':  0\n\n\nAttribute: An attribute is a value associated with an object, defined within the class of the object.\nMethod: A method is a function associated with an object, defined within the class of the object, and has access to the attributes associated with the object.\nFor looking at attributes and methods associated with an object, say obj, press tab key after typing obj..\nConsider the example below of a class example_class:\n\nclass example_class:\n    class_name = 'My Class'\n    def my_method(self):\n        print('Hello World!')\n\ne = example_class()\n\nIn the above class, class_name is an attribute, while my_method is a method.\n\n\n1.3.2 Assigning variable name to object\nWhen an object is assigned to a variable name, the variable name serves as a reference to the object. For example, consider the following assignment:\n\nx = [5,3]\n\nThe variable name x is a reference to the memory location where the object [5, 3] is stored. Now, suppose assign x to a new variable y:\n\ny = x\n\nIn the above statement the variable name y now refers to the same object [5,3]. The object [5,3] does not get copied to a new memory location referred by y. To prove this, let us add an element to y:\n\ny.append(4)\nprint(y)\n\n[5, 3, 4]\n\n\n\nprint(x)\n\n[5, 3, 4]\n\n\nWhen we changed y, note that x also changed to the same object, showing that x and y refer to the same object, instead of referring to different copies of the same object.\n\n\n1.3.3 Importing libraries\nThere are several built-in functions in python like print(), abs(), max(), sum() etc., which do not require importing any library. However, these functions will typically be insufficient for a analyzing data. Some of the popular libraries and their primary purposes are as follows:\n\nNumPy: Performing numerical operations and efficiently storing numerical data.\nPandas: Reading, cleaning and manipulating data.\nMatplotlib, Seaborn: Visualizing data.\nSciPy: Performing scientific computing such as solving differential equations, optimization, statistical tests, etc.\nScikit-learn: Data pre-processing and machine learning, with a focus on prediction.\nStatsmodels: Developing statistical models with a focus on inference\n\nA library can be imported using the import keyword. For example, a NumPy library can be imported as:\n\nimport numpy as np\n\nUsing the as keyboard, the NumPy library has been given the name np. All the functions and attributes of the library can be called using the ‘np.’ prefix. For example, let us generate a sequence of whole numbers upto 10 using the NumPy function arange():\n\nnp.arange(8)\n\narray([0, 1, 2, 3, 4, 5, 6, 7])\n\n\n\n\n1.3.4 Built-in objects\nThere are several built-in objects, modules and functions in python. Below are a few examples:\nScalar objects: Python has some built-in datatypes for handling scalar objects such as number, string, boolean values, and date/time. The built-in function type() function can be used to determine the datatype of an object:\n\nvar = 2.2\ntype(var)\n\nfloat\n\n\nDate time: Python as a built-in datetime module for handling date/time objects:\n\nimport datetime as dt\n\n\n#Defining a date-time object \ndt_object = dt.datetime(2022, 9, 20, 11,30,0)\n\nInformation about date and time can be accessed with the relevant attribute of the datetime object.\nrange(): The range() function returns a sequence of evenly-spaced integer values. It is commonly used in for loops to define the sequence of elements over which the iterations are performed.\nBelow is an example where the range() function is used to create a sequence of whole numbers upto 10:\n\nprint(list(range(1,10)))\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\ndt_object.day\n\n20\n\n\n\ndt_object.year\n\n2022\n\n\nThe strftime method of the datetime module formats a datetime object as a string. There are several types of formats for representing date as a string:\n\ndt_object.strftime('%m/%d/%Y')\n\n'09/20/2022'\n\n\n\ndt_object.strftime('%m/%d/%y %H:%M')\n\n'09/20/22 11:30'\n\n\n\ndt_object.strftime('%h-%d-%Y')\n\n'Sep-20-2022'\n\n\n\n\n1.3.5 Control flow\nAs in other languages, python has built-in keywords that provide conditional flow of control in the code.\nIf-elif-else: The if-elif-else statement can check several conditions, and execute the code corresponding to the condition that is true. Note that there can be as many elif statements as required.\n\n#Example of if-elif-else\nx = 5\nif x>0:\n    print(\"x is positive\")\nelif x==0:\n    print(\"x is zero\")\nelse:\n    print(\"X is negative\")\n    print(\"This was the last condition checked\")\n\nx is positive\n\n\nfor loop: A for loop iterates over the elements of an object, and executes the statements within the loop in each iteration. For example, below is a for loop that prints odd natural numbers upto 10:\n\nfor i in range(10):\n    if i%2!=0:\n        print(i)\n\n1\n3\n5\n7\n9\n\n\nwhile loop: A while loop iterates over a set of statements while a condition is satisfied. For example, below is a while loop that prints odd numbers upto 10:\n\ni=0\nwhile i<10:\n    if i%2!=0:\n        print(i)\n    i=i+1\n\n1\n3\n5\n7\n9"
  },
  {
    "objectID": "data_structures.html",
    "href": "data_structures.html",
    "title": "2  Data structures",
    "section": "",
    "text": "In this chapter we’ll learn about the python data structures that are often used or appear while analyzing data."
  },
  {
    "objectID": "data_structures.html#tuple",
    "href": "data_structures.html#tuple",
    "title": "2  Data structures",
    "section": "2.1 Tuple",
    "text": "2.1 Tuple\nTuple is a sequence of python objects, with two key characeterisics: (1) the number of objects are fixed, and (2) the objects are immutable, i.e., they cannot be changed.\nTuple can be defined as a sequence of python objects separated by commas, and enclosed in rounded brackets (). For example, below is a tuple containing three integers.\n\ntuple_example = (2,7,4)\n\nWe can check the data type of a python object using the type() function. Let us check the data type of the object tuple_example.\n\ntype(tuple_example)\n\ntuple\n\n\nElements of a tuple can be extracted using their index within square brackets. For example the second element of the tuple tuple_example can be extracted as follows:\n\ntuple_example[1]\n\n7\n\n\nNote that an object of a tuple cannot be modified. For example, consider the following attempt in changing the second element of the tuple tuple_example.\n\ntuple_example[1] = 8\n\nTypeError: 'tuple' object does not support item assignment\n\n\nThe above code results in an error as tuple elements cannot be modified.\n\n2.1.1 Concatenating tuples\nTuples can be concatenated using the + operator to produce a longer tuple:\n\n(2,7,4) + (\"another\", \"tuple\") + (\"mixed\",\"datatypes\",5)\n\n(2, 7, 4, 'another', 'tuple', 'mixed', 'datatypes', 5)\n\n\nMultiplying a tuple by an integer results in repeitition of the tuple:\n\n(2,7,\"hi\") * 3\n\n(2, 7, 'hi', 2, 7, 'hi', 2, 7, 'hi')\n\n\n\n\n2.1.2 Unpacking tuples\nIf tuples are assigned to an expression containing multiple variables, the tuple will be unpacked and each variable will be assigned a value as per the order in which is appears. See the example below.\n\nx,y,z  = (4.5, \"this is a string\", ((\"Nested tuple\",5)))\n\n\nx\n\n4.5\n\n\n\ny\n\n'this is a string'\n\n\n\nz\n\n('Nested tuple', 5)\n\n\nIf we are interested in retrieving only some values of the tuple, the expression *_ can be used to discard the other values. Let’s say we are interested in retrieving only the first and the last two values of the tuple:\n\nx,*_,y,z  = (4.5, \"this is a string\", ((\"Nested tuple\",5)),\"99\",99)\n\n\nx\n\n4.5\n\n\n\ny\n\n'99'\n\n\n\nz\n\n99\n\n\n\n\n2.1.3 Tuple methods\nA couple of useful tuple methods are count, which counts the occurences of an element in the tuple and index, which returns the position of the first occurance of an element in the tuple:\n\ntuple_example = (2,5,64,7,2,2)\n\n\ntuple_example.count(2)\n\n3\n\n\n\ntuple_example.index(2)\n\n0\n\n\nNow that we have an idea about tuple, let us try to think where it can be used."
  },
  {
    "objectID": "data_structures.html#list",
    "href": "data_structures.html#list",
    "title": "2  Data structures",
    "section": "2.2 List",
    "text": "2.2 List\nList is a sequence of python objects, with two key characeterisics that differentiate them from tuples: (1) the number of objects are variable, i.e., objects can be added or removed from a list, and (2) the objects are mutable, i.e., they can be changed.\nList can be defined as a sequence of python objects separated by commas, and enclosed in square brackets []. For example, below is a list containing three integers.\n\nlist_example = [2,7,4]\n\n\n2.2.1 Adding and removing elements in a list\nWe can add elements at the end of the list using the append method. For example, we append the string ‘red’ to the list list_example below.\n\nlist_example.append('red')\n\n\nlist_example\n\n[2, 7, 4, 'red']\n\n\nNote that the objects of a list or tuple can be of different datatypes.\nAn element can be added at a specific location of the list using the insert method. For example, if we wish to insert the number 2.32 as the second element of the list list_example, we can do it as follows:\n\nlist_example.insert(1,2.32)\n\n\nlist_example\n\n[2, 2.32, 7, 4, 'red']\n\n\nFor removing an element from the list, the pop and remove methods may be used. The pop method removes an element at particular index, while the remove method removes the element’s first occurence in the list by its value. See the examples below.\nLet us say, we need to remove the third element of the list.\n\nlist_example.pop(2)\n\n7\n\n\n\nlist_example\n\n[2, 2.32, 4, 'red']\n\n\nLet us say, we need to remove the element ‘red’.\n\nlist_example.remove('red')\n\n\nlist_example\n\n[2, 2.32, 4]\n\n\n\n#If there are multiple occurences of an element in the list, the first occurence will be removed\nlist_example2 = [2,3,2,4,4]\nlist_example2.remove(2)\nlist_example2\n\n[3, 2, 4, 4]\n\n\nFor removing multiple elements in a list, either pop or remove can be used in a for loop, or a for loop can be used with a condition. See the examples below.\nLet’s say we need to remove intergers less than 100 from the following list.\n\nlist_example3 = list(range(95,106))\nlist_example3\n\n[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]\n\n\n\n#Method 1: For loop with remove\nlist_example3_filtered = list(list_example3) #\nfor element in list_example3:\n    #print(element)\n    if element<100:\n        list_example3_filtered.remove(element)\nprint(list_example3_filtered)\n\n[100, 101, 102, 103, 104, 105]\n\n\n\\(\\color{red}{\\text{Q: What's the need to define a new variable `list\\_example3\\_filtered` in the above code?}}\\)\nReplace list_example3_filtered with list_example3 and identify the issue.\n\n#Method 2: For loop with condition\n[element for element in list_example3 if element>100]\n\n[101, 102, 103, 104, 105]\n\n\n\n\n2.2.2 List comprehensions\nList comprehension is a compact way to create new lists based on elements of an existing list.\nExample: Create a list that has squares of natural numbers from 5 to 15.\n\nsqrt_natural_no_5_15 = [(x**2) for x in range(5,16)]\nprint(sqrt_natural_no_5_15)\n\n[25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225]\n\n\nExample: Create a list of tuples, where each tuple consists of a natural number and its square, for natural numbers ranging from 5 to 15.\n\nsqrt_natural_no_5_15 = [(x,x**2) for x in range(5,16)]\nprint(sqrt_natural_no_5_15)\n\n[(5, 25), (6, 36), (7, 49), (8, 64), (9, 81), (10, 100), (11, 121), (12, 144), (13, 169), (14, 196), (15, 225)]\n\n\n\n\n2.2.3 Concatenating lists\nAs in tuples, lists can be concatenated using the + operator:\n\nimport time as tm\n\n\nlist_example4 = [5,'hi',4] \nlist_example4 = list_example4 + [None,'7',9]\nlist_example4\n\n[5, 'hi', 4, None, '7', 9]\n\n\nFor adding elements to a list, the extend method is preferred over the + operator. This is because using the + operator creates a new list, while the extend method adds elements to an existing list.\n\nlist_example4 = [5,'hi',4]\nlist_example4.extend([None, '7', 9])\nlist_example4\n\n[5, 'hi', 4, None, '7', 9]\n\n\n\n\n2.2.4 Sorting a list\nA list can be sorted using the sort method:\n\nlist_example5 = [6,78,9]\nlist_example5.sort(reverse=True) #the reverse argument is used to specify if the sorting is in ascending or descending order\nlist_example5\n\n[78, 9, 6]\n\n\n\n\n2.2.5 Slicing a list\nWe may extract or update a section of the list by passing the starting index (say start) and the stopping index (say stop) as start:stop to the index operator []. This is called slicing a list. For example, see the following example.\n\nlist_example6 = [4,7,3,5,7,1,5,87,5]\n\nLet us extract a slice containing all the elements starting from the the 3rd position upto the 7th position.\n\nlist_example6[2:7]\n\n[3, 5, 7, 1, 5]\n\n\nNote that while the element at the start index is included, the element with the stop index is excluded in the above slice.\nIf either the start or stop index is not mentioned, the slicing will be done from the beginning or utop the end of the list, respectively.\n\nlist_example6[:7]\n\n[4, 7, 3, 5, 7, 1, 5]\n\n\n\nlist_example6[2:]\n\n[3, 5, 7, 1, 5, 87, 5]\n\n\nTo slice the list relative to the end, we can use negative indices:\n\nlist_example6[-4:]\n\n[1, 5, 87, 5]\n\n\n\nlist_example6[-4:-2:]\n\n[1, 5]\n\n\nAn extra colon (‘:’) can be used to slice every ith element of a list.\n\n#Selecting every 3rd element of a list\nlist_example6[::3]\n\n[4, 5, 5]\n\n\n\n#Selecting every 3rd element of a list from the end\nlist_example6[::-3]\n\n[5, 1, 3]\n\n\n\n#Selecting every element of a list from the end or reversing a list \nlist_example6[::-1]\n\n[5, 87, 5, 1, 7, 5, 3, 7, 4]\n\n\n\n\n2.2.6 Practice exercise\nStart with the list [8,9,10]. Do the following: 1. Set the second entry (index 1) to 17 2. Add 4, 5, and 6 to the end of the list 3. Remove the first entry from the list 4. Sort the list 5. Double the list (concatenate the list to itself) 6. Insert 25 at index 3\nThe final list should equal [4,5,6,25,10,17,4,5,6,10,17]\n\n\n[4, 5, 6, 25, 10, 17, 4, 5, 6, 10, 17]\n\n\nNow that we have an idea about lists, let us try to think where it can be used.\n\n\n\n\n\n \n        \n\n\nNow that we have learned about lists and tuples, let us compare them.\n\\(\\color{red}{\\text{Q: A list seems to be much more flexible than tuple, and can replace a tuple almost everywhere. Then why use tuple at all?}}\\)\nA: The additional flexibility of a list comes at the cost of efficiency. Some of the advatages of a tuple over a list are as follows:\n\nSince a list can be extended, space is over-allocated when creating a list. A tuple takes less storage space as compared to a list of the same length.\nTuples are not copied. If a tuple is assigned to another tuple, both tuples point to the same memory location. However, if a list is assigned to another list, a new list is created consuming the same memory space as the orignial list.\nTuples refer to their element directly, while in a list, there is an extra layer of pointers that refers to their elements. Thus it is faster to retrieve elements from a tuple.\n\nThe examples below illustrate the above advantages of a tuple.\n\n#Example showing tuples take less storage space than lists for the same elements\ntuple_ex = (1, 2, 'Obama')\nlist_ex = [1, 2, 'Obama']\nprint(\"Space taken by tuple =\",tuple_ex.__sizeof__(),\" bytes\")\nprint(\"Space taken by list =\",list_ex.__sizeof__(),\" bytes\")\n\nSpace taken by tuple = 48  bytes\nSpace taken by list = 64  bytes\n\n\n\n#Examples showing that a tuples are not copied, while lists can be copied\ntuple_copy = tuple(tuple_ex)\nprint(\"Is tuple_copy same as tuple_ex?\", tuple_ex is tuple_copy)\nlist_copy = list(list_ex)\nprint(\"Is list_copy same as list_ex?\",list_ex is list_copy)\n\nIs tuple_copy same as tuple_ex? True\nIs list_copy same as list_ex? False\n\n\n\n#Examples showing tuples takes lesser time to retrieve elements\nimport time as tm\ntt = tm.time()\nlist_ex = list(range(1000000)) #List containinig whole numbers upto 1 million\na=(list_ex[::-2])\nprint(\"Time take to retrieve every 2nd element from a list = \", tm.time()-tt)\n\ntt = tm.time()\ntuple_ex = tuple(range(1000000)) #tuple containinig whole numbers upto 1 million\na=(tuple_ex[::-2])\nprint(\"Time take to retrieve every 2nd element from a tuple = \", tm.time()-tt)\n\nTime take to retrieve every 2nd element from a list =  0.03579902648925781\nTime take to retrieve every 2nd element from a tuple =  0.02684164047241211"
  },
  {
    "objectID": "data_structures.html#dictionary",
    "href": "data_structures.html#dictionary",
    "title": "2  Data structures",
    "section": "2.3 Dictionary",
    "text": "2.3 Dictionary\nA dictionary consists of key-value pairs, where the keys and values are python objects. While values can be any python object, keys need to be immutable python objects, like strings, intergers, tuples, etc. Thus, a list can be a value, but not a key, as a elements of list can be changed. A dictionary is defined using the keyword dict along with curly braces, colons to separate keys and values, and commas to separate elements of a dictionary:\n\ndict_example = {'USA':'Joe Biden', 'India':'Narendra Modi', 'China':'Xi Jinping'}\n\nElements of a dictionary can be retrieved by using the corresponding key.\n\ndict_example['India']\n\n'Narendra Modi'\n\n\n\n2.3.1 Adding and removing elements in a dictionary\nNew elements can be added to a dictionary by defining a key in square brackets and assiging it to a value:\n\ndict_example['Japan'] = 'Fumio Kishida'\ndict_example['Countries'] = 4\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Japan': 'Fumio Kishida',\n 'Countries': 4}\n\n\nElements can be removed from the dictionary using the del method or the pop method:\n\n#Removing the element having key as 'Countries'\ndel dict_example['Countries']\n\n\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Japan': 'Fumio Kishida'}\n\n\n\n#Removing the element having key as 'USA'\ndict_example.pop('USA')\n\n'Joe Biden'\n\n\n\ndict_example\n\n{'India': 'Narendra Modi', 'China': 'Xi Jinping', 'Japan': 'Fumio Kishida'}\n\n\nNew elements can be added, and values of exisiting keys can be changed using the update method:\n\ndict_example = {'USA':'Joe Biden', 'India':'Narendra Modi', 'China':'Xi Jinping','Countries':3}\ndict_example\n\n{'USA': ['Joe Biden'],\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Countries': 3}\n\n\n\ndict_example.update({'Countries':4, 'Japan':'Fumio Kishida'})\n\n\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Countries': 4,\n 'Japan': 'Fumio Kishida'}\n\n\n\n\n2.3.2 Practice exercise\nThe GDP per capita of USA for most years from 1960 to 2021 is given by the dictionary D given in the code cell below.\nFind:\n\nThe GDP per capita in 2015\nThe GDP per capita of 2014 is missing. Update the dictionary to include the GDP per capita of 2014 as the average of the GDP per capita of 2013 and 2015.\nImpute the GDP per capita of other missing years in the same manner as in (2), i.e., as the average GDP per capita of the previous year and the next year. Note that the GDP per capita is not missing for any two consecutive years.\n\n\nD = {'1960':3007,'1961':3067,'1962':3244,'1963':3375,'1964':3574,'1965':3828,'1966':4146,'1967':4336,'1968':4696,'1970':5234,'1971':5609,'1972':6094,'1973':6726,'1974':7226,'1975':7801,'1976':8592,'1978':10565,'1979':11674, '1980':12575,'1981':13976,'1982':14434,'1983':15544,'1984':17121,'1985':18237,  '1986':19071,'1987':20039,'1988':21417,'1989':22857,'1990':23889,'1991':24342,  '1992':25419,'1993':26387,'1994':27695,'1995':28691,'1996':29968,'1997':31459,  '1998':32854,'2000':36330,'2001':37134,'2002':37998,'2003':39490,'2004':41725,  '2005':44123,'2006':46302,'2007':48050,'2008':48570,'2009':47195,'2010':48651,  '2011':50066,'2012':51784,'2013':53291,'2015':56763,'2016':57867,'2017':59915,'2018':62805, '2019':65095,'2020':63028,'2021':69288}"
  },
  {
    "objectID": "data_structures.html#functions",
    "href": "data_structures.html#functions",
    "title": "2  Data structures",
    "section": "2.4 Functions",
    "text": "2.4 Functions\nIf an algorithm or block of code is being used several times in a code, then it can be separatey defined as a function. This makes the code more organized and readable. For example, let us define a function that prints prime numbers between a and b, and returns the number of prime numbers found.\n\n#Function definition\ndef prime_numbers (a,b=100):\n    num_prime_nos = 0\n    \n    #Iterating over all numbers between a and b\n    for i in range(a,b):\n        num_divisors=0\n        \n        #Checking if the ith number has any factors\n        for j in range(2, i):\n            if i%j == 0:\n                num_divisors=1;break;\n                \n        #If there are no factors, then printing and counting the number as prime        \n        if num_divisors==0:\n            print(i)\n            num_prime_nos = num_prime_nos+1\n            \n    #Return count of the number of prime numbers\n    return num_prime_nos\n\nIn the above function, the keyword def is used to define the function, prime_numbers is the name of the function, a and b are the arguments that the function uses to compute the output.\nLet us use the defined function to print and count the prime numbers between 40 and 60.\n\n#Printing prime numbers between 40 and 60\nnum_prime_nos_found = prime_numbers(40,60)\n\n41\n43\n47\n53\n59\n\n\n\nnum_prime_nos_found\n\n5\n\n\nIf the user calls the function without specifying the value of the argument b, then it will take the default value of 100, as mentioned in the function definition. However, for the argument a, the user will need to specify a value, as their is no value defined as a default value in the function definition.\n\n2.4.1 Global and local variables with respect to a function\nA variable defined within a function is local to that function, while a variable defined outside the function is global with respect to that function. In case a variable with the same name is defined both outside and inside a function, it will refer to its global or local value, depending on where it occurs.\nThe example below shows a variable with the name var referring to its local value when called within the function, and global value when called outside the function.\n\nvar = 5\ndef sample_function(var):    \n    print(\"Local value of 'var' within 'sample_function()'= \",var)\n\nsample_function(4)\nprint(\"Global value of 'var' outside 'sample_function()' = \",var)\n\nLocal value of 'var' within 'sample_function()'=  4\nGlobal value of 'var' outside 'sample_function()' =  5\n\n\n\n\n2.4.2 Practice exercise\nThe object deck defined below corresponds to a deck of cards. Estimate the probablity that a five card hand will be a flush, as follows:\n\nWrite a function that accepts a hand of 5 cards as argument, and returns whether the hand is a flush or not.\nRandomly pull a hand of 5 cards from the deck. Call the function developed in (1) to determine if the hand is a flush.\nRepeat (2) 10,000 times.\nEstimate the probability of the hand being a flush from the results of the 10,000 simulations.\n\nYou may use the function shuffle() from the random library to shuffle the deck everytime before pulling a hand of 5 cards.\n\ndeck = [{'value':i, 'suit':c}\nfor c in ['spades', 'clubs', 'hearts', 'diamonds']\nfor i in range(2,15)]"
  },
  {
    "objectID": "data_structures.html#in-class-exercise",
    "href": "data_structures.html#in-class-exercise",
    "title": "2  Data structures",
    "section": "2.5 In-class exercise",
    "text": "2.5 In-class exercise\nThe code cell below defines an object having the nutrition information of drinks in starbucks.\n\nstarbucks_drinks_nutrition={'Cool Lime Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories',\n   'value': 45},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 11},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Strawberry Acai Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories',\n   'value': 80},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 18},\n  {'Nutrition_type': 'Fiber', 'value': 1},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Very Berry Hibiscus Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories',\n   'value': 60},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 14},\n  {'Nutrition_type': 'Fiber', 'value': 1},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Evolution Fresh™ Organic Ginger Limeade': [{'Nutrition_type': 'Calories',\n   'value': 110},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 28},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 5}],\n 'Iced Coffee': [{'Nutrition_type': 'Calories', 'value': 5},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 0},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 5}],\n 'Iced Espresso Classics - Vanilla Latte': [{'Nutrition_type': 'Calories',\n   'value': 130},\n  {'Nutrition_type': 'Fat', 'value': 2.5},\n  {'Nutrition_type': 'Carb', 'value': 21},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 5},\n  {'Nutrition_type': 'Sodium', 'value': 65}],\n 'Iced Espresso Classics - Caffe Mocha': [{'Nutrition_type': 'Calories',\n   'value': 140},\n  {'Nutrition_type': 'Fat', 'value': 2.5},\n  {'Nutrition_type': 'Carb', 'value': 23},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 5},\n  {'Nutrition_type': 'Sodium', 'value': 90}],\n 'Iced Espresso Classics - Caramel Macchiato': [{'Nutrition_type': 'Calories',\n   'value': 130},\n  {'Nutrition_type': 'Fat', 'value': 2.5},\n  {'Nutrition_type': 'Carb', 'value': 21},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 5},\n  {'Nutrition_type': 'Sodium', 'value': 65}],\n 'Shaken Sweet Tea': [{'Nutrition_type': 'Calories', 'value': 80},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 19},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Tazo® Bottled Berry Blossom White': [{'Nutrition_type': 'Calories',\n   'value': 60},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 15},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Tazo® Bottled Black Mango': [{'Nutrition_type': 'Calories', 'value': 150},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 38},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 15}],\n 'Tazo® Bottled Black with Lemon': [{'Nutrition_type': 'Calories',\n   'value': 140},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 35},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Tazo® Bottled Brambleberry': [{'Nutrition_type': 'Calories', 'value': 140},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 35},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 15}],\n 'Tazo® Bottled Giant Peach': [{'Nutrition_type': 'Calories', 'value': 150},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 37},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 15}],\n 'Tazo® Bottled Iced Passion': [{'Nutrition_type': 'Calories', 'value': 70},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 17},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Tazo® Bottled Lemon Ginger': [{'Nutrition_type': 'Calories', 'value': 120},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 31},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Tazo® Bottled Organic Black Lemonade': [{'Nutrition_type': 'Calories',\n   'value': 140},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 35},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Tazo® Bottled Organic Iced Black Tea': [{'Nutrition_type': 'Calories',\n   'value': 60},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 15},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Tazo® Bottled Organic Iced Green Tea': [{'Nutrition_type': 'Calories',\n   'value': 120},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 31},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Tazo® Bottled Plum Pomegranate': [{'Nutrition_type': 'Calories',\n   'value': 140},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 35},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Tazo® Bottled Tazoberry': [{'Nutrition_type': 'Calories', 'value': 150},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 38},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 15}],\n 'Tazo® Bottled White Cranberry': [{'Nutrition_type': 'Calories',\n   'value': 140},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 35},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Teavana® Shaken Iced Black Tea': [{'Nutrition_type': 'Calories',\n   'value': 30},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 8},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 5}],\n 'Teavana® Shaken Iced Black Tea Lemonade': [{'Nutrition_type': 'Calories',\n   'value': 70},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 17},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 0}],\n 'Teavana® Shaken Iced Green Tea': [{'Nutrition_type': 'Calories',\n   'value': 30},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 8},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 5}],\n 'Teavana® Shaken Iced Green Tea Lemonade': [{'Nutrition_type': 'Calories',\n   'value': 70},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 17},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 5}],\n 'Teavana® Shaken Iced Passion Tango™ Tea': [{'Nutrition_type': 'Calories',\n   'value': 30},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 8},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 5}],\n 'Teavana® Shaken Iced Passion Tango™ Tea Lemonade': [{'Nutrition_type': 'Calories',\n   'value': 90},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 24},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 0}],\n 'Teavana® Shaken Iced Peach Green Tea': [{'Nutrition_type': 'Calories',\n   'value': 60},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 15},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 0}],\n 'Starbucks Refreshers™ Raspberry Pomegranate': [{'Nutrition_type': 'Calories',\n   'value': 90},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 27},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 0}],\n 'Starbucks Refreshers™ Strawberry Lemonade': [{'Nutrition_type': 'Calories',\n   'value': 90},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 27},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 0}],\n 'Starbucks® Doubleshot Protein Dark Chocolate': [{'Nutrition_type': 'Calories',\n   'value': 210},\n  {'Nutrition_type': 'Fat', 'value': 2.5},\n  {'Nutrition_type': 'Carb', 'value': 33},\n  {'Nutrition_type': 'Fiber', 'value': 2},\n  {'Nutrition_type': 'Protein', 'value': 20},\n  {'Nutrition_type': 'Sodium', 'value': 115}],\n 'Starbucks® Doubleshot Protein Vanilla': [{'Nutrition_type': 'Calories',\n   'value': 200},\n  {'Nutrition_type': 'Fat', 'value': 2.5},\n  {'Nutrition_type': 'Carb', 'value': 34},\n  {'Nutrition_type': 'Fiber', 'value': 2},\n  {'Nutrition_type': 'Protein', 'value': 20},\n  {'Nutrition_type': 'Sodium', 'value': 120}],\n 'Starbucks® Iced Coffee Caramel': [{'Nutrition_type': 'Calories',\n   'value': 60},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 13},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 1},\n  {'Nutrition_type': 'Sodium', 'value': 0}],\n 'Starbucks® Iced Coffee Light Sweetened': [{'Nutrition_type': 'Calories',\n   'value': 50},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 11},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 1},\n  {'Nutrition_type': 'Sodium', 'value': 0}],\n 'Starbucks® Iced Coffee Unsweetened': [{'Nutrition_type': 'Calories',\n   'value': 10},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 2},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 1},\n  {'Nutrition_type': 'Sodium', 'value': 0}],\n 'Blonde Roast': [{'Nutrition_type': 'Calories', 'value': 5},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 0},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 1},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Clover® Brewed Coffee': [{'Nutrition_type': 'Calories', 'value': 10},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 0},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 1},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Decaf Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 0},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 1},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Featured Dark Roast': [{'Nutrition_type': 'Calories', 'value': 5},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 0},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 1},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Nariño 70 Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 0},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 15}],\n 'Nariño 70 Cold Brew with Milk': [{'Nutrition_type': 'Calories', 'value': 0},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 0},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 0}],\n 'Nitro Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 0},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 0},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Nitro Cold Brew with Sweet Cream': [{'Nutrition_type': 'Calories',\n   'value': 70},\n  {'Nutrition_type': 'Fat', 'value': 5.0},\n  {'Nutrition_type': 'Carb', 'value': 5},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 1},\n  {'Nutrition_type': 'Sodium', 'value': 20}],\n 'Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 0},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 1},\n  {'Nutrition_type': 'Sodium', 'value': 10}],\n 'Vanilla Sweet Cream Cold Brew': [{'Nutrition_type': 'Calories',\n   'value': 110},\n  {'Nutrition_type': 'Fat', 'value': 6.0},\n  {'Nutrition_type': 'Carb', 'value': 14},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 1},\n  {'Nutrition_type': 'Sodium', 'value': 25}],\n 'Hot Chocolate': [{'Nutrition_type': 'Calories', 'value': 320},\n  {'Nutrition_type': 'Fat', 'value': 9.0},\n  {'Nutrition_type': 'Carb', 'value': 47},\n  {'Nutrition_type': 'Fiber', 'value': 4},\n  {'Nutrition_type': 'Protein', 'value': 14},\n  {'Nutrition_type': 'Sodium', 'value': 160}],\n 'Starbucks® Signature Hot Chocolate': [{'Nutrition_type': 'Calories',\n   'value': 430},\n  {'Nutrition_type': 'Fat', 'value': 26.0},\n  {'Nutrition_type': 'Carb', 'value': 45},\n  {'Nutrition_type': 'Fiber', 'value': 5},\n  {'Nutrition_type': 'Protein', 'value': 12},\n  {'Nutrition_type': 'Sodium', 'value': 115}],\n 'Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 190},\n  {'Nutrition_type': 'Fat', 'value': 7.0},\n  {'Nutrition_type': 'Carb', 'value': 19},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 13},\n  {'Nutrition_type': 'Sodium', 'value': 170}],\n 'Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 290},\n  {'Nutrition_type': 'Fat', 'value': 8.0},\n  {'Nutrition_type': 'Carb', 'value': 42},\n  {'Nutrition_type': 'Fiber', 'value': 4},\n  {'Nutrition_type': 'Protein', 'value': 13},\n  {'Nutrition_type': 'Sodium', 'value': 140}],\n 'Cappuccino': [{'Nutrition_type': 'Calories', 'value': 120},\n  {'Nutrition_type': 'Fat', 'value': 4.0},\n  {'Nutrition_type': 'Carb', 'value': 12},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 8},\n  {'Nutrition_type': 'Sodium', 'value': 100}],\n 'Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250},\n  {'Nutrition_type': 'Fat', 'value': 7.0},\n  {'Nutrition_type': 'Carb', 'value': 35},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 10},\n  {'Nutrition_type': 'Sodium', 'value': 150}],\n 'Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 260},\n  {'Nutrition_type': 'Fat', 'value': 6.0},\n  {'Nutrition_type': 'Carb', 'value': 40},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 11},\n  {'Nutrition_type': 'Sodium', 'value': 150}],\n 'Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories', 'value': 250},\n  {'Nutrition_type': 'Fat', 'value': 9.0},\n  {'Nutrition_type': 'Carb', 'value': 32},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 12},\n  {'Nutrition_type': 'Sodium', 'value': 180}],\n 'Flat White': [{'Nutrition_type': 'Calories', 'value': 180},\n  {'Nutrition_type': 'Fat', 'value': 7.0},\n  {'Nutrition_type': 'Carb', 'value': 18},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 12},\n  {'Nutrition_type': 'Sodium', 'value': 160}],\n 'Iced Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 130},\n  {'Nutrition_type': 'Fat', 'value': 4.5},\n  {'Nutrition_type': 'Carb', 'value': 13},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 8},\n  {'Nutrition_type': 'Sodium', 'value': 115}],\n 'Iced Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 230},\n  {'Nutrition_type': 'Fat', 'value': 6.0},\n  {'Nutrition_type': 'Carb', 'value': 36},\n  {'Nutrition_type': 'Fiber', 'value': 4},\n  {'Nutrition_type': 'Protein', 'value': 9},\n  {'Nutrition_type': 'Sodium', 'value': 90}],\n 'Iced Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250},\n  {'Nutrition_type': 'Fat', 'value': 7.0},\n  {'Nutrition_type': 'Carb', 'value': 37},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 10},\n  {'Nutrition_type': 'Sodium', 'value': 150}],\n 'Iced Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 200},\n  {'Nutrition_type': 'Fat', 'value': 4.0},\n  {'Nutrition_type': 'Carb', 'value': 34},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 7},\n  {'Nutrition_type': 'Sodium', 'value': 95}],\n 'Iced Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories',\n   'value': 260},\n  {'Nutrition_type': 'Fat', 'value': 9.0},\n  {'Nutrition_type': 'Carb', 'value': 34},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 11},\n  {'Nutrition_type': 'Sodium', 'value': 180}],\n 'Iced Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 190},\n  {'Nutrition_type': 'Fat', 'value': 4.0},\n  {'Nutrition_type': 'Carb', 'value': 30},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 7},\n  {'Nutrition_type': 'Sodium', 'value': 100}],\n 'Iced White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 300},\n  {'Nutrition_type': 'Fat', 'value': 8.0},\n  {'Nutrition_type': 'Carb', 'value': 47},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 10},\n  {'Nutrition_type': 'Sodium', 'value': 190}],\n 'Latte Macchiato': [{'Nutrition_type': 'Calories', 'value': 190},\n  {'Nutrition_type': 'Fat', 'value': 7.0},\n  {'Nutrition_type': 'Carb', 'value': 19},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 12},\n  {'Nutrition_type': 'Sodium', 'value': 160}],\n 'Starbucks Doubleshot® on Ice Beverage': [{'Nutrition_type': 'Calories',\n   'value': 45},\n  {'Nutrition_type': 'Fat', 'value': 1.0},\n  {'Nutrition_type': 'Carb', 'value': 5},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 3},\n  {'Nutrition_type': 'Sodium', 'value': 40}],\n 'Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 250},\n  {'Nutrition_type': 'Fat', 'value': 6.0},\n  {'Nutrition_type': 'Carb', 'value': 37},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 12},\n  {'Nutrition_type': 'Sodium', 'value': 150}],\n 'White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 360},\n  {'Nutrition_type': 'Fat', 'value': 11.0},\n  {'Nutrition_type': 'Carb', 'value': 53},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 14},\n  {'Nutrition_type': 'Sodium', 'value': 240}],\n 'Cinnamon Dolce Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories',\n   'value': 350},\n  {'Nutrition_type': 'Fat', 'value': 4.5},\n  {'Nutrition_type': 'Carb', 'value': 64},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 15},\n  {'Nutrition_type': 'Sodium', 'value': 0}],\n 'Coffee Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories',\n   'value': 110},\n  {'Nutrition_type': 'Fat', 'value': 0.0},\n  {'Nutrition_type': 'Carb', 'value': 24},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 3},\n  {'Nutrition_type': 'Sodium', 'value': 200}],\n 'Mocha Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories',\n   'value': 280},\n  {'Nutrition_type': 'Fat', 'value': 2.5},\n  {'Nutrition_type': 'Carb', 'value': 60},\n  {'Nutrition_type': 'Fiber', 'value': 2},\n  {'Nutrition_type': 'Protein', 'value': 4},\n  {'Nutrition_type': 'Sodium', 'value': 220}],\n 'Mocha Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories',\n   'value': 140},\n  {'Nutrition_type': 'Fat', 'value': 0.5},\n  {'Nutrition_type': 'Carb', 'value': 28},\n  {'Nutrition_type': 'Fiber', 'value': 1},\n  {'Nutrition_type': 'Protein', 'value': 4},\n  {'Nutrition_type': 'Sodium', 'value': 180}],\n 'Cinnamon Dolce Crème': [{'Nutrition_type': 'Calories', 'value': 200},\n  {'Nutrition_type': 'Fat', 'value': 6.0},\n  {'Nutrition_type': 'Carb', 'value': 28},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 10},\n  {'Nutrition_type': 'Sodium', 'value': 135}],\n 'Vanilla Crème': [{'Nutrition_type': 'Calories', 'value': 200},\n  {'Nutrition_type': 'Fat', 'value': 6.0},\n  {'Nutrition_type': 'Carb', 'value': 28},\n  {'Nutrition_type': 'Fiber', 'value': 0},\n  {'Nutrition_type': 'Protein', 'value': 10},\n  {'Nutrition_type': 'Sodium', 'value': 135}],\n 'Chocolate Smoothie': [{'Nutrition_type': 'Calories', 'value': 320},\n  {'Nutrition_type': 'Fat', 'value': 5.0},\n  {'Nutrition_type': 'Carb', 'value': 53},\n  {'Nutrition_type': 'Fiber', 'value': 8},\n  {'Nutrition_type': 'Protein', 'value': 20},\n  {'Nutrition_type': 'Sodium', 'value': 170}],\n 'Strawberry Smoothie': [{'Nutrition_type': 'Calories', 'value': 300},\n  {'Nutrition_type': 'Fat', 'value': 2.0},\n  {'Nutrition_type': 'Carb', 'value': 60},\n  {'Nutrition_type': 'Fiber', 'value': 7},\n  {'Nutrition_type': 'Protein', 'value': 16},\n  {'Nutrition_type': 'Sodium', 'value': 130}]}\n\nUse the object above to answer the following questions: 1. What is the datatype of the object? 2. What is the datatype of the elements of the object? 3. What is the datatype of the elements withing the elements of the object? 4. How many calories are there in Iced Coffee? 5. Which drink(s) have the highest amount of protein in them, and what is that protein amount? 6. Which drink(s) have a fat content of more than 10g, and what is their fat content?\n\nprint(\"1. Datatype=\",type(starbucks_drinks_nutrition)) \n\n\nprint(\"2. Datatype=\",type(starbucks_drinks_nutrition[list(starbucks_drinks_nutrition.keys())[0]]))\n\n\nprint(\"3. Datatype=\",type(starbucks_drinks_nutrition[list(starbucks_drinks_nutrition.keys())[0]][0]))\n\n\nprint(\"4. Calories = \",starbucks_drinks_nutrition['Iced Coffee'][0]['value'])\n\n\nprotein={}\nfor key,value in starbucks_drinks_nutrition.items():\n    for nutrition in value:        \n        if nutrition['Nutrition_type']=='Protein':\n            protein[key]=(nutrition['value'])\n{key:value for key, value in protein.items() if value == max(protein.values())}\n\n\nfat={}\nfor key,value in starbucks_drinks_nutrition.items():\n    for nutrition in value:        \n        if nutrition['Nutrition_type']=='Fat':\n            fat[key]=(nutrition['value'])\n{key:value for key, value in fat.items() if value>=10}"
  },
  {
    "objectID": "Chap 2 Reading data.html",
    "href": "Chap 2 Reading data.html",
    "title": "3  Reading data",
    "section": "",
    "text": "Reading data is the first step to extract information from it. Data can exist broadly in two formats:\n\nStructured data and,\nUntructured data.\n\nStructured data is typically stored in a tabular form, where rows in the data correspond to “observations” and columns correspond to “variables”. For example, the following dataset contains 5 observations, where each observation (or row) consists of information about a movie. The variables (or columns) contain different pieces of information about a given movie. As all variables for a given row are related to the same movie, the data below is also called as relational data.\n\n\n\n\n\n\n  \n    \n      \n      Title\n      US Gross\n      Production Budget\n      Release Date\n      Major Genre\n      Creative Type\n      Rotten Tomatoes Rating\n      IMDB Rating\n    \n  \n  \n    \n      0\n      The Shawshank Redemption\n      28241469\n      25000000\n      Sep 23 1994\n      Drama\n      Fiction\n      88\n      9.2\n    \n    \n      1\n      Inception\n      285630280\n      160000000\n      Jul 16 2010\n      Horror/Thriller\n      Fiction\n      87\n      9.1\n    \n    \n      2\n      One Flew Over the Cuckoo's Nest\n      108981275\n      4400000\n      Nov 19 1975\n      Comedy\n      Fiction\n      96\n      8.9\n    \n    \n      3\n      The Dark Knight\n      533345358\n      185000000\n      Jul 18 2008\n      Action/Adventure\n      Fiction\n      93\n      8.9\n    \n    \n      4\n      Schindler's List\n      96067179\n      25000000\n      Dec 15 1993\n      Drama\n      Non-Fiction\n      97\n      8.9\n    \n  \n\n\n\n\nUnstructured data is data that is not organized in any pre-defined manner. Examples of unstructured data can be text files, audio/video files, images, Internet of Things (IoT) data, etc. Unstructured data is relatively harder to analyze as most of the analytical methods and tools are oriented towards structured data. However, an unstructured data can be used to obtain structured data, which in turn can be analyzed. For example, an image can be converted to an array of pixels - which will be structured data. Machine learning algorithms can then be used on the array to classify the image as that of a dog or a cat.\nIn this course, we will focus on analyzing structured data."
  },
  {
    "objectID": "Chap 2 Reading data.html#reading-a-csv-file-with-pandas",
    "href": "Chap 2 Reading data.html#reading-a-csv-file-with-pandas",
    "title": "3  Reading data",
    "section": "3.2 Reading a csv file with Pandas",
    "text": "3.2 Reading a csv file with Pandas\nStructured data can be stored in a variety of formats. The most popular format is data_file_name.csv, where the extension csv stands for comma separated values. The variable values of each observation are separated by a comma in a .csv file. In other words, the delimiter is a comma in a csv file. However, the comma is not visible when a .csv file is opened with Microsoft Excel.\n\n3.2.1 Using the read_csv function\nWe will use functions from the Pandas library of Python to read data. Let us import Pandas to use its functions.\n\nimport pandas as pd\n\nNote that pd is the acronym that we will use to call a Pandas function. This acronym can be anything as desired by the user.\nThe function to read a csv file is read_csv(). It reads the dataset into an object of type Pandas DataFrame. Let us read the dataset movie_ratings.csv in Python.\n\nmovie_ratings = pd.read_csv('movie_ratings.csv')\n\nThe built-in python function type can be used to check the dataype of an object:\n\ntype(movie_ratings)\n\npandas.core.frame.DataFrame\n\n\nNote that the file movie_ratings.csv is stored at the same location as the python script containing the above code. If that is not the case, we’ll need to specify the location of the file as in the following code.\n\nmovie_ratings = pd.read_csv('D:/Books/DataScience_Intro_python/movie_ratings.csv')\n\nNote that forward slash is used instead of backslash while specifying the path of the data file. Another option is to use two consecutive backslashes instead of a single forward slash.\n\n\n3.2.2 Specifying the working directory\nIn case we need to read several datasets from a given location, it may be inconvenient to specify the path every time. In such a case we can change the current working directory to the location where the datasets are located.\nWe’ll use the os library of Python to view and/or change the current working directory.\n\nimport os #Importing the 'os' library\nos.getcwd() #Getting the path to the current working directory\n\n'C:\\\\Users\\\\akl0407\\\\Desktop\\\\STAT303-1\\\\Quarto Book\\\\DataScience_Intro_python'\n\n\nThe function getcwd() stands for get current working directory.\nSuppose the dataset to be read is located at 'D:\\Books\\DataScience_Intro_python\\Datasets'. Then, we’ll use the function chdir to change the current working directory to this location.\n\nos.chdir('D:/Books/DataScience_Intro_python/Datasets')\n\nNow we can read the dataset from this location without mentioning the entire path as shown below.\n\nmovie_ratings = pd.read_csv('movie_ratings.csv')\n\n\n\n3.2.3 Data overview and summary statistics\nOnce the data has been read, we may want to see what the data looks like. We’ll use another Pandas function head() to view the first few rows of the data.\n\nmovie_ratings.head()\n\n\n\n\n\n  \n    \n      \n      Title\n      US Gross\n      Worldwide Gross\n      Production Budget\n      Release Date\n      MPAA Rating\n      Source\n      Major Genre\n      Creative Type\n      IMDB Rating\n      IMDB Votes\n    \n  \n  \n    \n      0\n      Opal Dreams\n      14443\n      14443\n      9000000\n      Nov 22 2006\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      6.5\n      468\n    \n    \n      1\n      Major Dundee\n      14873\n      14873\n      3800000\n      Apr 07 1965\n      PG/PG-13\n      Adapted screenplay\n      Western/Musical\n      Fiction\n      6.7\n      2588\n    \n    \n      2\n      The Informers\n      315000\n      315000\n      18000000\n      Apr 24 2009\n      R\n      Adapted screenplay\n      Horror/Thriller\n      Fiction\n      5.2\n      7595\n    \n    \n      3\n      Buffalo Soldiers\n      353743\n      353743\n      15000000\n      Jul 25 2003\n      R\n      Adapted screenplay\n      Comedy\n      Fiction\n      6.9\n      13510\n    \n    \n      4\n      The Last Sin Eater\n      388390\n      388390\n      2200000\n      Feb 09 2007\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      5.7\n      1012\n    \n  \n\n\n\n\nRow Indices and column names (axis labels):\nThe bold integers on the left are the indices of the DataFrame. Each index refers to a distinct row. For example, the index 2 correponds to the row of the movie The Informers. By default, the indices are integers starting from 0. However, they can be changed (to even non-integer values) if desired by the user.\nThe bold text on top of the DataFrame refers to column names. For example, the column US Gross consists of the gross revenue of a movie in the US.\nCollectively, the indices and column names are referred as axis labels.\nShape of DataFrame:\nFor finding the number of rows and columns in the data, you may use the shape() function.\n\n#Finding the shape of movie_ratings dataset\nmovie_ratings.shape\n\n(2228, 11)\n\n\nThe movie_ratings dataset contains 2,809 observations (or rows) and 15 variables (or columns).\nFor obtaining summary statistics of data, you may use the describe() function.\n\n#Finding summary statistics of movie_ratings dataset\nmovie_ratings.describe()\n\n\n\n\n\n  \n    \n      \n      US Gross\n      Worldwide Gross\n      Production Budget\n      IMDB Rating\n      IMDB Votes\n    \n  \n  \n    \n      count\n      2.228000e+03\n      2.228000e+03\n      2.228000e+03\n      2228.000000\n      2228.000000\n    \n    \n      mean\n      5.076370e+07\n      1.019370e+08\n      3.816055e+07\n      6.239004\n      33585.154847\n    \n    \n      std\n      6.643081e+07\n      1.648589e+08\n      3.782604e+07\n      1.243285\n      47325.651561\n    \n    \n      min\n      0.000000e+00\n      8.840000e+02\n      2.180000e+02\n      1.400000\n      18.000000\n    \n    \n      25%\n      9.646188e+06\n      1.320737e+07\n      1.200000e+07\n      5.500000\n      6659.250000\n    \n    \n      50%\n      2.838649e+07\n      4.266892e+07\n      2.600000e+07\n      6.400000\n      18169.000000\n    \n    \n      75%\n      6.453140e+07\n      1.200000e+08\n      5.300000e+07\n      7.100000\n      40092.750000\n    \n    \n      max\n      7.601676e+08\n      2.767891e+09\n      3.000000e+08\n      9.200000\n      519541.000000\n    \n  \n\n\n\n\nAnswer the following questions based on the above table."
  },
  {
    "objectID": "Chap 2 Reading data.html#reading-other-data-formats---txt-html-json",
    "href": "Chap 2 Reading data.html#reading-other-data-formats---txt-html-json",
    "title": "3  Reading data",
    "section": "3.3 Reading other data formats - txt, html, json",
    "text": "3.3 Reading other data formats - txt, html, json\nAlthough csv is a very popular format for strucutred data, data is found in several other formats as well. Some of the other data formats are txt, html and json.\n\n3.3.1 Reading txt files\nThe txt format offers some additional flexibility as compared to the csv format. In the csv format, the delimiter is a comma (or the column values are separated by a comma). However, in a txt file, the delimiter can be anything as desired by the user. Let us read the file movie_ratings.txt, where the variable values are separated by a tab character.\n\nmovie_ratings_txt = pd.read_csv('movie_ratings.txt',sep='\\t')\n\nWe use the function read_csv to read a txt file. However, we mention the tab character (’) as a separater of variable values.\nNote that there is no need to remember the argument name - sep for specifying the delimiter. You can always refer to the read_csv() documentation to find the relevant argument.\n\n\n3.3.2 Reading HTML data\nThe Pandas function read_html searches for tabular data, i.e., data contained within the <table> tags of an html file. Let us read the tables in the GDP per capita page on Wikipedia.\n\n#Reading all the tables from the Wikipedia page on GDP per capita\ntables = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita')\n\nAll the tables will be read and stored in the variable named as tables. Let us find the datatype of the variable tables.\n\n#Finidng datatype of the variable - tables\ntype(tables)\n\nlist\n\n\nThe variable - tables is a list of all the tables read from the HTML data.\n\n#Number of tables read from the page\nlen(tables)\n\n6\n\n\nThe in-built function len can be used to find the length of the list - tables or the number of tables read from the Wikipedia page. Let us check out the first table.\n\n#Checking out the first table. Note that the index of the first table will be 0.\ntables[0]\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      .mw-parser-output .legend{page-break-inside:av...\n      $20,000 - $30,000 $10,000 - $20,000 $5,000 - $...\n      $1,000 - $2,500 $500 - $1,000 <$500 No data\n    \n  \n\n\n\n\nThe above table doesn’t seem to be useful. Let us check out the second table.\n\n#Checking out the second table. Note that the index of the first table will be 1.\ntables[1]\n\n\n\n\n\n  \n    \n      \n      Country/Territory\n      UN Region\n      IMF[4][5]\n      United Nations[6]\n      World Bank[7]\n    \n    \n      \n      Country/Territory\n      UN Region\n      Estimate\n      Year\n      Estimate\n      Year\n      Estimate\n      Year\n    \n  \n  \n    \n      0\n      Liechtenstein *\n      Europe\n      —\n      —\n      180227\n      2020\n      169049\n      2019\n    \n    \n      1\n      Monaco *\n      Europe\n      —\n      —\n      173696\n      2020\n      173688\n      2020\n    \n    \n      2\n      Luxembourg *\n      Europe\n      135046\n      2022\n      117182\n      2020\n      135683\n      2021\n    \n    \n      3\n      Bermuda *\n      Americas\n      —\n      —\n      123945\n      2020\n      110870\n      2021\n    \n    \n      4\n      Ireland *\n      Europe\n      101509\n      2022\n      86251\n      2020\n      85268\n      2020\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      212\n      Central AfricanRepublic *\n      Africa\n      527\n      2022\n      481\n      2020\n      477\n      2020\n    \n    \n      213\n      Sierra Leone *\n      Africa\n      513\n      2022\n      475\n      2020\n      485\n      2020\n    \n    \n      214\n      Madagascar *\n      Africa\n      504\n      2022\n      470\n      2020\n      496\n      2020\n    \n    \n      215\n      South Sudan *\n      Africa\n      393\n      2022\n      1421\n      2020\n      1120\n      2015\n    \n    \n      216\n      Burundi *\n      Africa\n      272\n      2022\n      286\n      2020\n      274\n      2020\n    \n  \n\n217 rows × 8 columns\n\n\n\nThe above table contains the estimated GDP per capita of all countries. This is the table that is likely to be relevant to a user interested in analyzing GDP per capita of countries. Instead of reading all tables of an HTML file, we can focus the search to tables containing certain relevant keywords. Let us try searching all table containing the word ‘Country’.\n\n#Reading all the tables from the Wikipedia page on GDP per capita, containing the word 'Country'\ntables = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita', match = 'Country')\n\nThe match argument can be used to specify the kewyords to be present in the table to be read.\n\nlen(tables)\n\n1\n\n\nOnly one table contains the keyword - ‘Country’. Let us check out the table obtained.\n\n#Table having the keyword - 'Country' from the HTML page\ntables[0]\n\n\n\n\n\n  \n    \n      \n      Country/Territory\n      UN Region\n      IMF[4][5]\n      United Nations[6]\n      World Bank[7]\n    \n    \n      \n      Country/Territory\n      UN Region\n      Estimate\n      Year\n      Estimate\n      Year\n      Estimate\n      Year\n    \n  \n  \n    \n      0\n      Liechtenstein *\n      Europe\n      —\n      —\n      180227\n      2020\n      169049\n      2019\n    \n    \n      1\n      Monaco *\n      Europe\n      —\n      —\n      173696\n      2020\n      173688\n      2020\n    \n    \n      2\n      Luxembourg *\n      Europe\n      135046\n      2022\n      117182\n      2020\n      135683\n      2021\n    \n    \n      3\n      Bermuda *\n      Americas\n      —\n      —\n      123945\n      2020\n      110870\n      2021\n    \n    \n      4\n      Ireland *\n      Europe\n      101509\n      2022\n      86251\n      2020\n      85268\n      2020\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      212\n      Central AfricanRepublic *\n      Africa\n      527\n      2022\n      481\n      2020\n      477\n      2020\n    \n    \n      213\n      Sierra Leone *\n      Africa\n      513\n      2022\n      475\n      2020\n      485\n      2020\n    \n    \n      214\n      Madagascar *\n      Africa\n      504\n      2022\n      470\n      2020\n      496\n      2020\n    \n    \n      215\n      South Sudan *\n      Africa\n      393\n      2022\n      1421\n      2020\n      1120\n      2015\n    \n    \n      216\n      Burundi *\n      Africa\n      272\n      2022\n      286\n      2020\n      274\n      2020\n    \n  \n\n217 rows × 8 columns\n\n\n\nThe argument match helps with a more focussed search, and helps us discard irrelevant tables.\n\n\n3.3.3 Reading JSON data\nJSON stands for JavaScript Object Notation, in which the data is stored and transmitted as plain text. Since the format is text only, JSON data can easily be exchanged between web applications, and used by any programming language. Unlinke the csv format, JSON supports a hierarchical data structure, and is easier to integrate with APIs.\nLets read JSON data on Ted Talks. The Pandas function read_json converts JSON data to a dataframe.\n\ntedtalks_data = pd.read_json('https://raw.githubusercontent.com/cwkenwaysun/TEDmap/master/data/TED_Talks.json')\n\n\ntedtalks_data.head()\n\n\n\n\n\n  \n    \n      \n      id\n      speaker\n      headline\n      URL\n      description\n      transcript_URL\n      month_filmed\n      year_filmed\n      event\n      duration\n      date_published\n      tags\n      newURL\n      date\n      views\n      rates\n    \n  \n  \n    \n      0\n      7\n      David Pogue\n      Simplicity sells\n      http://www.ted.com/talks/view/id/7\n      New York Times columnist David Pogue takes aim...\n      http://www.ted.com/talks/view/id/7/transcript?...\n      2\n      2006\n      TED2006\n      0:21:26\n      6/27/06\n      simplicity,computers,software,interface design...\n      https://www.ted.com/talks/david_pogue_says_sim...\n      2006-06-27\n      1646773\n      [{'id': 7, 'name': 'Funny', 'count': 968}, {'i...\n    \n    \n      1\n      6\n      Craig Venter\n      Sampling the ocean's DNA\n      http://www.ted.com/talks/view/id/6\n      Genomics pioneer Craig Venter takes a break fr...\n      http://www.ted.com/talks/view/id/6/transcript?...\n      7\n      2005\n      TEDGlobal 2005\n      0:16:51\n      2004/05/07\n      biotech,invention,oceans,genetics,DNA,biology,...\n      https://www.ted.com/talks/craig_venter_on_dna_...\n      2004-05-07\n      562625\n      [{'id': 3, 'name': 'Courageous', 'count': 21},...\n    \n    \n      2\n      4\n      Burt Rutan\n      The real future of space exploration\n      http://www.ted.com/talks/view/id/4\n      In this passionate talk, legendary spacecraft ...\n      http://www.ted.com/talks/view/id/4/transcript?...\n      2\n      2006\n      TED2006\n      0:19:37\n      10/25/06\n      aircraft,flight,industrial design,NASA,rocket ...\n      https://www.ted.com/talks/burt_rutan_sees_the_...\n      2006-10-25\n      2046869\n      [{'id': 3, 'name': 'Courageous', 'count': 169}...\n    \n    \n      3\n      3\n      Ashraf Ghani\n      How to rebuild a broken state\n      http://www.ted.com/talks/view/id/3\n      Ashraf Ghani's passionate and powerful 10-minu...\n      http://www.ted.com/talks/view/id/3/transcript?...\n      7\n      2005\n      TEDGlobal 2005\n      0:18:45\n      10/18/06\n      corruption,poverty,economics,investment,milita...\n      https://www.ted.com/talks/ashraf_ghani_on_rebu...\n      2006-10-18\n      814554\n      [{'id': 3, 'name': 'Courageous', 'count': 140}...\n    \n    \n      4\n      5\n      Chris Bangle\n      Great cars are great art\n      http://www.ted.com/talks/view/id/5\n      American designer Chris Bangle explains his ph...\n      http://www.ted.com/talks/view/id/5/transcript?...\n      2\n      2002\n      TED2002\n      0:20:04\n      2004/05/07\n      cars,industrial design,transportation,inventio...\n      https://www.ted.com/talks/chris_bangle_says_gr...\n      2004-05-07\n      870950\n      [{'id': 1, 'name': 'Beautiful', 'count': 89}, ...\n    \n  \n\n\n\n\n\n\n\n\n\n \n        \n\n\n\n\n3.3.4 Reading data from web APIs\nAPI, an acronym for Application programming interface, is a way of transferring information between systems. Many websites have public APIs that provide data via JSON or other formats. For example, the IMDb-API is a web service for receiving movies, serial, and cast information. API results are in the JSON format and include items such as movie specifications, ratings, Wikipedia page content, etc. One of these APIs contains ratings of the top 250 movies on IMDB. Let us read this data using the IMDB API.\nWe’ll use the get function from the python library requests to request data from the API and obtain a response code. The response code will let us know if our request to pull data from this API was successful.\n\n#Importing the requests library\nimport requests as rq\n\n\n# Downloading imdb top 250 movie's data\nurl = 'https://imdb-api.com/en/API/Top250Movies/k_v6gf8ppf' #URL of the API containing top 250 movies based on IMDB ratings\nresponse = rq.get(url) #Requesting data from the API\nresponse\n\n<Response [200]>\n\n\nWe have a response code of 200, which indicates that the request was successful.\nThe response object’s JSON method will return a dictionary containing JSON parsed into native Python objects.\n\nmovie_data = response.json()\n\n\nmovie_data.keys()\n\ndict_keys(['items', 'errorMessage'])\n\n\nThe movie_data contains only two keys. The items key seems likely to contain information about the top 250 movies. Let us convert the values of the items key (which is list of dictionaries) to a dataframe, so that we can view it in a tabular form.\n\n#Converting a list of dictionaries to a dataframe\nmovie_data_df = pd.DataFrame(movie_data['items'])\n\n\n#Checking the movie data pulled using the API\nmovie_data_df.head()\n\n\n\n\n\n  \n    \n      \n      id\n      rank\n      title\n      fullTitle\n      year\n      image\n      crew\n      imDbRating\n      imDbRatingCount\n    \n  \n  \n    \n      0\n      tt0111161\n      1\n      The Shawshank Redemption\n      The Shawshank Redemption (1994)\n      1994\n      https://m.media-amazon.com/images/M/MV5BMDFkYT...\n      Frank Darabont (dir.), Tim Robbins, Morgan Fre...\n      9.2\n      2624065\n    \n    \n      1\n      tt0068646\n      2\n      The Godfather\n      The Godfather (1972)\n      1972\n      https://m.media-amazon.com/images/M/MV5BM2MyNj...\n      Francis Ford Coppola (dir.), Marlon Brando, Al...\n      9.2\n      1817542\n    \n    \n      2\n      tt0468569\n      3\n      The Dark Knight\n      The Dark Knight (2008)\n      2008\n      https://m.media-amazon.com/images/M/MV5BMTMxNT...\n      Christopher Nolan (dir.), Christian Bale, Heat...\n      9.0\n      2595637\n    \n    \n      3\n      tt0071562\n      4\n      The Godfather Part II\n      The Godfather Part II (1974)\n      1974\n      https://m.media-amazon.com/images/M/MV5BMWMwMG...\n      Francis Ford Coppola (dir.), Al Pacino, Robert...\n      9.0\n      1248050\n    \n    \n      4\n      tt0050083\n      5\n      12 Angry Men\n      12 Angry Men (1957)\n      1957\n      https://m.media-amazon.com/images/M/MV5BMWU4N2...\n      Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb\n      8.9\n      775140\n    \n  \n\n\n\n\n\n#Rows and columns of the movie data\nmovie_data_df.shape\n\n(250, 9)\n\n\nThis API provides the names of the top 250 movies along with the year of release, IMDB ratings, and cast information."
  },
  {
    "objectID": "Chap 2 Reading data.html#writing-data",
    "href": "Chap 2 Reading data.html#writing-data",
    "title": "3  Reading data",
    "section": "3.4 Writing data",
    "text": "3.4 Writing data\nThe Pandas function to_csv can be used to write (or export) data to a csv or txt file. Below are some examples.\nExample 1: Let us export the movies data of the top 250 movies to a csv file.\n\n#Exporting the data of the top 250 movies to a csv file\nmovie_data_df.to_csv('movie_data_exported.csv')\n\nThe file movie_data_exported.csv will appear in the working directory.\nExample 2: Let us export the movies data of the top 250 movies to a txt file with a semi-colon as the delimiter.\n\nmovie_data_df.to_csv('movie_data_exported.txt',sep=';')\n\nExample 3: Let us export the movies data of the top 250 movies to a JSON file.\n\nwith open('movie_data.json', 'w') as f:\n    json.dump(movie_data, f)"
  },
  {
    "objectID": "Chap 2 Reading data.html#sub-setting-data-loc-and-iloc",
    "href": "Chap 2 Reading data.html#sub-setting-data-loc-and-iloc",
    "title": "3  Reading data",
    "section": "3.5 Sub-setting data: loc and iloc",
    "text": "3.5 Sub-setting data: loc and iloc\nSometimes we may be interested in working with a subset of rows and columns of the data, instead of working with the entire dataset. The indexing operators loc and iloc provide a convenient of selecting a subset of desired rows and columns. The operator loc uses axis labels (row indices and column names) to subset the data, while iloc uses the index (this is different from the row index) corresponding to the position of the row or columns. Note that the index of the position for both the row and column starts from 0.\nLet us read the file movie_IMDBratings_sorted.csv, which has movies sorted in the descending order of their IMDB ratings.\n\nmovies_sorted = pd.read_csv('./Datasets/movie_IMDBratings_sorted.csv',index_col = 0)\n\nThe argument index_col=0 assigns the first column of the file as the row indices of the DataFrame.\n\nmovies_sorted.head()\n\n\n\n\n\n  \n    \n      \n      Title\n      US Gross\n      Worldwide Gross\n      Production Budget\n      Release Date\n      MPAA Rating\n      Source\n      Major Genre\n      Creative Type\n      IMDB Rating\n      IMDB Votes\n    \n    \n      Rank\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      The Shawshank Redemption\n      28241469\n      28241469\n      25000000\n      Sep 23 1994\n      R\n      Adapted screenplay\n      Drama\n      Fiction\n      9.2\n      519541\n    \n    \n      2\n      Inception\n      285630280\n      753830280\n      160000000\n      Jul 16 2010\n      PG/PG-13\n      Original Screenplay\n      Horror/Thriller\n      Fiction\n      9.1\n      188247\n    \n    \n      3\n      The Dark Knight\n      533345358\n      1022345358\n      185000000\n      Jul 18 2008\n      PG/PG-13\n      Adapted screenplay\n      Action/Adventure\n      Fiction\n      8.9\n      465000\n    \n    \n      4\n      Schindler's List\n      96067179\n      321200000\n      25000000\n      Dec 15 1993\n      R\n      Adapted screenplay\n      Drama\n      Non-Fiction\n      8.9\n      276283\n    \n    \n      5\n      Pulp Fiction\n      107928762\n      212928762\n      8000000\n      Oct 14 1994\n      R\n      Original Screenplay\n      Drama\n      Fiction\n      8.9\n      417703\n    \n  \n\n\n\n\nLet us say, we wish to subset the title, worldwide gross, production budget, and IMDB raring of top 3 movies.\n\n# Subsetting the DataFrame by loc - using axis labels\nmovies_subset = movies_sorted.loc[1:3,['Title','Worldwide Gross','Production Budget','IMDB Rating']]\nmovies_subset\n\n\n\n\n\n  \n    \n      \n      Title\n      Worldwide Gross\n      Production Budget\n      IMDB Rating\n    \n    \n      Rank\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      The Shawshank Redemption\n      28241469\n      25000000\n      9.2\n    \n    \n      2\n      Inception\n      753830280\n      160000000\n      9.1\n    \n    \n      3\n      The Dark Knight\n      1022345358\n      185000000\n      8.9\n    \n  \n\n\n\n\n\n# Subsetting the DataFrame by iloc - using index of the position of rows and columns\nmovies_subset = movies_sorted.iloc[0:3,[0,2,3,9]]\nmovies_subset\n\n\n\n\n\n  \n    \n      \n      Title\n      Worldwide Gross\n      Production Budget\n      IMDB Rating\n    \n    \n      Rank\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      The Shawshank Redemption\n      28241469\n      25000000\n      9.2\n    \n    \n      2\n      Inception\n      753830280\n      160000000\n      9.1\n    \n    \n      3\n      The Dark Knight\n      1022345358\n      185000000\n      8.9"
  },
  {
    "objectID": "NumPy.html",
    "href": "NumPy.html",
    "title": "4  NumPy",
    "section": "",
    "text": "NumPy, short for Numerical Python is used to analyze numeric data with Python. Although numeric operations may be performed without NumPy, NumPy is preferred for its efficiency, especially when working with large arrays of data. A couple of reasons that make NumPy more efficient are:\nWe’ll see the above two advantages of NumPy with the examples below.\nLet us import the NumPy library to use its methods and functions.\nExample 1: This example shows that computations using NumPy arrays are typically much faster than computations with other data structures such as a list.\nQ: Multiply whole numbers upto 1 million by an integer, say 2. Compare the time taken for the computation if the numbers are stored in a NumPy array vs a list.\nUse the numpy function arange() to define a one-dimensional NumPy array."
  },
  {
    "objectID": "NumPy.html#vectorized-computation-with-numpy",
    "href": "NumPy.html#vectorized-computation-with-numpy",
    "title": "4  NumPy",
    "section": "4.1 Vectorized computation with NumPy",
    "text": "4.1 Vectorized computation with NumPy\nSeveral matrix algebra operations such as multiplications, decompositions, determinants, etc. can be performed convenienetly with NumPy. However, we’ll focus on matrix multiplication as it is very commonly used to avoid python for loops and make computations faster. The dot function is used to multiply matrices:\n\n#Defining a 2x3 matrix\na = np.array([[0,1],[3,4]])\na\n\narray([[0, 1],\n       [3, 4]])\n\n\n\n#Defining a 3x2 matrix\nb = np.array([[6,-1],[2,1]])\nb\n\narray([[ 6, -1],\n       [ 2,  1]])\n\n\n\n#Multiplying matrices 'a' and 'b' using the dot function\na.dot(b)\n\narray([[ 2,  1],\n       [26,  1]])\n\n\n\n#Note that * results in element-wise multiplication\na*b\n\narray([[ 0, -1],\n       [ 6,  4]])\n\n\nExample 2: This example will show vectorized computations with NumPy. Vectorized computations help perform computations more efficiently, and also make the code concise.\nQ: Read the (1) quantities of roll, bun, cake and bread required by 3 people - Ben, Barbara & Beth, from food_quantity.csv, (2) price of these food items in two shops - Target and Kroger, from price.csv. Find out which shop should each person go to minimize their expenses.\n\n#Reading the datasets on food quantity and price\nimport pandas as pd\nfood_qty = pd.read_csv('./Datasets/food_quantity.csv')\nprice = pd.read_csv('./Datasets/price.csv')\n\n\nfood_qty\n\n\n\n\n\n  \n    \n      \n      Person\n      roll\n      bun\n      cake\n      bread\n    \n  \n  \n    \n      0\n      Ben\n      6\n      5\n      3\n      1\n    \n    \n      1\n      Barbara\n      3\n      6\n      2\n      2\n    \n    \n      2\n      Beth\n      3\n      4\n      3\n      1\n    \n  \n\n\n\n\n\nprice\n\n\n\n\n\n  \n    \n      \n      Item\n      Target\n      Kroger\n    \n  \n  \n    \n      0\n      roll\n      1.5\n      1.0\n    \n    \n      1\n      bun\n      2.0\n      2.5\n    \n    \n      2\n      cake\n      5.0\n      4.5\n    \n    \n      3\n      bread\n      16.0\n      17.0\n    \n  \n\n\n\n\nFirst, let’s start from a simple problem. We’ll compute the expenses of Ben if he prefers to buy all food items from Target\n\n#Method 1: Using loop\nbens_target_expense = 0 #Initializing Ben's expenses to 0\nfor k in range(4):   #Iterating over all the four desired food items\n    bens_target_expense += food_qty.iloc[0,k+1]*price.iloc[k,1] #Total expenses on the kth item\nbens_target_expense    #Total expenses for Ben if he goes to Target\n\n50.0\n\n\n\n#Method 2: Using NumPy array\nfood_num = food_qty.iloc[0,1:].to_numpy()  #Converting food quantity (for Ben) dataframe to NumPy array\nprice_num = price.iloc[:,1].to_numpy()     #Converting price (for Target) dataframe to NumPy array\nfood_num.dot(price_num)   #Matrix multiplication of the quantity vector with the price vector directly yields the result\n\n50.0\n\n\nBen will spend $50 if he goes to Target\nNow, let’s add another layer of complication. We’ll compute Ben’s expenses for both stores - Target and Kroger\n\n#Method 1: Using loops\n\n#Initializing a Series of length two to store the expenses in Target and Kroger for Ben\nbens_store_expense = pd.Series(0.0,index=price.columns[1:3])\nfor j in range(2):      #Iterating over both the stores - Target and Kroger\n    for k in range(4):        #Iterating over all the four desired food items\n        bens_store_expense[j] += food_qty.iloc[0,k+1]*price.iloc[k,j+1]\nbens_store_expense\n\nTarget    50.0\nKroger    49.0\ndtype: float64\n\n\n\n#Method 2: Using NumPy array\nfood_num = food_qty.iloc[0,1:].to_numpy()  #Converting food quantity (for Ben) dataframe to NumPy array\nprice_num = price.iloc[:,1:].to_numpy()    #Converting price dataframe to NumPy array\nfood_num.dot(price_num)      #Matrix multiplication of the quantity vector with the price matrix directly yields the result\n\narray([50.0, 49.0], dtype=object)\n\n\nBen will spend \\$50 if he goes to Target, and $49 if he goes to Kroger. Thus, he should choose Kroger.\nNow, let’s add the final layer of complication, and solve the problem. We’ll compute everyone’s expenses for both stores - Target and Kroger\n\n#Method 1: Using loops\nstore_expense = pd.DataFrame(0.0,index=price.columns[1:3],columns = food_qty['Person'])\nfor i in range(3):    #Iterating over all the three people - Ben, Barbara, and Beth\n    for j in range(2):     #Iterating over both the stores - Target and Kroger\n        for k in range(4):        #Iterating over all the four desired food items\n            store_expense.iloc[j,i] += food_qty.iloc[i,k+1]*price.iloc[k,j+1]\nstore_expense\n\n\n\n\n\n  \n    \n      Person\n      Ben\n      Barbara\n      Beth\n    \n  \n  \n    \n      Target\n      50.0\n      58.5\n      43.5\n    \n    \n      Kroger\n      49.0\n      61.0\n      43.5\n    \n  \n\n\n\n\n\n#Method 2: Using NumPy array\nfood_num = food_qty.iloc[:,1:].to_numpy() #Converting food quantity dataframe to NumPy array\nprice_num = price.iloc[:,1:].to_numpy()  #Converting price dataframe to NumPy array\nfood_num.dot(price_num)  #Matrix multiplication of the quantity matrix with the price matrix directly yields the result\n\narray([[50. , 49. ],\n       [58.5, 61. ],\n       [43.5, 43.5]])\n\n\nBased on the above table, Ben should go to Kroger, Barbara to Target and Beth can go to either store.\nNote that, with each layer of complication, the number of for loops keep increasing, thereby increasing the complexity of Method 1, while the method with NumPy array does not change much. Vectorized computations with arrays are much more efficient.\n\nIn-class exercise\nUse matrix multiplication to find the average IMDB rating and average Rotten tomatoes rating for each genre - comedy, action, drama and horror. Use the data: movies_cleaned.csv. Which is the most preferred genre for IMDB users, and which is the least preferred genre for Rotten Tomatoes users?\nHint: 1. Create two matrices - one containing the IMDB and Rotten Tomatoes ratings, and the other containing the genre flags (comedy/action/drama/horror). 2. Multiply the two matrices created in 1. 3. Divide each row/column of the resulting matrix by a vector having the number of ratings in each genre to get the average rating for the genre."
  },
  {
    "objectID": "NumPy.html#pseudorandom-number-generation",
    "href": "NumPy.html#pseudorandom-number-generation",
    "title": "4  NumPy",
    "section": "4.2 Pseudorandom number generation",
    "text": "4.2 Pseudorandom number generation\nRandom numbers often need to be generated to analyze processes or systems, especially in cases when these processes or systems are governed by known probability distrbutions. For example, the number of personnel required to answer calls at a call center can be analyzed by simulating occurence and duration of calls.\nNumPy’s random module can be used to generate arrays of random numbers from several different probability distributions. For example, a 3x5 array of uniformly distributed random numbers can be generated using the uniform function of teh random module.\n\nnp.random.uniform(size = (3,5))\n\narray([[0.69256322, 0.69259973, 0.03515058, 0.45186048, 0.43513769],\n       [0.07373366, 0.07465425, 0.92195975, 0.72915895, 0.8906299 ],\n       [0.15816734, 0.88144978, 0.05954028, 0.81403832, 0.97725557]])\n\n\nRandom numbers can also be generated by Python’s built-in random module. However, it generates one random number at a time, which makes it much slower than NumPy’s random module.\nExample 3: Suppose 500 people eat at Mod Pizza, and another 500 eat at Viet nom nom, everyday.\nThe waiting time at Viet nom nom has a normal distribution with mean 8 minutes and standard deviation 3 minutes, while the waiting time at Mod Pizza has a uniform distribution with minimum 5 minutes and maximum 25 minutes.\nSimulate a dataset containing waiting times for 500 ppl for 30 days in each of the food joints. Assume that the waiting time is meansured simultaneously at a certain time in both places, i.e., the observations are paired.\nOn how many days is the average waiting time at Viet Nom Nom higher than that at Mod Pizza?\nWhat percentage of times the waiting time at Viet nom nom was higher than the waiting time at Mod Pizza?\nTry both approaches: (1) Using loops to generate data, (2) numpy array to generate data. Compare the time taken in both approaches.\n\nimport time as tm\n\n\n#Method 1: Using loops\nstart_time = tm.time() #Current system time\n\n#Initializing waiting times for 500 ppl over 30 days\nwaiting_times_MOD = pd.DataFrame(0,index=range(500),columns=range(30)) #Mod pizza\nwaiting_times_Vnom = pd.DataFrame(0,index=range(500),columns=range(30)) #Viet nom nom\nimport random as rm\nfor i in range(500):  #Iterating over 500 ppl\n    for j in range(30): #Iterating over 30 days\n        waiting_times_Vnom.iloc[i,j] = rm.gauss(8,3) #Simulating waiting time in Viet nom nom for the ith person on jth day\n        waiting_times_MOD.iloc[i,j] = rm.uniform(5,25) #Simulating waiting time in Mod pizza for the ith person on jth day\ntime_diff = waiting_times_Vnom-waiting_times_MOD\n\nprint(\"On \",sum(time_diff.mean()>0),\" days, the average waiting time at Viet Nom Nom higher than that at Mod Pizza\")\nprint(\"Percentage of times waiting time at Viet nom nom was greater than that at Mod Pizza = \",100*(time_diff>0).sum().sum()/(30*500),\"%\")\nend_time = tm.time() #Current system time\nprint(\"Time taken = \", end_time-start_time)\n\nOn  0  days, the average waiting time at Viet Nom Nom higher than that at Mod Pizza\nPercentage of times waiting time at Viet nom nom was greater than that at Mod Pizza =  16.58 %\nTime taken =  3.5454351902008057\n\n\n\n#Method 2: Using NumPy arrays\nstart_time = tm.time()\nwaiting_time_Vnom = np.random.normal(8,3,size = (500,30)) #Simultaneously generating the waiting times of 500 ppl over 30 days in Viet nom nom\nwaiting_time_MOD = np.random.uniform(5,25,size = (500,30)) #Simultaneously generating the waiting times of 500 ppl over 30 days in MOD pizza\ntime_diff = waiting_time_Vnom-waiting_time_MOD\nprint(\"On \",(time_diff.mean()>0).sum(),\" days, the average waiting time at Viet Nom Nom higher than that at Mod Pizza\")\nprint(\"Percentage of times waiting time at Viet nom nom was greater than that at Mod Pizza = \",100*(time_diff>0).sum()/15000,\"%\")\nend_time = tm.time()\nprint(\"Time taken = \", end_time-start_time)\n\nOn  0  days, the average waiting time at Viet Nom Nom higher than that at Mod Pizza\nPercentage of times waiting time at Viet nom nom was greater than that at Mod Pizza =  16.486666666666668 %\nTime taken =  0.001995563507080078\n\n\nThe approach with NumPy is much faster than the one with loops.\n\nIn-class exercise\nLab Question: Bootstrapping\nQuestion) Find the 95% confidence interval of Profit for ‘Action’ movies, using Bootstrapping\nAnswer) Bootstrapping is a non-parametric method for obtaining confidence interval. The Bootstrapping method for finding the confidence interval is as follows.\n(a) Find the profit for each of the ‘Action’ movies. Suppose there are N such movies. We will have a Profit column with N values.\n(b) Randomly sample N values with replacement from the Profit column\n(c) Find the mean of the N values obtained in (b)\n(d) Repeat steps (b) and (c) M=1000 times\n(e) The 95% Confidence interval is the range between the 2.5% and 97.5% percentile values of the 1000 means obtained in (c)\nUse the movies_cleaned.csv dataset.\nGo ahead, code this up, and find the confidence interval!"
  },
  {
    "objectID": "Pandas.html",
    "href": "Pandas.html",
    "title": "5  Pandas",
    "section": "",
    "text": "The Pandas library contains several methods and functions for cleaning, manipulating and analyzing data. While NumPy is suited for working with homogenous numerical array data, Pandas is designed for working with tabular or heterogenous data.\nLet is import the Pandas library to use its methods and functions."
  },
  {
    "objectID": "Pandas.html#pandas-data-stuctures---series-and-dataframe",
    "href": "Pandas.html#pandas-data-stuctures---series-and-dataframe",
    "title": "5  Pandas",
    "section": "5.1 Pandas data stuctures - Series and DataFrame",
    "text": "5.1 Pandas data stuctures - Series and DataFrame\nA DataFrame is a two-dimensional object - conmprising of tabular data organized in rows and columns, where individual columns can be of different value types (numeric / string / boolean etc.). A DataFrame has row indices which refer to individual rows, and column names that refer to indvidual columns. By default, the row indices are integers starting from zero. However, both the row indices and column names can be customized by the user.\nLet us read the spotify data - spotify_data.csv, using the Pandas function read_csv().\n\nspotify_data = pd.read_csv('./Datasets/spotify_data.csv')\nspotify_data.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      0\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      6\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n    \n    \n      2\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      7\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n    \n    \n      3\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      2\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n    \n    \n      4\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      8\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\nThe object spotify_data is a pandas DataFrame:\n\ntype(spotify_data)\n\npandas.core.frame.DataFrame\n\n\nA Series is a one-dimensional object, containing a sequence of values, where each value has an index. Each column of a DataFrame is Series as shown in the example below.\n\n#Extracting movie titles from the movie_ratings DataFrame\nspotify_songs = movie_ratings['track_name']\nspotify_songs\n\n0                      Eno Ide\n1            Ee Tanuvu Ninnade\n2            Munjaane Manjalli\n3         Gudugudiya Sedi Nodo\n4                        Ambar\n                  ...         \n245618         Coming Up Roses\n245619               Young Kid\n245620                Apricots\n245621    Time I Love to Waste\n245622                 Call me\nName: track_name, Length: 245623, dtype: object\n\n\n\n#The object movie_titles is a Series\ntype(spotify_songs)\n\npandas.core.series.Series"
  },
  {
    "objectID": "Pandas.html#data-manipulations-with-pandas",
    "href": "Pandas.html#data-manipulations-with-pandas",
    "title": "5  Pandas",
    "section": "5.2 Data manipulations with Pandas",
    "text": "5.2 Data manipulations with Pandas\n\n5.2.1 Sub-setting data\nIn the chapter on reading data, we learned about operators loc and iloc that can be used to subset data based on axis labels and position of rows/columns respectively. However, usually we are not aware of the relevant row indices, and we may want to subset data based on some condition(s). For example, suppose we wish to analyze only those songs whose track popularity is higher than 50.\nQ: Do we need to subset rows or columns in this case?\nA: Rows, as songs correspond to rows, while features of songs correspond to columns.\nAs we need to subset rows, the filter must be applied at the starting index. As we don’t need to subset any specific features of the songs, there is no subsetting to be done on the columns. A : at the ending index means that all columns need to selected.\n\npopular_songs = spotify_data.loc[spotify_data.track_popularity>=50,:]\npopular_songs.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      181\n      1277325\n      hip hop\n      Dave\n      77\n      Titanium\n      69\n      127750\n      1\n      2021\n      0.959\n      ...\n      0\n      -8.687\n      0\n      0.4370\n      0.152000\n      0.000001\n      0.1050\n      0.510\n      121.008\n      4\n    \n    \n      191\n      1123869\n      rap\n      Jay Wheeler\n      85\n      Viendo el Techo\n      64\n      188955\n      0\n      2021\n      0.741\n      ...\n      11\n      -6.029\n      0\n      0.2290\n      0.306000\n      0.000327\n      0.1000\n      0.265\n      179.972\n      4\n    \n    \n      208\n      3657199\n      rap\n      Polo G\n      91\n      RAPSTAR\n      89\n      165926\n      1\n      2021\n      0.789\n      ...\n      6\n      -6.862\n      1\n      0.2420\n      0.410000\n      0.000000\n      0.1290\n      0.437\n      81.039\n      4\n    \n    \n      263\n      1461700\n      pop & rock\n      Teoman\n      67\n      Gecenin Sonuna Yolculuk\n      52\n      280600\n      0\n      2021\n      0.686\n      ...\n      11\n      -7.457\n      0\n      0.0268\n      0.119000\n      0.000386\n      0.1080\n      0.560\n      100.932\n      4\n    \n    \n      293\n      299746\n      pop & rock\n      Lars Winnerbäck\n      62\n      Själ och hjärta\n      55\n      271675\n      0\n      2021\n      0.492\n      ...\n      2\n      -6.005\n      0\n      0.0349\n      0.000735\n      0.000207\n      0.0953\n      0.603\n      142.042\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\nSuppose we wish to analyze only track_name, release year and track_popularity of songs. Then, we can subset the revelant columns:\n\nrelevant_columns = spotify_data.loc[:,['track_name','release_year','track_popularity']]\nrelevant_columns.head()\n\n\n\n\n\n  \n    \n      \n      track_name\n      release_year\n      track_popularity\n    \n  \n  \n    \n      0\n      All Girls Are The Same\n      2021\n      0\n    \n    \n      1\n      Lucid Dreams\n      2021\n      0\n    \n    \n      2\n      Hear Me Calling\n      2021\n      0\n    \n    \n      3\n      Robbery\n      2021\n      0\n    \n    \n      4\n      Big Stepper\n      2021\n      0\n    \n  \n\n\n\n\n\n\n5.2.2 Sorting data\nSorting dataset is a very common operation. The sort_values() function of Pandas can be used to sort a Pandas DataFrame or Series. Let us sort the spotify data in decreasing order of track_popularity:\n\nspotify_sorted = spotify_data.sort_values(by = 'track_popularity', ascending = False)\nspotify_sorted.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      2398\n      1444702\n      pop\n      Olivia Rodrigo\n      88\n      drivers license\n      99\n      242014\n      1\n      2021\n      0.585\n      ...\n      10\n      -8.761\n      1\n      0.0601\n      0.72100\n      0.000013\n      0.1050\n      0.132\n      143.874\n      4\n    \n    \n      2442\n      177401\n      hip hop\n      Masked Wolf\n      85\n      Astronaut In The Ocean\n      98\n      132780\n      0\n      2021\n      0.778\n      ...\n      4\n      -6.865\n      0\n      0.0913\n      0.17500\n      0.000000\n      0.1500\n      0.472\n      149.996\n      4\n    \n    \n      3133\n      1698014\n      pop\n      Kali Uchis\n      88\n      telepatía\n      97\n      160191\n      0\n      2020\n      0.653\n      ...\n      11\n      -9.016\n      0\n      0.0502\n      0.11200\n      0.000000\n      0.2030\n      0.553\n      83.970\n      4\n    \n    \n      6702\n      31308207\n      pop\n      The Weeknd\n      96\n      Save Your Tears\n      97\n      215627\n      1\n      2020\n      0.680\n      ...\n      0\n      -5.487\n      1\n      0.0309\n      0.02120\n      0.000012\n      0.5430\n      0.644\n      118.051\n      4\n    \n    \n      6703\n      31308207\n      pop\n      The Weeknd\n      96\n      Blinding Lights\n      96\n      200040\n      0\n      2020\n      0.514\n      ...\n      1\n      -5.934\n      1\n      0.0598\n      0.00146\n      0.000095\n      0.0897\n      0.334\n      171.005\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\nDrivers license is the most popular song!\n\n\n\n\n\n \n        \n\n\n\n\n5.2.3 Unique values, value counts and membership\nThe Pandas function unique provides the unique values of a Series. For example, let us find the number of unique genres of songs in the spotify dataset:\n\nspotify_data.genres.unique()\n\narray(['rap', 'pop', 'miscellaneous', 'metal', 'hip hop', 'rock',\n       'pop & rock', 'hoerspiel', 'folk', 'electronic', 'jazz', 'country',\n       'latin'], dtype=object)\n\n\nThe Pandas function value_counts() provides the number of observations of each value of a Series. For example, let us find the number of songs of each genre in the spotify dataset:\n\nspotify_data.genres.value_counts()\n\npop              70441\nrock             49785\npop & rock       43437\nmiscellaneous    35848\njazz             13363\nhoerspiel        12514\nhip hop           7373\nfolk              2821\nlatin             2125\nrap               1798\nmetal             1659\ncountry           1236\nelectronic         790\nName: genres, dtype: int64\n\n\nMore than half the songs in the dataset are pop, rock or pop & rock.\nThe Pandas function isin() provides a boolean Series indicating the position of certain values in a Series. The function is helpful in sub-setting data. For example, let us subset the songs that are either latin, rap, or metal:\n\nlatin_rap_metal_songs = spotify_data.loc[spotify_data.genres.isin(['latin','rap','metal']),:]\nlatin_rap_metal_songs.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      0\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      6\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n    \n    \n      2\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      7\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n    \n    \n      3\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      2\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n    \n    \n      4\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      8\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n    \n  \n\n5 rows × 21 columns"
  },
  {
    "objectID": "Pandas.html#operations-between-dataframe-and-series",
    "href": "Pandas.html#operations-between-dataframe-and-series",
    "title": "5  Pandas",
    "section": "5.3 Operations between DataFrame and Series",
    "text": "5.3 Operations between DataFrame and Series\nLet us learn arithematic operations between DataFrame and Series with the help of an example.\nExample: Spotify recommends songs based on songs listened by the user. Suppose you have listned to the song drivers license. Spotify intends to recommend you 5 songs that are similar to drivers license. Which songs should it recommend?\nLet us see what information do we have about songs that can help us identify songs similar to drivers license. The columns attribute of DataFrame will display all the columns names. The description of some of the column names relating to audio features is here.\n\nspotify_data.columns\n\nIndex(['artist_followers', 'genres', 'artist_name', 'artist_popularity',\n       'track_name', 'track_popularity', 'duration_ms', 'explicit',\n       'release_year', 'danceability', 'energy', 'key', 'loudness', 'mode',\n       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n       'valence', 'tempo', 'time_signature'],\n      dtype='object')\n\n\nSolution approach: We have serveral features of a song. Let us find songs similar to drivers license in terms of danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, time_signature and tempo. Note that we are considering only audio features for simplicity.\nTo find the songs most similar to drivers license, we need to define a measure that quantifies the similarity. Let us define simililarity of a song with drivers license as the Euclidean distance of the song from drivers license, where the coordinates of a song are: (danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, time_signature, tempo). Thus, similarity can be formulated as:\n\\[Similarity_{DL-S} = \\sqrt{(danceability_{DL}-danceability_{S})^2+(energy_{DL}-energy_{S})^2 +...+ (tempo_{DL}-tempo_{S})^2)},\\]\nwhere the subscript DL stands for drivers license and S stands for any song. The top 5 songs with the least value of \\(Similarity_{DL-S}\\) will be the most similar to drivers lincense and should be recommended.\nLet us subset the columns that we need to use to compute the Euclidean distance.\n\naudio_features = spotify_data[['danceability', 'energy', 'key', 'loudness','mode','speechiness',\n                               'acousticness', 'instrumentalness', 'liveness','valence', 'tempo', 'time_signature']]\n\n\naudio_features.head()\n\n\n\n\n\n  \n    \n      \n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      0.673\n      0.529\n      0\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n    \n    \n      1\n      0.511\n      0.566\n      6\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n    \n    \n      2\n      0.699\n      0.687\n      7\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n    \n    \n      3\n      0.708\n      0.690\n      2\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n    \n    \n      4\n      0.753\n      0.597\n      8\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n    \n  \n\n\n\n\n\n#Distribution of values of audio_features\naudio_features.describe()\n\n\n\n\n\n  \n    \n      \n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      count\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n    \n    \n      mean\n      0.568357\n      0.580633\n      5.240326\n      -9.432548\n      0.670928\n      0.111984\n      0.383938\n      0.071169\n      0.223756\n      0.552302\n      119.335060\n      3.884177\n    \n    \n      std\n      0.159444\n      0.236631\n      3.532546\n      4.449731\n      0.469877\n      0.198068\n      0.321142\n      0.209555\n      0.198076\n      0.250017\n      29.864219\n      0.458082\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      -60.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.462000\n      0.405000\n      2.000000\n      -11.990000\n      0.000000\n      0.033200\n      0.070000\n      0.000000\n      0.098100\n      0.353000\n      96.099250\n      4.000000\n    \n    \n      50%\n      0.579000\n      0.591000\n      5.000000\n      -8.645000\n      1.000000\n      0.043100\n      0.325000\n      0.000011\n      0.141000\n      0.560000\n      118.002000\n      4.000000\n    \n    \n      75%\n      0.685000\n      0.776000\n      8.000000\n      -6.131000\n      1.000000\n      0.075300\n      0.671000\n      0.002220\n      0.292000\n      0.760000\n      137.929000\n      4.000000\n    \n    \n      max\n      0.988000\n      1.000000\n      11.000000\n      3.744000\n      1.000000\n      0.969000\n      0.996000\n      1.000000\n      1.000000\n      1.000000\n      243.507000\n      5.000000\n    \n  \n\n\n\n\nNote that the audio features differ in terms of scale. Some features like key have a wide range of [0,11], while others like danceability have a very narrow range of [0,0.988]. If we use them directly, features like danceability will have a much higher influence on \\(Similarity_{DL-S}\\) as compared to features like key. Assuming we wish all the features to have equal weight in quantifying a song’s similarity to drivers license, we should scale the features, so that their values are comparable.\nLet us scale the value of each column to a standard uniform distribtion: \\(U[0,1]\\).\nFor scaling the values of a column to \\(U[0,1]\\), we need to subtract the minimum value of the column from each value, and divide by the range of values of the column. For example, danceability can be standardized as follows:\n\n#Scaling danceability to U[0,1]\ndanceability_value_range = audio_features.danceability.max()-audio_features.danceability.min()\ndanceability_std = (audio_features.danceability-audio_features.danceability.min())/danceability_value_range\ndanceability_std\n\n0         0.681174\n1         0.517206\n2         0.707490\n3         0.716599\n4         0.762146\n            ...   \n243185    0.621457\n243186    0.797571\n243187    0.533401\n243188    0.565789\n243189    0.750000\nName: danceability, Length: 243190, dtype: float64\n\n\nHowever, it will be cumbersome to repeat the above code for each audio feature. We can instead write a function that scales values of a column to \\(U[0,1]\\), and apply the function on all the audio features.\n\n#Function to scale a column to U[0,1]\ndef scale_uniform(x):\n    return (x-x.min())/(x.max()-x.min())\n\nWe will use the Pandas function apply() to apply the above function to the DataFrame audio_features.\n\n#Scaling all audio features to U[0,1]\naudio_features_scaled = audio_features.apply(scale_uniform)\n\nlambda function: Note that one line functions can be conveniently written as lambda functions in Python. These functions do not require a name, and can be defined using the keyword lambda. The above two blocks of code can be concisely written as:\n\naudio_features_scaled = audio_features.apply(lambda x: (x-x.min())/(x.max()-x.min()))\n\n\n#All the audio features are scaled to U[0,1]\naudio_features_scaled.describe()\n\n\n\n\n\n  \n    \n      \n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      count\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n    \n    \n      mean\n      0.575260\n      0.580633\n      0.476393\n      0.793290\n      0.670928\n      0.115566\n      0.385480\n      0.071169\n      0.223756\n      0.552302\n      0.490068\n      0.776835\n    \n    \n      std\n      0.161380\n      0.236631\n      0.321141\n      0.069806\n      0.469877\n      0.204405\n      0.322431\n      0.209555\n      0.198076\n      0.250017\n      0.122642\n      0.091616\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.467611\n      0.405000\n      0.181818\n      0.753169\n      0.000000\n      0.034262\n      0.070281\n      0.000000\n      0.098100\n      0.353000\n      0.394647\n      0.800000\n    \n    \n      50%\n      0.586032\n      0.591000\n      0.454545\n      0.805644\n      1.000000\n      0.044479\n      0.326305\n      0.000011\n      0.141000\n      0.560000\n      0.484594\n      0.800000\n    \n    \n      75%\n      0.693320\n      0.776000\n      0.727273\n      0.845083\n      1.000000\n      0.077709\n      0.673695\n      0.002220\n      0.292000\n      0.760000\n      0.566427\n      0.800000\n    \n    \n      max\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n    \n  \n\n\n\n\nSince we need to find the Euclidean distance from the song drivers license, let us find the index of the row containing features of *drivers license.\n\ndrivers_license_index = spotify_data[spotify_data.track_name=='drivers license'].index[0]\n\nNow, we’ll subtract the audio features of drivers license from all other songs:\n\nsongs_minus_DL = audio_features_scaled-audio_features_scaled.loc[drivers_license_index,:]\n\nNow, let us square the difference computed above. We’ll use the in-built python function pow() to square the difference:\n\nsongs_minus_DL_sq = songs_minus_DL.pow(2)\nsongs_minus_DL_sq.head()\n\n\n\n\n\n  \n    \n      \n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      0.007933\n      0.008649\n      0.826446\n      0.000580\n      0.0\n      0.064398\n      0.418204\n      1.055600e-07\n      0.000376\n      0.005041\n      0.005535\n      0.0\n    \n    \n      1\n      0.005610\n      0.016900\n      0.132231\n      0.000577\n      1.0\n      0.020844\n      0.139498\n      1.716100e-10\n      0.055225\n      0.007396\n      0.060654\n      0.0\n    \n    \n      2\n      0.013314\n      0.063001\n      0.074380\n      0.005586\n      1.0\n      0.002244\n      0.171942\n      5.382400e-10\n      0.000256\n      0.134689\n      0.050906\n      0.0\n    \n    \n      3\n      0.015499\n      0.064516\n      0.528926\n      0.003154\n      0.0\n      0.000269\n      0.140249\n      1.716100e-10\n      0.013689\n      0.168921\n      0.068821\n      0.0\n    \n    \n      4\n      0.028914\n      0.025921\n      0.033058\n      0.000021\n      0.0\n      0.057274\n      0.456981\n      1.716100e-10\n      0.008464\n      0.234256\n      0.075428\n      0.0\n    \n  \n\n\n\n\nNow, we’ll sum the squares of differences from all audio features to compute the similarity of all songs to drivers license.\n\ndistance_squared = songs_minus_DL_sq.sum(axis = 1)\ndistance_squared.head()\n\n0    1.337163\n1    1.438935\n2    1.516317\n3    1.004043\n4    0.920316\ndtype: float64\n\n\nNow, we’ll sort these distances to find the top 5 songs closest to drivers’s license.\n\ndistances_sorted = distance_squared.sort_values()\ndistances_sorted.head()\n\n2398      0.000000\n81844     0.008633\n4397      0.011160\n130789    0.015018\n143744    0.015058\ndtype: float64\n\n\nUsing the indices of the top 5 distances, we will identify the top 5 songs most similar to drivers license:\n\nspotify_data.loc[distances_sorted.index[0:6],:]\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      2398\n      1444702\n      pop\n      Olivia Rodrigo\n      88\n      drivers license\n      99\n      242014\n      1\n      2021\n      0.585\n      ...\n      10\n      -8.761\n      1\n      0.0601\n      0.721\n      0.000013\n      0.105\n      0.132\n      143.874\n      4\n    \n    \n      81844\n      2264501\n      pop\n      Jay Chou\n      74\n      安靜\n      49\n      334240\n      0\n      2001\n      0.513\n      ...\n      10\n      -7.853\n      1\n      0.0281\n      0.688\n      0.000008\n      0.116\n      0.123\n      143.924\n      4\n    \n    \n      4397\n      25457\n      pop\n      Terence Lam\n      60\n      拼命無恙 in Bb major\n      52\n      241062\n      0\n      2020\n      0.532\n      ...\n      10\n      -9.690\n      1\n      0.0269\n      0.674\n      0.000000\n      0.117\n      0.190\n      151.996\n      4\n    \n    \n      130789\n      176266\n      pop\n      Alan Tam\n      54\n      從後趕上\n      8\n      258427\n      0\n      1988\n      0.584\n      ...\n      10\n      -11.889\n      1\n      0.0282\n      0.707\n      0.000002\n      0.107\n      0.124\n      140.147\n      4\n    \n    \n      143744\n      396326\n      pop & rock\n      Laura Branigan\n      64\n      How Am I Supposed to Live Without You\n      40\n      263320\n      0\n      1983\n      0.559\n      ...\n      10\n      -8.260\n      1\n      0.0355\n      0.813\n      0.000083\n      0.134\n      0.185\n      139.079\n      4\n    \n    \n      35627\n      1600562\n      pop\n      Tiziano Ferro\n      68\n      Non Me Lo So Spiegare\n      44\n      240040\n      0\n      2014\n      0.609\n      ...\n      11\n      -7.087\n      1\n      0.0352\n      0.706\n      0.000000\n      0.130\n      0.207\n      146.078\n      4\n    \n  \n\n6 rows × 21 columns\n\n\n\nWe can see the top 5 songs most similar to drivers license in the track_name column above. Interestingly, three of the five songs are Asian! These songs indeed sound similart to drivers license!"
  },
  {
    "objectID": "Pandas.html#correlation",
    "href": "Pandas.html#correlation",
    "title": "5  Pandas",
    "section": "5.4 Correlation",
    "text": "5.4 Correlation\nCorrelation may refer to any kind of association between two random variables. However, in this book, we will always consider correlation as the linear association between two random variables, or the Pearson’s correlation coefficient. Note that correlation does not imply causalty and vice-versa.\nThe Pandas function corr() provides the pairwise correlation between all columns of a DataFrame, or between two Series. The function corrwith() provides the pairwise correlation of a DataFrame with another DataFrame or Series.\n\n#Pairwise correlation amongst all columns\nspotify_data.corr()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      artist_popularity\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      artist_followers\n      1.000000\n      0.577861\n      0.197426\n      0.040435\n      0.082857\n      0.098589\n      -0.010120\n      0.080085\n      -0.000119\n      0.123771\n      0.004313\n      -0.059933\n      -0.107475\n      -0.033986\n      0.002425\n      -0.053317\n      0.016524\n      0.030826\n    \n    \n      artist_popularity\n      0.577861\n      1.000000\n      0.285565\n      -0.097996\n      0.092147\n      0.062007\n      0.038784\n      0.039583\n      -0.011005\n      0.045165\n      0.018758\n      0.236942\n      -0.075715\n      -0.066679\n      0.099678\n      -0.034501\n      -0.032036\n      -0.033423\n    \n    \n      track_popularity\n      0.197426\n      0.285565\n      1.000000\n      0.060474\n      0.193685\n      0.568329\n      0.158507\n      0.217342\n      0.013369\n      0.296350\n      -0.022486\n      -0.056537\n      -0.284433\n      -0.124283\n      -0.090479\n      -0.038859\n      0.058408\n      0.071741\n    \n    \n      duration_ms\n      0.040435\n      -0.097996\n      0.060474\n      1.000000\n      -0.024226\n      0.067665\n      -0.145779\n      0.075990\n      0.007710\n      0.078586\n      -0.034818\n      -0.332585\n      -0.133960\n      0.067055\n      -0.034631\n      -0.155354\n      0.051046\n      0.085015\n    \n    \n      explicit\n      0.082857\n      0.092147\n      0.193685\n      -0.024226\n      1.000000\n      0.215656\n      0.138522\n      0.104734\n      0.011818\n      0.124410\n      -0.060350\n      0.077268\n      -0.129363\n      -0.039472\n      -0.024283\n      -0.032549\n      0.006585\n      0.043538\n    \n    \n      release_year\n      0.098589\n      0.062007\n      0.568329\n      0.067665\n      0.215656\n      1.000000\n      0.204743\n      0.338096\n      0.021497\n      0.430054\n      -0.071338\n      -0.032968\n      -0.369038\n      -0.149644\n      -0.045160\n      -0.070025\n      0.079382\n      0.089485\n    \n    \n      danceability\n      -0.010120\n      0.038784\n      0.158507\n      -0.145779\n      0.138522\n      0.204743\n      1.000000\n      0.137615\n      0.020128\n      0.142239\n      -0.051130\n      0.198509\n      -0.143936\n      -0.179213\n      -0.114999\n      0.505350\n      -0.125061\n      0.111015\n    \n    \n      energy\n      0.080085\n      0.039583\n      0.217342\n      0.075990\n      0.104734\n      0.338096\n      0.137615\n      1.000000\n      0.030824\n      0.747829\n      -0.053374\n      -0.043377\n      -0.678745\n      -0.131269\n      0.126050\n      0.348158\n      0.205960\n      0.170854\n    \n    \n      key\n      -0.000119\n      -0.011005\n      0.013369\n      0.007710\n      0.011818\n      0.021497\n      0.020128\n      0.030824\n      1.000000\n      0.024674\n      -0.139688\n      -0.003533\n      -0.023179\n      -0.006600\n      -0.011566\n      0.024206\n      0.008336\n      0.007738\n    \n    \n      loudness\n      0.123771\n      0.045165\n      0.296350\n      0.078586\n      0.124410\n      0.430054\n      0.142239\n      0.747829\n      0.024674\n      1.000000\n      -0.028151\n      -0.173444\n      -0.493020\n      -0.269008\n      0.002959\n      0.209588\n      0.171926\n      0.146030\n    \n    \n      mode\n      0.004313\n      0.018758\n      -0.022486\n      -0.034818\n      -0.060350\n      -0.071338\n      -0.051130\n      -0.053374\n      -0.139688\n      -0.028151\n      1.000000\n      -0.037237\n      0.043773\n      -0.024695\n      0.005657\n      0.010305\n      0.015399\n      -0.015225\n    \n    \n      speechiness\n      -0.059933\n      0.236942\n      -0.056537\n      -0.332585\n      0.077268\n      -0.032968\n      0.198509\n      -0.043377\n      -0.003533\n      -0.173444\n      -0.037237\n      1.000000\n      0.112061\n      -0.094796\n      0.263630\n      0.052171\n      -0.127945\n      -0.150350\n    \n    \n      acousticness\n      -0.107475\n      -0.075715\n      -0.284433\n      -0.133960\n      -0.129363\n      -0.369038\n      -0.143936\n      -0.678745\n      -0.023179\n      -0.493020\n      0.043773\n      0.112061\n      1.000000\n      0.112107\n      0.007415\n      -0.175674\n      -0.173152\n      -0.163243\n    \n    \n      instrumentalness\n      -0.033986\n      -0.066679\n      -0.124283\n      0.067055\n      -0.039472\n      -0.149644\n      -0.179213\n      -0.131269\n      -0.006600\n      -0.269008\n      -0.024695\n      -0.094796\n      0.112107\n      1.000000\n      -0.031301\n      -0.150172\n      -0.027369\n      -0.022034\n    \n    \n      liveness\n      0.002425\n      0.099678\n      -0.090479\n      -0.034631\n      -0.024283\n      -0.045160\n      -0.114999\n      0.126050\n      -0.011566\n      0.002959\n      0.005657\n      0.263630\n      0.007415\n      -0.031301\n      1.000000\n      -0.011137\n      -0.027716\n      -0.040789\n    \n    \n      valence\n      -0.053317\n      -0.034501\n      -0.038859\n      -0.155354\n      -0.032549\n      -0.070025\n      0.505350\n      0.348158\n      0.024206\n      0.209588\n      0.010305\n      0.052171\n      -0.175674\n      -0.150172\n      -0.011137\n      1.000000\n      0.100947\n      0.084783\n    \n    \n      tempo\n      0.016524\n      -0.032036\n      0.058408\n      0.051046\n      0.006585\n      0.079382\n      -0.125061\n      0.205960\n      0.008336\n      0.171926\n      0.015399\n      -0.127945\n      -0.173152\n      -0.027369\n      -0.027716\n      0.100947\n      1.000000\n      0.017423\n    \n    \n      time_signature\n      0.030826\n      -0.033423\n      0.071741\n      0.085015\n      0.043538\n      0.089485\n      0.111015\n      0.170854\n      0.007738\n      0.146030\n      -0.015225\n      -0.150350\n      -0.163243\n      -0.022034\n      -0.040789\n      0.084783\n      0.017423\n      1.000000\n    \n  \n\n\n\n\nQ: Which audio feature is the most correlated with track_popularity?\n\nspotify_data.corrwith(spotify_data.track_popularity).sort_values(ascending = False)\n\ntrack_popularity     1.000000\nrelease_year         0.568329\nloudness             0.296350\nartist_popularity    0.285565\nenergy               0.217342\nartist_followers     0.197426\nexplicit             0.193685\ndanceability         0.158507\ntime_signature       0.071741\nduration_ms          0.060474\ntempo                0.058408\nkey                  0.013369\nmode                -0.022486\nvalence             -0.038859\nspeechiness         -0.056537\nliveness            -0.090479\ninstrumentalness    -0.124283\nacousticness        -0.284433\ndtype: float64\n\n\nLoudness is the audio feature having the highest correlation with track_popularity."
  },
  {
    "objectID": "Data visualization.html",
    "href": "Data visualization.html",
    "title": "6  Data visualization",
    "section": "",
    "text": "It is generally easier for humans to comprehend information with plots, diagrams and pictures, rather than with text and numbers. This makes data visualizations a vital part of data science. Some of the key purposes of data visualization are:\nWe’ll use a couple of libraries for making data visualizations - matplotlib and seaborn. Matplotlib is mostly used for creating relatively simple two-dimensional plots. Its plotting interface that is similar to the plot() function in MATLAB, so those who have used MATLAB should find it familiar. Seaborn is a recently developed data visualization library based on matplotlib. It is more oriented towards visualizing data with Pandas DataFrame and NumPy arrays. While matplotlib may also be used to create complex plots, seaborn has some built-in themes that may make it more convenient to make complex plots. Seaborn also has color schemes and plot styles that improve the readability and aesthetics of malplotlib plots. However, preferences depend on the user and their coding style, and it is perfectly fine to use either library for making the same visualization."
  },
  {
    "objectID": "Data visualization.html#matplotlib",
    "href": "Data visualization.html#matplotlib",
    "title": "6  Data visualization",
    "section": "6.1 Matplotlib",
    "text": "6.1 Matplotlib\nLet’s visualize the life expectancy of different countries with GDP per capita. We’ll read the data file gdp_lifeExpectancy.csv, which contains the GDP per capita and life expectancy of countries from 1952 to 2007.\n\nimport pandas as pd\nimport numpy as np\n\n\ngdp_data = pd.read_csv('./Datasets/gdp_lifeExpectancy.csv')\ngdp_data.head()\n\n\n\n\n\n  \n    \n      \n      country\n      continent\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      Asia\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1\n      Afghanistan\n      Asia\n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      2\n      Afghanistan\n      Asia\n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      3\n      Afghanistan\n      Asia\n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      4\n      Afghanistan\n      Asia\n      1972\n      36.088\n      13079460\n      739.981106\n    \n  \n\n\n\n\n\n6.1.1 Scatterplots and trendline\nNow, we’ll import the pyplot module of matplotlib to make plots. We’ll use the plot() function to make the scatter plot, and the functions xlabel() and ylabel() for lablelling the plot axes.\n\nimport matplotlib.pyplot as plt\n\nQ: Make a scatterplot of Life expectancy vs GDP per capita.\n\n#Making a scatterplot of Life expectancy vs GDP per capita\nx = gdp_data.gdpPercap\ny = gdp_data.lifeExp\nplt.plot(x,y,'o') #By default, the plot() function makes a lineplot. The 'o' arguments specifies a scatterplot\nplt.xlabel('GDP per capita')  #Labelling the horizontal X-axis\nplt.ylabel('Life expectancy') #Labelling the verical Y-axis\n\nText(0, 0.5, 'Life expectancy')\n\n\n\n\n\nFrom the above plot, we observe that life expectancy seems to be positively correlated with the GDP per capita of the country, as one may expect. However, there are a few outliers in the data - which are countries having extremely high GDP per capita, but not a correspondingly high life expectancy.\nSometimes it is difficult to get an idea of the overall trend (positive or negative correlation). In such cases, it may help to add a trendline to the scatter plot. In the plot below we add a trendline over the scatterplot showing that the life expectancy on an average increases with increasing GDP per capita. The trendline is actually a linear regression of life expectancy on GDP per capita. However, we’ll not discuss linear regression is this book.\nQ: Add a trendline over the scatterplot of life expectancy vs GDP per capita.\n\n#Making a scatterplot of Life expectancy vs GDP per capita\nx = gdp_data.gdpPercap\ny = gdp_data.lifeExp\nplt.plot(x,y,'o') #By default, the plot() function makes a lineplot. The 'o' arguments specifies a scatterplot\nplt.xlabel('GDP per capita')  #Labelling the horizontal X-axis\nplt.ylabel('Life expectancy') #Labelling the verical Y-axis\n\n#Plotting a trendline (linear regression) on the scatterplot\nslope_intercept_trendline = np.polyfit(x,y,1)   #Finding the slope and intercept for the trendline\ncompute_y_given_x = np.poly1d(slope_intercept_trendline) #Defining a function that computes the 'y' given 'x' for the trendline\nplt.plot(x,compute_y_given_x(x)) #Plotting the trendline\n\n\n\n\nThe above plot shows that our earlier intuition of a postive correlation between Life expectancy and GDP per capita was correct.\nWe used the NumPy function polyfit() to compute the slope and intercept of the trendline. Then, we defined an object compute_y_given_x of poly1d class and used it to compute the trendline.\n\n\n6.1.2 Subplots\nThere is often a need to make a few plots together to compare them. See the example below.\nQ: Make scatterplots of life expectancy vs GDP per capita separately for each of the 4 continents of Asia, Europe, Africa and America. Arrange the plots in a 2 x 2 grid.\n\n#Defining a 2x2 grid of subplots\nfig, axes = plt.subplots(2,2,figsize=(15,10))\nplt.subplots_adjust(wspace=0.4) #adjusting white space between individual plots\n\n#Making a scatterplot of Life expectancy vs GDP per capita for each continent\ncontinents = np.array([['Asia', 'Europe'], ['Africa', 'Americas']])\n\n#Looping over the 2x2 grid\nfor i in range(2):\n    for j in range(2):\n        x = gdp_data.loc[gdp_data.continent==continents[i,j],:].gdpPercap\n        y = gdp_data.loc[gdp_data.continent==continents[i,j],:].lifeExp\n        axes[i,j].plot(x,y,'o') \n        axes[i,j].set_xlim([gdp_data.gdpPercap.min(), gdp_data.gdpPercap.max()])\n        axes[i,j].set_ylim([gdp_data.lifeExp.min(), gdp_data.lifeExp.max()])\n        axes[i,j].set_xlabel('GDP per capita for '+ continents[i,j])  \n        axes[i,j].set_ylabel('Life expectancy for '+ continents[i,j]) \n\n\n\n\nWe observe that for each continent, except Africa, initially life expectancy increases rapidly with increasing GDP per capita. However, after a certain threshold of GDP per capita, life expectancy increases slowly. Several countries in Europe enjoy a relatively high GDP per capita as well as high life expectancy. Some countries in Asia have an extremely high GDP per capita, but a relatively low life expectancy. It will be interesting to see the proportion of GDP associated with healthcare for these outlying Asian countries, and European countries.\nWe used the subplot function of matplotlib to define the 2x2 grid of subplots. The function subplots_adjust() can be used to adjust white spaces around the plot. We used a for loop to iterate over each subplot. The axes object returned by the subplot() function was used to refer to individual subplots.\n\n\n6.1.3 Overlapping plots with legend\nWe can also have the scatterplot of all the continents on the sample plot, with a distinct color for each continent. A legend will be required to identify the continent’s color.\n\ncontinents = np.array([['Asia', 'Europe'], ['Africa', 'Americas']])\nfor i in range(2):\n    for j in range(2):\n        x = gdp_data.loc[gdp_data.continent==continents[i,j],:].gdpPercap\n        y = gdp_data.loc[gdp_data.continent==continents[i,j],:].lifeExp\n        plt.plot(x,y,'o',label = continents[i,j]) \n        plt.xlim([gdp_data.gdpPercap.min(), gdp_data.gdpPercap.max()])\n        plt.ylim([gdp_data.lifeExp.min(), gdp_data.lifeExp.max()])\n        plt.xlabel('GDP per capita')  \n        plt.ylabel('Life expectancy') \nplt.legend()\n\n<matplotlib.legend.Legend at 0x1d09bf00040>"
  },
  {
    "objectID": "Data visualization.html#pandas",
    "href": "Data visualization.html#pandas",
    "title": "6  Data visualization",
    "section": "6.2 Pandas",
    "text": "6.2 Pandas\nMatplotlib is low-level tool, in which different components of the plot, such as points, legend, axis titles, etc. need to be specified separately. The Pandas plot() function can be used directly with a DataFrame or Series to make plots.\n\n6.2.1 Scatterplots and lineplots\n\n#Plotting life expectancy vs GDP per capita using the Pandas plot() function\ngdp_data.plot(x = 'gdpPercap', y = 'lifeExp', kind = 'scatter')\n\n<AxesSubplot:xlabel='gdpPercap', ylabel='lifeExp'>\n\n\n\n\n\nNote that with matplolib, it will take 3 lines to make the same plot - one for the scatterplot, and two for the axis titles.\nLet us re-arrange the data to show other benefits of the Pandas plot() function. Note that data resphaping is explained in Chapter 8 of the book, so you may ignore the code block below that uses the pivot_table() function.\n\n#You may ignore this code block until Chapter 8.\nmean_gdp_per_capita = gdp_data.pivot_table(index = 'year', columns = 'continent',values = 'gdpPercap')\nmean_gdp_per_capita.head()\n\n\n\n\n\n  \n    \n      continent\n      Africa\n      Americas\n      Asia\n      Europe\n      Oceania\n    \n    \n      year\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1952\n      1252.572466\n      4079.062552\n      5195.484004\n      5661.057435\n      10298.085650\n    \n    \n      1957\n      1385.236062\n      4616.043733\n      5787.732940\n      6963.012816\n      11598.522455\n    \n    \n      1962\n      1598.078825\n      4901.541870\n      5729.369625\n      8365.486814\n      12696.452430\n    \n    \n      1967\n      2050.363801\n      5668.253496\n      5971.173374\n      10143.823757\n      14495.021790\n    \n    \n      1972\n      2339.615674\n      6491.334139\n      8187.468699\n      12479.575246\n      16417.333380\n    \n  \n\n\n\n\nWe have reshaped the data to obtain the mean GDP per capita of each continent for each year.\nThe pandas plot() function can be directly used with this DataFrame to create line plots showing mean GDP per capita of each continent with year.\n\nmean_gdp_per_capita.plot(ylabel = 'GDP per capita')\n\n<AxesSubplot:xlabel='year', ylabel='GDP per capita'>\n\n\n\n\n\nWe observe that the mean GDP per capita of of Europe and Oceania have increased rapidly, while that for Africa is incresing very slowly.\nThe above plot will take several lines of code if developed using only matplotlib. The pandas plot() function has a framework to conveniently make commonly used plots.\n\n\n6.2.2 Bar plots\nBar plots can be made using the pandas bar function with the DataFrame or Series, just like the line plots and scatterplots.\nBelow, we are reading the dataset of noise complaints of type Loud music/Party received the police in New York City in 2016.\n\nnyc_party_complaints = pd.read_csv('./Datasets/party_nyc.csv')\nnyc_party_complaints.head()\n\n\n\n\n\n  \n    \n      \n      Created Date\n      Closed Date\n      Location Type\n      Incident Zip\n      City\n      Borough\n      Latitude\n      Longitude\n      Hour_of_the_day\n      Month_of_the_year\n    \n  \n  \n    \n      0\n      12/31/2015 0:01\n      12/31/2015 3:48\n      Store/Commercial\n      10034.0\n      NEW YORK\n      MANHATTAN\n      40.866183\n      -73.918930\n      0\n      12\n    \n    \n      1\n      12/31/2015 0:02\n      12/31/2015 4:36\n      Store/Commercial\n      10040.0\n      NEW YORK\n      MANHATTAN\n      40.859324\n      -73.931237\n      0\n      12\n    \n    \n      2\n      12/31/2015 0:03\n      12/31/2015 0:40\n      Residential Building/House\n      10026.0\n      NEW YORK\n      MANHATTAN\n      40.799415\n      -73.953371\n      0\n      12\n    \n    \n      3\n      12/31/2015 0:03\n      12/31/2015 1:53\n      Residential Building/House\n      11231.0\n      BROOKLYN\n      BROOKLYN\n      40.678285\n      -73.994668\n      0\n      12\n    \n    \n      4\n      12/31/2015 0:05\n      12/31/2015 3:49\n      Residential Building/House\n      10033.0\n      NEW YORK\n      MANHATTAN\n      40.850304\n      -73.938516\n      0\n      12\n    \n  \n\n\n\n\nLet us visualise the locations from where the the complaints are coming.\n\n#Using the pandas function bar() to create bar plot\nnyc_party_complaints['Location Type'].value_counts().plot.bar(ylabel = 'Number of complaints')\n\n<AxesSubplot:ylabel='Number of complaints'>\n\n\n\n\n\nFrom the above plot, we observe that most of the complaints come from residential buildings and houses, as one may expect.\nLet is visualize the time of the year when most complaints occur.\n\n#Using the pandas function bar() to create bar plot\nnyc_party_complaints['Month_of_the_year'].value_counts().sort_index().plot.bar(ylabel = 'Number of complaints')\n\n<AxesSubplot:ylabel='Number of complaints'>\n\n\n\n\n\nTry executing the code without sort_index() to figure out the purpose of using the function.\nFrom the above plot, we observe that most of the complaints occur during summer and early Fall.\nLet us create a stacked bar chart that combines both the above plots into a single plot. You may ignore the code used for re-shaping the data until Chapter 8. The purpose here is to show the utility of the pandas bar() function.\n\n#Reshaping the data to make it suitable for a stacked barplot - ignore this code until chapter 8\ncomplaints_location=pd.crosstab(nyc_party_complaints.Month_of_the_year, nyc_party_complaints['Location Type'])\ncomplaints_location.head()\n\n\n\n\n\n  \n    \n      Location Type\n      Club/Bar/Restaurant\n      House of Worship\n      Park/Playground\n      Residential Building/House\n      Store/Commercial\n      Street/Sidewalk\n    \n    \n      Month_of_the_year\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      748\n      24\n      17\n      9393\n      1157\n      832\n    \n    \n      2\n      570\n      29\n      16\n      8383\n      1197\n      782\n    \n    \n      3\n      747\n      39\n      90\n      9689\n      1480\n      1835\n    \n    \n      4\n      848\n      53\n      129\n      11984\n      1761\n      2943\n    \n    \n      5\n      2091\n      72\n      322\n      15676\n      1941\n      5090\n    \n  \n\n\n\n\n\n#Stacked bar plot showing number of complaints at different months of the year, and from different locations\ncomplaints_location.plot.bar(stacked=True,ylabel = 'Number of complaints',figsize=(15, 10))\n\n<AxesSubplot:xlabel='Month_of_the_year', ylabel='Number of complaints'>\n\n\n\n\n\nThe above plots gives the insights about location and day of the year simultaneously that were previously separately obtained by the individual plots."
  },
  {
    "objectID": "Data visualization.html#seaborn",
    "href": "Data visualization.html#seaborn",
    "title": "6  Data visualization",
    "section": "6.3 Seaborn",
    "text": "6.3 Seaborn\nSeaborn offers the flexibility of simultaneously visualizing multiple variables in a single plot, and offers several themes to develop plots.\n\n#Importing the seaborn library\nimport seaborn as sns\n\n\n6.3.1 Bar plots with confidence interevals\nWe’ll group the data to obtain the total complaints for each Location Type, Borough, Month_of_the_year, and Hour_of_the_day. Note that you’ll learn grouping data in Chapter 9, so you may ignore the next code block. The grouping is done to shape the data in a suitable form for visualization.\n\n#Grouping the data to make it suitable for visualization using Seaborn. Ignore this code block until learn chapter 9.\nnyc_complaints_grouped = nyc_party_complaints[['Location Type','Borough','Month_of_the_year','Latitude','Hour_of_the_day']].groupby(['Location Type','Borough','Month_of_the_year','Hour_of_the_day'])['Latitude'].agg([('complaints','count')]).reset_index()\nnyc_complaints_grouped.head()\n\n\n\n\n\n  \n    \n      \n      Location Type\n      Borough\n      Month_of_the_year\n      Hour_of_the_day\n      complaints\n    \n  \n  \n    \n      0\n      Club/Bar/Restaurant\n      BRONX\n      1\n      0\n      10\n    \n    \n      1\n      Club/Bar/Restaurant\n      BRONX\n      1\n      1\n      10\n    \n    \n      2\n      Club/Bar/Restaurant\n      BRONX\n      1\n      2\n      6\n    \n    \n      3\n      Club/Bar/Restaurant\n      BRONX\n      1\n      3\n      6\n    \n    \n      4\n      Club/Bar/Restaurant\n      BRONX\n      1\n      4\n      3\n    \n  \n\n\n\n\nLet us create a bar plot visualizing the average number of complaints with the time of the day.\n\nsns.barplot(x=\"Hour_of_the_day\", y = 'complaints',  data=nyc_complaints_grouped)\n\n<AxesSubplot:xlabel='Hour_of_the_day', ylabel='complaints'>\n\n\n\n\n\nFrom the above plot, we observe that most of the complaints are made around midnight. However, interestingly, there are some complaints at each hour of the day.\nNote that the above barplot shows the mean number of complaints in a month at each hour of the day. The black lines are the 95% confidence intervals of the mean number of complaints.\n\n\n6.3.2 Facetgrid: Multi-plot grid for plotting conditional relationships\nWith pandas, we simultaneously visualized the number of complaints with month of the year and location type. We’ll use Seaborn to add another variable - Borough to the visualization.\nQ: Visualize the number of complaints with Month_of_the_year, Location Type, and Borough.\nThe seaborn class FacetGrid is used to design the plot, i.e., specify the way the data will be divided in mutually exclusive subsets for visualization. Then the [map] function of the FacetGrid class is used to apply a plotting function to each subset of the data.\n\n#Visualizing the number of complaints with Month_of_the_year, Location Type, and Borough.\na = sns.FacetGrid(nyc_complaints_grouped, hue = 'Location Type', col = 'Borough',col_wrap=3,height=3.5,aspect = 1)\na.map(sns.lineplot,'Month_of_the_year','complaints')\na.add_legend()\n\n<seaborn.axisgrid.FacetGrid at 0x1d0e52ff580>\n\n\n\n\n\nFrom the above plot, we get a couple of interesting insights: 1. For Queens and Staten Island, most of the complaints occur in summer, for Manhattan and Bronx it is mostly during late spring, while Brooklyn has a spike of complaints in early Fall. 2. In most of the Boroughs, the majority complaints always occur in residential areas. However, for Manhattan, the number of street/sidewalk complaints in the summer are comparable to those from residential areas.\nWe have visualized 4 variables simultaneously in the above plot.\nLet us consider another example, where we will visualize the weather in a few cities of Australia. The file Australia_weather.csv consists of weather details of Syndey, Canberra, and Melbourne from 2007 to 2017.\n\naussie_weather = pd.read_csv('./Datasets/Australia_weather.csv')\naussie_weather.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Location\n      MinTemp\n      MaxTemp\n      Rainfall\n      Evaporation\n      Sunshine\n      WindGustDir\n      WindGustSpeed\n      WindDir9am\n      ...\n      Humidity3pm\n      Pressure9am\n      Pressure3pm\n      Cloud9am\n      Cloud3pm\n      Temp9am\n      Temp3pm\n      RainToday\n      RISK_MM\n      RainTomorrow\n    \n  \n  \n    \n      0\n      10/20/2010\n      Sydney\n      12.9\n      20.3\n      0.2\n      3.0\n      10.9\n      ENE\n      37\n      W\n      ...\n      57\n      1028.8\n      1025.6\n      3\n      1\n      16.9\n      19.8\n      No\n      0.0\n      No\n    \n    \n      1\n      10/21/2010\n      Sydney\n      13.3\n      21.5\n      0.0\n      6.6\n      11.0\n      ENE\n      41\n      W\n      ...\n      58\n      1025.9\n      1022.4\n      2\n      5\n      17.6\n      21.3\n      No\n      0.0\n      No\n    \n    \n      2\n      10/22/2010\n      Sydney\n      15.3\n      23.0\n      0.0\n      5.6\n      11.0\n      NNE\n      41\n      W\n      ...\n      63\n      1021.4\n      1017.8\n      1\n      4\n      19.0\n      22.2\n      No\n      0.0\n      No\n    \n    \n      3\n      10/26/2010\n      Sydney\n      12.9\n      26.7\n      0.2\n      3.8\n      12.1\n      NE\n      33\n      W\n      ...\n      56\n      1018.0\n      1015.0\n      1\n      5\n      17.8\n      22.5\n      No\n      0.0\n      No\n    \n    \n      4\n      10/27/2010\n      Sydney\n      14.8\n      23.8\n      0.0\n      6.8\n      9.6\n      SSE\n      54\n      SSE\n      ...\n      69\n      1016.0\n      1014.7\n      2\n      7\n      20.2\n      20.6\n      No\n      1.8\n      Yes\n    \n  \n\n5 rows × 24 columns\n\n\n\n\naussie_weather.shape\n\n(4666, 24)\n\n\nQ: Visualize if it rains the next day (RainTomorrow) given whether it has rained today (RainToday), the current day’s humidity (Humidity9am), maximum temperature (MaxTemp) and the city (Location).\n\na = sns.FacetGrid(aussie_weather,col='Location',row='RainToday',height = 4,aspect = 0.8,hue = 'RainTomorrow')\na.map(plt.scatter,'MaxTemp','Humidity9am')\na.add_legend()\n\n<seaborn.axisgrid.FacetGrid at 0x1d0e77b0610>\n\n\n\n\n\nHumidity tends to be higher when it is going to rain the next day. However, the correlation is much more pronounced for Syndey. In case it is not raining on the current day, humidity seems to be slightly negatively correlated with temperature.\n\n\n6.3.3 Histogram and density plots\nHistogram and density plots visualize the data distribution. A histogram plots the number of observations occuring within discrete, evenly spaced bins of a random variable, to visualize the distribution of the variable. It may be considered a special case of bar plot as bars are used to plot the observation counts.\nA density plot uses a kernel density estimate to approximate the distribution of random variable.\nWe can use the Seaborn displot() function to make both kinds of plots - histogram or density plot.\nExample: Make a histogram showing the distributions of maximum temperature in Sydney, Canberra and Melbourne.\n\nsns.displot(data = aussie_weather, x = 'MaxTemp',kind = 'hist',col='Location')\n\n<seaborn.axisgrid.FacetGrid at 0x1d0ec989e50>\n\n\n\n\n\nFrom the above plot, we observe that: 1. Melbourne has a right skewed distribution with the median temperature being smaller than the mean. 2. Canberra seems to have the highest variation in the temperture.\nExample: Make a density plot showing the distributions of maximum temperature in Sydney, Canberra and Melbourne.\n\nsns.displot(data = aussie_weather, x = 'MaxTemp',kind = 'kde', col = 'Location')\n\n<seaborn.axisgrid.FacetGrid at 0x1d0e963f8e0>\n\n\n\n\n\nExample: Show the distributions of the maximum and minimum temperatures in a single plot.\n\nsns.histplot(data=aussie_weather, x=\"MaxTemp\", color=\"skyblue\", label=\"MaxTemp\", kde=True)\nsns.histplot(data=aussie_weather, x=\"MinTemp\", color=\"red\", label=\"MinTemp\", kde=True)\nplt.legend()\n\n<matplotlib.legend.Legend at 0x1d0eb6b9790>\n\n\n\n\n\nThe Seaborn function histplot() can be used to make a density plot overlapping on a histogram.\n\n\n6.3.4 Boxplots\nBoxplots is a standardized way of visualizing the data distribution. They show five key metrics that describe the data distribution - median, 25th percentile value, 75th percentile value, minimum and maximum, as shown in the figure below. Note that the minimum and maximum exclude the outliers.\n\n\n\n\n\nExample: Make a boxplot comparing the distributions of maximum temperatures of Sydney, Canberra and Melbourne, given whether or not it has rained on the day.\n\nsns.boxplot(data = aussie_weather,x = 'Location', y = 'MaxTemp',hue = 'RainToday')\n\n<AxesSubplot:xlabel='Location', ylabel='MaxTemp'>\n\n\n\n\n\nFrom the above plot, we observe that: 1. The maximum temperature of the day, on an average, is lower if it rained on the day. 2. Sydney and Meloboure have some extremly high outlying values of maximum temperature.\nWe have used the Seaborn boxplot() function for the above plot."
  },
  {
    "objectID": "Data cleaning and preparation.html",
    "href": "Data cleaning and preparation.html",
    "title": "7  Data cleaning and preparation",
    "section": "",
    "text": "Missing values in a dataset can occur due to several reasons such as breakdown of measuring equipment, accidental removal of observations, lack of response by respondents, error on the part of the reearcher, etc.\nRemoving all rows / columns with even a single missing value results in loss of data that is non-missing in the respective rows/columns. In this section, we’ll see how to make a smart guess for the missing values, so that we can (hopefully) maximize the information that can be extracted from the data.\n\n\nRubin (1976) classified missing values into three categories, depending on how the values could have been missing.\n\n\n\nIf the probablity that a random variable’s value is missing is the same for all the observations, then the data is said to be missing completely at random. An example of MCAR is a weighing scale that ran out of batteries. Some of the data will be missing simply because of bad luck.\n\n\n\nIf the probability of being missing is the same only within groups defined by the observed data, then the data are missing at random (MAR). MAR is a much broader class than MCAR. For example, when placed on a soft surface, a weighing scale may produce more missing values than when placed on a hard surface. Such data are thus not MCAR. If, however, we know surface type and if we can assume MCAR within the type of surface, then the data are MAR\n\n\n\nMNAR means that the probability of being missing varies for reasons that are unknown to us. For example, the weighing scale mechanism may wear out over time, producing more missing data as time progresses, but we may fail to note this. If the heavier objects are measured later in time, then we obtain a distribution of the measurements that will be distorted. MNAR includes the possibility that the scale produces more missing values for the heavier objects (as above), a situation that might be difficult to recognize and handle.\nSource: https://stefvanbuuren.name/fimd/sec-MCAR.html\n\n\n\n\nRubin, Donald B. 1976. “Inference and Missing Data.” Biometrika 63 (3): 581–92."
  },
  {
    "objectID": "Data wrangling.html",
    "href": "Data wrangling.html",
    "title": "8  Data wrangling",
    "section": "",
    "text": "Data wrangling"
  },
  {
    "objectID": "Data aggregation.html",
    "href": "Data aggregation.html",
    "title": "9  Data aggregation",
    "section": "",
    "text": "Data aggregation"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Rubin, Donald B. 1976. “Inference and Missing Data.”\nBiometrika 63 (3): 581–92."
  }
]