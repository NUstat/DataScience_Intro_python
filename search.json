[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science with Python",
    "section": "",
    "text": "This book is developed for the course STAT303-1 (Data Science with Python-1). The first two chapters of the book are a review of python, and will be covered very quickly. Students are expected to know the contents of these chapters beforehand, or be willing to learn it quickly. The core part of the course begins from the third chapter - Reading data.\nNote that this book is still being edited. Please let the instructors know in case of any typos/mistakes/general feedback."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html",
    "href": "Introduction to Python and Jupyter Notebooks.html",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "",
    "text": "This chapter is a very brief introduction to python and Jupyter notebooks. We only discuss the content relevant for applying python to analyze data."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#installation",
    "href": "Introduction to Python and Jupyter Notebooks.html#installation",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "1.1 Installation",
    "text": "1.1 Installation\nAnaconda: If you are new to python, we recommend downloading the Anaconda installer and following the instructions for installation. Once installed, we’ll use the Jupyter Notebook interface to write code.\nQuarto: We’ll use Quarto to publish the .ipynb file containing text, python code, and the output. Download and install Quarto from here."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#jupyter-notebook",
    "href": "Introduction to Python and Jupyter Notebooks.html#jupyter-notebook",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "1.2 Jupyter notebook",
    "text": "1.2 Jupyter notebook\n\n1.2.1 Introduction\nJupyter notebook is an interactive platform, where you can write code and text, and make visualizations. You can access Jupyter notebook from the Anaconda Navigator, or directly open the Jupyter Notebook application itself. It should automatically open up in your default browser. The figure below shows a Jupyter Notebook opened with Google Chrome. This page is called the landing page of the notebook.\n\n\n\n\n\nTo create a new notebook, click on the New button and select the Python 3 option. You should see a blank notebook as in the figure below.\n\n\n\n\n\n\n\n1.2.2 Writing and executing code\nCode cell: By default, a cell is of type Code, i.e., for typing code, as seen as the default choice in the dropdown menu below the Widgets tab. Try typing a line of python code (say, 2+3) in an empty code cell and execute it by pressing Shift+Enter. This should execute the code, and create an new code cell. Pressing Ctlr+Enter for Windows (or Cmd+Enter for Mac) will execute the code without creating a new cell.\nCommenting code in a code cell: Comments should be made while writing the code to explain the purpose of the code or a brief explanation of the tasks being performed by the code. A comment can be added in a code cell by preceding it with a # sign. For example, see the comment in the code below.\nWriting comments will help other users understand your code. It is also useful for the coder to keep track of the tasks being performed by their code.\n\n#This code adds 3 and 5\n3+5\n\n8\n\n\nMarkdown cell: Although a comment can be written in a code cell, a code cell cannot be used for writing headings/sub-headings, and is not appropriate for writing lengthy chunks of text. In such cases, change the cell type to Markdown from the dropdown menu below the Widgets tab. Use any markdown cheat sheet found online, for example, this one to format text in the markdown cells.\nGive a name to the notebook by clicking on the text, which says ‘Untitled’.\n\n\n1.2.3 Saving and loading notebooks\nSave the notebook by clicking on File, and selecting Save as, or clicking on the Save and Checkpoint icon (below the File tab). Your notebook will be saved as a file with an extension ipynb. This file will contain all the code as well as the outputs, and can be loaded and edited by a Jupyter user. To load an existing Jupyter notebook, navigate to the folder of the notebook on the landing page, and then click on the file to open it.\n\n\n1.2.4 Rendering notebook as HTML\nWe’ll use Quarto to print the **.ipynb* file as HTML. Check the procedure for rendering a notebook as HTML here. You have several options to format the file.\nYou will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#in-class-exercise",
    "href": "Introduction to Python and Jupyter Notebooks.html#in-class-exercise",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "In-class exercise",
    "text": "In-class exercise\n\nCreate a new notebook.\nSave the file as In_class_exercise1.\nGive a heading to the file - First HTML file.\nPrint Today is day 1 of class.\nCompute and print the number of hours of this course in the quarter (that will be 10 weeks x 2 classes per week x 1.33 hours per class).\n\nThe HTML file should look like the picture below."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#python-language-basics",
    "href": "Introduction to Python and Jupyter Notebooks.html#python-language-basics",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "1.3 Python language basics",
    "text": "1.3 Python language basics\n\n1.3.1 Object Oriented Programming\nPython is an object-oriented programming language. In layman terms, it means that every number, string, data structure, function, class, module, etc., exists in the python interpreter as a python object. An object may have attributes and methods associated with it. For example, let us define a variable that stores an integer:\n\nvar = 2\n\nThe variable var is an object that has attributes and methods associated with it. For example a couple of its attributes are real and imag, which store the real and imaginary parts respectively, of the object var:\n\nprint(\"Real part of 'var': \",var.real)\nprint(\"Real part of 'var': \",var.imag)\n\nReal part of 'var':  2\nReal part of 'var':  0\n\n\nAttribute: An attribute is a value associated with an object, defined within the class of the object.\nMethod: A method is a function associated with an object, defined within the class of the object, and has access to the attributes associated with the object.\nFor looking at attributes and methods associated with an object, say obj, press tab key after typing obj..\nConsider the example below of a class example_class:\n\nclass example_class:\n    class_name = 'My Class'\n    def my_method(self):\n        print('Hello World!')\n\ne = example_class()\n\nIn the above class, class_name is an attribute, while my_method is a method.\n\n\n1.3.2 Assigning variable name to object\nWhen an object is assigned to a variable name, the variable name serves as a reference to the object. For example, consider the following assignment:\n\nx = [5,3]\n\nThe variable name x is a reference to the memory location where the object [5, 3] is stored. Now, suppose we assign x to a new variable y:\n\ny = x\n\nIn the above statement the variable name y now refers to the same object [5,3]. The object [5,3] does not get copied to a new memory location referred by y. To prove this, let us add an element to y:\n\ny.append(4)\nprint(y)\n\n[5, 3, 4]\n\n\n\nprint(x)\n\n[5, 3, 4]\n\n\nWhen we changed y, note that x also changed to the same object, showing that x and y refer to the same object, instead of referring to different copies of the same object.\n\n\n1.3.3 Importing libraries\nThere are several built-in functions in python like print(), abs(), max(), sum() etc., which do not require importing any library. However, these functions will typically be insufficient for a analyzing data. Some of the popular libraries and their primary purposes are as follows:\n\nNumPy: Performing numerical operations and efficiently storing numerical data.\nPandas: Reading, cleaning and manipulating data.\nMatplotlib, Seaborn: Visualizing data.\nSciPy: Performing scientific computing such as solving differential equations, optimization, statistical tests, etc.\nScikit-learn: Data pre-processing and machine learning, with a focus on prediction.\nStatsmodels: Developing statistical models with a focus on inference\n\nA library can be imported using the import keyword. For example, a NumPy library can be imported as:\n\nimport numpy as np\n\nUsing the as keyboard, the NumPy library has been given the name np. All the functions and attributes of the library can be called using the ‘np.’ prefix. For example, let us generate a sequence of whole numbers upto 10 using the NumPy function arange():\n\nnp.arange(8)\n\narray([0, 1, 2, 3, 4, 5, 6, 7])\n\n\n\n\n1.3.4 Built-in objects\nThere are several built-in objects, modules and functions in python. Below are a few examples:\nScalar objects: Python has some built-in datatypes for handling scalar objects such as number, string, boolean values, and date/time. The built-in function type() function can be used to determine the datatype of an object:\n\nvar = 2.2\ntype(var)\n\nfloat\n\n\nrange(): The range() function returns a sequence of evenly-spaced integer values. It is commonly used in for loops to define the sequence of elements over which the iterations are performed.\nBelow is an example where the range() function is used to create a sequence of whole numbers upto 10:\n\nprint(list(range(1,10)))\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nDate time: Python as a built-in datetime module for handling date/time objects:\n\nimport datetime as dt\n\n\n#Defining a date-time object \ndt_object = dt.datetime(2022, 9, 20, 11,30,0)\n\nInformation about date and time can be accessed with the relevant attribute of the datetime object.\n\ndt_object.day\n\n20\n\n\n\ndt_object.year\n\n2022\n\n\nThe strftime method of the datetime module formats a datetime object as a string. There are several types of formats for representing date as a string:\n\ndt_object.strftime('%m/%d/%Y')\n\n'09/20/2022'\n\n\n\ndt_object.strftime('%m/%d/%y %H:%M')\n\n'09/20/22 11:30'\n\n\n\ndt_object.strftime('%h-%d-%Y')\n\n'Sep-20-2022'\n\n\n\n\n1.3.5 Control flow\nAs in other languages, python has built-in keywords that provide conditional flow of control in the code.\nIf-elif-else: The if-elif-else statement can check several conditions, and execute the code corresponding to the condition that is true. Note that there can be as many elif statements as required.\n\n#Example of if-elif-else\nx = 5\nif x>0:\n    print(\"x is positive\")\nelif x==0:\n    print(\"x is zero\")\nelse:\n    print(\"X is negative\")\n    print(\"This was the last condition checked\")\n\nx is positive\n\n\nfor loop: A for loop iterates over the elements of an object, and executes the statements within the loop in each iteration. For example, below is a for loop that prints odd natural numbers upto 10:\n\nfor i in range(10):\n    if i%2!=0:\n        print(i)\n\n1\n3\n5\n7\n9\n\n\nwhile loop: A while loop iterates over a set of statements while a condition is satisfied. For example, below is a while loop that prints odd numbers upto 10:\n\ni=0\nwhile i<10:\n    if i%2!=0:\n        print(i)\n    i=i+1\n\n1\n3\n5\n7\n9"
  },
  {
    "objectID": "data_structures.html",
    "href": "data_structures.html",
    "title": "2  Data structures",
    "section": "",
    "text": "In this chapter we’ll learn about the python data structures that are often used or appear while analyzing data."
  },
  {
    "objectID": "data_structures.html#tuple",
    "href": "data_structures.html#tuple",
    "title": "2  Data structures",
    "section": "2.1 Tuple",
    "text": "2.1 Tuple\nTuple is a sequence of python objects, with two key characteristics: (1) the number of objects are fixed, and (2) the objects are immutable, i.e., they cannot be changed.\nTuple can be defined as a sequence of python objects separated by commas, and enclosed in rounded brackets (). For example, below is a tuple containing three integers.\n\ntuple_example = (2,7,4)\n\nWe can check the data type of a python object using the in-built python function type(). Let us check the data type of the object tuple_example.\n\ntype(tuple_example)\n\ntuple\n\n\nElements of a tuple can be extracted using their index within square brackets. For example the second element of the tuple tuple_example can be extracted as follows:\n\ntuple_example[1]\n\n7\n\n\nNote that an element of a tuple cannot be modified. For example, consider the following attempt in changing the second element of the tuple tuple_example.\n\ntuple_example[1] = 8\n\nTypeError: 'tuple' object does not support item assignment\n\n\nThe above code results in an error as tuple elements cannot be modified.\n\n2.1.1 Concatenating tuples\nTuples can be concatenated using the + operator to produce a longer tuple:\n\n(2,7,4) + (\"another\", \"tuple\") + (\"mixed\",\"datatypes\",5)\n\n(2, 7, 4, 'another', 'tuple', 'mixed', 'datatypes', 5)\n\n\nMultiplying a tuple by an integer results in repetition of the tuple:\n\n(2,7,\"hi\") * 3\n\n(2, 7, 'hi', 2, 7, 'hi', 2, 7, 'hi')\n\n\n\n\n2.1.2 Unpacking tuples\nIf tuples are assigned to an expression containing multiple variables, the tuple will be unpacked and each variable will be assigned a value as per the order in which it appears. See the example below.\n\nx,y,z  = (4.5, \"this is a string\", ((\"Nested tuple\",5)))\n\n\nx\n\n4.5\n\n\n\ny\n\n'this is a string'\n\n\n\nz\n\n('Nested tuple', 5)\n\n\nIf we are interested in retrieving only some values of the tuple, the expression *_ can be used to discard the other values. Let’s say we are interested in retrieving only the first and the last two values of the tuple:\n\nx,*_,y,z  = (4.5, \"this is a string\", ((\"Nested tuple\",5)),\"99\",99)\n\n\nx\n\n4.5\n\n\n\ny\n\n'99'\n\n\n\nz\n\n99\n\n\n\n\n2.1.3 Tuple methods\nA couple of useful tuple methods are count, which counts the occurrences of an element in the tuple and index, which returns the position of the first occurrence of an element in the tuple:\n\ntuple_example = (2,5,64,7,2,2)\n\n\ntuple_example.count(2)\n\n3\n\n\n\ntuple_example.index(2)\n\n0\n\n\nNow that we have an idea about tuple, let us try to think where it can be used."
  },
  {
    "objectID": "data_structures.html#list",
    "href": "data_structures.html#list",
    "title": "2  Data structures",
    "section": "2.2 List",
    "text": "2.2 List\nList is a sequence of python objects, with two key characeterisics that differentiates it from tuple: (1) the number of objects are variable, i.e., objects can be added or removed from a list, and (2) the objects are mutable, i.e., they can be changed.\nList can be defined as a sequence of python objects separated by commas, and enclosed in square brackets []. For example, below is a list consisting of three integers.\n\nlist_example = [2,7,4]\n\n\n2.2.1 Adding and removing elements in a list\nWe can add elements at the end of the list using the append method. For example, we append the string ‘red’ to the list list_example below.\n\nlist_example.append('red')\n\n\nlist_example\n\n[2, 7, 4, 'red']\n\n\nNote that the objects of a list or a tuple can be of different datatypes.\nAn element can be added at a specific location of the list using the insert method. For example, if we wish to insert the number 2.32 as the second element of the list list_example, we can do it as follows:\n\nlist_example.insert(1,2.32)\n\n\nlist_example\n\n[2, 2.32, 7, 4, 'red']\n\n\nFor removing an element from the list, the pop and remove methods may be used. The pop method removes an element at a particular index, while the remove method removes the element’s first occurence in the list by its value. See the examples below.\nLet us say, we need to remove the third element of the list.\n\nlist_example.pop(2)\n\n7\n\n\n\nlist_example\n\n[2, 2.32, 4, 'red']\n\n\nLet us say, we need to remove the element ‘red’.\n\nlist_example.remove('red')\n\n\nlist_example\n\n[2, 2.32, 4]\n\n\n\n#If there are multiple occurences of an element in the list, the first occurence will be removed\nlist_example2 = [2,3,2,4,4]\nlist_example2.remove(2)\nlist_example2\n\n[3, 2, 4, 4]\n\n\nFor removing multiple elements in a list, either pop or remove can be used in a for loop, or a for loop can be used with a condition. See the examples below.\nLet’s say we need to remove intergers less than 100 from the following list.\n\nlist_example3 = list(range(95,106))\nlist_example3\n\n[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]\n\n\n\n#Method 1: For loop with remove\nlist_example3_filtered = list(list_example3) #\nfor element in list_example3:\n    if element<100:\n        list_example3_filtered.remove(element)\nprint(list_example3_filtered)\n\n[100, 101, 102, 103, 104, 105]\n\n\n\\(\\color{red}{\\text{Q1}}\\): What’s the need to define a new variable list\\_example3\\_filtered in the above code?\n\\(\\color{blue}{\\text{A1}}\\): Replace list_example3_filtered with list_example3 and identify the issue.\n\n#Method 2: For loop with condition\n[element for element in list_example3 if element>100]\n\n[101, 102, 103, 104, 105]\n\n\n\n\n2.2.2 List comprehensions\nList comprehension is a compact way to create new lists based on elements of an existing list.\nExample: Create a list that has squares of natural numbers from 5 to 15.\n\nsqrt_natural_no_5_15 = [(x**2) for x in range(5,16)]\nprint(sqrt_natural_no_5_15)\n\n[25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225]\n\n\nExample: Create a list of tuples, where each tuple consists of a natural number and its square, for natural numbers ranging from 5 to 15.\n\nsqrt_natural_no_5_15 = [(x,x**2) for x in range(5,16)]\nprint(sqrt_natural_no_5_15)\n\n[(5, 25), (6, 36), (7, 49), (8, 64), (9, 81), (10, 100), (11, 121), (12, 144), (13, 169), (14, 196), (15, 225)]\n\n\n\n\n2.2.3 Practice exercise 1\nBelow is a list consisting of responses to the question: “At what age do you think you will marry?” from students of the STAT303-1 Fall 2022 class.\n\nexp_marriage_age=['24','30','28','29','30','27','26','28','30+','26','28','30','30','30','probably never','30','25','25','30','28','30+ ','30','25','28','28','25','25','27','28','30','30','35','26','28','27','27','30','25','30','26','32','27','26','27','26','28','37','28','28','28','35','28','27','28','26','28','26','30','27','30','28','25','26','28','35','29','27','27','30','24','25','29','27','33','30','30','25','26','30','32','26','30','30','I wont','25','27','27','25','27','27','32','26','25','never','28','33','28','35','25','30','29','30','31','28','28','30','40','30','28','30','27','by 30','28','27','28','30-35','35','30','30','never','30','35','28','31','30','27','33','32','27','27','26','N/A','25','26','29','28','34','26','24','28','30','120','25','33','27','28','32','30','26','30','30','28','27','27','27','27','27','27','28','30','30','30','28','30','28','30','30','28','28','30','27','30','28','25','never','69','28','28','33','30','28','28','26','30','26','27','30','25','Never','27','27','25']\n\nUse list comprehension to:\n\n2.2.3.1 \nRemove the elements that are not integers - such as ‘probably never’, ‘30+’, etc. What is the length of the new list?\nHint: The built-in python function of the str class - isdigit() may be useful to check if the string contains only digits.\nSolution:\n\nexp_marriage_age_num = [x for x in exp_marriage_age if x.isdigit()==True]\nprint(\"Length of the new list = \",len(exp_marriage_age_num))\n\nLength of the new list =  181\n\n\n\n\n2.2.3.2 \nCap the values greater than 80 to 80, in the clean list obtained in (1). What is the mean age when people expect to marry in the new list?\n\nexp_marriage_age_capped = [min(int(x),80) for x in exp_marriage_age_num]\nprint(\"Mean age when people expect to marry = \", sum(exp_marriage_age_capped)/len(exp_marriage_age_capped))\n\nMean age when people expect to marry =  28.955801104972377\n\n\n\n\n2.2.3.3 \nDetermine the percentage of people who expect to marry at an age of 30 or more.\n\nprint(\"Percentage of people who expect to marry at an age of 30 or more =\", str(100*sum([1 for x in exp_marriage_age_capped if x>=30])/len(exp_marriage_age_capped)),\"%\")\n\nPercentage of people who expect to marry at an age of 30 or more = 37.01657458563536 %\n\n\n\n\n\n2.2.4 Concatenating lists\nAs in tuples, lists can be concatenated using the + operator:\n\nimport time as tm\n\n\nlist_example4 = [5,'hi',4] \nlist_example4 = list_example4 + [None,'7',9]\nlist_example4\n\n[5, 'hi', 4, None, '7', 9]\n\n\nFor adding elements to a list, the extend method is preferred over the + operator. This is because the + operator creates a new list, while the extend method adds elements to an existing list. Thus, the extend operator is more memory efficient.\n\nlist_example4 = [5,'hi',4]\nlist_example4.extend([None, '7', 9])\nlist_example4\n\n[5, 'hi', 4, None, '7', 9]\n\n\n\n\n2.2.5 Sorting a list\nA list can be sorted using the sort method:\n\nlist_example5 = [6,78,9]\nlist_example5.sort(reverse=True) #the reverse argument is used to specify if the sorting is in ascending or descending order\nlist_example5\n\n[78, 9, 6]\n\n\n\n\n2.2.6 Slicing a list\nWe may extract or update a section of the list by passing the starting index (say start) and the stopping index (say stop) as start:stop to the index operator []. This is called slicing a list. For example, see the following example.\n\nlist_example6 = [4,7,3,5,7,1,5,87,5]\n\nLet us extract a slice containing all the elements from the the 3rd position to the 7th position.\n\nlist_example6[2:7]\n\n[3, 5, 7, 1, 5]\n\n\nNote that while the element at the start index is included, the element with the stop index is excluded in the above slice.\nIf either the start or stop index is not mentioned, the slicing will be done from the beginning or until the end of the list, respectively.\n\nlist_example6[:7]\n\n[4, 7, 3, 5, 7, 1, 5]\n\n\n\nlist_example6[2:]\n\n[3, 5, 7, 1, 5, 87, 5]\n\n\nTo slice the list relative to the end, we can use negative indices:\n\nlist_example6[-4:]\n\n[1, 5, 87, 5]\n\n\n\nlist_example6[-4:-2:]\n\n[1, 5]\n\n\nAn extra colon (‘:’) can be used to slice every \\(n\\)th element of a list.\n\n#Selecting every 3rd element of a list\nlist_example6[::3]\n\n[4, 5, 5]\n\n\n\n#Selecting every 3rd element of a list from the end\nlist_example6[::-3]\n\n[5, 1, 3]\n\n\n\n#Selecting every element of a list from the end or reversing a list \nlist_example6[::-1]\n\n[5, 87, 5, 1, 7, 5, 3, 7, 4]\n\n\n\n\n2.2.7 Practice exercise 2\nStart with the list [8,9,10]. Do the following:\n\n2.2.7.1 \nSet the second entry (index 1) to 17\n\nL = [8,9,10]\nL[1]=17\n\n\n\n2.2.7.2 \nAdd 4, 5, and 6 to the end of the list\n\nL = L+[4,5,6]\n\n\n\n2.2.7.3 \nRemove the first entry from the list\n\nL.pop(0)\n\n8\n\n\n\n\n2.2.7.4 \nSort the list\n\nL.sort()\n\n\n\n2.2.7.5 \nDouble the list (concatenate the list to itself)\n\nL=L+L\n\n\n\n2.2.7.6 \nInsert 25 at index 3\nThe final list should equal [4,5,6,25,10,17,4,5,6,10,17]\n\nL.insert(3,25)\nL\n\n[4, 5, 6, 25, 10, 17, 4, 5, 6, 10, 17]\n\n\nNow that we have an idea about lists, let us try to think where it can be used.\n\n\n\n\n\n \n        \n\n\nNow that we have learned about lists and tuples, let us compare them.\n\\(\\color{red}{\\text{Q2}}\\): A list seems to be much more flexible than tuple, and can replace a tuple almost everywhere. Then why use tuple at all?\n\\(\\color{blue}{\\text{A2}}\\): The additional flexibility of a list comes at the cost of efficiency. Some of the advantages of a tuple over a list are as follows:\n\nSince a list can be extended, space is over-allocated when creating a list. A tuple takes less storage space as compared to a list of the same length.\nTuples are not copied. If a tuple is assigned to another tuple, both tuples point to the same memory location. However, if a list is assigned to another list, a new list is created consuming the same memory space as the original list.\nTuples refer to their element directly, while in a list, there is an extra layer of pointers that refers to their elements. Thus it is faster to retrieve elements from a tuple.\n\nThe examples below illustrate the above advantages of a tuple.\n\n#Example showing tuples take less storage space than lists for the same elements\ntuple_ex = (1, 2, 'Obama')\nlist_ex = [1, 2, 'Obama']\nprint(\"Space taken by tuple =\",tuple_ex.__sizeof__(),\" bytes\")\nprint(\"Space taken by list =\",list_ex.__sizeof__(),\" bytes\")\n\nSpace taken by tuple = 48  bytes\nSpace taken by list = 64  bytes\n\n\n\n#Examples showing that a tuples are not copied, while lists can be copied\ntuple_copy = tuple(tuple_ex)\nprint(\"Is tuple_copy same as tuple_ex?\", tuple_ex is tuple_copy)\nlist_copy = list(list_ex)\nprint(\"Is list_copy same as list_ex?\",list_ex is list_copy)\n\nIs tuple_copy same as tuple_ex? True\nIs list_copy same as list_ex? False\n\n\n\n#Examples showing tuples takes lesser time to retrieve elements\nimport time as tm\ntt = tm.time()\nlist_ex = list(range(1000000)) #List containinig whole numbers upto 1 million\na=(list_ex[::-2])\nprint(\"Time take to retrieve every 2nd element from a list = \", tm.time()-tt)\n\ntt = tm.time()\ntuple_ex = tuple(range(1000000)) #tuple containinig whole numbers upto 1 million\na=(tuple_ex[::-2])\nprint(\"Time take to retrieve every 2nd element from a tuple = \", tm.time()-tt)\n\nTime take to retrieve every 2nd element from a list =  0.03579902648925781\nTime take to retrieve every 2nd element from a tuple =  0.02684164047241211"
  },
  {
    "objectID": "data_structures.html#dictionary",
    "href": "data_structures.html#dictionary",
    "title": "2  Data structures",
    "section": "2.3 Dictionary",
    "text": "2.3 Dictionary\nA dictionary consists of key-value pairs, where the keys and values are python objects. While values can be any python object, keys need to be immutable python objects, like strings, intergers, tuples, etc. Thus, a list can be a value, but not a key, as elements of list can be changed. A dictionary is defined using the keyword dict along with curly braces, colons to separate keys and values, and commas to separate elements of a dictionary:\n\ndict_example = {'USA':'Joe Biden', 'India':'Narendra Modi', 'China':'Xi Jinping'}\n\nElements of a dictionary can be retrieved by using the corresponding key.\n\ndict_example['India']\n\n'Narendra Modi'\n\n\n\n2.3.1 Adding and removing elements in a dictionary\nNew elements can be added to a dictionary by defining a key in square brackets and assiging it to a value:\n\ndict_example['Japan'] = 'Fumio Kishida'\ndict_example['Countries'] = 4\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Japan': 'Fumio Kishida',\n 'Countries': 4}\n\n\nElements can be removed from the dictionary using the del method or the pop method:\n\n#Removing the element having key as 'Countries'\ndel dict_example['Countries']\n\n\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Japan': 'Fumio Kishida'}\n\n\n\n#Removing the element having key as 'USA'\ndict_example.pop('USA')\n\n'Joe Biden'\n\n\n\ndict_example\n\n{'India': 'Narendra Modi', 'China': 'Xi Jinping', 'Japan': 'Fumio Kishida'}\n\n\nNew elements can be added, and values of exisiting keys can be changed using the update method:\n\ndict_example = {'USA':'Joe Biden', 'India':'Narendra Modi', 'China':'Xi Jinping','Countries':3}\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Countries': 3}\n\n\n\ndict_example.update({'Countries':4, 'Japan':'Fumio Kishida'})\n\n\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Countries': 4,\n 'Japan': 'Fumio Kishida'}\n\n\n\n\n2.3.2 Iterating over elements of a dictionary\nThe items() attribute of a dictionary can be used to iterate over elements of a dictionary.\n\nfor key,value in dict_example.items():\n    print(\"The Head of State of\",key,\"is\",value)\n\nThe Head of State of USA is Joe Biden\nThe Head of State of India is Narendra Modi\nThe Head of State of China is Xi Jinping\nThe Head of State of Countries is 4\nThe Head of State of Japan is Fumio Kishida\n\n\n\n\n2.3.3 Practice exercise 3\nThe GDP per capita of USA for most years from 1960 to 2021 is given by the dictionary D given in the code cell below.\nFind:\n\nThe GDP per capita in 2015\nThe GDP per capita of 2014 is missing. Update the dictionary to include the GDP per capita of 2014 as the average of the GDP per capita of 2013 and 2015.\nImpute the GDP per capita of other missing years in the same manner as in (2), i.e., as the average GDP per capita of the previous year and the next year. Note that the GDP per capita is not missing for any two consecutive years.\nPrint the years and the imputed GDP per capita for the years having a missing value of GDP per capita in (3).\n\n\nD = {'1960':3007,'1961':3067,'1962':3244,'1963':3375,'1964':3574,'1965':3828,'1966':4146,'1967':4336,'1968':4696,'1970':5234,'1971':5609,'1972':6094,'1973':6726,'1974':7226,'1975':7801,'1976':8592,'1978':10565,'1979':11674, '1980':12575,'1981':13976,'1982':14434,'1983':15544,'1984':17121,'1985':18237,  '1986':19071,'1987':20039,'1988':21417,'1989':22857,'1990':23889,'1991':24342,  '1992':25419,'1993':26387,'1994':27695,'1995':28691,'1996':29968,'1997':31459,  '1998':32854,'2000':36330,'2001':37134,'2002':37998,'2003':39490,'2004':41725,  '2005':44123,'2006':46302,'2007':48050,'2008':48570,'2009':47195,'2010':48651,  '2011':50066,'2012':51784,'2013':53291,'2015':56763,'2016':57867,'2017':59915,'2018':62805, '2019':65095,'2020':63028,'2021':69288}\n\nSolution:\n\nprint(\"GDP per capita in 2015 =\", D['2015'])\nD['2014'] = (D['2013']+D['2015'])/2\nfor i in range(1960,2021):\n    if str(i) not in D.keys():    \n        D[str(i)] = (D[str(i-1)]+D[str(i+1)])/2\n        print(\"Imputed GDP per capita for the year\",i,\"is $\",D[str(i)])\n\nGDP per capita in 2015 = 56763\nImputed GDP per capita for the year 1969 is $ 4965.0\nImputed GDP per capita for the year 1977 is $ 9578.5\nImputed GDP per capita for the year 1999 is $ 34592.0"
  },
  {
    "objectID": "data_structures.html#functions",
    "href": "data_structures.html#functions",
    "title": "2  Data structures",
    "section": "2.4 Functions",
    "text": "2.4 Functions\nIf an algorithm or block of code is being used several times in a code, then it can be separately defined as a function. This makes the code more organized and readable. For example, let us define a function that prints prime numbers between a and b, and returns the number of prime numbers found.\n\n#Function definition\ndef prime_numbers (a,b=100):\n    num_prime_nos = 0\n    \n    #Iterating over all numbers between a and b\n    for i in range(a,b):\n        num_divisors=0\n        \n        #Checking if the ith number has any factors\n        for j in range(2, i):\n            if i%j == 0:\n                num_divisors=1;break;\n                \n        #If there are no factors, then printing and counting the number as prime        \n        if num_divisors==0:\n            print(i)\n            num_prime_nos = num_prime_nos+1\n            \n    #Return count of the number of prime numbers\n    return num_prime_nos\n\nIn the above function, the keyword def is used to define the function, prime_numbers is the name of the function, a and b are the arguments that the function uses to compute the output.\nLet us use the defined function to print and count the prime numbers between 40 and 60.\n\n#Printing prime numbers between 40 and 60\nnum_prime_nos_found = prime_numbers(40,60)\n\n41\n43\n47\n53\n59\n\n\n\nnum_prime_nos_found\n\n5\n\n\nIf the user calls the function without specifying the value of the argument b, then it will take the default value of 100, as mentioned in the function definition. However, for the argument a, the user will need to specify a value, as there is no value defined as a default value in the function definition.\n\n2.4.1 Global and local variables with respect to a function\nA variable defined within a function is local to that function, while a variable defined outside the function is global to that function. In case a variable with the same name is defined both outside and inside a function, it will refer to its global value outside the function and local value within the function.\nThe example below shows a variable with the name var referring to its local value when called within the function, and global value when called outside the function.\n\nvar = 5\ndef sample_function(var):    \n    print(\"Local value of 'var' within 'sample_function()'= \",var)\n\nsample_function(4)\nprint(\"Global value of 'var' outside 'sample_function()' = \",var)\n\nLocal value of 'var' within 'sample_function()'=  4\nGlobal value of 'var' outside 'sample_function()' =  5\n\n\n\n\n2.4.2 Practice exercise 4\nThe object deck defined below corresponds to a deck of cards. Estimate the probablity that a five card hand will be a flush, as follows:\n\nWrite a function that accepts a hand of 5 cards as argument, and returns whether the hand is a flush or not.\nRandomly pull a hand of 5 cards from the deck. Call the function developed in (1) to determine if the hand is a flush.\nRepeat (2) 10,000 times.\nEstimate the probability of the hand being a flush from the results of the 10,000 simulations.\n\nYou may use the function shuffle() from the random library to shuffle the deck everytime before pulling a hand of 5 cards.\n\ndeck = [{'value':i, 'suit':c}\nfor c in ['spades', 'clubs', 'hearts', 'diamonds']\nfor i in range(2,15)]\n\nSolution:\n\nimport random as rm\n\n#Function to check if a 5-card hand is a flush\ndef chck_flush(hands):  \n    \n    #Assuming that the hand is a flush, before checking the cards\n    yes_flush =1\n    \n    #Storing the suit of the first card in 'first_suit'\n    first_suit = hands[0]['suit']\n    \n    #Iterating over the remaining 4 cards of the hand\n    for j in range(1,len(hands)):\n        \n        #If the suit of any of the cards does not match the suit of the first card, the hand is not a flush\n        if first_suit!=hands[j]['suit']:\n            yes_flush = 0; \n            \n            #As soon as a card with a different suit is found, the hand is not a flush and there is no need to check other cards. So, we 'break' out of the loop\n            break;\n    return yes_flush\n\nflush=0\nfor i in range(10000):\n    \n    #Shuffling the deck\n    rm.shuffle(deck)\n    \n    #Picking out the first 5 cards of the deck as a hand and checking if they are a flush\n    #If the hand is a flush it is counted\n    flush=flush+chck_flush(deck[0:5])\n    \nprint(\"Probability of obtaining a flush=\", 100*(flush/10000),\"%\")\n\nProbability of obtaining a flush= 0.18 %"
  },
  {
    "objectID": "data_structures.html#practice-exercise-5",
    "href": "data_structures.html#practice-exercise-5",
    "title": "2  Data structures",
    "section": "2.5 Practice exercise 5",
    "text": "2.5 Practice exercise 5\nThe code cell below defines an object having the nutrition information of drinks in starbucks. Assume that the manner in which the information is structured is consistent throughout the object.\n\nstarbucks_drinks_nutrition={'Cool Lime Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 45}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 11}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Strawberry Acai Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 80}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 18}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Very Berry Hibiscus Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 14}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Evolution Fresh™ Organic Ginger Limeade': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Iced Coffee': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Iced Espresso Classics - Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 21}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 65}], 'Iced Espresso Classics - Caffe Mocha': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 23}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 90}], 'Iced Espresso Classics - Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 21}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 65}], 'Shaken Sweet Tea': [{'Nutrition_type': 'Calories', 'value': 80}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Berry Blossom White': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Black Mango': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 38}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Black with Lemon': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Brambleberry': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Giant Peach': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Iced Passion': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Lemon Ginger': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 31}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Black Lemonade': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Iced Black Tea': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Iced Green Tea': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 31}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Plum Pomegranate': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Tazoberry': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 38}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled White Cranberry': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Teavana® Shaken Iced Black Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Black Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Teavana® Shaken Iced Green Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Green Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Passion Tango™ Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Passion Tango™ Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 24}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Teavana® Shaken Iced Peach Green Tea': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks Refreshers™ Raspberry Pomegranate': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 27}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks Refreshers™ Strawberry Lemonade': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 27}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Doubleshot Protein Dark Chocolate': [{'Nutrition_type': 'Calories', 'value': 210}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 33}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Starbucks® Doubleshot Protein Vanilla': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 120}], 'Starbucks® Iced Coffee Caramel': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 13}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Iced Coffee Light Sweetened': [{'Nutrition_type': 'Calories', 'value': 50}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 11}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Iced Coffee Unsweetened': [{'Nutrition_type': 'Calories', 'value': 10}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 2}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Blonde Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Clover® Brewed Coffee': [{'Nutrition_type': 'Calories', 'value': 10}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Decaf Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Featured Dark Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Nariño 70 Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Nariño 70 Cold Brew with Milk': [{'Nutrition_type': 'Calories', 'value': 0}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Nitro Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Nitro Cold Brew with Sweet Cream': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 5.0}, {'Nutrition_type': 'Carb', 'value': 5}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 20}], 'Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Vanilla Sweet Cream Cold Brew': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 14}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 25}], 'Hot Chocolate': [{'Nutrition_type': 'Calories', 'value': 320}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 47}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 14}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Starbucks® Signature Hot Chocolate': [{'Nutrition_type': 'Calories', 'value': 430}, {'Nutrition_type': 'Fat', 'value': 26.0}, {'Nutrition_type': 'Carb', 'value': 45}, {'Nutrition_type': 'Fiber', 'value': 5}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 13}, {'Nutrition_type': 'Sodium', 'value': 170}], 'Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 290}, {'Nutrition_type': 'Fat', 'value': 8.0}, {'Nutrition_type': 'Carb', 'value': 42}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 13}, {'Nutrition_type': 'Sodium', 'value': 140}], 'Cappuccino': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 12}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 8}, {'Nutrition_type': 'Sodium', 'value': 100}], 'Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 260}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 40}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 11}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 32}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Flat White': [{'Nutrition_type': 'Calories', 'value': 180}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 18}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Iced Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 4.5}, {'Nutrition_type': 'Carb', 'value': 13}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 8}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Iced Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 230}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 36}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 9}, {'Nutrition_type': 'Sodium', 'value': 90}], 'Iced Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Iced Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 7}, {'Nutrition_type': 'Sodium', 'value': 95}], 'Iced Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories', 'value': 260}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 11}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Iced Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 30}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 7}, {'Nutrition_type': 'Sodium', 'value': 100}], 'Iced White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 300}, {'Nutrition_type': 'Fat', 'value': 8.0}, {'Nutrition_type': 'Carb', 'value': 47}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 190}], 'Latte Macchiato': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Starbucks Doubleshot® on Ice Beverage': [{'Nutrition_type': 'Calories', 'value': 45}, {'Nutrition_type': 'Fat', 'value': 1.0}, {'Nutrition_type': 'Carb', 'value': 5}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 3}, {'Nutrition_type': 'Sodium', 'value': 40}], 'Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 150}], 'White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 360}, {'Nutrition_type': 'Fat', 'value': 11.0}, {'Nutrition_type': 'Carb', 'value': 53}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 14}, {'Nutrition_type': 'Sodium', 'value': 240}], 'Cinnamon Dolce Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 350}, {'Nutrition_type': 'Fat', 'value': 4.5}, {'Nutrition_type': 'Carb', 'value': 64}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 15}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Coffee Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 24}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 3}, {'Nutrition_type': 'Sodium', 'value': 200}], 'Mocha Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 280}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 60}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 4}, {'Nutrition_type': 'Sodium', 'value': 220}], 'Mocha Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.5}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 4}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Cinnamon Dolce Crème': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 135}], 'Vanilla Crème': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 135}], 'Chocolate Smoothie': [{'Nutrition_type': 'Calories', 'value': 320}, {'Nutrition_type': 'Fat', 'value': 5.0}, {'Nutrition_type': 'Carb', 'value': 53}, {'Nutrition_type': 'Fiber', 'value': 8}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 170}], 'Strawberry Smoothie': [{'Nutrition_type': 'Calories', 'value': 300}, {'Nutrition_type': 'Fat', 'value': 2.0}, {'Nutrition_type': 'Carb', 'value': 60}, {'Nutrition_type': 'Fiber', 'value': 7}, {'Nutrition_type': 'Protein', 'value': 16}, {'Nutrition_type': 'Sodium', 'value': 130}]}\n\nUse the object above to answer the following questions:\n\n2.5.1 \nWhat is the datatype of the object?\n\nprint(\"Datatype=\",type(starbucks_drinks_nutrition)) \n\nDatatype= <class 'dict'>\n\n\n\n2.5.1.1 \nIf the object in (1) is a dictonary, what is the datatype of the values of the dictionary?\n\nprint(\"Datatype=\",type(starbucks_drinks_nutrition[list(starbucks_drinks_nutrition.keys())[0]]))\n\nDatatype= <class 'list'>\n\n\n\n\n2.5.1.2 \nIf the object in (1) is a dictonary, what is the datatype of the elements within the values of the dictionary?\n\nprint(\"Datatype=\",type(starbucks_drinks_nutrition[list(starbucks_drinks_nutrition.keys())[0]][0]))\n\nDatatype= <class 'dict'>\n\n\n\n\n2.5.1.3 \nHow many calories are there in Iced Coffee?\n\nprint(\"Calories = \",starbucks_drinks_nutrition['Iced Coffee'][0]['value'])\n\nCalories =  5\n\n\n\n\n2.5.1.4 \nWhich drink(s) have the highest amount of protein in them, and what is that protein amount?\n\n#Defining an empty dictionary that will be used to store the protein of each drink\nprotein={}\n\nfor key,value in starbucks_drinks_nutrition.items():\n    for nutrition in value:        \n        if nutrition['Nutrition_type']=='Protein':\n            protein[key]=(nutrition['value'])\n\n#Using dictionary comprehension to find the key-value pair having the maximum value in the dictionary\n{key:value for key, value in protein.items() if value == max(protein.values())}\n\n{'Starbucks® Doubleshot Protein Dark Chocolate': 20,\n 'Starbucks® Doubleshot Protein Vanilla': 20,\n 'Chocolate Smoothie': 20}\n\n\n\n\n2.5.1.5 \nWhich drink(s) have a fat content of more than 10g, and what is their fat content?\n\n#Defining an empty dictionary that will be used to store the fat of each drink\nfat={}\nfor key,value in starbucks_drinks_nutrition.items():\n    for nutrition in value:        \n        if nutrition['Nutrition_type']=='Fat':\n            fat[key]=(nutrition['value'])\n            \n#Using dictionary comprehension to find the key-value pair having the value more than 10\n{key:value for key, value in fat.items() if value>=10}\n\n{'Starbucks® Signature Hot Chocolate': 26.0, 'White Chocolate Mocha': 11.0}"
  },
  {
    "objectID": "Reading data.html",
    "href": "Reading data.html",
    "title": "3  Reading data",
    "section": "",
    "text": "Reading data is the first step to extract information from it. Data can exist broadly in two formats:\n\nStructured data, and\nUntructured data.\n\nStructured data is typically stored in a tabular form, where rows in the data correspond to “observations” and columns correspond to “variables”. For example, the following dataset contains 5 observations, where each observation (or row) consists of information about a movie. The variables (or columns) contain different pieces of information about a given movie. As all variables for a given row are related to the same movie, the data below is also called relational data.\n\n\n\n\n\n\n  \n    \n      \n      Title\n      US Gross\n      Production Budget\n      Release Date\n      Major Genre\n      Creative Type\n      Rotten Tomatoes Rating\n      IMDB Rating\n    \n  \n  \n    \n      0\n      The Shawshank Redemption\n      28241469\n      25000000\n      Sep 23 1994\n      Drama\n      Fiction\n      88\n      9.2\n    \n    \n      1\n      Inception\n      285630280\n      160000000\n      Jul 16 2010\n      Horror/Thriller\n      Fiction\n      87\n      9.1\n    \n    \n      2\n      One Flew Over the Cuckoo's Nest\n      108981275\n      4400000\n      Nov 19 1975\n      Comedy\n      Fiction\n      96\n      8.9\n    \n    \n      3\n      The Dark Knight\n      533345358\n      185000000\n      Jul 18 2008\n      Action/Adventure\n      Fiction\n      93\n      8.9\n    \n    \n      4\n      Schindler's List\n      96067179\n      25000000\n      Dec 15 1993\n      Drama\n      Non-Fiction\n      97\n      8.9\n    \n  \n\n\n\n\nUnstructured data is data that is not organized in any pre-defined manner. Examples of unstructured data can be text files, audio/video files, images, Internet of Things (IoT) data, etc. Unstructured data is relatively harder to analyze as most of the analytical methods and tools are oriented towards structured data. However, an unstructured data can be used to obtain structured data, which in turn can be analyzed. For example, an image can be converted to an array of pixels - which will be structured data. Machine learning algorithms can then be used on the array to classify the image as that of a dog or a cat.\nIn this course, we will focus on analyzing structured data."
  },
  {
    "objectID": "Reading data.html#reading-a-csv-file-with-pandas",
    "href": "Reading data.html#reading-a-csv-file-with-pandas",
    "title": "3  Reading data",
    "section": "3.2 Reading a csv file with Pandas",
    "text": "3.2 Reading a csv file with Pandas\nStructured data can be stored in a variety of formats. The most popular format is data_file_name.csv, where the extension csv stands for comma separated values. The variable values of each observation are separated by a comma in a .csv file. In other words, the delimiter is a comma in a csv file. However, the comma is not visible when a .csv file is opened with Microsoft Excel.\n\n3.2.1 Using the read_csv function\nWe will use functions from the Pandas library of Python to read data. Let us import Pandas to use its functions.\n\nimport pandas as pd\n\nNote that pd is the acronym that we will use to call a Pandas function. This acronym can be anything as desired by the user.\nThe function to read a csv file is read_csv(). It reads the dataset into an object of type Pandas DataFrame. Let us read the dataset movie_ratings.csv in Python.\n\nmovie_ratings = pd.read_csv('movie_ratings.csv')\n\nThe built-in python function type can be used to check the dataype of an object:\n\ntype(movie_ratings)\n\npandas.core.frame.DataFrame\n\n\nNote that the file movie_ratings.csv is stored at the same location as the python script containing the above code. If that is not the case, we’ll need to specify the location of the file as in the following code.\n\nmovie_ratings = pd.read_csv('D:/Books/DataScience_Intro_python/movie_ratings.csv')\n\nNote that forward slash is used instead of backslash while specifying the path of the data file. Another option is to use two consecutive backslashes instead of a single forward slash.\n\n\n3.2.2 Specifying the working directory\nIn case we need to read several datasets from a given location, it may be inconvenient to specify the path every time. In such a case we can change the current working directory to the location where the datasets are located.\nWe’ll use the os library of Python to view and/or change the current working directory.\n\nimport os #Importing the 'os' library\nos.getcwd() #Getting the path to the current working directory\n\n\n\nC:\\Users\\username\\STAT303-1\\Quarto Book\\DataScience_Intro_python\n\n\nThe function getcwd() stands for get current working directory.\nSuppose the dataset to be read is located at 'D:\\Books\\DataScience_Intro_python\\Datasets'. Then, we’ll use the function chdir to change the current working directory to this location.\n\nos.chdir('D:/Books/DataScience_Intro_python/Datasets')\n\nNow we can read the dataset from this location without mentioning the entire path as shown below.\n\nmovie_ratings = pd.read_csv('movie_ratings.csv')\n\n\n\n3.2.3 Data overview\nOnce the data has been read, we may want to see what the data looks like. We’ll use another Pandas function head() to view the first few rows of the data.\n\nmovie_ratings.head()\n\n\n\n\n\n  \n    \n      \n      Title\n      US Gross\n      Worldwide Gross\n      Production Budget\n      Release Date\n      MPAA Rating\n      Source\n      Major Genre\n      Creative Type\n      IMDB Rating\n      IMDB Votes\n    \n  \n  \n    \n      0\n      Opal Dreams\n      14443\n      14443\n      9000000\n      Nov 22 2006\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      6.5\n      468\n    \n    \n      1\n      Major Dundee\n      14873\n      14873\n      3800000\n      Apr 07 1965\n      PG/PG-13\n      Adapted screenplay\n      Western/Musical\n      Fiction\n      6.7\n      2588\n    \n    \n      2\n      The Informers\n      315000\n      315000\n      18000000\n      Apr 24 2009\n      R\n      Adapted screenplay\n      Horror/Thriller\n      Fiction\n      5.2\n      7595\n    \n    \n      3\n      Buffalo Soldiers\n      353743\n      353743\n      15000000\n      Jul 25 2003\n      R\n      Adapted screenplay\n      Comedy\n      Fiction\n      6.9\n      13510\n    \n    \n      4\n      The Last Sin Eater\n      388390\n      388390\n      2200000\n      Feb 09 2007\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      5.7\n      1012\n    \n  \n\n\n\n\n\n3.2.3.1 Row Indices and column names (axis labels)\nThe bold integers on the left are the indices of the DataFrame. Each index refers to a distinct row. For example, the index 2 correponds to the row of the movie The Informers. By default, the indices are integers starting from 0. However, they can be changed (to even non-integer values) if desired by the user.\nThe bold text on top of the DataFrame refers to column names. For example, the column US Gross consists of the gross revenue of a movie in the US.\nCollectively, the indices and column names are referred as axis labels.\n\n\n3.2.3.2 Shape of DataFrame\nFor finding the number of rows and columns in the data, you may use the shape() function.\n\n#Finding the shape of movie_ratings dataset\nmovie_ratings.shape\n\n(2228, 11)\n\n\nThe movie_ratings dataset contains 2,228 observations (or rows) and 11 variables (or columns).\n\n\n\n3.2.4 Summary statistics\n\n3.2.4.1 Numeric columns summary\nThe Pandas function of the DataFrame class, describe() can be used very conviniently to print the summary statistics of numeric columns of the data.\n\n#Finding summary statistics of movie_ratings dataset\nmovie_ratings.describe()\n\n\n\n\n\nTable 3.1:  Summary statistics of numeric variables \n  \n    \n      \n      US Gross\n      Worldwide Gross\n      Production Budget\n      IMDB Rating\n      IMDB Votes\n      Release Year\n    \n  \n  \n    \n      count\n      2.228000e+03\n      2.228000e+03\n      2.228000e+03\n      2228.000000\n      2228.000000\n      2228.000000\n    \n    \n      mean\n      5.076370e+07\n      1.019370e+08\n      3.816055e+07\n      6.239004\n      33585.154847\n      2002.005386\n    \n    \n      std\n      6.643081e+07\n      1.648589e+08\n      3.782604e+07\n      1.243285\n      47325.651561\n      5.524324\n    \n    \n      min\n      0.000000e+00\n      8.840000e+02\n      2.180000e+02\n      1.400000\n      18.000000\n      1953.000000\n    \n    \n      25%\n      9.646188e+06\n      1.320737e+07\n      1.200000e+07\n      5.500000\n      6659.250000\n      1999.000000\n    \n    \n      50%\n      2.838649e+07\n      4.266892e+07\n      2.600000e+07\n      6.400000\n      18169.000000\n      2002.000000\n    \n    \n      75%\n      6.453140e+07\n      1.200000e+08\n      5.300000e+07\n      7.100000\n      40092.750000\n      2006.000000\n    \n    \n      max\n      7.601676e+08\n      2.767891e+09\n      3.000000e+08\n      9.200000\n      519541.000000\n      2039.000000\n    \n  \n\n\n\n\n\nAnswer the following questions based on the above table.\n\n\n\n\n\n \n        \n\n\n\n\n\n\n\n \n        \n\n\n\n\n3.2.4.2 Summary statistics across rows/columns\nThe Pandas DataFrame class has functions such as sum() and mean() to compute sum over rows or columns of a DataFrame.\nLet us compute the mean of all the numeric columns of the data:\n\nmovie_ratings.mean(axis = 0)\n\nUS Gross             5.076370e+07\nWorldwide Gross      1.019370e+08\nProduction Budget    3.816055e+07\nIMDB Rating          6.239004e+00\nIMDB Votes           3.358515e+04\ndtype: float64\n\n\nThe argument axis=0 denotes that the mean is taken over all the rows of the DataFrame. For computing a statistic across column the argument axis=1 will be used.\nIf mean over a subset of columns is desired, then those column names can be subset from the data. For example, let us compute the mean IMDB rating, and mean IMDB votes of all the movies:\n\nmovie_ratings[['IMDB Rating','IMDB Votes']].mean(axis = 0)\n\nIMDB Rating        6.239004\nIMDB Votes     33585.154847\ndtype: float64\n\n\n\n\n\n3.2.5 Practice exercise 1\nRead the file Top 10 Albums By Year.csv. This file contains the top 10 albums for each year from 1990 to 2021. Each row corresponds to a unique album.\n\n3.2.5.1 \nPrint the first 5 rows of the data.\n\nalbum_data = pd.read_csv('./Datasets/Top 10 Albums By Year.csv')\nalbum_data.head()\n\n\n\n\n\n  \n    \n      \n      Year\n      Ranking\n      Artist\n      Album\n      Worldwide Sales\n      CDs\n      Tracks\n      Album Length\n      Hours\n      Minutes\n      Seconds\n      Genre\n    \n  \n  \n    \n      0\n      1990\n      8\n      Phil Collins\n      Serious Hits... Live!\n      9956520\n      1\n      15\n      1:16:53\n      1.28\n      76.88\n      4613\n      Rock\n    \n    \n      1\n      1990\n      1\n      Madonna\n      The Immaculate Collection\n      30000000\n      1\n      17\n      1:13:32\n      1.23\n      73.53\n      4412\n      Pop\n    \n    \n      2\n      1990\n      10\n      The Three Tenors\n      Carreras Domingo Pavarotti In Concert 1990\n      8533000\n      1\n      17\n      1:07:55\n      1.13\n      67.92\n      4075\n      Classical\n    \n    \n      3\n      1990\n      4\n      MC Hammer\n      Please Hammer Don't Hurt Em\n      18000000\n      1\n      13\n      0:59:04\n      0.98\n      59.07\n      3544\n      Hip Hop\n    \n    \n      4\n      1990\n      6\n      Movie Soundtrack\n      Aashiqui\n      15000000\n      1\n      12\n      0:58:13\n      0.97\n      58.22\n      3493\n      World\n    \n  \n\n\n\n\n\n\n3.2.5.2 \nHow many rows and columns are there in the data?\n\nalbum_data.shape\n\n(320, 12)\n\n\nThere are 320 rows and 12 columns in the data\n\n\n3.2.5.3 \nPrint the summary statistics of the data, and answer the following questions:\n\nWhat proportion of albums have 15 or lesser tracks? Mention a range for the proportion.\nWhat is the mean length of a track (in minutes)?\n\n\nalbum_data.describe()\n\n\n\n\n\n  \n    \n      \n      Year\n      Ranking\n      CDs\n      Tracks\n      Hours\n      Minutes\n      Seconds\n    \n  \n  \n    \n      count\n      320.000000\n      320.00000\n      320.000000\n      320.000000\n      320.000000\n      320.000000\n      320.000000\n    \n    \n      mean\n      2005.500000\n      5.50000\n      1.043750\n      14.306250\n      0.941406\n      56.478500\n      3388.715625\n    \n    \n      std\n      9.247553\n      2.87678\n      0.246528\n      5.868995\n      0.382895\n      22.970109\n      1378.209812\n    \n    \n      min\n      1990.000000\n      1.00000\n      1.000000\n      6.000000\n      0.320000\n      19.430000\n      1166.000000\n    \n    \n      25%\n      1997.750000\n      3.00000\n      1.000000\n      12.000000\n      0.740000\n      44.137500\n      2648.250000\n    \n    \n      50%\n      2005.500000\n      5.50000\n      1.000000\n      13.000000\n      0.860000\n      51.555000\n      3093.500000\n    \n    \n      75%\n      2013.250000\n      8.00000\n      1.000000\n      15.000000\n      1.090000\n      65.112500\n      3906.750000\n    \n    \n      max\n      2021.000000\n      10.00000\n      4.000000\n      67.000000\n      5.070000\n      304.030000\n      18242.000000\n    \n  \n\n\n\n\nAt least 75% of the albums have 15 tracks since the 75th percentile value of the number of tracks is 15. However, albums between those having 75th percentile value for the number of tracks and those having the maximum number of tracks can also have 15 tracks. Thus, the proportion of albums having 15 or lesser tracks = [75%-99.99%].\n\nprint(\"Mean length of a track =\",56.478500/14.306250, \"minutes\")\n\nMean length of a track = 3.9478200087374398 minutes\n\n\n\n\n\n3.2.6 Creating new columns from existing columns\nNew variables (or columns) can be created based on existing variables, or with external data (we’ll see adding external data later). For example, let us create a new variable ratio_wgross_by_budget, which is the ratio of Worldwide Gross and Production Budget for each movie:\n\nmovie_ratings['ratio_wgross_by_budget'] = movie_ratings['Worldwide Gross']/movie_ratings['Production Budget']\n\nThe new variable can be seen at the right end of the updated DataFrame as shown below.\n\nmovie_ratings.head()\n\n\n\n\n\n  \n    \n      \n      Title\n      US Gross\n      Worldwide Gross\n      Production Budget\n      Release Date\n      MPAA Rating\n      Source\n      Major Genre\n      Creative Type\n      IMDB Rating\n      IMDB Votes\n      ratio_wgross_by_budget\n    \n  \n  \n    \n      0\n      Opal Dreams\n      14443\n      14443\n      9000000\n      Nov 22 2006\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      6.5\n      468\n      0.001605\n    \n    \n      1\n      Major Dundee\n      14873\n      14873\n      3800000\n      Apr 07 1965\n      PG/PG-13\n      Adapted screenplay\n      Western/Musical\n      Fiction\n      6.7\n      2588\n      0.003914\n    \n    \n      2\n      The Informers\n      315000\n      315000\n      18000000\n      Apr 24 2009\n      R\n      Adapted screenplay\n      Horror/Thriller\n      Fiction\n      5.2\n      7595\n      0.017500\n    \n    \n      3\n      Buffalo Soldiers\n      353743\n      353743\n      15000000\n      Jul 25 2003\n      R\n      Adapted screenplay\n      Comedy\n      Fiction\n      6.9\n      13510\n      0.023583\n    \n    \n      4\n      The Last Sin Eater\n      388390\n      388390\n      2200000\n      Feb 09 2007\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      5.7\n      1012\n      0.176541\n    \n  \n\n\n\n\n\n\n3.2.7 Datatype of variables\nNote that in Table 3.1 (summary statistics), we don’t see Release Date. This is because the datatype of Release Data is not numeric.\nThe datatype of each variable can be seen using the dtypes() function of the DataFrame class.\n\n#Checking the datatypes of the variables \nmovie_ratings.dtypes\n\nTitle                 object\nUS Gross               int64\nWorldwide Gross        int64\nProduction Budget      int64\nRelease Date          object\nMPAA Rating           object\nSource                object\nMajor Genre           object\nCreative Type         object\nIMDB Rating          float64\nIMDB Votes             int64\ndtype: object\n\n\nOften, we wish to convert the datatypes of some of the variables to make them suitable for analysis. For example, the datatype of Release Date in the DataFrame movie_ratings is object. To perform numerical computations on this variable, we’ll need to convert it to a datatime format. We’ll use the Pandas function to_datatime() to covert it to a datatime format. Similar functions such as to_numeric(), to_string() etc., can be used for other conversions.\n\npd.to_datetime(movie_ratings['Release Date'])\n\n0      2006-11-22\n1      1965-04-07\n2      2009-04-24\n3      2003-07-25\n4      2007-02-09\n          ...    \n2223   2004-07-07\n2224   1998-06-19\n2225   2010-05-14\n2226   1991-06-14\n2227   1998-01-23\nName: Release Date, Length: 2228, dtype: datetime64[ns]\n\n\nWe can see above that the function to_datetime() converts Release Date to a datetime format.\nNow, we’ll update the variable Release Date in the DataFrame to be in the datetime format:\n\nmovie_ratings['Release Date'] = pd.to_datetime(movie_ratings['Release Date'])\n\n\nmovie_ratings.dtypes\n\nTitle                        object\nUS Gross                      int64\nWorldwide Gross               int64\nProduction Budget             int64\nRelease Date         datetime64[ns]\nMPAA Rating                  object\nSource                       object\nMajor Genre                  object\nCreative Type                object\nIMDB Rating                 float64\nIMDB Votes                    int64\ndtype: object\n\n\nWe can see that the datatype of Release Date has changed to datetime in the updated DataFrame, movie_ratings. Now we can perform computations on Release Date. Suppose we wish to create a new variable Release_year that consists of the year of release of the movie. We’ll use the attribute year of the datetime module to extract the year from Release Date:\n\n#Extracting year from Release Date\nmovie_ratings['Release Year'] = movie_ratings['Release Date'].dt.year\n\n\nmovie_ratings.head()\n\n\n\n\n\n  \n    \n      \n      Title\n      US Gross\n      Worldwide Gross\n      Production Budget\n      Release Date\n      MPAA Rating\n      Source\n      Major Genre\n      Creative Type\n      IMDB Rating\n      IMDB Votes\n      Release Year\n    \n  \n  \n    \n      0\n      Opal Dreams\n      14443\n      14443\n      9000000\n      2006-11-22\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      6.5\n      468\n      2006\n    \n    \n      1\n      Major Dundee\n      14873\n      14873\n      3800000\n      1965-04-07\n      PG/PG-13\n      Adapted screenplay\n      Western/Musical\n      Fiction\n      6.7\n      2588\n      1965\n    \n    \n      2\n      The Informers\n      315000\n      315000\n      18000000\n      2009-04-24\n      R\n      Adapted screenplay\n      Horror/Thriller\n      Fiction\n      5.2\n      7595\n      2009\n    \n    \n      3\n      Buffalo Soldiers\n      353743\n      353743\n      15000000\n      2003-07-25\n      R\n      Adapted screenplay\n      Comedy\n      Fiction\n      6.9\n      13510\n      2003\n    \n    \n      4\n      The Last Sin Eater\n      388390\n      388390\n      2200000\n      2007-02-09\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      5.7\n      1012\n      2007\n    \n  \n\n\n\n\nAs year is a numeric variable, it will appear in the numeric summary statistics with the describe() function, as shown below.\n\nmovie_ratings.describe()\n\n\n\n\n\n  \n    \n      \n      US Gross\n      Worldwide Gross\n      Production Budget\n      IMDB Rating\n      IMDB Votes\n      Release Year\n    \n  \n  \n    \n      count\n      2.228000e+03\n      2.228000e+03\n      2.228000e+03\n      2228.000000\n      2228.000000\n      2228.000000\n    \n    \n      mean\n      5.076370e+07\n      1.019370e+08\n      3.816055e+07\n      6.239004\n      33585.154847\n      2002.005386\n    \n    \n      std\n      6.643081e+07\n      1.648589e+08\n      3.782604e+07\n      1.243285\n      47325.651561\n      5.524324\n    \n    \n      min\n      0.000000e+00\n      8.840000e+02\n      2.180000e+02\n      1.400000\n      18.000000\n      1953.000000\n    \n    \n      25%\n      9.646188e+06\n      1.320737e+07\n      1.200000e+07\n      5.500000\n      6659.250000\n      1999.000000\n    \n    \n      50%\n      2.838649e+07\n      4.266892e+07\n      2.600000e+07\n      6.400000\n      18169.000000\n      2002.000000\n    \n    \n      75%\n      6.453140e+07\n      1.200000e+08\n      5.300000e+07\n      7.100000\n      40092.750000\n      2006.000000\n    \n    \n      max\n      7.601676e+08\n      2.767891e+09\n      3.000000e+08\n      9.200000\n      519541.000000\n      2039.000000\n    \n  \n\n\n\n\n\n\n3.2.8 Practice exercise 2\n\n3.2.8.1 \nWhy is Worldwide Sales not included in the summary statistics table printed in Practice exercise 1?\n\nalbum_data.dtypes\n\nYear                 int64\nRanking              int64\nArtist              object\nAlbum               object\nWorldwide Sales     object\nCDs                  int64\nTracks               int64\nAlbum Length        object\nHours              float64\nMinutes            float64\nSeconds              int64\nGenre               object\ndtype: object\n\n\nWorldwide Sales is not included in the summary statistics table printed in Practice exercise 1 because its data type is object and not int or float\n\n\n3.2.8.2 \nUpdate the DataFrame so that Worldwide Sales is included in the summary statistics table. Print the summary statistics table.\nHint: Sometimes it may not be possible to convert an object to numeric(). For example, the object ‘hi’ cannot be converted to a numeric() by the python compiler. To avoid getting an error, use the errors argument of to_numeric() to force such conversions to NaN (missing value).\n\nalbum_data['Worldwide Sales'] = pd.to_numeric(album_data['Worldwide Sales'], errors = 'coerce')\nalbum_data.describe()\n\n\n\n\n\n  \n    \n      \n      Year\n      Ranking\n      Worldwide Sales\n      CDs\n      Tracks\n      Hours\n      Minutes\n      Seconds\n    \n  \n  \n    \n      count\n      320.000000\n      320.00000\n      3.190000e+02\n      320.000000\n      320.000000\n      320.000000\n      320.000000\n      320.000000\n    \n    \n      mean\n      2005.500000\n      5.50000\n      1.071093e+07\n      1.043750\n      14.306250\n      0.941406\n      56.478500\n      3388.715625\n    \n    \n      std\n      9.247553\n      2.87678\n      7.566796e+06\n      0.246528\n      5.868995\n      0.382895\n      22.970109\n      1378.209812\n    \n    \n      min\n      1990.000000\n      1.00000\n      1.909009e+06\n      1.000000\n      6.000000\n      0.320000\n      19.430000\n      1166.000000\n    \n    \n      25%\n      1997.750000\n      3.00000\n      5.000000e+06\n      1.000000\n      12.000000\n      0.740000\n      44.137500\n      2648.250000\n    \n    \n      50%\n      2005.500000\n      5.50000\n      8.255866e+06\n      1.000000\n      13.000000\n      0.860000\n      51.555000\n      3093.500000\n    \n    \n      75%\n      2013.250000\n      8.00000\n      1.400000e+07\n      1.000000\n      15.000000\n      1.090000\n      65.112500\n      3906.750000\n    \n    \n      max\n      2021.000000\n      10.00000\n      4.500000e+07\n      4.000000\n      67.000000\n      5.070000\n      304.030000\n      18242.000000\n    \n  \n\n\n\n\n\n\n3.2.8.3 \nCreate a new column that computes the average worldwide sales per year for each album, assuming that the worldwide sales are as of 2022. Print the first 5 rows of the updated DataFrame.\n\nalbum_data['mean_sales_per_year'] = album_data['Worldwide Sales']/(2022-album_data['Year'])\nalbum_data.head()\n\n\n\n\n\n  \n    \n      \n      Year\n      Ranking\n      Artist\n      Album\n      Worldwide Sales\n      CDs\n      Tracks\n      Album Length\n      Hours\n      Minutes\n      Seconds\n      Genre\n      mean_sales_per_year\n    \n  \n  \n    \n      0\n      1990\n      8\n      Phil Collins\n      Serious Hits... Live!\n      9956520.0\n      1\n      15\n      1:16:53\n      1.28\n      76.88\n      4613\n      Rock\n      311141.25\n    \n    \n      1\n      1990\n      1\n      Madonna\n      The Immaculate Collection\n      30000000.0\n      1\n      17\n      1:13:32\n      1.23\n      73.53\n      4412\n      Pop\n      937500.00\n    \n    \n      2\n      1990\n      10\n      The Three Tenors\n      Carreras Domingo Pavarotti In Concert 1990\n      8533000.0\n      1\n      17\n      1:07:55\n      1.13\n      67.92\n      4075\n      Classical\n      266656.25\n    \n    \n      3\n      1990\n      4\n      MC Hammer\n      Please Hammer Don't Hurt Em\n      18000000.0\n      1\n      13\n      0:59:04\n      0.98\n      59.07\n      3544\n      Hip Hop\n      562500.00\n    \n    \n      4\n      1990\n      6\n      Movie Soundtrack\n      Aashiqui\n      15000000.0\n      1\n      12\n      0:58:13\n      0.97\n      58.22\n      3493\n      World\n      468750.00\n    \n  \n\n\n\n\n\n\n\n3.2.9 Reading a sub-set of data: loc and iloc\nSometimes we may be interested in working with a subset of rows and columns of the data, instead of working with the entire dataset. The indexing operators loc and iloc provide a convenient way of selecting a subset of desired rows and columns. The operator loc uses axis labels (row indices and column names) to subset the data, while iloc uses the position of rows or columns, where position has values 0,1,2,3,…and so on, for rows from top to bottom and columns from left to right. In other words, the first row has position 0, the second row has position 1, the third row has position 2, and so on. Similarly, the first column from left has position 0, the second column from left has position 1, the third column from left has position 2, and so on.\nLet us read the file movie_IMDBratings_sorted.csv, which has movies sorted in the descending order of their IMDB ratings.\n\nmovies_sorted = pd.read_csv('./Datasets/movie_IMDBratings_sorted.csv',index_col = 0)\n\nThe argument index_col=0 assigns the first column of the file as the row indices of the DataFrame.\n\nmovies_sorted.head()\n\n\n\n\n\n  \n    \n      \n      Title\n      US Gross\n      Worldwide Gross\n      Production Budget\n      Release Date\n      MPAA Rating\n      Source\n      Major Genre\n      Creative Type\n      IMDB Rating\n      IMDB Votes\n    \n    \n      Rank\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      The Shawshank Redemption\n      28241469\n      28241469\n      25000000\n      Sep 23 1994\n      R\n      Adapted screenplay\n      Drama\n      Fiction\n      9.2\n      519541\n    \n    \n      2\n      Inception\n      285630280\n      753830280\n      160000000\n      Jul 16 2010\n      PG/PG-13\n      Original Screenplay\n      Horror/Thriller\n      Fiction\n      9.1\n      188247\n    \n    \n      3\n      The Dark Knight\n      533345358\n      1022345358\n      185000000\n      Jul 18 2008\n      PG/PG-13\n      Adapted screenplay\n      Action/Adventure\n      Fiction\n      8.9\n      465000\n    \n    \n      4\n      Schindler's List\n      96067179\n      321200000\n      25000000\n      Dec 15 1993\n      R\n      Adapted screenplay\n      Drama\n      Non-Fiction\n      8.9\n      276283\n    \n    \n      5\n      Pulp Fiction\n      107928762\n      212928762\n      8000000\n      Oct 14 1994\n      R\n      Original Screenplay\n      Drama\n      Fiction\n      8.9\n      417703\n    \n  \n\n\n\n\nLet us say, we wish to subset the title, worldwide gross, production budget, and IMDB raring of top 3 movies.\n\n# Subsetting the DataFrame by loc - using axis labels\nmovies_subset = movies_sorted.loc[1:3,['Title','Worldwide Gross','Production Budget','IMDB Rating']]\nmovies_subset\n\n\n\n\n\n  \n    \n      \n      Title\n      Worldwide Gross\n      Production Budget\n      IMDB Rating\n    \n    \n      Rank\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      The Shawshank Redemption\n      28241469\n      25000000\n      9.2\n    \n    \n      2\n      Inception\n      753830280\n      160000000\n      9.1\n    \n    \n      3\n      The Dark Knight\n      1022345358\n      185000000\n      8.9\n    \n  \n\n\n\n\n\n# Subsetting the DataFrame by iloc - using index of the position of rows and columns\nmovies_subset = movies_sorted.iloc[0:3,[0,2,3,9]]\nmovies_subset\n\n\n\n\n\n  \n    \n      \n      Title\n      Worldwide Gross\n      Production Budget\n      IMDB Rating\n    \n    \n      Rank\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      The Shawshank Redemption\n      28241469\n      25000000\n      9.2\n    \n    \n      2\n      Inception\n      753830280\n      160000000\n      9.1\n    \n    \n      3\n      The Dark Knight\n      1022345358\n      185000000\n      8.9\n    \n  \n\n\n\n\nLet us find the movie with the maximum Worldwide Gross.\nWe will use the argmax() function of the Pandas Series class to find the position of the movie with the maximum worldwide gross, and then use the position to find the movie.\n\nposition_max_wgross = movies_sorted['Worldwide Gross'].argmax()\n\n\nmovies_sorted.iloc[position_max_wgross,:]\n\nTitle                             Avatar\nUS Gross                       760167650\nWorldwide Gross               2767891499\nProduction Budget              237000000\nRelease Date                 Dec 18 2009\nMPAA Rating                     PG/PG-13\nSource               Original Screenplay\nMajor Genre             Action/Adventure\nCreative Type                    Fiction\nIMDB Rating                          8.3\nIMDB Votes                        261439\nName: 59, dtype: object\n\n\nAvatar has the highest worldwide gross of all the movies. Note that the : indicates that all the columns of the DataFrame are selected.\n\n\n3.2.10 Practice exercise 3\n\n3.2.10.1 \nFind the album having the highest worldwide sales per year, and its artist.\n\nalbum_data.iloc[album_data['mean_sales_per_year'].argmax(),:]\n\nYear                        2021\nRanking                        1\nArtist                     Adele\nAlbum                         30\nWorldwide Sales        4485025.0\nCDs                            1\nTracks                        12\nAlbum Length             0:58:14\nHours                       0.97\nMinutes                    58.23\nSeconds                     3494\nGenre                        Pop\nmean_sales_per_year    4485025.0\nName: 312, dtype: object\n\n\n‘30’ has the highest worldwide sales and its artist is Adele.\n\n\n3.2.10.2 \nSubset the data to include only Hip-Hop albums. How many Hip_Hop albums are there?\n\nhiphop_albums = album_data.loc[album_data['Genre']=='Hip Hop',:]\nprint(\"There are\",hiphop_albums.shape[0], \"hip-hop albums\")\n\nThere are 42 hip-hop albums\n\n\n\n\n3.2.10.3 \nWhich album amongst hip-hop has the higest mean sales per year per track, and who is its artist?\n\nhiphop_albums.loc[:,'mean_sales_per_year_track'] = hiphop_albums.loc[:,'Worldwide Sales']/((2022-hiphop_albums.loc[:,'Year'])*(hiphop_albums.loc[:,'Tracks']))\nhiphop_albums.iloc[hiphop_albums['mean_sales_per_year_track'].argmax(),:]\n\nYear                                  2021\nRanking                                  6\nArtist                           Cai Xukun\nAlbum                                    迷\nWorldwide Sales                  3402981.0\nCDs                                      1\nTracks                                  11\nAlbum Length                       0:24:16\nHours                                  0.4\nMinutes                              24.27\nSeconds                               1456\nGenre                              Hip Hop\nmean_sales_per_year              3402981.0\nmean_sales_per_year_track    309361.909091\nName: 318, dtype: object\n\n\n迷 has the higest mean sales per year per track amongst hip-hop albumns, and its artist is Cai Xukun."
  },
  {
    "objectID": "Reading data.html#reading-other-data-formats---txt-html-json",
    "href": "Reading data.html#reading-other-data-formats---txt-html-json",
    "title": "3  Reading data",
    "section": "3.3 Reading other data formats - txt, html, json",
    "text": "3.3 Reading other data formats - txt, html, json\nAlthough csv is a very popular format for structured data, data is found in several other formats as well. Some of the other data formats are txt, html and json.\n\n3.3.1 Reading txt files\nThe txt format offers some additional flexibility as compared to the csv format. In the csv format, the delimiter is a comma (or the column values are separated by a comma). However, in a txt file, the delimiter can be anything as desired by the user. Let us read the file movie_ratings.txt, where the variable values are separated by a tab character.\n\nmovie_ratings_txt = pd.read_csv('movie_ratings.txt',sep='\\t')\n\nWe use the function read_csv to read a txt file. However, we mention the tab character (r”) as a separator of variable values.\nNote that there is no need to remember the argument name - sep for specifying the delimiter. You can always refer to the read_csv() documentation to find the relevant argument.\n\n\n3.3.2 Practice exercise 4\nRead the file bestseller_books.txt. It contains top 50 best-selling books on amazon from 2009 to 2019. Identify the delimiter without opening the file with Notepad or a text-editing software. How many rows and columns are there in the dataset?\nSolution:\n\n#Reading some lines with 'error_bad_lines=False' to identify the delimiter\nbestseller_books = pd.read_csv('./Datasets/bestseller_books.txt',error_bad_lines=False)\nbestseller_books.head()\n\nb'Skipping line 6: expected 1 fields, saw 2\\nSkipping line 10: expected 1 fields, saw 3\\nSkipping line 16: expected 1 fields, saw 5\\nSkipping line 17: expected 1 fields, saw 4\\nSkipping line 20: expected 1 fields, saw 3\\nSkipping line 29: expected 1 fields, saw 2\\nSkipping line 33: expected 1 fields, saw 2\\nSkipping line 40: expected 1 fields, saw 2\\nSkipping line 41: expected 1 fields, saw 2\\nSkipping line 42: expected 1 fields, saw 3\\nSkipping line 43: expected 1 fields, saw 3\\nSkipping line 44: expected 1 fields, saw 2\\nSkipping line 60: expected 1 fields, saw 4\\nSkipping line 61: expected 1 fields, saw 3\\nSkipping line 63: expected 1 fields, saw 2\\nSkipping line 64: expected 1 fields, saw 2\\nSkipping line 70: expected 1 fields, saw 3\\nSkipping line 71: expected 1 fields, saw 2\\nSkipping line 72: expected 1 fields, saw 2\\nSkipping line 73: expected 1 fields, saw 2\\nSkipping line 80: expected 1 fields, saw 4\\nSkipping line 82: expected 1 fields, saw 2\\nSkipping line 94: expected 1 fields, saw 4\\nSkipping line 95: expected 1 fields, saw 2\\nSkipping line 96: expected 1 fields, saw 2\\nSkipping line 101: expected 1 fields, saw 3\\nSkipping line 119: expected 1 fields, saw 3\\nSkipping line 130: expected 1 fields, saw 2\\nSkipping line 131: expected 1 fields, saw 2\\nSkipping line 132: expected 1 fields, saw 2\\nSkipping line 133: expected 1 fields, saw 2\\nSkipping line 148: expected 1 fields, saw 3\\nSkipping line 149: expected 1 fields, saw 3\\nSkipping line 150: expected 1 fields, saw 3\\nSkipping line 154: expected 1 fields, saw 3\\nSkipping line 155: expected 1 fields, saw 2\\nSkipping line 156: expected 1 fields, saw 3\\nSkipping line 157: expected 1 fields, saw 2\\nSkipping line 158: expected 1 fields, saw 2\\nSkipping line 159: expected 1 fields, saw 2\\nSkipping line 177: expected 1 fields, saw 4\\nSkipping line 178: expected 1 fields, saw 2\\nSkipping line 179: expected 1 fields, saw 2\\nSkipping line 183: expected 1 fields, saw 3\\nSkipping line 209: expected 1 fields, saw 2\\nSkipping line 215: expected 1 fields, saw 3\\nSkipping line 224: expected 1 fields, saw 3\\nSkipping line 230: expected 1 fields, saw 2\\nSkipping line 241: expected 1 fields, saw 2\\nSkipping line 247: expected 1 fields, saw 2\\nSkipping line 248: expected 1 fields, saw 2\\nSkipping line 249: expected 1 fields, saw 2\\nSkipping line 250: expected 1 fields, saw 2\\nSkipping line 251: expected 1 fields, saw 2\\nSkipping line 252: expected 1 fields, saw 2\\nSkipping line 253: expected 1 fields, saw 2\\nSkipping line 254: expected 1 fields, saw 2\\nSkipping line 259: expected 1 fields, saw 3\\nSkipping line 273: expected 1 fields, saw 2\\nSkipping line 274: expected 1 fields, saw 2\\nSkipping line 275: expected 1 fields, saw 2\\nSkipping line 276: expected 1 fields, saw 2\\nSkipping line 277: expected 1 fields, saw 2\\nSkipping line 278: expected 1 fields, saw 2\\nSkipping line 279: expected 1 fields, saw 2\\nSkipping line 280: expected 1 fields, saw 2\\nSkipping line 281: expected 1 fields, saw 2\\nSkipping line 282: expected 1 fields, saw 2\\nSkipping line 292: expected 1 fields, saw 4\\nSkipping line 293: expected 1 fields, saw 4\\nSkipping line 295: expected 1 fields, saw 7\\nSkipping line 296: expected 1 fields, saw 7\\nSkipping line 297: expected 1 fields, saw 2\\nSkipping line 302: expected 1 fields, saw 3\\nSkipping line 315: expected 1 fields, saw 3\\nSkipping line 321: expected 1 fields, saw 2\\nSkipping line 346: expected 1 fields, saw 3\\nSkipping line 347: expected 1 fields, saw 3\\nSkipping line 365: expected 1 fields, saw 2\\nSkipping line 408: expected 1 fields, saw 2\\nSkipping line 420: expected 1 fields, saw 2\\nSkipping line 421: expected 1 fields, saw 2\\nSkipping line 430: expected 1 fields, saw 2\\nSkipping line 434: expected 1 fields, saw 2\\nSkipping line 446: expected 1 fields, saw 2\\nSkipping line 448: expected 1 fields, saw 2\\nSkipping line 449: expected 1 fields, saw 4\\nSkipping line 451: expected 1 fields, saw 3\\nSkipping line 458: expected 1 fields, saw 2\\nSkipping line 459: expected 1 fields, saw 2\\nSkipping line 460: expected 1 fields, saw 2\\nSkipping line 465: expected 1 fields, saw 2\\nSkipping line 470: expected 1 fields, saw 2\\nSkipping line 471: expected 1 fields, saw 2\\nSkipping line 476: expected 1 fields, saw 2\\nSkipping line 495: expected 1 fields, saw 2\\nSkipping line 496: expected 1 fields, saw 2\\nSkipping line 497: expected 1 fields, saw 2\\nSkipping line 512: expected 1 fields, saw 5\\nSkipping line 513: expected 1 fields, saw 2\\nSkipping line 515: expected 1 fields, saw 2\\nSkipping line 517: expected 1 fields, saw 3\\nSkipping line 518: expected 1 fields, saw 3\\nSkipping line 519: expected 1 fields, saw 3\\nSkipping line 520: expected 1 fields, saw 3\\nSkipping line 521: expected 1 fields, saw 3\\nSkipping line 525: expected 1 fields, saw 3\\nSkipping line 533: expected 1 fields, saw 3\\nSkipping line 534: expected 1 fields, saw 3\\n'\n\n\n\n\n\n\n  \n    \n      \n      ;Unnamed: 0;Name;Author;User Rating;Reviews;Price;Year;Genre\n    \n  \n  \n    \n      0\n      0;0;10-Day Green Smoothie Cleanse;JJ Smith;4.7...\n    \n    \n      1\n      1;1;11/22/63: A Novel;Stephen King;4.6;2052;22...\n    \n    \n      2\n      2;2;12 Rules for Life: An Antidote to Chaos;Jo...\n    \n    \n      3\n      3;3;1984 (Signet Classics);George Orwell;4.7;2...\n    \n    \n      4\n      5;5;A Dance with Dragons (A Song of Ice and Fi...\n    \n  \n\n\n\n\n\n#The delimiter seems to be ';' based on the output of the above code\nbestseller_books = pd.read_csv('./Datasets/bestseller_books.txt',sep=';')\nbestseller_books.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Unnamed: 0.1\n      Name\n      Author\n      User Rating\n      Reviews\n      Price\n      Year\n      Genre\n    \n  \n  \n    \n      0\n      0\n      0\n      10-Day Green Smoothie Cleanse\n      JJ Smith\n      4.7\n      17350\n      8\n      2016\n      Non Fiction\n    \n    \n      1\n      1\n      1\n      11/22/63: A Novel\n      Stephen King\n      4.6\n      2052\n      22\n      2011\n      Fiction\n    \n    \n      2\n      2\n      2\n      12 Rules for Life: An Antidote to Chaos\n      Jordan B. Peterson\n      4.7\n      18979\n      15\n      2018\n      Non Fiction\n    \n    \n      3\n      3\n      3\n      1984 (Signet Classics)\n      George Orwell\n      4.7\n      21424\n      6\n      2017\n      Fiction\n    \n    \n      4\n      4\n      4\n      5,000 Awesome Facts (About Everything!) (Natio...\n      National Geographic Kids\n      4.8\n      7665\n      12\n      2019\n      Non Fiction\n    \n  \n\n\n\n\n\n#The file read with ';' as the delimited is correct\nprint(\"The file has\",bestseller_books.shape[0],\"rows and\",bestseller_books.shape[1],\"columns\")\n\nThe file has 550 rows and 9 columns\n\n\nAlternatively, you can use the argument sep = None, and engine = 'python'. The default engine is C. However, the ‘python’ engine has a ‘sniffer’ tool which may identify the delimiter automatically.\n\nbestseller_books = pd.read_csv('./Datasets/bestseller_books.txt',sep=None, engine = 'python')\nbestseller_books.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Unnamed: 0.1\n      Name\n      Author\n      User Rating\n      Reviews\n      Price\n      Year\n      Genre\n    \n  \n  \n    \n      0\n      0\n      0\n      10-Day Green Smoothie Cleanse\n      JJ Smith\n      4.7\n      17350\n      8\n      2016\n      Non Fiction\n    \n    \n      1\n      1\n      1\n      11/22/63: A Novel\n      Stephen King\n      4.6\n      2052\n      22\n      2011\n      Fiction\n    \n    \n      2\n      2\n      2\n      12 Rules for Life: An Antidote to Chaos\n      Jordan B. Peterson\n      4.7\n      18979\n      15\n      2018\n      Non Fiction\n    \n    \n      3\n      3\n      3\n      1984 (Signet Classics)\n      George Orwell\n      4.7\n      21424\n      6\n      2017\n      Fiction\n    \n    \n      4\n      4\n      4\n      5,000 Awesome Facts (About Everything!) (Natio...\n      National Geographic Kids\n      4.8\n      7665\n      12\n      2019\n      Non Fiction\n    \n  \n\n\n\n\n\n\n3.3.3 Reading HTML data\nThe Pandas function read_html searches for tabular data, i.e., data contained within the <table> tags of an html file. Let us read the tables in the GDP per capita page on Wikipedia.\n\n#Reading all the tables from the Wikipedia page on GDP per capita\ntables = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita')\n\nAll the tables will be read and stored in the variable named as tables. Let us find the datatype of the variable tables.\n\n#Finidng datatype of the variable - tables\ntype(tables)\n\nlist\n\n\nThe variable - tables is a list of all the tables read from the HTML data.\n\n#Number of tables read from the page\nlen(tables)\n\n6\n\n\nThe in-built function len can be used to find the length of the list - tables or the number of tables read from the Wikipedia page. Let us check out the first table.\n\n#Checking out the first table. Note that the index of the first table will be 0.\ntables[0]\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      .mw-parser-output .legend{page-break-inside:av...\n      $20,000 - $30,000 $10,000 - $20,000 $5,000 - $...\n      $1,000 - $2,500 $500 - $1,000 <$500 No data\n    \n  \n\n\n\n\nThe above table doesn’t seem to be useful. Let us check out the second table.\n\n#Checking out the second table. Note that the index of the first table will be 1.\ntables[1]\n\n\n\n\n\n  \n    \n      \n      Country/Territory\n      UN Region\n      IMF[4][5]\n      United Nations[6]\n      World Bank[7]\n    \n    \n      \n      Country/Territory\n      UN Region\n      Estimate\n      Year\n      Estimate\n      Year\n      Estimate\n      Year\n    \n  \n  \n    \n      0\n      Liechtenstein *\n      Europe\n      —\n      —\n      180227\n      2020\n      169049\n      2019\n    \n    \n      1\n      Monaco *\n      Europe\n      —\n      —\n      173696\n      2020\n      173688\n      2020\n    \n    \n      2\n      Luxembourg *\n      Europe\n      135046\n      2022\n      117182\n      2020\n      135683\n      2021\n    \n    \n      3\n      Bermuda *\n      Americas\n      —\n      —\n      123945\n      2020\n      110870\n      2021\n    \n    \n      4\n      Ireland *\n      Europe\n      101509\n      2022\n      86251\n      2020\n      85268\n      2020\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      212\n      Central AfricanRepublic *\n      Africa\n      527\n      2022\n      481\n      2020\n      477\n      2020\n    \n    \n      213\n      Sierra Leone *\n      Africa\n      513\n      2022\n      475\n      2020\n      485\n      2020\n    \n    \n      214\n      Madagascar *\n      Africa\n      504\n      2022\n      470\n      2020\n      496\n      2020\n    \n    \n      215\n      South Sudan *\n      Africa\n      393\n      2022\n      1421\n      2020\n      1120\n      2015\n    \n    \n      216\n      Burundi *\n      Africa\n      272\n      2022\n      286\n      2020\n      274\n      2020\n    \n  \n\n217 rows × 8 columns\n\n\n\nThe above table contains the estimated GDP per capita of all countries. This is the table that is likely to be relevant to a user interested in analyzing GDP per capita of countries. Instead of reading all tables of an HTML file, we can focus the search to tables containing certain relevant keywords. Let us try searching all table containing the word ‘Country’.\n\n#Reading all the tables from the Wikipedia page on GDP per capita, containing the word 'Country'\ntables = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita', match = 'Country')\n\nThe match argument can be used to specify the keywords to be present in the table to be read.\n\nlen(tables)\n\n1\n\n\nOnly one table contains the keyword - ‘Country’. Let us check out the table obtained.\n\n#Table having the keyword - 'Country' from the HTML page\ntables[0]\n\n\n\n\n\n  \n    \n      \n      Country/Territory\n      UN Region\n      IMF[4][5]\n      United Nations[6]\n      World Bank[7]\n    \n    \n      \n      Country/Territory\n      UN Region\n      Estimate\n      Year\n      Estimate\n      Year\n      Estimate\n      Year\n    \n  \n  \n    \n      0\n      Liechtenstein *\n      Europe\n      —\n      —\n      180227\n      2020\n      169049\n      2019\n    \n    \n      1\n      Monaco *\n      Europe\n      —\n      —\n      173696\n      2020\n      173688\n      2020\n    \n    \n      2\n      Luxembourg *\n      Europe\n      135046\n      2022\n      117182\n      2020\n      135683\n      2021\n    \n    \n      3\n      Bermuda *\n      Americas\n      —\n      —\n      123945\n      2020\n      110870\n      2021\n    \n    \n      4\n      Ireland *\n      Europe\n      101509\n      2022\n      86251\n      2020\n      85268\n      2020\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      212\n      Central AfricanRepublic *\n      Africa\n      527\n      2022\n      481\n      2020\n      477\n      2020\n    \n    \n      213\n      Sierra Leone *\n      Africa\n      513\n      2022\n      475\n      2020\n      485\n      2020\n    \n    \n      214\n      Madagascar *\n      Africa\n      504\n      2022\n      470\n      2020\n      496\n      2020\n    \n    \n      215\n      South Sudan *\n      Africa\n      393\n      2022\n      1421\n      2020\n      1120\n      2015\n    \n    \n      216\n      Burundi *\n      Africa\n      272\n      2022\n      286\n      2020\n      274\n      2020\n    \n  \n\n217 rows × 8 columns\n\n\n\nThe argument match helps with a more focussed search, and helps us discard irrelevant tables.\n\n\n3.3.4 Practice exercise 5\nRead the table(s) consisting of attendance of spectators in FIFA worlds cup from this page. Read only those table(s) that have the word ‘attendance’ in them. How many rows and columns are there in the table(s)?\n\ndfs = pd.read_html('https://en.wikipedia.org/wiki/FIFA_World_Cup',\n                       match='attendance')\nprint(len(dfs))\ndata = dfs[0]\nprint(\"Number of rows =\",data.shape[0], \"and number of columns=\",data.shape[1])\n\n1\nNumber of rows = 22 and number of columns= 9\n\n\n\n\n3.3.5 Reading JSON data\nJSON stands for JavaScript Object Notation, in which the data is stored and transmitted as plain text. A couple of benefits of the JSON format are:\n\nSince the format is text only, JSON data can easily be exchanged between web applications, and used by any programming language.\nUnlike the csv format, JSON supports a hierarchical data structure, and is easier to integrate with APIs.\n\nThe JSON format can support a hierachical data structure, as it is built on the following two data structures (Source: technical documentation):\n\nA collection of name/value pairs. In various languages, this is realized as an object, record, struct, dictionary, hash table, keyed list, or associative array.\nAn ordered list of values. In most languages, this is realized as an array, vector, list, or sequence.\n\nThese are universal data structures. Virtually all modern programming languages support them in one form or another. It makes sense that a data format that is interchangeable with programming languages also be based on these structures.\nThe Pandas function read_json converts a JSON string to a Pandas DataFrame. The function dumps() of the json library converts a Python object to a JSON string.\nLets read the JSON data on Ted Talks.\n\ntedtalks_data = pd.read_json('https://raw.githubusercontent.com/cwkenwaysun/TEDmap/master/data/TED_Talks.json')\n\n\ntedtalks_data.head()\n\n\n\n\n\n  \n    \n      \n      id\n      speaker\n      headline\n      URL\n      description\n      transcript_URL\n      month_filmed\n      year_filmed\n      event\n      duration\n      date_published\n      tags\n      newURL\n      date\n      views\n      rates\n    \n  \n  \n    \n      0\n      7\n      David Pogue\n      Simplicity sells\n      http://www.ted.com/talks/view/id/7\n      New York Times columnist David Pogue takes aim...\n      http://www.ted.com/talks/view/id/7/transcript?...\n      2\n      2006\n      TED2006\n      0:21:26\n      6/27/06\n      simplicity,computers,software,interface design...\n      https://www.ted.com/talks/david_pogue_says_sim...\n      2006-06-27\n      1646773\n      [{'id': 7, 'name': 'Funny', 'count': 968}, {'i...\n    \n    \n      1\n      6\n      Craig Venter\n      Sampling the ocean's DNA\n      http://www.ted.com/talks/view/id/6\n      Genomics pioneer Craig Venter takes a break fr...\n      http://www.ted.com/talks/view/id/6/transcript?...\n      7\n      2005\n      TEDGlobal 2005\n      0:16:51\n      2004/05/07\n      biotech,invention,oceans,genetics,DNA,biology,...\n      https://www.ted.com/talks/craig_venter_on_dna_...\n      2004-05-07\n      562625\n      [{'id': 3, 'name': 'Courageous', 'count': 21},...\n    \n    \n      2\n      4\n      Burt Rutan\n      The real future of space exploration\n      http://www.ted.com/talks/view/id/4\n      In this passionate talk, legendary spacecraft ...\n      http://www.ted.com/talks/view/id/4/transcript?...\n      2\n      2006\n      TED2006\n      0:19:37\n      10/25/06\n      aircraft,flight,industrial design,NASA,rocket ...\n      https://www.ted.com/talks/burt_rutan_sees_the_...\n      2006-10-25\n      2046869\n      [{'id': 3, 'name': 'Courageous', 'count': 169}...\n    \n    \n      3\n      3\n      Ashraf Ghani\n      How to rebuild a broken state\n      http://www.ted.com/talks/view/id/3\n      Ashraf Ghani's passionate and powerful 10-minu...\n      http://www.ted.com/talks/view/id/3/transcript?...\n      7\n      2005\n      TEDGlobal 2005\n      0:18:45\n      10/18/06\n      corruption,poverty,economics,investment,milita...\n      https://www.ted.com/talks/ashraf_ghani_on_rebu...\n      2006-10-18\n      814554\n      [{'id': 3, 'name': 'Courageous', 'count': 140}...\n    \n    \n      4\n      5\n      Chris Bangle\n      Great cars are great art\n      http://www.ted.com/talks/view/id/5\n      American designer Chris Bangle explains his ph...\n      http://www.ted.com/talks/view/id/5/transcript?...\n      2\n      2002\n      TED2002\n      0:20:04\n      2004/05/07\n      cars,industrial design,transportation,inventio...\n      https://www.ted.com/talks/chris_bangle_says_gr...\n      2004-05-07\n      870950\n      [{'id': 1, 'name': 'Beautiful', 'count': 89}, ...\n    \n  \n\n\n\n\n\n\n3.3.6 Practice exercise 6\nRead the movies dataset from here. How many rows and columns are there in the data?\n\nmovies_data = pd.read_json('https://raw.githubusercontent.com/vega/vega-datasets/master/data/movies.json')\nprint(\"Number of rows =\",movies_data.shape[0], \"and number of columns=\",movies_data.shape[1])\n\nNumber of rows = 3201 and number of columns= 16\n\n\n\n\n3.3.7 Reading data from web APIs\nAPI, an acronym for Application programming interface, is a way of transferring information between systems. Many websites have public APIs that provide data via JSON or other formats. For example, the IMDb-API is a web service for receiving movies, serial, and cast information. API results are in the JSON format and include items such as movie specifications, ratings, Wikipedia page content, etc. One of these APIs contains ratings of the top 250 movies on IMDB. Let us read this data using the IMDB API.\nWe’ll use the get function from the python library requests to request data from the API and obtain a response code. The response code will let us know if our request to pull data from this API was successful.\n\n#Importing the requests library\nimport requests as rq\n\n\n# Downloading imdb top 250 movie's data\nurl = 'https://imdb-api.com/en/API/Top250Movies/k_v6gf8ppf' #URL of the API containing top 250 movies based on IMDB ratings\nresponse = rq.get(url) #Requesting data from the API\nresponse\n\n<Response [200]>\n\n\nWe have a response code of 200, which indicates that the request was successful.\nThe response object’s JSON method will return a dictionary containing JSON parsed into native Python objects.\n\nmovie_data = response.json()\n\n\nmovie_data.keys()\n\ndict_keys(['items', 'errorMessage'])\n\n\nThe movie_data contains only two keys. The items key seems likely to contain information about the top 250 movies. Let us convert the values of the items key (which is list of dictionaries) to a dataframe, so that we can view it in a tabular form.\n\n#Converting a list of dictionaries to a dataframe\nmovie_data_df = pd.DataFrame(movie_data['items'])\n\n\n#Checking the movie data pulled using the API\nmovie_data_df.head()\n\n\n\n\n\n  \n    \n      \n      id\n      rank\n      title\n      fullTitle\n      year\n      image\n      crew\n      imDbRating\n      imDbRatingCount\n    \n  \n  \n    \n      0\n      tt0111161\n      1\n      The Shawshank Redemption\n      The Shawshank Redemption (1994)\n      1994\n      https://m.media-amazon.com/images/M/MV5BMDFkYT...\n      Frank Darabont (dir.), Tim Robbins, Morgan Fre...\n      9.2\n      2624065\n    \n    \n      1\n      tt0068646\n      2\n      The Godfather\n      The Godfather (1972)\n      1972\n      https://m.media-amazon.com/images/M/MV5BM2MyNj...\n      Francis Ford Coppola (dir.), Marlon Brando, Al...\n      9.2\n      1817542\n    \n    \n      2\n      tt0468569\n      3\n      The Dark Knight\n      The Dark Knight (2008)\n      2008\n      https://m.media-amazon.com/images/M/MV5BMTMxNT...\n      Christopher Nolan (dir.), Christian Bale, Heat...\n      9.0\n      2595637\n    \n    \n      3\n      tt0071562\n      4\n      The Godfather Part II\n      The Godfather Part II (1974)\n      1974\n      https://m.media-amazon.com/images/M/MV5BMWMwMG...\n      Francis Ford Coppola (dir.), Al Pacino, Robert...\n      9.0\n      1248050\n    \n    \n      4\n      tt0050083\n      5\n      12 Angry Men\n      12 Angry Men (1957)\n      1957\n      https://m.media-amazon.com/images/M/MV5BMWU4N2...\n      Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb\n      8.9\n      775140\n    \n  \n\n\n\n\n\n#Rows and columns of the movie data\nmovie_data_df.shape\n\n(250, 9)\n\n\nThis API provides the names of the top 250 movies along with the year of release, IMDB ratings, and cast information."
  },
  {
    "objectID": "Reading data.html#writing-data",
    "href": "Reading data.html#writing-data",
    "title": "3  Reading data",
    "section": "3.4 Writing data",
    "text": "3.4 Writing data\nThe Pandas function to_csv can be used to write (or export) data to a csv or txt file. Below are some examples.\nExample 1: Let us export the movies data of the top 250 movies to a csv file.\n\n#Exporting the data of the top 250 movies to a csv file\nmovie_data_df.to_csv('movie_data_exported.csv')\n\nThe file movie_data_exported.csv will appear in the working directory.\nExample 2: Let us export the movies data of the top 250 movies to a txt file with a semi-colon as the delimiter.\n\nmovie_data_df.to_csv('movie_data_exported.txt',sep=';')\n\nExample 3: Let us export the movies data of the top 250 movies to a JSON file.\n\nwith open('movie_data.json', 'w') as f:\n    json.dump(movie_data, f)"
  },
  {
    "objectID": "NumPy.html",
    "href": "NumPy.html",
    "title": "4  NumPy",
    "section": "",
    "text": "NumPy, short for Numerical Python is used to analyze numeric data with Python. NumPy arrays are primarily used to create homogeneous \\(n\\)-dimensional arrays (\\(n = 1,...,n\\)). Let us import the NumPy library to use its methods and functions, and the NumPy function array() to define a NumPy array.\nThe NumPy function array() creates an object of type numpy.ndarray."
  },
  {
    "objectID": "NumPy.html#why-do-we-need-numpy-arrays",
    "href": "NumPy.html#why-do-we-need-numpy-arrays",
    "title": "4  NumPy",
    "section": "4.1 Why do we need NumPy arrays?",
    "text": "4.1 Why do we need NumPy arrays?\nNumPy arrays can store data just like other data structures such as such as lists, tuples, and Pandas DataFrame. Computations performed using NumPy arrays can also be performed with data stored in the other data structures. However, NumPy is preferred for its efficiency, especially when working with large arrays of data.\n\n4.1.1 Numpy arrays are memory efficient\nA NumPy array is a collection of homogeneous data-types that are stored in contiguous memory locations. On the other hand, data structures such as lists are a collection of heterogeneous data types stored in non-contiguous memory locations. Homogenous data elements let the NumPy array be densely packed resulting in lesser memory consumption. The following example illustrates the smaller size of NumPy arrays as compared to other data structures.\n\n#Example showing NumPy arrays take less storage space than lists, tuples and Pandas DataFrame for the same elements\ntuple_ex = tuple(range(1000))\nlist_ex = list(range(1000))\nnumpy_ex = np.array([range(1000)])\npandas_df = pd.DataFrame(range(1000))\nprint(\"Space taken by tuple =\",tuple_ex.__sizeof__(),\" bytes\")\nprint(\"Space taken by list =\",list_ex.__sizeof__(),\" bytes\")\nprint(\"Space taken by Pandas DataFrame =\",pandas_df.__sizeof__(),\" bytes\")\nprint(\"Space taken by NumPy array =\",numpy_ex.__sizeof__(),\" bytes\")\n\nSpace taken by tuple = 8024  bytes\nSpace taken by list = 8040  bytes\nSpace taken by Pandas DataFrame = 8128  bytes\nSpace taken by NumPy array = 4120  bytes\n\n\nNote that NumPy arrays are memory efficient as long as they are homogenous. They will lose the memory efficiency if they are used to store elements of multiple data types.\nThe example below compares the size of a homogenous NumPy array to that of a similar heterogenous NumPy array to illustrate the point.\n\nnumpy_homogenous = np.array([[1,2],[3,3]])\nprint(\"Size of a homogenous numpy array = \",numpy_homogenous.__sizeof__(), \"bytes\")\n\nSize of homogenous numpy array =  136 bytes\n\n\nNow let us convert an element of the above array to a string, and check the size of the array.\n\nnumpy_homogenous = np.array([[1,'2'],[3,3]])\nprint(\"Size of a heterogenous numpy array = \",numpy_homogenous.__sizeof__(), \"bytes\")\n\nSize of a heterogenous numpy array =  296 bytes\n\n\nThe size of the homogenous NumPy array is much lesser than that of the one with heterogenous data. Thus, NumPy arrays are primarily used for storing homogenous data.\nOn the other hand, the size of other data structures, such as a list, does not depend on whether the elements in them are homogenous or heterogenous, as shown by the example below.\n\nlist_homogenous = list([1,2,3,4])\nprint(\"Size of a homogenous list = \",list_homogenous.__sizeof__(), \"bytes\")\nlist_heterogenous = list([1,'2',3,4])\nprint(\"Size of a heterogenous list = \",list_heterogenous.__sizeof__(), \"bytes\")\n\nSize of a homogenous list =  72 bytes\nSize of a heterogenous list =  72 bytes\n\n\nNote that the memory efficiency of NumPy arrays does not come into play with a very small amount of data. Thus, a list with four elements - 1,2,3 and 4, has a lesser size than a NumPy array with the same elements. However, with larger datasets, such as the one shown earlier (sequence of integers from 0 to 999), the memory efficiency of NumPy arrays can be seen.\nUnlike data structures such as lists, tuples, and dictionary, all elements of a NumPy array should be of same type to leverage the memory efficiency of NumPy arrays.\n\n\n4.1.2 NumPy arrays are fast\nWith NumPy arrays, mathematical computations can be performed faster, as compared to other data structures, due to the following reasons:\n\nAs the NumPy array is densely packed with homogenous data, it helps retrieve the data faster as well, thereby making computations faster.\nWith NumPy, vectorized computations can replace the relatively more expensive python for loops. The NumPy package breaks down the vectorized computations into multiple fragments and then processes all the fragments parallelly. However, with a for loop, computations will be one at a time.\nThe NumPy package integrates C, and C++ codes in Python. These programming languages have very little execution time as compared to Python.\n\nWe’ll see the faster speed on NumPy computations in the example below.\nExample: This example shows that computations using NumPy arrays are typically much faster than computations with other data structures.\nQ: Multiply whole numbers upto 1 million by an integer, say 2. Compare the time taken for the computation if the numbers are stored in a NumPy array vs a list.\nUse the numpy function arange() to define a one-dimensional NumPy array.\n\n#Examples showing NumPy arrays are more efficient for numerical computation\nimport time as tm\nstart_time = tm.time()\nlist_ex = list(range(1000000)) #List containinig whole numbers upto 1 million\na=(list_ex*2)\nprint(\"Time take to multiply numbers in a list = \", tm.time()-start_time)\n\nstart_time = tm.time()\ntuple_ex = tuple(range(1000000)) #Tuple containinig whole numbers upto 1 million\na=(tuple_ex*2)\nprint(\"Time take to multiply numbers in a tuple = \", tm.time()-start_time)\n\nstart_time = tm.time()\ndf_ex = pd.DataFrame(range(1000000)) #Pandas DataFrame containinig whole numbers upto 1 million\na=(df_ex*2)\nprint(\"Time take to multiply numbers in a Pandas DataFrame = \", tm.time()-start_time)\n\nstart_time = tm.time()\nnumpy_ex = np.arange(1000000) #NumPy array containinig whole numbers upto 1 million\na=(numpy_ex*2)\nprint(\"Time take to multiply numbers in a NumPy array = \", tm.time()-start_time)\n\nTime take to multiply numbers in a list =  0.023949384689331055\nTime take to multiply numbers in a tuple =  0.03192734718322754\nTime take to multiply numbers in a Pandas DataFrame =  0.047330617904663086\nTime take to multiply numbers in a NumPy array =  0.0"
  },
  {
    "objectID": "NumPy.html#numpy-array-basic-attributes",
    "href": "NumPy.html#numpy-array-basic-attributes",
    "title": "4  NumPy",
    "section": "4.2 NumPy array: Basic attributes",
    "text": "4.2 NumPy array: Basic attributes\nLet us define a NumPy array:\n\nnumpy_ex = np.array([[1,2,3],[4,5,6]])\nnumpy_ex\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\nThe attributes of numpy_ex can be seen by typing numpy_ex followed by a ., and then pressing the tab key.\nSome of the basic attributes of a NumPy array are the following:\n\n4.2.1 ndim\nShows the number of dimensions (or axes) of the array.\n\nnumpy_ex.ndim\n\n2\n\n\n\n\n4.2.2 shape\nThis is a tuple of integers indicating the size of the array in each dimension. For a matrix with n rows and m columns, the shape will be (n,m). The length of the shape tuple is therefore the rank, or the number of dimensions, ndim.\n\nnumpy_ex.shape\n\n(2, 3)\n\n\n\n\n4.2.3 size\nThis is the total number of elements of the array, which is the product of the elements of shape.\n\nnumpy_ex.size\n\n6\n\n\n\n\n4.2.4 dtype\nThis is an object describing the type of the elements in the array. One can create or specify dtype’s using standard Python types. NumPy provides many, for example bool_, character, int_, int8, int16, int32, int64, float_, float8, float16, float32, float64, complex_, complex64, object_.\n\nnumpy_ex.dtype\n\ndtype('int32')\n\n\n\n\n4.2.5 T\nThis attribute is used to transpose the NumPy array. This is often used to make matrices (2-dimensional arrays) compatible for multiplication.\nFor matrix multiplication, the columns of the first matrix must be equal to the rows of the second matrix. For example, consider the matrix below:\n\nmatrix_to_multiply = np.array([[1,2,1],[0,1,0]])\n\nSuppose we wish to multiply this matrix with numpy_ex. Note the shape of both the matrices below.\n\nmatrix_to_multiply.shape\n\n(2, 3)\n\n\n\nnumpy_ex.shape\n\n(2, 3)\n\n\nTo multiply the above matrices the number of columns of the one of the matrices must be the same as the number of rows of the other matrix. With the current matrices, this is not true as the number of columns of the first matrix is 3, and the the number of rows of the second matrix is 2 (no matter which matrix is considered to be the first one).\nHowever, if we transpose one of the matrices, their shapes will be compatible for multiplication. Let’s transpose matrix_to_multiply:\n\nmatrix_to_multiply_transpose = matrix_to_multiply.T\nmatrix_to_multiply_transpose\n\narray([[1, 0],\n       [2, 1],\n       [1, 0]])\n\n\nThe shape of matrix_to_multiply_transpose is:\n\nmatrix_to_multiply_transpose.shape\n\n(3, 2)\n\n\nThe matrices matrix_to_multiply_transpose and numpy_ex are compatible for matrix multiplication. However, the result will depend on the order in which the matrices are multiplied:\n\n#Matrix multiplication with matrix_to_multiply_transpose before numpy_ex\nmatrix_to_multiply_transpose.dot(numpy_ex)\n\narray([[ 1,  2,  3],\n       [ 6,  9, 12],\n       [ 1,  2,  3]])\n\n\n\n#Matrix multiplication with numpy_ex before matrix_to_multiply_transpose\nnumpy_ex.dot(matrix_to_multiply_transpose)\n\narray([[ 8,  2],\n       [20,  5]])\n\n\nThe shape of the resulting matrix is equal to the rows of the first matrix and the columns of the second matrix. The order of matrices must be decided as per the requirements of the problem."
  },
  {
    "objectID": "NumPy.html#arithmetic-operations",
    "href": "NumPy.html#arithmetic-operations",
    "title": "4  NumPy",
    "section": "4.3 Arithmetic operations",
    "text": "4.3 Arithmetic operations\nNumpy arrays support arithmetic operators like +, -, *, etc. We can perform an arithmetic operation on an array either with a single number (also called scalar) or with another array of the same shape. However, we cannot perform an arithmetic operation on an array with an array of a different shape.\nBelow are some examples of arithmetic operations on arrays.\n\n#Defining two arrays of the same shape\narr1 = np.array([[1, 2, 3, 4], \n                 [5, 6, 7, 8], \n                 [9, 1, 2, 3]])\narr2 = np.array([[11, 12, 13, 14], \n                 [15, 16, 17, 18], \n                 [19, 11, 12, 13]])\n\n\n#Element-wise summation of arrays\narr1 + arr2\n\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 12, 14, 16]])\n\n\n\n# Element-wise subtraction\narr2 - arr1\n\narray([[10, 10, 10, 10],\n       [10, 10, 10, 10],\n       [10, 10, 10, 10]])\n\n\n\n# Adding a scalar to an array adds the scalar to each element of the array\narr1 + 3\n\narray([[ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12,  4,  5,  6]])\n\n\n\n# Dividing an array by a scalar divides all elements of the array by the scalar\narr1 / 2\n\narray([[0.5, 1. , 1.5, 2. ],\n       [2.5, 3. , 3.5, 4. ],\n       [4.5, 0.5, 1. , 1.5]])\n\n\n\n# Element-wise multiplication\narr1 * arr2\n\narray([[ 11,  24,  39,  56],\n       [ 75,  96, 119, 144],\n       [171,  11,  24,  39]])\n\n\n\n# Modulus operator with scalar\narr1 % 4\n\narray([[1, 2, 3, 0],\n       [1, 2, 3, 0],\n       [1, 1, 2, 3]], dtype=int32)"
  },
  {
    "objectID": "NumPy.html#broadcasting",
    "href": "NumPy.html#broadcasting",
    "title": "4  NumPy",
    "section": "4.4 Broadcasting",
    "text": "4.4 Broadcasting\nBroadcasting allows arithmetic operations between two arrays with different numbers of dimensions but compatible shapes.\nThe Broadcasting documentation succinctly explains it as the following:\n“The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is broadcast across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations.”\nThe example below shows the broadcasting of two arrays.\n\narr1 = np.array([[1, 2, 3, 4], \n                 [5, 6, 7, 8], \n                 [9, 1, 2, 3]])\narr2 = np.array([4, 5, 6, 7])\n\n\narr1 + arr2\n\narray([[ 5,  7,  9, 11],\n       [ 9, 11, 13, 15],\n       [13,  6,  8, 10]])\n\n\nWhen the expression arr1 + arr2 is evaluated, arr2 (which has the shape (4,)) is replicated three times to match the shape (3, 4) of arr1. Numpy performs the replication without actually creating three copies of the smaller dimension array, thus improving performance and using lower memory.\nIn the above addition of arrays, arr2 was stretched or broadcast to the shape of arr1. However, this broadcasting was possible only because the right dimension of both the arrays is 4, and the left dimension of one of the arrays is 1.\nSee the broadcasting documentation to understand the rules for broadcasting:\n“When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when:\n\nthey are equal, or\none of them is 1”\n\nIf the rightmost dimension of arr2 is 3, broadcasting will not occur, as it is not equal to the rightmost dimension of arr1:\n\n#Defining arr2 as an array of shape (3,)\narr2 = np.array([4, 5, 6])\n\n\n#Broadcasting will not happen when the broadcasting rules are violated\narr1 + arr2\n\nValueError: operands could not be broadcast together with shapes (3,4) (3,)"
  },
  {
    "objectID": "NumPy.html#comparison",
    "href": "NumPy.html#comparison",
    "title": "4  NumPy",
    "section": "4.5 Comparison",
    "text": "4.5 Comparison\nNumpy arrays support comparison operations like ==, !=, > etc. The result is an array of booleans.\n\narr1 = np.array([[1, 2, 3], [3, 4, 5]])\narr2 = np.array([[2, 2, 3], [1, 2, 5]])\n\n\narr1 == arr2\n\narray([[False,  True,  True],\n       [False, False,  True]])\n\n\n\narr1 != arr2\n\narray([[ True, False, False],\n       [ True,  True, False]])\n\n\n\narr1 >= arr2\n\narray([[False,  True,  True],\n       [ True,  True,  True]])\n\n\n\narr1 < arr2\n\narray([[ True, False, False],\n       [False, False, False]])\n\n\nArray comparison is frequently used to count the number of equal elements in two arrays using the sum method. Remember that True evaluates to 1 and False evaluates to 0 when booleans are used in arithmetic operations.\n\n(arr1 == arr2).sum()\n\n3"
  },
  {
    "objectID": "NumPy.html#concatenating-arrays",
    "href": "NumPy.html#concatenating-arrays",
    "title": "4  NumPy",
    "section": "4.6 Concatenating arrays",
    "text": "4.6 Concatenating arrays\nArrays can be concatenated along an axis with NumPy’s concatenate function. The axis argument specifies the dimension for concatenation. The arrays should have the same number of dimensions, and the same length along each axis except the axis used for concatenation.\nThe examples below show concatenation of arrays.\n\narr1 = np.array([[1, 2, 3], [3, 4, 5]])\narr2 = np.array([[2, 2, 3], [1, 2, 5]])\nprint(\"Array 1:\\n\",arr1)\nprint(\"Array 2:\\n\",arr2)\n\nArray 1:\n [[1 2 3]\n [3 4 5]]\nArray 2:\n [[2 2 3]\n [1 2 5]]\n\n\n\n#Concatenating the arrays along the default axis: axis=0\nnp.concatenate((arr1,arr2))\n\narray([[1, 2, 3],\n       [3, 4, 5],\n       [2, 2, 3],\n       [1, 2, 5]])\n\n\n\n#Concatenating the arrays along axis = 1\nnp.concatenate((arr1,arr2),axis=1)\n\narray([[1, 2, 3, 2, 2, 3],\n       [3, 4, 5, 1, 2, 5]])\n\n\nSince the arrays need to have the same dimension only along the axis of concatenation, let us try concatenate the array below (arr3) with arr1, along axis = 0.\n\narr3 = np.array([2, 2, 3])\n\n\nnp.concatenate((arr1,arr3),axis=0)\n\nValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)\n\n\nNote the above error, which indicates that arr3 has only one dimension. Let us check the shape of arr3.\n\narr3.shape\n\n(3,)\n\n\nWe can reshape arr3 to a shape of (1,3) to make it compatible for concatenation with arr1 along axis = 0.\n\narr3_reshaped = arr3.reshape(1,3)\narr3_reshaped\n\narray([[2, 2, 3]])\n\n\nNow we can concatenate the reshaped arr3 with arr1 along axis = 0.\n\nnp.concatenate((arr1,arr3_reshaped),axis=0)\n\narray([[1, 2, 3],\n       [3, 4, 5],\n       [2, 2, 3]])"
  },
  {
    "objectID": "NumPy.html#practice-exercise-1",
    "href": "NumPy.html#practice-exercise-1",
    "title": "4  NumPy",
    "section": "4.7 Practice exercise 1",
    "text": "4.7 Practice exercise 1\n\n4.7.0.1 \nRead the coordinates of the capital cities of the world from http://techslides.com/list-of-countries-and-capitals . Use NumPy to print the name and coordinates of the capital city closest to the US capital - Washington DC.\nNote that:\n\nThe Country Name for US is given as United States in the data.\nThe ‘closeness’ of capital cities from the US capital is based on the Euclidean distance of their coordinates to those of the US capital.\n\nHints:\n\nUse read_html() from the Pandas library to read the table.\nUse the to_numpy() function of the Pandas DataFrame class to convert a DataFrame to a Numpy array\nUse broadcasting to compute the euclidean distance of capital cities from Washington DC.\n\nSolution:\n\nimport pandas as pd\ncapital_cities = pd.read_html('http://techslides.com/list-of-countries-and-capitals',header=0)[0]\ncoordinates_capital_cities = capital_cities.iloc[:,2:4].to_numpy()\nus_coordinates = capital_cities.loc[capital_cities['Country Name']=='United States',['Capital Latitude','Capital Longitude']].to_numpy()\n\n#Broadcasting\ndistance_from_DC = np.sqrt(np.sum((us_coordinates-coordinates_capital_cities)**2,axis=1))\n\n#Assigning a high value of distance to DC, otherwise it will itself be selected as being closest to DC\ndistance_from_DC[distance_from_DC==0]=9999\nclosest_capital_index = np.argmin(distance_from_DC)\nprint(\"Closest capital city is:\" ,capital_cities.loc[closest_capital_index,'Capital Name'])\nprint(\"Coordinates of the closest capital city are:\",coordinates_capital_cities[closest_capital_index,:])\n\nClosest capital city is: Ottawa\nCoordinates of the closest capital city are: [ 45.41666667 -75.7       ]\n\n\n\n\n4.7.0.2 \nUse NumPy to:\n\nPrint the names of the countries of the top 10 capital cities closest to the US capital - Washington DC.\nCreate and print a NumPy array containing the coordinates of the top 10 cities.\n\nHint: Use the concatenate() function from the NumPy library to stack the coordinates of the top 10 cities.\n\ntop10_cities_coordinates = coordinates_capital_cities[closest_capital_index,:].reshape(1,2)\nprint(\"Top 10 countries closest to Washington DC are:\\n Canada\")\nfor i in range(9):\n    distance_from_DC[closest_capital_index]=9999\n    closest_capital_index = np.argmin(distance_from_DC)\n    print(capital_cities.loc[closest_capital_index,'Country Name'])\n    top10_cities_coordinates=np.concatenate((top10_cities_coordinates,coordinates_capital_cities[closest_capital_index,:].reshape(1,2)))\nprint(\"Coordinates of the top 10 cities closest to US are: \\n\",top10_cities_coordinates)\n\nTop 10 countries closest to Washington DC are:\n Canada\nBahamas\nBermuda\nCuba\nTurks and Caicos Islands\nCayman Islands\nHaiti\nJamaica\nDominican Republic\nSaint Pierre and Miquelon\nCoordinates of the top 10 cities closest to US are: \n [[ 45.41666667 -75.7       ]\n [ 25.08333333 -77.35      ]\n [ 32.28333333 -64.783333  ]\n [ 23.11666667 -82.35      ]\n [ 21.46666667 -71.133333  ]\n [ 19.3        -81.383333  ]\n [ 18.53333333 -72.333333  ]\n [ 18.         -76.8       ]\n [ 18.46666667 -69.9       ]\n [ 46.76666667 -56.183333  ]]"
  },
  {
    "objectID": "NumPy.html#vectorized-computation-with-numpy",
    "href": "NumPy.html#vectorized-computation-with-numpy",
    "title": "4  NumPy",
    "section": "4.8 Vectorized computation with NumPy",
    "text": "4.8 Vectorized computation with NumPy\nSeveral matrix algebra operations such as multiplications, decompositions, determinants, etc. can be performed conveniently with NumPy. However, we’ll focus on matrix multiplication as it is very commonly used to avoid python for loops and make computations faster. The dot function is used to multiply matrices:\n\n#Defining a 2x2 matrix\na = np.array([[0,1],[3,4]])\na\n\narray([[0, 1],\n       [3, 4]])\n\n\n\n#Defining a 2x2 matrix\nb = np.array([[6,-1],[2,1]])\nb\n\narray([[ 6, -1],\n       [ 2,  1]])\n\n\n\n#Multiplying matrices 'a' and 'b' using the dot function\na.dot(b)\n\narray([[ 2,  1],\n       [26,  1]])\n\n\n\n#Note that * results in element-wise multiplication\na*b\n\narray([[ 0, -1],\n       [ 6,  4]])\n\n\nExample 2: This example will show vectorized computations with NumPy. Vectorized computations help perform computations more efficiently, and also make the code concise.\nQ: Read the (1) quantities of roll, bun, cake and bread required by 3 people - Ben, Barbara & Beth, from food_quantity.csv, (2) price of these food items in two shops - Target and Kroger, from price.csv. Find out which shop should each person go to minimize their expenses.\n\n#Reading the datasets on food quantity and price\nimport pandas as pd\nfood_qty = pd.read_csv('./Datasets/food_quantity.csv')\nprice = pd.read_csv('./Datasets/price.csv')\n\n\nfood_qty\n\n\n\n\n\n  \n    \n      \n      Person\n      roll\n      bun\n      cake\n      bread\n    \n  \n  \n    \n      0\n      Ben\n      6\n      5\n      3\n      1\n    \n    \n      1\n      Barbara\n      3\n      6\n      2\n      2\n    \n    \n      2\n      Beth\n      3\n      4\n      3\n      1\n    \n  \n\n\n\n\n\nprice\n\n\n\n\n\n  \n    \n      \n      Item\n      Target\n      Kroger\n    \n  \n  \n    \n      0\n      roll\n      1.5\n      1.0\n    \n    \n      1\n      bun\n      2.0\n      2.5\n    \n    \n      2\n      cake\n      5.0\n      4.5\n    \n    \n      3\n      bread\n      16.0\n      17.0\n    \n  \n\n\n\n\nFirst, let’s start from a simple problem. We’ll compute the expenses of Ben if he prefers to buy all food items from Target\n\n#Method 1: Using loop\nbens_target_expense = 0 #Initializing Ben's expenses to 0\nfor k in range(4):   #Iterating over all the four desired food items\n    bens_target_expense += food_qty.iloc[0,k+1]*price.iloc[k,1] #Total expenses on the kth item\nbens_target_expense    #Total expenses for Ben if he goes to Target\n\n50.0\n\n\n\n#Method 2: Using NumPy array\nfood_num = food_qty.iloc[0,1:].to_numpy()  #Converting food quantity (for Ben) dataframe to NumPy array\nprice_num = price.iloc[:,1].to_numpy()     #Converting price (for Target) dataframe to NumPy array\nfood_num.dot(price_num)   #Matrix multiplication of the quantity vector with the price vector directly yields the result\n\n50.0\n\n\nBen will spend $50 if he goes to Target\nNow, let’s add another layer of complication. We’ll compute Ben’s expenses for both stores - Target and Kroger\n\n#Method 1: Using loops\n\n#Initializing a Series of length two to store the expenses in Target and Kroger for Ben\nbens_store_expense = pd.Series(0.0,index=price.columns[1:3])\nfor j in range(2):      #Iterating over both the stores - Target and Kroger\n    for k in range(4):        #Iterating over all the four desired food items\n        bens_store_expense[j] += food_qty.iloc[0,k+1]*price.iloc[k,j+1]\nbens_store_expense\n\nTarget    50.0\nKroger    49.0\ndtype: float64\n\n\n\n#Method 2: Using NumPy array\nfood_num = food_qty.iloc[0,1:].to_numpy()  #Converting food quantity (for Ben) dataframe to NumPy array\nprice_num = price.iloc[:,1:].to_numpy()    #Converting price dataframe to NumPy array\nfood_num.dot(price_num)      #Matrix multiplication of the quantity vector with the price matrix directly yields the result\n\narray([50.0, 49.0], dtype=object)\n\n\nBen will spend \\$50 if he goes to Target, and $49 if he goes to Kroger. Thus, he should choose Kroger.\nNow, let’s add the final layer of complication, and solve the problem. We’ll compute everyone’s expenses for both stores - Target and Kroger\n\n#Method 1: Using loops\nstore_expense = pd.DataFrame(0.0,index=price.columns[1:3],columns = food_qty['Person'])\nfor i in range(3):    #Iterating over all the three people - Ben, Barbara, and Beth\n    for j in range(2):     #Iterating over both the stores - Target and Kroger\n        for k in range(4):        #Iterating over all the four desired food items\n            store_expense.iloc[j,i] += food_qty.iloc[i,k+1]*price.iloc[k,j+1]\nstore_expense\n\n\n\n\n\n  \n    \n      Person\n      Ben\n      Barbara\n      Beth\n    \n  \n  \n    \n      Target\n      50.0\n      58.5\n      43.5\n    \n    \n      Kroger\n      49.0\n      61.0\n      43.5\n    \n  \n\n\n\n\n\n#Method 2: Using NumPy array\nfood_num = food_qty.iloc[:,1:].to_numpy() #Converting food quantity dataframe to NumPy array\nprice_num = price.iloc[:,1:].to_numpy()  #Converting price dataframe to NumPy array\nfood_num.dot(price_num)  #Matrix multiplication of the quantity matrix with the price matrix directly yields the result\n\narray([[50. , 49. ],\n       [58.5, 61. ],\n       [43.5, 43.5]])\n\n\nBased on the above table, Ben should go to Kroger, Barbara to Target and Beth can go to either store.\nNote that, with each layer of complication, the number of for loops keep increasing, thereby increasing the complexity of Method 1, while the method with NumPy array does not change much. Vectorized computations with arrays are much more efficient.\n\n4.8.1 Practice exercise 2\nUse matrix multiplication to find the average IMDB rating and average Rotten tomatoes rating for each genre - comedy, action, drama and horror. Use the data: movies_cleaned.csv. Which is the most preferred genre for IMDB users, and which is the least preferred genre for Rotten Tomatoes users?\nHint: 1. Create two matrices - one containing the IMDB and Rotten Tomatoes ratings, and the other containing the genre flags (comedy/action/drama/horror).\n\nMultiply the two matrices created in 1.\nDivide each row/column of the resulting matrix by a vector having the number of ratings in each genre to get the average rating for the genre.\n\nSolution:\n\nimport pandas as pd\ndata = pd.read_csv('./Datasets/movies_cleaned.csv')\ndata.head()\n\n\n\n\n\n  \n    \n      \n      Title\n      IMDB Rating\n      Rotten Tomatoes Rating\n      Running Time min\n      Release Date\n      US Gross\n      Worldwide Gross\n      Production Budget\n      comedy\n      Action\n      drama\n      horror\n    \n  \n  \n    \n      0\n      Broken Arrow\n      5.8\n      55\n      108\n      Feb 09 1996\n      70645997\n      148345997\n      65000000\n      0\n      1\n      0\n      0\n    \n    \n      1\n      Brazil\n      8.0\n      98\n      136\n      Dec 18 1985\n      9929135\n      9929135\n      15000000\n      1\n      0\n      0\n      0\n    \n    \n      2\n      The Cable Guy\n      5.8\n      52\n      95\n      Jun 14 1996\n      60240295\n      102825796\n      47000000\n      1\n      0\n      0\n      0\n    \n    \n      3\n      Chain Reaction\n      5.2\n      13\n      106\n      Aug 02 1996\n      21226204\n      60209334\n      55000000\n      0\n      1\n      0\n      0\n    \n    \n      4\n      Clash of the Titans\n      5.9\n      65\n      108\n      Jun 12 1981\n      30000000\n      30000000\n      15000000\n      0\n      1\n      0\n      0\n    \n  \n\n\n\n\n\n# Getting ratings of all movies\ndrating = data[['IMDB Rating','Rotten Tomatoes Rating']]\ndrating_num = drating.to_numpy() #Converting the data to NumPy array\ndrating_num\n\narray([[ 5.8, 55. ],\n       [ 8. , 98. ],\n       [ 5.8, 52. ],\n       ...,\n       [ 7. , 65. ],\n       [ 5.7, 26. ],\n       [ 6.7, 82. ]])\n\n\n\n# Getting the matrix indicating the genre of all movies\ndgenre = data.iloc[:,8:12]\ndgenre_num = dgenre.to_numpy() #Converting the data to NumPy array\ndgenre_num\n\narray([[0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       ...,\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 1, 0, 0]], dtype=int64)\n\n\nWe’ll first find the total IMDB and Rotten tomatoes ratings for all movies of each genre, and then divide them by the number of movies of the corresponding genre to find the average rating for the genre.\nFor finding the total IMDB and Rotten tomatoes ratings, we’ll multiply drating_num with dgenre_num. However, before multiplying, we’ll check if their shapes are compatible for matrix multiplication.\n\n#Shape of drating_num\ndrating_num.shape\n\n(980, 2)\n\n\n\n#Shape of dgenre_num\ndgenre_num.shape\n\n(980, 4)\n\n\nNote that the above shapes are not compatible for matrix multiplication. We’ll transpose dgenre_num to make the shapes compatible.\n\n#Total IMDB and Rotten tomatoes ratings for each genre\nratings_sum_genre = drating_num.T.dot(dgenre_num)\nratings_sum_genre\n\narray([[ 1785.6,  1673.1,  1630.3,   946.2],\n       [14119. , 13725. , 14535. ,  6533. ]])\n\n\n\n#Number of movies in the data will be stored in 'rows', and number of columns stored in 'cols'\nrows, cols = data.shape\n\n\n#Getting number of movies in each genre\nmovies_count_genre = dgenre_num.T.dot(np.ones(rows))\nmovies_count_genre\n\narray([302., 264., 239., 154.])\n\n\n\n#Finding the average IMDB and average Rotten tomatoes ratings for each genre\nratings_sum_genre/movies_count_genre\n\narray([[ 5.91258278,  6.3375    ,  6.82133891,  6.14415584],\n       [46.75165563, 51.98863636, 60.81589958, 42.42207792]])\n\n\n\npd.DataFrame(ratings_sum_genre/movies_count_genre,columns = ['comedy','Action','drama','horror'],\n             index = ['IMDB Rating','Rotten Tomatoes Rating'])\n\n\n\n\n\n  \n    \n      \n      comedy\n      Action\n      drama\n      horror\n    \n  \n  \n    \n      IMDB Rating\n      5.912583\n      6.337500\n      6.821339\n      6.144156\n    \n    \n      Rotten Tomatoes Rating\n      46.751656\n      51.988636\n      60.815900\n      42.422078\n    \n  \n\n\n\n\nIMDB users prefer drama, and are amused the least by comedy movies, on an average. However, Rotten tomatoes critics would rather watch comedy than horror movies, on an average."
  },
  {
    "objectID": "NumPy.html#pseudorandom-number-generation",
    "href": "NumPy.html#pseudorandom-number-generation",
    "title": "4  NumPy",
    "section": "4.9 Pseudorandom number generation",
    "text": "4.9 Pseudorandom number generation\nRandom numbers often need to be generated to analyze processes or systems, especially in cases when these processes or systems are governed by known probability distrbutions. For example, the number of personnel required to answer calls at a call center can be analyzed by simulating occurence and duration of calls.\nNumPy’s random module can be used to generate arrays of random numbers from several different probability distributions. For example, a 3x5 array of uniformly distributed random numbers can be generated using the uniform function of the random module.\n\nnp.random.uniform(size = (3,5))\n\narray([[0.69256322, 0.69259973, 0.03515058, 0.45186048, 0.43513769],\n       [0.07373366, 0.07465425, 0.92195975, 0.72915895, 0.8906299 ],\n       [0.15816734, 0.88144978, 0.05954028, 0.81403832, 0.97725557]])\n\n\nRandom numbers can also be generated by Python’s built-in random module. However, it generates one random number at a time, which makes it much slower than NumPy’s random module.\nExample: Suppose 500 people eat at Food cart 1, and another 500 eat at Food cart 2, everyday.\nThe waiting time at Food cart 2 has a normal distribution with mean 8 minutes and standard deviation 3 minutes, while the waiting time at Food cart 1 has a uniform distribution with minimum 5 minutes and maximum 25 minutes.\nSimulate a dataset containing waiting times for 500 ppl for 30 days in each of the food joints. Assume that the waiting times are measured simultaneously at a certain time in both places, i.e., the observations are paired.\nOn how many days is the average waiting time at Food cart 2 higher than that at Food cart 1?\nWhat percentage of times the waiting time at Food cart 2 was higher than the waiting time at Food cart 1?\nTry both approaches: (1) Using loops to generate data, (2) numpy array to generate data. Compare the time taken in both approaches.\n\nimport time as tm\n\n\n#Method 1: Using loops\nstart_time = tm.time() #Current system time\n\n#Initializing waiting times for 500 ppl over 30 days\nwaiting_times_FoodCart1 = pd.DataFrame(0,index=range(500),columns=range(30)) #FoodCart1\nwaiting_times_FoodCart2 = pd.DataFrame(0,index=range(500),columns=range(30)) #FoodCart2\nimport random as rm\nfor i in range(500):  #Iterating over 500 ppl\n    for j in range(30): #Iterating over 30 days\n        waiting_times_FoodCart2.iloc[i,j] = rm.gauss(8,3) #Simulating waiting time in FoodCart2 for the ith person on jth day\n        waiting_times_FoodCart1.iloc[i,j] = rm.uniform(5,25) #Simulating waiting time in FoodCart1 for the ith person on jth day\ntime_diff = waiting_times_FoodCart2-waiting_times_FoodCart1\n\nprint(\"On \",sum(time_diff.mean()>0),\" days, the average waiting time at FoodCart2 higher than that at FoodCart1\")\nprint(\"Percentage of times waiting time at FoodCart2 was greater than that at FoodCart1 = \",100*(time_diff>0).sum().sum()/(30*500),\"%\")\nend_time = tm.time() #Current system time\nprint(\"Time taken = \", end_time-start_time)\n\nOn  0  days, the average waiting time at FoodCart2 higher than that at FoodCart1\nPercentage of times waiting time at FoodCart2 was greater than that at FoodCart1 =  16.226666666666667 %\nTime taken =  4.521248817443848\n\n\n\n#Method 2: Using NumPy arrays\nstart_time = tm.time()\nwaiting_time_FoodCart2 = np.random.normal(8,3,size = (500,30)) #Simultaneously generating the waiting times of 500 ppl over 30 days in FoodCart2\nwaiting_time_FoodCart1 = np.random.uniform(5,25,size = (500,30)) #Simultaneously generating the waiting times of 500 ppl over 30 days in FoodCart1\ntime_diff = waiting_time_FoodCart2-waiting_time_FoodCart1\nprint(\"On \",(time_diff.mean()>0).sum(),\" days, the average waiting time at FoodCart2 higher than that at FoodCart1\")\nprint(\"Percentage of times waiting time at FoodCart2 was greater than that at FoodCart1 = \",100*(time_diff>0).sum()/15000,\"%\")\nend_time = tm.time()\nprint(\"Time taken = \", end_time-start_time)\n\nOn  0  days, the average waiting time at FoodCart2 higher than that at FoodCart1\nPercentage of times waiting time at FoodCart2 was greater than that at FoodCart1 =  16.52 %\nTime taken =  0.008000850677490234\n\n\nThe approach with NumPy is much faster than the one with loops.\n\n4.9.1 Practice exercise 3\nBootstrapping: Find the 95% confidence interval of mean profit for ‘Action’ movies, using Bootstrapping.\nBootstrapping is a non-parametric method for obtaining confidence interval. Use the algorithm below to find the confidence interval:\n\nFind the profit for each of the ‘Action’ movies. Suppose there are N such movies. We will have a Profit column with N values.\n\nRandomly sample N values with replacement from the Profit column\n\nFind the mean of the N values obtained in (b)\n\nRepeat steps (b) and (c) M=1000 times\n\nThe 95% Confidence interval is the range between the 2.5% and 97.5% percentile values of the 1000 means obtained in (c)\nUse the movies_cleaned.csv dataset.\n\nSolution:\n\n#Reading data\nmovies = pd.read_csv('./Datasets/movies_cleaned.csv')\n\n#Filtering action movies\nmovies_action = movies.loc[movies['Action']==1,:]\n\n#Computing profit of movies\nmovies_action.loc[:,'Profit'] = movies_action.loc[:,'Worldwide Gross'] - movies_action.loc[:,'Production Budget']\n\n#Subsetting the profit column\nprofit_vec = movies_action['Profit']\n\n#Creating a matrix of 1000 samples with replacement from the profit column\nbootstrap_samples=np.random.choice(profit_vec,size = (1000,len(profit_vec)))\n\n#Computing the mean of each of the 1000 samples\nbootstrap_sample_means = bootstrap_samples.mean(axis=1)\n\n#The confidence interval is the 2.5th and 97.5th percentile of the mean of the 1000 samples\nprint(\"Confidence interval = [$\"+str(np.round(np.percentile(bootstrap_sample_means,2.5)/1e6,2))+\" million, $\"+str(np.round(np.percentile(bootstrap_sample_means,97.5)/1e6,2))+\" million]\")\n\nConfidence interval = [$132.53 million, $182.69 million]"
  },
  {
    "objectID": "Pandas.html",
    "href": "Pandas.html",
    "title": "5  Pandas",
    "section": "",
    "text": "The Pandas library contains several methods and functions for cleaning, manipulating and analyzing data. While NumPy is suited for working with homogenous numerical array data, Pandas is designed for working with tabular or heterogenous data.\nPandas is built on top of the NumPy package. Thus, there are some similarities between the two libraries. Like NumPy, Pandas provides the basic mathematical functionalities like addition, subtraction, conditional operations and broadcasting. However, unlike NumPy library which provides objects for multi-dimensional arrays, Pandas provides the 2D table object called Dataframe.\nData in pandas is often used to feed statistical analysis in SciPy, plotting functions from Matplotlib, and machine learning algorithms in Scikit-learn.\nTypically, the Pandas library is used for:\n\nCleaning the data by tasks such as removing missing values, filtering rows / columns, aggregating data, mutating data, etc.\nComputing summary statistics such as the mean, median, max, min, standard deviation, etc.\nComputing correlation among columns in the data\nComputing the data distribution\nVisualizing the data with help from the Matplotlib library\nWriting the cleaned and transformed data into a CSV file or other database formats\n\nLet’s import the Pandas library to use its methods and functions.\n\nimport pandas as pd"
  },
  {
    "objectID": "Pandas.html#pandas-data-structures---series-and-dataframe",
    "href": "Pandas.html#pandas-data-structures---series-and-dataframe",
    "title": "5  Pandas",
    "section": "5.2 Pandas data structures - Series and DataFrame",
    "text": "5.2 Pandas data structures - Series and DataFrame\nThere are two core components of the Pandas library - Series and DataFrame.\nA DataFrame is a two-dimensional object - comprising of tabular data organized in rows and columns, where individual columns can be of different value types (numeric / string / boolean etc.). A DataFrame has row labels (also called row indices) which refer to individual rows, and column labels (also called column names) that refer to individual columns. By default, the row indices are integers starting from zero. However, both the row indices and column names can be customized by the user.\nLet us read the spotify data - spotify_data.csv, using the Pandas function read_csv().\n\nspotify_data = pd.read_csv('./Datasets/spotify_data.csv')\nspotify_data.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      0\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      6\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n    \n    \n      2\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      7\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n    \n    \n      3\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      2\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n    \n    \n      4\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      8\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\nThe object spotify_data is a pandas DataFrame:\n\ntype(spotify_data)\n\npandas.core.frame.DataFrame\n\n\nA Series is a one-dimensional object, containing a sequence of values, where each value has an index. Each column of a DataFrame is Series as shown in the example below.\n\n#Extracting movie titles from the movie_ratings DataFrame\nspotify_songs = spotify_data['track_name']\nspotify_songs\n\n0                           All Girls Are The Same\n1                                     Lucid Dreams\n2                                  Hear Me Calling\n3                                          Robbery\n4                                      Big Stepper\n                            ...                   \n243185                                    Stardust\n243186             Knockin' A Jug - 78 rpm Version\n243187            When It's Sleepy Time Down South\n243188    On The Sunny Side Of The Street - Part 2\n243189                                    My Sweet\nName: track_name, Length: 243190, dtype: object\n\n\n\n#The object movie_titles is a Series\ntype(spotify_songs)\n\npandas.core.series.Series\n\n\nA Series is essentially a column, and a DataFrame is a two-dimensional table made up of a collection of Series"
  },
  {
    "objectID": "Pandas.html#creating-a-pandas-series-dataframe",
    "href": "Pandas.html#creating-a-pandas-series-dataframe",
    "title": "5  Pandas",
    "section": "5.3 Creating a Pandas Series / DataFrame",
    "text": "5.3 Creating a Pandas Series / DataFrame\n\n5.3.1 Specifying data within the Series() / DataFrame() functions\nA Pandas Series and DataFrame can be created by specifying the data within the Series() / DataFrame() function. Below are examples of defining a Pandas Series / DataFrame.\n\n#Defining a Pandas Series\nseries_example = pd.Series(['these','are','english','words'])\nseries_example\n\n0      these\n1        are\n2    english\n3      words\ndtype: object\n\n\nNote that the default row indices are integers starting from 0. However, the index can be specified with the index argument if desired by the user:\n\n#Defining a Pandas Series with custom row labels\nseries_example = pd.Series(['these','are','english','words'], index = range(101,105))\nseries_example\n\n101      these\n102        are\n103    english\n104      words\ndtype: object\n\n\n\n\n5.3.2 Transforming in-built data structures\nA Pandas DataFrame can be created by converting the in-built python data structures such as lists, dictionaries, and list of dictionaries to DataFrame. See the examples below.\n\n#List consisting of expected age to marry of students of the STAT303-1 Fall 2022 class\nexp_marriage_age_list=['24','30','28','29','30','27','26','28','30+','26','28','30','30','30','probably never','30','25','25','30','28','30+ ','30','25','28','28','25','25','27','28','30','30','35','26','28','27','27','30','25','30','26','32','27','26','27','26','28','37','28','28','28','35','28','27','28','26','28','26','30','27','30','28','25','26','28','35','29','27','27','30','24','25','29','27','33','30','30','25','26','30','32','26','30','30','I wont','25','27','27','25','27','27','32','26','25','never','28','33','28','35','25','30','29','30','31','28','28','30','40','30','28','30','27','by 30','28','27','28','30-35','35','30','30','never','30','35','28','31','30','27','33','32','27','27','26','N/A','25','26','29','28','34','26','24','28','30','120','25','33','27','28','32','30','26','30','30','28','27','27','27','27','27','27','28','30','30','30','28','30','28','30','30','28','28','30','27','30','28','25','never','69','28','28','33','30','28','28','26','30','26','27','30','25','Never','27','27','25']\n\n\n#Example 1: Creating a Pandas Series from a list\nexp_marriage_age_series=pd.Series(exp_marriage_age_list,name = 'expected_marriage_age')\nexp_marriage_age_series.head()\n\n0    24\n1    30\n2    28\n3    29\n4    30\nName: expected_marriage_age, dtype: object\n\n\n\n#Dictionary consisting of the GDP per capita of the US from 1960 to 2021 with some missing values\nGDP_per_capita_dict = {'1960':3007,'1961':3067,'1962':3244,'1963':3375,'1964':3574,'1965':3828,'1966':4146,'1967':4336,'1968':4696,'1970':5234,'1971':5609,'1972':6094,'1973':6726,'1974':7226,'1975':7801,'1976':8592,'1978':10565,'1979':11674, '1980':12575,'1981':13976,'1982':14434,'1983':15544,'1984':17121,'1985':18237,  '1986':19071,'1987':20039,'1988':21417,'1989':22857,'1990':23889,'1991':24342,  '1992':25419,'1993':26387,'1994':27695,'1995':28691,'1996':29968,'1997':31459,  '1998':32854,'2000':36330,'2001':37134,'2002':37998,'2003':39490,'2004':41725,  '2005':44123,'2006':46302,'2007':48050,'2008':48570,'2009':47195,'2010':48651,  '2011':50066,'2012':51784,'2013':53291,'2015':56763,'2016':57867,'2017':59915,'2018':62805, '2019':65095,'2020':63028,'2021':69288}\n\n\n#Example 2: Creating a Pandas Series from a Dictionary\nGDP_per_capita_series = pd.Series(GDP_per_capita_dict)\nGDP_per_capita_series.head()\n\n1960    3007\n1961    3067\n1962    3244\n1963    3375\n1964    3574\ndtype: int64\n\n\n\n#List of dictionary consisting of 52 playing cards of the deck\ndeck_list_of_dictionaries = [{'value':i, 'suit':c}\nfor c in ['spades', 'clubs', 'hearts', 'diamonds']\nfor i in range(2,15)]\n\n\n#Example 3: Creating a Pandas DataFrame from a List of dictionaries\ndeck_df = pd.DataFrame(deck_list_of_dictionaries)\ndeck_df.head()\n\n\n\n\n\n  \n    \n      \n      value\n      suit\n    \n  \n  \n    \n      0\n      2\n      spades\n    \n    \n      1\n      3\n      spades\n    \n    \n      2\n      4\n      spades\n    \n    \n      3\n      5\n      spades\n    \n    \n      4\n      6\n      spades\n    \n  \n\n\n\n\n\n\n5.3.3 Importing data from files\nIn the real world, a Pandas DataFrame will typically be created by loading the datasets from existing storage such as SQL Database, CSV file, Excel file, text files, HTML files, etc., as we learned in the third chapter of the book on Reading data."
  },
  {
    "objectID": "Pandas.html#attributes-and-methods-of-a-pandas-dataframe",
    "href": "Pandas.html#attributes-and-methods-of-a-pandas-dataframe",
    "title": "5  Pandas",
    "section": "5.4 Attributes and Methods of a Pandas DataFrame",
    "text": "5.4 Attributes and Methods of a Pandas DataFrame\nAll attributes and methods of a Pandas DataFrame object can be viewed with the python’s built-in dir() function.\n\n#List of attributes and methods of a Pandas DataFrame\n#This code is not executed as the list is too long\ndir(spotify_data)\n\nAlthough we’ll see examples of attributes and methods of a Pandas DataFrame, please note that most of these attributes and methods are also applicable to the Pandas Series object.\n\n5.4.1 Attributes of a Pandas DataFrame\nSome of the attributes of the Pandas DataFrame class are the following.\n\n5.4.1.1 dtypes\nThis attribute is a Series consisting the datatypes of columns of a Pandas DataFrame.\n\nspotify_data.dtypes\n\nartist_followers       int64\ngenres                object\nartist_name           object\nartist_popularity      int64\ntrack_name            object\ntrack_popularity       int64\nduration_ms            int64\nexplicit               int64\nrelease_year           int64\ndanceability         float64\nenergy               float64\nkey                    int64\nloudness             float64\nmode                   int64\nspeechiness          float64\nacousticness         float64\ninstrumentalness     float64\nliveness             float64\nvalence              float64\ntempo                float64\ntime_signature         int64\ndtype: object\n\n\nThe table below describes the datatypes of columns in a Pandas DataFrame.\n\n\n\nPandas Type\nNative Python Type\nDescription\n\n\n\n\nobject\nstring\nThe most general dtype. This datatype is assigned to a column if the column has mixed types (numbers and strings)\n\n\nint64\nint\nThis datatype is for integers from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807 or for integers having a maximum size of 64 bits\n\n\nfloat64\nfloat\nThis datatype is for real numbers. If a column contains integers and NaNs, Pandas will default to float64. This is because the missing values may be a real number\n\n\ndatetime64, timedelta[ns]\nN/A (but see the datetime module in Python’s standard library)\nValues meant to hold time data. This datatype is useful for time series analysis\n\n\n\n\n\n5.4.1.2 columns\nThis attribute consists of the column labels (or column names) of a Pandas DataFrame.\n\nspotify_data.columns\n\nIndex(['artist_followers', 'genres', 'artist_name', 'artist_popularity',\n       'track_name', 'track_popularity', 'duration_ms', 'explicit',\n       'release_year', 'danceability', 'energy', 'key', 'loudness', 'mode',\n       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n       'valence', 'tempo', 'time_signature'],\n      dtype='object')\n\n\n\n\n5.4.1.3 index\nThis attribute consists of the row lables (or row indices) of a Pandas DataFrame.\n\nspotify_data.index\n\nRangeIndex(start=0, stop=243190, step=1)\n\n\n\n\n5.4.1.4 axes\nThis is a list of length two, where the first element is the row labels, and the second element is the columns labels. In other words, this attribute combines the information in the attributes - index and columns.\n\nspotify_data.axes\n\n[RangeIndex(start=0, stop=243190, step=1),\n Index(['artist_followers', 'genres', 'artist_name', 'artist_popularity',\n        'track_name', 'track_popularity', 'duration_ms', 'explicit',\n        'release_year', 'danceability', 'energy', 'key', 'loudness', 'mode',\n        'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n        'valence', 'tempo', 'time_signature'],\n       dtype='object')]\n\n\n\n\n5.4.1.5 ndim\nAs in NumPy, this attribute specifies the number of dimensions. However, unlike NumPy, a Pandas DataFrame has a fixed dimenstion of 2, and a Pandas Series has a fixed dimesion of 1.\n\nspotify_data.ndim\n\n2\n\n\n\n\n5.4.1.6 size\nThis attribute specifies the number of elements in a DataFrame. Its value is the product of the number of rows and columns.\n\nspotify_data.size\n\n5106990\n\n\n\n\n5.4.1.7 shape\nThis is a tuple consisting of the number of rows and columns in a Pandas DataFrame.\n\nspotify_data.shape\n\n(243190, 21)\n\n\n\n\n5.4.1.8 values\nThis provides a NumPy representation of a Pandas DataFrame.\n\nspotify_data.values\n\narray([[16996777, 'rap', 'Juice WRLD', ..., 0.203, 161.991, 4],\n       [16996777, 'rap', 'Juice WRLD', ..., 0.218, 83.903, 4],\n       [16996777, 'rap', 'Juice WRLD', ..., 0.499, 88.933, 4],\n       ...,\n       [2256652, 'jazz', 'Louis Armstrong', ..., 0.37, 105.093, 4],\n       [2256652, 'jazz', 'Louis Armstrong', ..., 0.576, 101.279, 4],\n       [2256652, 'jazz', 'Louis Armstrong', ..., 0.816, 105.84, 4]],\n      dtype=object)\n\n\n\n\n\n5.4.2 Methods of a Pandas DataFrame\nSome of the commonly used methods of the Pandas DataFrame class are the following.\n\n5.4.2.1 head()\nPrints the first n rows of a DataFrame.\n\nspotify_data.head(2)\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      0\n      -7.226\n      1\n      0.306\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      6\n      -7.230\n      0\n      0.200\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n    \n  \n\n2 rows × 21 columns\n\n\n\n\n\n5.4.2.2 tail()\nPrints the last n rows of a DataFrame.\n\nspotify_data.tail(3)\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      243187\n      2256652\n      jazz\n      Louis Armstrong\n      74\n      When It's Sleepy Time Down South\n      4\n      200200\n      0\n      1923\n      0.527\n      ...\n      3\n      -14.814\n      1\n      0.0793\n      0.989\n      0.00001\n      0.1040\n      0.370\n      105.093\n      4\n    \n    \n      243188\n      2256652\n      jazz\n      Louis Armstrong\n      74\n      On The Sunny Side Of The Street - Part 2\n      4\n      185973\n      0\n      1923\n      0.559\n      ...\n      0\n      -9.804\n      1\n      0.0512\n      0.989\n      0.84700\n      0.4480\n      0.576\n      101.279\n      4\n    \n    \n      243189\n      2256652\n      jazz\n      Louis Armstrong\n      74\n      My Sweet\n      4\n      195960\n      0\n      1923\n      0.741\n      ...\n      3\n      -10.406\n      1\n      0.0505\n      0.927\n      0.07880\n      0.0633\n      0.816\n      105.840\n      4\n    \n  \n\n3 rows × 21 columns\n\n\n\n\n\n5.4.2.3 describe()\nPrint summary statistics of a Pandas DataFrame, as seen in chapter 3 on Reading Data.\n\nspotify_data.describe()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      artist_popularity\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      count\n      2.431900e+05\n      243190.000000\n      243190.000000\n      2.431900e+05\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n    \n    \n      mean\n      1.960931e+06\n      65.342633\n      36.080772\n      2.263209e+05\n      0.050039\n      1992.475258\n      0.568357\n      0.580633\n      5.240326\n      -9.432548\n      0.670928\n      0.111984\n      0.383938\n      0.071169\n      0.223756\n      0.552302\n      119.335060\n      3.884177\n    \n    \n      std\n      5.028746e+06\n      10.289182\n      16.476836\n      9.973214e+04\n      0.218026\n      18.481463\n      0.159444\n      0.236631\n      3.532546\n      4.449731\n      0.469877\n      0.198068\n      0.321142\n      0.209555\n      0.198076\n      0.250017\n      29.864219\n      0.458082\n    \n    \n      min\n      2.300000e+01\n      51.000000\n      0.000000\n      3.344000e+03\n      0.000000\n      1923.000000\n      0.000000\n      0.000000\n      0.000000\n      -60.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      1.832620e+05\n      57.000000\n      25.000000\n      1.776670e+05\n      0.000000\n      1980.000000\n      0.462000\n      0.405000\n      2.000000\n      -11.990000\n      0.000000\n      0.033200\n      0.070000\n      0.000000\n      0.098100\n      0.353000\n      96.099250\n      4.000000\n    \n    \n      50%\n      5.352520e+05\n      64.000000\n      36.000000\n      2.188670e+05\n      0.000000\n      1994.000000\n      0.579000\n      0.591000\n      5.000000\n      -8.645000\n      1.000000\n      0.043100\n      0.325000\n      0.000011\n      0.141000\n      0.560000\n      118.002000\n      4.000000\n    \n    \n      75%\n      1.587332e+06\n      72.000000\n      48.000000\n      2.645465e+05\n      0.000000\n      2008.000000\n      0.685000\n      0.776000\n      8.000000\n      -6.131000\n      1.000000\n      0.075300\n      0.671000\n      0.002220\n      0.292000\n      0.760000\n      137.929000\n      4.000000\n    \n    \n      max\n      7.890023e+07\n      100.000000\n      99.000000\n      4.995083e+06\n      1.000000\n      2021.000000\n      0.988000\n      1.000000\n      11.000000\n      3.744000\n      1.000000\n      0.969000\n      0.996000\n      1.000000\n      1.000000\n      1.000000\n      243.507000\n      5.000000\n    \n  \n\n\n\n\n\n\n5.4.2.4 max()/min()\nReturns the max/min values of numeric columns. If the function is applied on non-numeric columns, it will return the maximum/minimum value based on the order of the alphabet.\n\n#The max() method applied on a Series\nspotify_data['artist_popularity'].max()\n\n100\n\n\n\n#The max() method applied on a DataFrame\nspotify_data.max()\n\nartist_followers                    78900234\ngenres                                  rock\nartist_name                          高爾宣 OSN\nartist_popularity                        100\ntrack_name           행복했던 날들이었다 days gone by\ntrack_popularity                          99\nduration_ms                          4995083\nexplicit                                   1\nrelease_year                            2021\ndanceability                           0.988\nenergy                                   1.0\nkey                                       11\nloudness                               3.744\nmode                                       1\nspeechiness                            0.969\nacousticness                           0.996\ninstrumentalness                         1.0\nliveness                                 1.0\nvalence                                  1.0\ntempo                                243.507\ntime_signature                             5\ndtype: object\n\n\n\n\n5.4.2.5 mean()/median()\nReturns the mean/median values of numeric columns.\n\nspotify_data.median()\n\nartist_followers     535252.000000\nartist_popularity        64.000000\ntrack_popularity         36.000000\nduration_ms          218867.000000\nexplicit                  0.000000\nrelease_year           1994.000000\ndanceability              0.579000\nenergy                    0.591000\nkey                       5.000000\nloudness                 -8.645000\nmode                      1.000000\nspeechiness               0.043100\nacousticness              0.325000\ninstrumentalness          0.000011\nliveness                  0.141000\nvalence                   0.560000\ntempo                   118.002000\ntime_signature            4.000000\ndtype: float64\n\n\n\n\n5.4.2.6 std()\nReturns the standard deviation of numeric columns.\n\nspotify_data.std()\n\nartist_followers     5.028746e+06\nartist_popularity    1.028918e+01\ntrack_popularity     1.647684e+01\nduration_ms          9.973214e+04\nexplicit             2.180260e-01\nrelease_year         1.848146e+01\ndanceability         1.594436e-01\nenergy               2.366309e-01\nkey                  3.532546e+00\nloudness             4.449731e+00\nmode                 4.698771e-01\nspeechiness          1.980684e-01\nacousticness         3.211417e-01\ninstrumentalness     2.095551e-01\nliveness             1.980759e-01\nvalence              2.500172e-01\ntempo                2.986422e+01\ntime_signature       4.580822e-01\ndtype: float64\n\n\n\n\n5.4.2.7 sample(n)\nReturns n random observations from a Pandas DataFrame.\n\nspotify_data.sample(4)\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      42809\n      385756\n      rock\n      Saxon\n      56\n      Never Surrender - 2009 Remastered Version\n      2\n      195933\n      0\n      2012\n      0.506\n      ...\n      6\n      -7.847\n      1\n      0.0633\n      0.0535\n      0.00001\n      0.3330\n      0.536\n      90.989\n      4\n    \n    \n      25730\n      810526\n      hip hop\n      Froid\n      68\n      Pseudosocial\n      54\n      135963\n      0\n      2016\n      0.644\n      ...\n      7\n      -9.098\n      0\n      0.3280\n      0.8270\n      0.00001\n      0.1630\n      0.886\n      117.170\n      4\n    \n    \n      147392\n      479209\n      jazz\n      Sarah Vaughan\n      59\n      Love Dance\n      14\n      204400\n      0\n      1982\n      0.386\n      ...\n      0\n      -23.819\n      1\n      0.0372\n      0.8970\n      0.00000\n      0.0943\n      0.102\n      110.981\n      3\n    \n    \n      233189\n      1201905\n      rock\n      Grateful Dead\n      72\n      Cold Rain and Snow - 2013 Remaster\n      29\n      151702\n      0\n      1967\n      0.412\n      ...\n      6\n      -10.476\n      0\n      0.0487\n      0.4090\n      0.58300\n      0.1630\n      0.875\n      168.803\n      4\n    \n  \n\n4 rows × 21 columns\n\n\n\n\n\n5.4.2.8 dropna()\nDrops all observations with at least one missing value.\n\n#This code is not executed to avoid prining a large table\nspotify_data.dropna()\n\n\n\n5.4.2.9 apply()\nThis method is used to apply a function over all columns or rows of a Pandas DataFrame. For example, let us find the range of values of artist_followers, artist_popularity and release_year.\n\n#Defining the function to compute range of values of a columns\ndef range_of_values(x):\n    return x.max()-x.min()\n\n#Applying the function to three coluumns for which we wish to find the range of values\nspotify_data[['artist_followers','artist_popularity','release_year']].apply(range_of_values, axis=0)\n\nartist_followers     78900211\nartist_popularity          49\nrelease_year               98\ndtype: int64\n\n\nThe apply() method is often used with the one line function known as lambda function in python. These functions do not require a name, and can be defined using the keyword lambda. The above block of code can be concisely written as:\n\nspotify_data[['artist_followers','artist_popularity','release_year']].apply(lambda x:x.max()-x.min(), axis=0)\n\nartist_followers     78900211\nartist_popularity          49\nrelease_year               98\ndtype: int64\n\n\nNote that the Series object also has an apply() method associated with it. The method can be used to apply a function to each value of a Series.\n\n\n5.4.2.10 map()\nThe function is used to map distinct values of a Pandas Series to another set of corresponding values.\nFor example, suppose we wish to create a new column in the spotify dataset which indicates the modality of the song - major (mode = 1) or minor (mode = 0). We’ll map the values of the mode column to the categories major and minor:\n\n#Creating a dictionary that maps the values 0 and 1 to minor and major respectively\nmap_mode = {0:'minor', 1:'major'}\n\n#The map() function requires a dictionary object, and maps the 'values' of the 'keys' in the dictionary\nspotify_data['modality'] = spotify_data['mode'].map(map_mode)\n\nWe can see the variable modality in the updated DataFrame.\n\nspotify_data.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n      modality\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n      major\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n      minor\n    \n    \n      2\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n      minor\n    \n    \n      3\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n      major\n    \n    \n      4\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n      major\n    \n  \n\n5 rows × 22 columns\n\n\n\n\n\n5.4.2.11 drop()\nThis function is used to drop rows/columns from a DataFrame.\nFor example, let us drop the columns mode from the spotify dataset:\n\n#Dropping the column 'mode'\nspotify_data_new = spotify_data.drop('mode',axis=1)\nspotify_data_new.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n      modality\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      0\n      -7.226\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n      major\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      6\n      -7.230\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n      minor\n    \n    \n      2\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      7\n      -3.997\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n      minor\n    \n    \n      3\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      2\n      -5.181\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n      major\n    \n    \n      4\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      8\n      -8.469\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n      major\n    \n  \n\n5 rows × 21 columns\n\n\n\nNote that if multiple columns or rows are to be dropped, they must be enclosed in box brackets.\n\n\n5.4.2.12 unique()\nThis functions provides the unique values of a Series. For example, let us find the number of unique genres of songs in the spotify dataset:\n\nspotify_data.genres.unique()\n\narray(['rap', 'pop', 'miscellaneous', 'metal', 'hip hop', 'rock',\n       'pop & rock', 'hoerspiel', 'folk', 'electronic', 'jazz', 'country',\n       'latin'], dtype=object)\n\n\n\n\n5.4.2.13 value_counts()\nThis function provides the number of observations of each value of a Series. For example, let us find the number of songs of each genre in the spotify dataset:\n\nspotify_data.genres.value_counts()\n\npop              70441\nrock             49785\npop & rock       43437\nmiscellaneous    35848\njazz             13363\nhoerspiel        12514\nhip hop           7373\nfolk              2821\nlatin             2125\nrap               1798\nmetal             1659\ncountry           1236\nelectronic         790\nName: genres, dtype: int64\n\n\nMore than half the songs in the dataset are pop, rock or pop & rock.\n\n\n5.4.2.14 isin()\nThis function provides a boolean Series indicating the position of certain values in a Series. The function is helpful in sub-setting data. For example, let us subset the songs that are either latin, rap, or metal:\n\nlatin_rap_metal_songs = spotify_data.loc[spotify_data.genres.isin(['latin','rap','metal']),:]\nlatin_rap_metal_songs.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      0\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      6\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n    \n    \n      2\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      7\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n    \n    \n      3\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      2\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n    \n    \n      4\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      8\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n    \n  \n\n5 rows × 21 columns"
  },
  {
    "objectID": "Pandas.html#data-manipulations-with-pandas",
    "href": "Pandas.html#data-manipulations-with-pandas",
    "title": "5  Pandas",
    "section": "5.5 Data manipulations with Pandas",
    "text": "5.5 Data manipulations with Pandas\n\n5.5.1 Sub-setting data\n\n5.5.1.1 loc and iloc with the original row / column index\nSubsetting observations: In the chapter on reading data, we learned about operators loc and iloc that can be used to subset data based on axis labels and position of rows/columns respectively. However, usually we are not aware of the relevant row indices, and we may want to subset data based on some condition(s). For example, suppose we wish to analyze only those songs whose track popularity is higher than 50.\nQ: Do we need to subset rows or columns in this case?\nA: Rows, as songs correspond to rows, while features of songs correspond to columns.\nAs we need to subset rows, the filter must be applied at the starting index, i.e., the index before the ,. As we don’t need to subset any specific features of the songs, there is no subsetting to be done on the columns. A : at the ending index means that all columns need to selected.\n\n#Subsetting spotify songs that have track popularity score of more than 50\npopular_songs = spotify_data.loc[spotify_data.track_popularity>=50,:]\npopular_songs.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      181\n      1277325\n      hip hop\n      Dave\n      77\n      Titanium\n      69\n      127750\n      1\n      2021\n      0.959\n      ...\n      0\n      -8.687\n      0\n      0.4370\n      0.152000\n      0.000001\n      0.1050\n      0.510\n      121.008\n      4\n    \n    \n      191\n      1123869\n      rap\n      Jay Wheeler\n      85\n      Viendo el Techo\n      64\n      188955\n      0\n      2021\n      0.741\n      ...\n      11\n      -6.029\n      0\n      0.2290\n      0.306000\n      0.000327\n      0.1000\n      0.265\n      179.972\n      4\n    \n    \n      208\n      3657199\n      rap\n      Polo G\n      91\n      RAPSTAR\n      89\n      165926\n      1\n      2021\n      0.789\n      ...\n      6\n      -6.862\n      1\n      0.2420\n      0.410000\n      0.000000\n      0.1290\n      0.437\n      81.039\n      4\n    \n    \n      263\n      1461700\n      pop & rock\n      Teoman\n      67\n      Gecenin Sonuna Yolculuk\n      52\n      280600\n      0\n      2021\n      0.686\n      ...\n      11\n      -7.457\n      0\n      0.0268\n      0.119000\n      0.000386\n      0.1080\n      0.560\n      100.932\n      4\n    \n    \n      293\n      299746\n      pop & rock\n      Lars Winnerbäck\n      62\n      Själ och hjärta\n      55\n      271675\n      0\n      2021\n      0.492\n      ...\n      2\n      -6.005\n      0\n      0.0349\n      0.000735\n      0.000207\n      0.0953\n      0.603\n      142.042\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\nSubsetting columns: Suppose we wish to analyze only track_name, release year and track_popularity of songs. Then, we can subset the revelant columns:\n\nrelevant_columns = spotify_data.loc[:,['track_name','release_year','track_popularity']]\nrelevant_columns.head()\n\n\n\n\n\n  \n    \n      \n      track_name\n      release_year\n      track_popularity\n    \n  \n  \n    \n      0\n      All Girls Are The Same\n      2021\n      0\n    \n    \n      1\n      Lucid Dreams\n      2021\n      0\n    \n    \n      2\n      Hear Me Calling\n      2021\n      0\n    \n    \n      3\n      Robbery\n      2021\n      0\n    \n    \n      4\n      Big Stepper\n      2021\n      0\n    \n  \n\n\n\n\nNote that when multiple columns are subset with loc they are enclosed in a box bracket, unlike the case with a single column. Similarly if multiple observations are selected using the row labels, the row labels must be enclosed in box brackets.\n\n\n5.5.1.2 Re-indexing rows followed by loc / iloc\nSuppose we wish to subset data based on the genres. If we want to subset hiphop songs, we may subset as:\n\n#Subsetting hiphop songs\nhiphop_songs = spotify_data.loc[spotify_data['genres']=='hip hop',:]\nhiphop_songs.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      64\n      6485079\n      hip hop\n      DaBaby\n      93\n      FIND MY WAY\n      0\n      139890\n      1\n      2021\n      0.836\n      ...\n      4\n      -6.750\n      0\n      0.1840\n      0.1870\n      0.00000\n      0.1380\n      0.7000\n      103.000\n      4\n    \n    \n      80\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      Hula Hoop\n      0\n      236493\n      0\n      2021\n      0.799\n      ...\n      6\n      -4.628\n      1\n      0.0801\n      0.1130\n      0.00315\n      0.0942\n      0.9510\n      175.998\n      4\n    \n    \n      81\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      Gasolina - Live\n      0\n      306720\n      0\n      2021\n      0.669\n      ...\n      1\n      -4.251\n      1\n      0.2700\n      0.1530\n      0.00000\n      0.1540\n      0.0814\n      96.007\n      4\n    \n    \n      87\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      La Nueva Y La Ex\n      0\n      197053\n      0\n      2021\n      0.639\n      ...\n      5\n      -3.542\n      1\n      0.1360\n      0.0462\n      0.00000\n      0.1410\n      0.6390\n      198.051\n      4\n    \n    \n      88\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      Que Tire Pa Lante\n      0\n      210520\n      0\n      2021\n      0.659\n      ...\n      7\n      -2.814\n      1\n      0.0358\n      0.0478\n      0.00000\n      0.1480\n      0.7040\n      93.979\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\nHowever, if we need to subset data by genres frequently in our analysis, and we don’t need the current row labels, we may replace the row labels as genres to shorten the code for filtering the observations based on genres.\nWe use the set_index() function to re-index the rows based on existing column(s) of the DataFrame.\n\n#Defining row labels as the values of the column `genres`\nspotify_data_reindexed = spotify_data.set_index(keys=spotify_data['genres'])\nspotify_data_reindexed.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n    \n      genres\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      rap\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      0\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n    \n    \n      rap\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      6\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n    \n    \n      rap\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      7\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n    \n    \n      rap\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      2\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n    \n    \n      rap\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      8\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\nNow, we can subset hiphop songs using the row label of the data:\n\n#Subsetting hiphop songs using row labels\nhiphop_songs = spotify_data_reindexed.loc['hip hop',:]\nhiphop_songs.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n    \n      genres\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      hip hop\n      6485079\n      hip hop\n      DaBaby\n      93\n      FIND MY WAY\n      0\n      139890\n      1\n      2021\n      0.836\n      ...\n      4\n      -6.750\n      0\n      0.1840\n      0.1870\n      0.00000\n      0.1380\n      0.7000\n      103.000\n      4\n    \n    \n      hip hop\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      Hula Hoop\n      0\n      236493\n      0\n      2021\n      0.799\n      ...\n      6\n      -4.628\n      1\n      0.0801\n      0.1130\n      0.00315\n      0.0942\n      0.9510\n      175.998\n      4\n    \n    \n      hip hop\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      Gasolina - Live\n      0\n      306720\n      0\n      2021\n      0.669\n      ...\n      1\n      -4.251\n      1\n      0.2700\n      0.1530\n      0.00000\n      0.1540\n      0.0814\n      96.007\n      4\n    \n    \n      hip hop\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      La Nueva Y La Ex\n      0\n      197053\n      0\n      2021\n      0.639\n      ...\n      5\n      -3.542\n      1\n      0.1360\n      0.0462\n      0.00000\n      0.1410\n      0.6390\n      198.051\n      4\n    \n    \n      hip hop\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      Que Tire Pa Lante\n      0\n      210520\n      0\n      2021\n      0.659\n      ...\n      7\n      -2.814\n      1\n      0.0358\n      0.0478\n      0.00000\n      0.1480\n      0.7040\n      93.979\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\n\n\n\n5.5.2 Sorting data\nSorting dataset is a very common operation. The sort_values() function of Pandas can be used to sort a Pandas DataFrame or Series. Let us sort the spotify data in decreasing order of track_popularity:\n\nspotify_sorted = spotify_data.sort_values(by = 'track_popularity', ascending = False)\nspotify_sorted.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      2398\n      1444702\n      pop\n      Olivia Rodrigo\n      88\n      drivers license\n      99\n      242014\n      1\n      2021\n      0.585\n      ...\n      10\n      -8.761\n      1\n      0.0601\n      0.72100\n      0.000013\n      0.1050\n      0.132\n      143.874\n      4\n    \n    \n      2442\n      177401\n      hip hop\n      Masked Wolf\n      85\n      Astronaut In The Ocean\n      98\n      132780\n      0\n      2021\n      0.778\n      ...\n      4\n      -6.865\n      0\n      0.0913\n      0.17500\n      0.000000\n      0.1500\n      0.472\n      149.996\n      4\n    \n    \n      3133\n      1698014\n      pop\n      Kali Uchis\n      88\n      telepatía\n      97\n      160191\n      0\n      2020\n      0.653\n      ...\n      11\n      -9.016\n      0\n      0.0502\n      0.11200\n      0.000000\n      0.2030\n      0.553\n      83.970\n      4\n    \n    \n      6702\n      31308207\n      pop\n      The Weeknd\n      96\n      Save Your Tears\n      97\n      215627\n      1\n      2020\n      0.680\n      ...\n      0\n      -5.487\n      1\n      0.0309\n      0.02120\n      0.000012\n      0.5430\n      0.644\n      118.051\n      4\n    \n    \n      6703\n      31308207\n      pop\n      The Weeknd\n      96\n      Blinding Lights\n      96\n      200040\n      0\n      2020\n      0.514\n      ...\n      1\n      -5.934\n      1\n      0.0598\n      0.00146\n      0.000095\n      0.0897\n      0.334\n      171.005\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\nDrivers license is the most popular song!\n\n\n\n\n\n \n        \n\n\n\n\n5.5.3 Ranking data\nWith the rank() function, we can rank the observations.\nFor example, let us add a new column to the spotify data that provides the rank of the track_popularity column:\n\nspotify_ranked = spotify_data.copy()\nspotify_ranked['track_popularity_rank']=spotify_sorted['track_popularity'].rank()\nspotify_ranked.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n      track_popularity_rank\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n      963.5\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n      963.5\n    \n    \n      2\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n      963.5\n    \n    \n      3\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n      963.5\n    \n    \n      4\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n      963.5\n    \n  \n\n5 rows × 22 columns\n\n\n\nNote the column track_popularity_rank. Why does it contain floating point numbers? Check the rank() documentation to find out!\n\n\n5.5.4 Practice exercise 1\n\n5.5.4.1 \nRead the file STAT303-1 survey for data analysis.csv.\n\nsurvey_data = pd.read_csv('./Datasets/STAT303-1 survey for data analysis.csv')\n\n\n\n5.5.4.2 \nHow many observations and variables are there in the data?\n\nprint(\"The data has \",survey_data.shape[0],\"observations, and\", survey_data.shape[1], \"columns\")\n\nThe data has  192 observations, and 51 columns\n\n\n\n\n5.5.4.3 \nRename all the columns of the data, except the first two columns, with the shorter names in the list new_col_names given below. The order of column names in the list is the same as the order in which the columns are to be renamed starting with the third column from the left.\n\nnew_col_names = ['parties_per_month', 'do_you_smoke', 'weed', 'are_you_an_introvert_or_extrovert', 'love_first_sight', 'learning_style', 'left_right_brained', 'personality_type', 'social_media', 'num_insta_followers', 'streaming_platforms', 'expected_marriage_age', 'expected_starting_salary', 'fav_sport', 'minutes_ex_per_week', 'sleep_hours_per_day', 'how_happy', 'farthest_distance_travelled', 'fav_number', 'fav_letter', 'internet_hours_per_day', 'only_child', 'birthdate_odd_even', 'birth_month', 'fav_season', 'living_location_on_campus', 'major', 'num_majors_minors', 'high_school_GPA', 'NU_GPA', 'age', 'height', 'height_father', 'height_mother', 'school_year', 'procrastinator', 'num_clubs', 'student_athlete', 'AP_stats', 'used_python_before', 'dominant_hand', 'childhood_in_US', 'gender', 'region_of_residence', 'political_affliation', 'cant_change_math_ability', 'can_change_math_ability', 'math_is_genetic', 'much_effort_is_lack_of_talent']\n\n\nsurvey_data.columns = list(survey_data.columns[0:2])+new_col_names\n\n\n\n5.5.4.4 \nRename the following columns again:\n\nRename do_you_smoke to smoke.\nRename are_you_an_introvert_or_extrovert to introvert_extrovert.\n\nHint: Use the function rename()\n\nsurvey_data.rename(columns={'do_you_smoke':'smoke','are_you_an_introvert_or_extrovert':'introvert_extrovert'},inplace=True)\n\n\n\n5.5.4.5 \nFind the proportion of people going to more than 4 parties per month. Use the variable parties_per_month.\n\nsurvey_data['parties_per_month']=pd.to_numeric(survey_data.parties_per_month,errors='coerce')\nsurvey_data.loc[survey_data['parties_per_month']>4,:].shape[0]/survey_data.shape[0]\n\n0.3385416666666667\n\n\n\n\n5.5.4.6 \nAmong the people who go to more than 4 parties a month, what proportion of them are introverts?\n\nsurvey_data.loc[((survey_data['parties_per_month']>4) & (survey_data.introvert_extrovert=='Introvert')),:].shape[0]/survey_data.loc[survey_data['parties_per_month']>4,:].shape[0]\n\n0.5076923076923077\n\n\n\n\n5.5.4.7 \nFind the proportion of people in each category of the variable how_happy.\n\nsurvey_data.how_happy.value_counts()/survey_data.shape[0]\n\nPretty happy     0.703125\nVery happy       0.151042\nNot too happy    0.088542\nDon't know       0.057292\nName: how_happy, dtype: float64\n\n\n\n\n5.5.4.8 \nAmong the people who go to more than 4 parties a month, what proportion of them are either Pretty happy or Very happy?\n\nsurvey_data.loc[((survey_data['parties_per_month']>4) & (survey_data.how_happy.isin(['Pretty happy','Very happy'])))].shape[0]/survey_data.loc[survey_data['parties_per_month']>4,:].shape[0]\n\n0.9076923076923077\n\n\n\n\n5.5.4.9 \nExamine the column num_insta_followers. Some numbers in the column contain a comma(,) or a tilde(~). Remove both these characters from the numbers in the column.\nHint: You may use the function str.replace() of the Pandas Series class.\n\nsurvey_data_insta = survey_data.copy()\nsurvey_data_insta['num_insta_followers']=survey_data_insta['num_insta_followers'].str.replace(',','')\nsurvey_data_insta['num_insta_followers']=survey_data_insta['num_insta_followers'].str.replace('~','')\n\n\n\n5.5.4.10 \nConvert the column num_insta_followers to numeric. Coerce the errors.\n\nsurvey_data_insta.num_insta_followers = pd.to_numeric(survey_data_insta.num_insta_followers,errors='coerce')\n\n\n\n5.5.4.11 \nDrop the observations consisting of missing values for num_insta_followers. Report the number of observations dropped.\n\nsurvey_data.num_insta_followers.isna().sum()\n\n3\n\n\nThere are 3 missing values of num_insta_followers.\n\n#Dropping observations with missing values of num_insta_followers\nsurvey_data=survey_data[~survey_data.num_insta_followers.isna()]\n\n\n\n5.5.4.12 \nWhat is the mean internet_hours_per_day for the top 46 people in terms of number of instagram followers?\n\nsurvey_data_insta.sort_values(by = 'num_insta_followers',ascending=False, inplace=True)\ntop_insta = survey_data_insta.iloc[:46,:]\ntop_insta.internet_hours_per_day = pd.to_numeric(top_insta.internet_hours_per_day,errors = 'coerce')\ntop_insta.internet_hours_per_day.mean()\n\n5.088888888888889\n\n\n\n\n5.5.4.13 \nWhat is the mean internet_hours_per_day for the remaining people?\n\nlow_insta = survey_data_insta.iloc[46:,:]\nlow_insta.internet_hours_per_day = pd.to_numeric(low_insta.internet_hours_per_day,errors = 'coerce')\nlow_insta.internet_hours_per_day.mean()\n\n13.118881118881118"
  },
  {
    "objectID": "Pandas.html#arithematic-operations",
    "href": "Pandas.html#arithematic-operations",
    "title": "5  Pandas",
    "section": "5.6 Arithematic operations",
    "text": "5.6 Arithematic operations\n\n5.6.1 Arithematic operations between DataFrames\nLet us create two toy DataFrames:\n\n#Creating two toy DataFrames\ntoy_df1 = pd.DataFrame([(1,2),(3,4),(5,6)], columns=['a','b'])\ntoy_df2 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'])\n\n\n#DataFrame 1\ntoy_df1\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      1\n      2\n    \n    \n      1\n      3\n      4\n    \n    \n      2\n      5\n      6\n    \n  \n\n\n\n\n\n#DataFrame 2\ntoy_df2\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      100\n      200\n    \n    \n      1\n      300\n      400\n    \n    \n      2\n      500\n      600\n    \n  \n\n\n\n\nElement by element operations between two DataFrames can be performed with the operators +, -, *,/,**, and %. Below is an example of element-by-element addition of two DataFrames:\n\n# Element-by-element arithmetic addition of the two DataFrames\ntoy_df1 + toy_df2\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      101\n      202\n    \n    \n      1\n      303\n      404\n    \n    \n      2\n      505\n      606\n    \n  \n\n\n\n\nNote that these operations create problems when the row indices and/or column names of the two DataFrames do not match. See the example below:\n\n#Creating another toy example of a DataFrame\ntoy_df3 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'], index=[1,2,3])\ntoy_df3\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      1\n      100\n      200\n    \n    \n      2\n      300\n      400\n    \n    \n      3\n      500\n      600\n    \n  \n\n\n\n\n\n#Adding DataFrames with some unmatching row indices\ntoy_df1 + toy_df3\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n    \n    \n      1\n      103.0\n      204.0\n    \n    \n      2\n      305.0\n      406.0\n    \n    \n      3\n      NaN\n      NaN\n    \n  \n\n\n\n\nNote that the rows whose indices match between the two DataFrames are added up. The rest of the values are missing (or NaN) because only one of the DataFrames has that index.\nAs in the case of row indices, missing values will also appear in the case of unmatching column names, as shown in the example below.\n\ntoy_df4 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['b','c'])\ntoy_df4\n\n\n\n\n\n  \n    \n      \n      b\n      c\n    \n  \n  \n    \n      0\n      100\n      200\n    \n    \n      1\n      300\n      400\n    \n    \n      2\n      500\n      600\n    \n  \n\n\n\n\n\n#Adding DataFrames with some unmatching column names\ntoy_df1 + toy_df4\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      NaN\n      102\n      NaN\n    \n    \n      1\n      NaN\n      304\n      NaN\n    \n    \n      2\n      NaN\n      506\n      NaN\n    \n  \n\n\n\n\n\n\n5.6.2 Arithematic operations between a Series and a DataFrame\nBroadcasting: As in NumPy, we can broadcast a Series to match the shape of another DataFrame:\n\n# Broadcasting: The row [1,2] (a Series) is added on every row in df2 \ntoy_df1.iloc[0,:] + toy_df2\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      101\n      202\n    \n    \n      1\n      301\n      402\n    \n    \n      2\n      501\n      602\n    \n  \n\n\n\n\nNote that the + operator is used to add values of a Series to a DataFrame based on column names. For adding a Series to a DataFrame based on row indices, we cannot use the + operator. Instead, we’ll need to use the add() function as explained below.\nBroadcasting based on row/column labels: We can use the add() function to broadcast a Series to a DataFrame. By default the Series adds based on column names, as in the case of the + operator.\n\n# Add the first row of df1 (a Series) to every row in df2 \ntoy_df2.add(toy_df1.iloc[0,:])\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      101\n      202\n    \n    \n      1\n      301\n      402\n    \n    \n      2\n      501\n      602\n    \n  \n\n\n\n\nFor broadcasting based on row indices, we use the axis argument of the add() function.\n\n# The second column of df1 (a Series) is added to every col in df2\ntoy_df2.add(toy_df1.iloc[:,1],axis='index')\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      102\n      202\n    \n    \n      1\n      304\n      404\n    \n    \n      2\n      506\n      606\n    \n  \n\n\n\n\n\n\n5.6.3 Case study\nTo see the application of arithematic operations on DataFrames, let us see the case study below.\nSong recommendation: Spotify recommends songs based on songs listened by the user. Suppose you have listened to the song drivers license. Spotify intends to recommend you 5 songs that are similar to drivers license. Which songs should it recommend?\nLet us see the available song information that can help us identify songs similar to drivers license. The columns attribute of DataFrame will display all the columns names. The description of some of the column names relating to audio features is here.\n\nspotify_data.columns\n\nIndex(['artist_followers', 'genres', 'artist_name', 'artist_popularity',\n       'track_name', 'track_popularity', 'duration_ms', 'explicit',\n       'release_year', 'danceability', 'energy', 'key', 'loudness', 'mode',\n       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n       'valence', 'tempo', 'time_signature'],\n      dtype='object')\n\n\nSolution approach: We have several features of a song. Let us find songs similar to drivers license in terms of danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, time_signature and tempo. Note that we are considering only audio features for simplicity.\nTo find the songs most similar to drivers license, we need to define a measure that quantifies the similarity. Let us define similarity of a song with drivers license as the Euclidean distance of the song from drivers license, where the coordinates of a song are: (danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, time_signature, tempo). Thus, similarity can be formulated as:\n\\[Similarity_{DL-S} = \\sqrt{(danceability_{DL}-danceability_{S})^2+(energy_{DL}-energy_{S})^2 +...+ (tempo_{DL}-tempo_{S})^2)},\\]\nwhere the subscript DL stands for drivers license and S stands for any song. The top 5 songs with the least value of \\(Similarity_{DL-S}\\) will be the most similar to drivers lincense and should be recommended.\nLet us subset the columns that we need to use to compute the Euclidean distance.\n\naudio_features = spotify_data[['danceability', 'energy', 'key', 'loudness','mode','speechiness',\n                               'acousticness', 'instrumentalness', 'liveness','valence', 'tempo', 'time_signature']]\n\n\naudio_features.head()\n\n\n\n\n\n  \n    \n      \n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      0.673\n      0.529\n      0\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n    \n    \n      1\n      0.511\n      0.566\n      6\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n    \n    \n      2\n      0.699\n      0.687\n      7\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n    \n    \n      3\n      0.708\n      0.690\n      2\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n    \n    \n      4\n      0.753\n      0.597\n      8\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n    \n  \n\n\n\n\n\n#Distribution of values of audio_features\naudio_features.describe()\n\n\n\n\n\n  \n    \n      \n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      count\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n    \n    \n      mean\n      0.568357\n      0.580633\n      5.240326\n      -9.432548\n      0.670928\n      0.111984\n      0.383938\n      0.071169\n      0.223756\n      0.552302\n      119.335060\n      3.884177\n    \n    \n      std\n      0.159444\n      0.236631\n      3.532546\n      4.449731\n      0.469877\n      0.198068\n      0.321142\n      0.209555\n      0.198076\n      0.250017\n      29.864219\n      0.458082\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      -60.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.462000\n      0.405000\n      2.000000\n      -11.990000\n      0.000000\n      0.033200\n      0.070000\n      0.000000\n      0.098100\n      0.353000\n      96.099250\n      4.000000\n    \n    \n      50%\n      0.579000\n      0.591000\n      5.000000\n      -8.645000\n      1.000000\n      0.043100\n      0.325000\n      0.000011\n      0.141000\n      0.560000\n      118.002000\n      4.000000\n    \n    \n      75%\n      0.685000\n      0.776000\n      8.000000\n      -6.131000\n      1.000000\n      0.075300\n      0.671000\n      0.002220\n      0.292000\n      0.760000\n      137.929000\n      4.000000\n    \n    \n      max\n      0.988000\n      1.000000\n      11.000000\n      3.744000\n      1.000000\n      0.969000\n      0.996000\n      1.000000\n      1.000000\n      1.000000\n      243.507000\n      5.000000\n    \n  \n\n\n\n\nNote that the audio features differ in terms of scale. Some features like key have a wide range of [0,11], while others like danceability have a very narrow range of [0,0.988]. If we use them directly, features like danceability will have a much higher influence on \\(Similarity_{DL-S}\\) as compared to features like key. Assuming we wish all the features to have equal weight in quantifying a song’s similarity to drivers license, we should scale the features, so that their values are comparable.\nLet us scale the value of each column to a standard uniform distribution: \\(U[0,1]\\).\nFor scaling the values of a column to \\(U[0,1]\\), we need to subtract the minimum value of the column from each value, and divide by the range of values of the column. For example, danceability can be standardized as follows:\n\n#Scaling danceability to U[0,1]\ndanceability_value_range = audio_features.danceability.max()-audio_features.danceability.min()\ndanceability_std = (audio_features.danceability-audio_features.danceability.min())/danceability_value_range\ndanceability_std\n\n0         0.681174\n1         0.517206\n2         0.707490\n3         0.716599\n4         0.762146\n            ...   \n243185    0.621457\n243186    0.797571\n243187    0.533401\n243188    0.565789\n243189    0.750000\nName: danceability, Length: 243190, dtype: float64\n\n\nHowever, it will be cumbersome to repeat the above code for each audio feature. We can instead write a function that scales values of a column to \\(U[0,1]\\), and apply the function on all the audio features.\n\n#Function to scale a column to U[0,1]\ndef scale_uniform(x):\n    return (x-x.min())/(x.max()-x.min())\n\nWe will use the Pandas function apply() to apply the above function to the DataFrame audio_features.\n\n#Scaling all audio features to U[0,1]\naudio_features_scaled = audio_features.apply(scale_uniform)\n\nThe above two blocks of code can be concisely written with the lambda function as:\n\naudio_features_scaled = audio_features.apply(lambda x: (x-x.min())/(x.max()-x.min()))\n\n\n#All the audio features are scaled to U[0,1]\naudio_features_scaled.describe()\n\n\n\n\n\n  \n    \n      \n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      count\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n    \n    \n      mean\n      0.575260\n      0.580633\n      0.476393\n      0.793290\n      0.670928\n      0.115566\n      0.385480\n      0.071169\n      0.223756\n      0.552302\n      0.490068\n      0.776835\n    \n    \n      std\n      0.161380\n      0.236631\n      0.321141\n      0.069806\n      0.469877\n      0.204405\n      0.322431\n      0.209555\n      0.198076\n      0.250017\n      0.122642\n      0.091616\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.467611\n      0.405000\n      0.181818\n      0.753169\n      0.000000\n      0.034262\n      0.070281\n      0.000000\n      0.098100\n      0.353000\n      0.394647\n      0.800000\n    \n    \n      50%\n      0.586032\n      0.591000\n      0.454545\n      0.805644\n      1.000000\n      0.044479\n      0.326305\n      0.000011\n      0.141000\n      0.560000\n      0.484594\n      0.800000\n    \n    \n      75%\n      0.693320\n      0.776000\n      0.727273\n      0.845083\n      1.000000\n      0.077709\n      0.673695\n      0.002220\n      0.292000\n      0.760000\n      0.566427\n      0.800000\n    \n    \n      max\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n    \n  \n\n\n\n\nSince we need to find the Euclidean distance from the song drivers license, let us find the index of the row containing features of drivers license.\n\n#Index of the row consisting of drivers license can be found with the index attribute\ndrivers_license_index = spotify_data[spotify_data.track_name=='drivers license'].index[0]\n\nNote that the object returned by the index attribute is of type pandas.core.indexes.numeric.Int64Index. The elements of this object can be retrieved like the elements of a python list. That is why the object is sliced with [0] to return the first element of the object. As there is only one observation with the track_name as drivers license, we sliced the first element. If there were multiple observations with track_name as drivers license, we will obtain the indices of all those observations with the index attribute.\nNow, we’ll subtract the audio features of drivers license from all other songs:\n\n#Audio features of drivers license are being subtracted from audio features of all songs by broadcasting\nsongs_minus_DL = audio_features_scaled-audio_features_scaled.loc[drivers_license_index,:]\n\nNow, let us square the difference computed above. We’ll use the in-built python function pow() to square the difference:\n\nsongs_minus_DL_sq = songs_minus_DL.pow(2)\nsongs_minus_DL_sq.head()\n\n\n\n\n\n  \n    \n      \n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      0.007933\n      0.008649\n      0.826446\n      0.000580\n      0.0\n      0.064398\n      0.418204\n      1.055600e-07\n      0.000376\n      0.005041\n      0.005535\n      0.0\n    \n    \n      1\n      0.005610\n      0.016900\n      0.132231\n      0.000577\n      1.0\n      0.020844\n      0.139498\n      1.716100e-10\n      0.055225\n      0.007396\n      0.060654\n      0.0\n    \n    \n      2\n      0.013314\n      0.063001\n      0.074380\n      0.005586\n      1.0\n      0.002244\n      0.171942\n      5.382400e-10\n      0.000256\n      0.134689\n      0.050906\n      0.0\n    \n    \n      3\n      0.015499\n      0.064516\n      0.528926\n      0.003154\n      0.0\n      0.000269\n      0.140249\n      1.716100e-10\n      0.013689\n      0.168921\n      0.068821\n      0.0\n    \n    \n      4\n      0.028914\n      0.025921\n      0.033058\n      0.000021\n      0.0\n      0.057274\n      0.456981\n      1.716100e-10\n      0.008464\n      0.234256\n      0.075428\n      0.0\n    \n  \n\n\n\n\nNow, we’ll sum the squares of differences from all audio features to compute the similarity of all songs to drivers license.\n\ndistance_squared = songs_minus_DL_sq.sum(axis = 1)\ndistance_squared.head()\n\n0    1.337163\n1    1.438935\n2    1.516317\n3    1.004043\n4    0.920316\ndtype: float64\n\n\nNow, we’ll sort these distances to find the top 5 songs closest to drivers’s license.\n\ndistances_sorted = distance_squared.sort_values()\ndistances_sorted.head()\n\n2398      0.000000\n81844     0.008633\n4397      0.011160\n130789    0.015018\n143744    0.015058\ndtype: float64\n\n\nUsing the indices of the top 5 distances, we will identify the top 5 songs most similar to drivers license:\n\nspotify_data.loc[distances_sorted.index[0:6],:]\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      2398\n      1444702\n      pop\n      Olivia Rodrigo\n      88\n      drivers license\n      99\n      242014\n      1\n      2021\n      0.585\n      ...\n      10\n      -8.761\n      1\n      0.0601\n      0.721\n      0.000013\n      0.105\n      0.132\n      143.874\n      4\n    \n    \n      81844\n      2264501\n      pop\n      Jay Chou\n      74\n      安靜\n      49\n      334240\n      0\n      2001\n      0.513\n      ...\n      10\n      -7.853\n      1\n      0.0281\n      0.688\n      0.000008\n      0.116\n      0.123\n      143.924\n      4\n    \n    \n      4397\n      25457\n      pop\n      Terence Lam\n      60\n      拼命無恙 in Bb major\n      52\n      241062\n      0\n      2020\n      0.532\n      ...\n      10\n      -9.690\n      1\n      0.0269\n      0.674\n      0.000000\n      0.117\n      0.190\n      151.996\n      4\n    \n    \n      130789\n      176266\n      pop\n      Alan Tam\n      54\n      從後趕上\n      8\n      258427\n      0\n      1988\n      0.584\n      ...\n      10\n      -11.889\n      1\n      0.0282\n      0.707\n      0.000002\n      0.107\n      0.124\n      140.147\n      4\n    \n    \n      143744\n      396326\n      pop & rock\n      Laura Branigan\n      64\n      How Am I Supposed to Live Without You\n      40\n      263320\n      0\n      1983\n      0.559\n      ...\n      10\n      -8.260\n      1\n      0.0355\n      0.813\n      0.000083\n      0.134\n      0.185\n      139.079\n      4\n    \n    \n      35627\n      1600562\n      pop\n      Tiziano Ferro\n      68\n      Non Me Lo So Spiegare\n      44\n      240040\n      0\n      2014\n      0.609\n      ...\n      11\n      -7.087\n      1\n      0.0352\n      0.706\n      0.000000\n      0.130\n      0.207\n      146.078\n      4\n    \n  \n\n6 rows × 21 columns\n\n\n\nWe can see the top 5 songs most similar to drivers license in the track_name column above. Interestingly, three of the five songs are Asian! These songs indeed sound similar to drivers license!"
  },
  {
    "objectID": "Pandas.html#correlation",
    "href": "Pandas.html#correlation",
    "title": "5  Pandas",
    "section": "5.7 Correlation",
    "text": "5.7 Correlation\nCorrelation may refer to any kind of association between two random variables. However, in this book, we will always consider correlation as the linear association between two random variables, or the Pearson’s correlation coefficient. Note that correlation does not imply causality and vice-versa.\nThe Pandas function corr() provides the pairwise correlation between all columns of a DataFrame, or between two Series. The function corrwith() provides the pairwise correlation of a DataFrame with another DataFrame or Series.\n\n#Pairwise correlation amongst all columns\nspotify_data.corr()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      artist_popularity\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      artist_followers\n      1.000000\n      0.577861\n      0.197426\n      0.040435\n      0.082857\n      0.098589\n      -0.010120\n      0.080085\n      -0.000119\n      0.123771\n      0.004313\n      -0.059933\n      -0.107475\n      -0.033986\n      0.002425\n      -0.053317\n      0.016524\n      0.030826\n    \n    \n      artist_popularity\n      0.577861\n      1.000000\n      0.285565\n      -0.097996\n      0.092147\n      0.062007\n      0.038784\n      0.039583\n      -0.011005\n      0.045165\n      0.018758\n      0.236942\n      -0.075715\n      -0.066679\n      0.099678\n      -0.034501\n      -0.032036\n      -0.033423\n    \n    \n      track_popularity\n      0.197426\n      0.285565\n      1.000000\n      0.060474\n      0.193685\n      0.568329\n      0.158507\n      0.217342\n      0.013369\n      0.296350\n      -0.022486\n      -0.056537\n      -0.284433\n      -0.124283\n      -0.090479\n      -0.038859\n      0.058408\n      0.071741\n    \n    \n      duration_ms\n      0.040435\n      -0.097996\n      0.060474\n      1.000000\n      -0.024226\n      0.067665\n      -0.145779\n      0.075990\n      0.007710\n      0.078586\n      -0.034818\n      -0.332585\n      -0.133960\n      0.067055\n      -0.034631\n      -0.155354\n      0.051046\n      0.085015\n    \n    \n      explicit\n      0.082857\n      0.092147\n      0.193685\n      -0.024226\n      1.000000\n      0.215656\n      0.138522\n      0.104734\n      0.011818\n      0.124410\n      -0.060350\n      0.077268\n      -0.129363\n      -0.039472\n      -0.024283\n      -0.032549\n      0.006585\n      0.043538\n    \n    \n      release_year\n      0.098589\n      0.062007\n      0.568329\n      0.067665\n      0.215656\n      1.000000\n      0.204743\n      0.338096\n      0.021497\n      0.430054\n      -0.071338\n      -0.032968\n      -0.369038\n      -0.149644\n      -0.045160\n      -0.070025\n      0.079382\n      0.089485\n    \n    \n      danceability\n      -0.010120\n      0.038784\n      0.158507\n      -0.145779\n      0.138522\n      0.204743\n      1.000000\n      0.137615\n      0.020128\n      0.142239\n      -0.051130\n      0.198509\n      -0.143936\n      -0.179213\n      -0.114999\n      0.505350\n      -0.125061\n      0.111015\n    \n    \n      energy\n      0.080085\n      0.039583\n      0.217342\n      0.075990\n      0.104734\n      0.338096\n      0.137615\n      1.000000\n      0.030824\n      0.747829\n      -0.053374\n      -0.043377\n      -0.678745\n      -0.131269\n      0.126050\n      0.348158\n      0.205960\n      0.170854\n    \n    \n      key\n      -0.000119\n      -0.011005\n      0.013369\n      0.007710\n      0.011818\n      0.021497\n      0.020128\n      0.030824\n      1.000000\n      0.024674\n      -0.139688\n      -0.003533\n      -0.023179\n      -0.006600\n      -0.011566\n      0.024206\n      0.008336\n      0.007738\n    \n    \n      loudness\n      0.123771\n      0.045165\n      0.296350\n      0.078586\n      0.124410\n      0.430054\n      0.142239\n      0.747829\n      0.024674\n      1.000000\n      -0.028151\n      -0.173444\n      -0.493020\n      -0.269008\n      0.002959\n      0.209588\n      0.171926\n      0.146030\n    \n    \n      mode\n      0.004313\n      0.018758\n      -0.022486\n      -0.034818\n      -0.060350\n      -0.071338\n      -0.051130\n      -0.053374\n      -0.139688\n      -0.028151\n      1.000000\n      -0.037237\n      0.043773\n      -0.024695\n      0.005657\n      0.010305\n      0.015399\n      -0.015225\n    \n    \n      speechiness\n      -0.059933\n      0.236942\n      -0.056537\n      -0.332585\n      0.077268\n      -0.032968\n      0.198509\n      -0.043377\n      -0.003533\n      -0.173444\n      -0.037237\n      1.000000\n      0.112061\n      -0.094796\n      0.263630\n      0.052171\n      -0.127945\n      -0.150350\n    \n    \n      acousticness\n      -0.107475\n      -0.075715\n      -0.284433\n      -0.133960\n      -0.129363\n      -0.369038\n      -0.143936\n      -0.678745\n      -0.023179\n      -0.493020\n      0.043773\n      0.112061\n      1.000000\n      0.112107\n      0.007415\n      -0.175674\n      -0.173152\n      -0.163243\n    \n    \n      instrumentalness\n      -0.033986\n      -0.066679\n      -0.124283\n      0.067055\n      -0.039472\n      -0.149644\n      -0.179213\n      -0.131269\n      -0.006600\n      -0.269008\n      -0.024695\n      -0.094796\n      0.112107\n      1.000000\n      -0.031301\n      -0.150172\n      -0.027369\n      -0.022034\n    \n    \n      liveness\n      0.002425\n      0.099678\n      -0.090479\n      -0.034631\n      -0.024283\n      -0.045160\n      -0.114999\n      0.126050\n      -0.011566\n      0.002959\n      0.005657\n      0.263630\n      0.007415\n      -0.031301\n      1.000000\n      -0.011137\n      -0.027716\n      -0.040789\n    \n    \n      valence\n      -0.053317\n      -0.034501\n      -0.038859\n      -0.155354\n      -0.032549\n      -0.070025\n      0.505350\n      0.348158\n      0.024206\n      0.209588\n      0.010305\n      0.052171\n      -0.175674\n      -0.150172\n      -0.011137\n      1.000000\n      0.100947\n      0.084783\n    \n    \n      tempo\n      0.016524\n      -0.032036\n      0.058408\n      0.051046\n      0.006585\n      0.079382\n      -0.125061\n      0.205960\n      0.008336\n      0.171926\n      0.015399\n      -0.127945\n      -0.173152\n      -0.027369\n      -0.027716\n      0.100947\n      1.000000\n      0.017423\n    \n    \n      time_signature\n      0.030826\n      -0.033423\n      0.071741\n      0.085015\n      0.043538\n      0.089485\n      0.111015\n      0.170854\n      0.007738\n      0.146030\n      -0.015225\n      -0.150350\n      -0.163243\n      -0.022034\n      -0.040789\n      0.084783\n      0.017423\n      1.000000\n    \n  \n\n\n\n\nQ: Which audio feature is the most correlated with track_popularity?\n\nspotify_data.corrwith(spotify_data.track_popularity).sort_values(ascending = False)\n\ntrack_popularity     1.000000\nrelease_year         0.568329\nloudness             0.296350\nartist_popularity    0.285565\nenergy               0.217342\nartist_followers     0.197426\nexplicit             0.193685\ndanceability         0.158507\ntime_signature       0.071741\nduration_ms          0.060474\ntempo                0.058408\nkey                  0.013369\nmode                -0.022486\nvalence             -0.038859\nspeechiness         -0.056537\nliveness            -0.090479\ninstrumentalness    -0.124283\nacousticness        -0.284433\ndtype: float64\n\n\nLoudness is the audio feature having the highest correlation with track_popularity.\nQ: Which audio feature is the most weakly correlated with track_popularity?\n\n5.7.1 Practice exercise 2\n\n5.7.1.1 \nUse the updated dataset from Practice exercise 1.\nThe last four variables in the dataset are:\n\ncant_change_math_ability\ncan_change_math_ability\nmath_is_genetic\nmuch_effort_is_lack_of_talent\n\nEach of the above variables has values - Agree / Disagree. Replace Agree with 1 and Disagree with 0.\nHint : You can do it with any one of the following methods:\n\nUse the map() function\nUse the apply() function with the lambda function\nUse the replace() function\nUse the applymap() function\n\nTwo of the above methods avoid a for-loop. Which ones?\nSolution:\n\n#Making a copy of data to avoid changing the original data. \nsurvey_data_copy = survey_data.copy()\n\n#Using the map function\nfor i in range(47,51):\n    survey_data_copy.iloc[:,i] = survey_data_copy.iloc[:,i].map({'Agree':1,'Disagree':0})\nsurvey_data_copy.iloc[:,47:51].head()\n\n\n\n\n\n  \n    \n      \n      cant_change_math_ability\n      can_change_math_ability\n      math_is_genetic\n      much_effort_is_lack_of_talent\n    \n  \n  \n    \n      0\n      0\n      1\n      0\n      0\n    \n    \n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      0\n      1\n      0\n      0\n    \n    \n      3\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      0\n      0\n      0\n    \n  \n\n\n\n\n\n#Making a copy of data to avoid changing the original data. \nsurvey_data_copy = survey_data.copy()\n\n#Using the lambda function with apply()\nfor i in range(47,51):\n    survey_data_copy.iloc[:,i] = survey_data_copy.iloc[:,i].apply(lambda x: 1 if x=='Agree' else 0)\nsurvey_data_copy.iloc[:,47:51].head()\n\n\n\n\n\n  \n    \n      \n      cant_change_math_ability\n      can_change_math_ability\n      math_is_genetic\n      much_effort_is_lack_of_talent\n    \n  \n  \n    \n      0\n      0\n      1\n      0\n      0\n    \n    \n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      0\n      1\n      0\n      0\n    \n    \n      3\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      0\n      0\n      0\n    \n  \n\n\n\n\n\n#Making a copy of data to avoid changing the original data. \nsurvey_data_copy = survey_data.copy()\n\n#Using the replace() function\nsurvey_data_copy.iloc[:,47:51] = survey_data_copy.iloc[:,47:51].replace('Agree','1')\nsurvey_data_copy.iloc[:,47:51] = survey_data_copy.iloc[:,47:51].replace('Disagree','0')\nsurvey_data_copy.iloc[:,47:51].head()\n\n\n\n\n\n  \n    \n      \n      cant_change_math_ability\n      can_change_math_ability\n      math_is_genetic\n      much_effort_is_lack_of_talent\n    \n  \n  \n    \n      0\n      0\n      1\n      0\n      0\n    \n    \n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      0\n      1\n      0\n      0\n    \n    \n      3\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      0\n      0\n      0\n    \n  \n\n\n\n\n\n#Making a copy of data to avoid changing the original data. \nsurvey_data_copy = survey_data.copy()\n\n#Using the lambda function with applymap()\nsurvey_data_copy.iloc[:,47:51] = survey_data_copy.iloc[:,47:51].applymap(lambda x: 1 if x=='Agree' else 0)\nsurvey_data_copy.iloc[:,47:51].head()\n\n\n\n\n\n  \n    \n      \n      cant_change_math_ability\n      can_change_math_ability\n      math_is_genetic\n      much_effort_is_lack_of_talent\n    \n  \n  \n    \n      0\n      0\n      1\n      0\n      0\n    \n    \n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      0\n      1\n      0\n      0\n    \n    \n      3\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      0\n      0\n      0\n    \n  \n\n\n\n\n\n\n5.7.1.2 \nAmong the four variables, which one is the most negatively correlated with math_is_genetic?\n\n#Computing correlation\nsurvey_data_copy.iloc[:,47:51].corrwith(survey_data_copy.math_is_genetic)\n\ncant_change_math_ability         0.294544\ncan_change_math_ability         -0.361546\nmath_is_genetic                  1.000000\nmuch_effort_is_lack_of_talent    0.154083\ndtype: float64\n\n\nThe variable can_change_math_ability is the most negatively correlated wtih math_is_genetic."
  },
  {
    "objectID": "Data visualization.html",
    "href": "Data visualization.html",
    "title": "6  Data visualization",
    "section": "",
    "text": "“One picture is worth a thousand words” - Fred R. Barnard\nVisual perception offers the highest bandwidth channel, as we acquire much more information through visual perception than with all of the other channels combined, as billions of our neurons are dedicated to this task. Moreover, the processing of visual information is, at its first stages, a highly parallel process. Thus, it is generally easier for humans to comprehend information with plots, diagrams and pictures, rather than with text and numbers. This makes data visualizations a vital part of data science. Some of the key purposes of data visualization are:\nWe’ll use a couple of libraries for making data visualizations - matplotlib and seaborn. Matplotlib is mostly used for creating relatively simple two-dimensional plots. Its plotting interface that is similar to the plot() function in MATLAB, so those who have used MATLAB should find it familiar. Seaborn is a recently developed data visualization library based on matplotlib. It is more oriented towards visualizing data with Pandas DataFrame and NumPy arrays. While matplotlib may also be used to create complex plots, seaborn has some built-in themes that may make it more convenient to make complex plots. Seaborn also has color schemes and plot styles that improve the readability and aesthetics of malplotlib plots. However, preferences depend on the user and their coding style, and it is perfectly fine to use either library for making the same visualization."
  },
  {
    "objectID": "Data visualization.html#matplotlib",
    "href": "Data visualization.html#matplotlib",
    "title": "6  Data visualization",
    "section": "6.1 Matplotlib",
    "text": "6.1 Matplotlib\nMatplotlib is:\n\na low-level graph plotting library in python that strives to emulate MATLAB,\ncan be used in Python scripts, Python and IPython shells, Jupyter notebooks and web application servers.\nis mostly written in python, a few segments are written in C, Objective-C and Javascript for Platform compatibility.\n\nConceptual model: Plotting requires action on a range of levels, ranging from the size of the figure to the text object in the plot. Matplotlib provides object-oriented interface in the hierarchical fashion to provide complete control over the plot. The user generates and keeps track of the figure and axes objects. These axes objects are then used for most plotting actions.\n\n6.1.1 Matplotlib: Object hierarchy\nA hierarchy means that there is a tree-like structure of matplotlib objects underlying each plot.\nA Figure object is the outermost container for a matplotlib graphic, which can contain multiple Axes objects. Note that an Axes actually translates into what we think of as an individual plot or graph (rather than the plural of axis as we might expect).\nThe Figure object is a box-like container holding one or more Axes (actual plots), as shown in Figure 6.1. Below the Axes in the hierarchy are smaller objects such as tick marks, individual lines, legends, and text boxes. Almost every element of a chart is its own manipulable Python object, all the way down to the ticks and labels.\n\n\n\n\nFigure 6.1: Matplotlib Object hierarchy\n\n\n\nHowever, Matplotlib presents this as a figure anatomy, rather than an explicit hierarchy. Figure 6.2 shows the components of a figure that can be customized with Matplotlib. (Source: https://matplotlib.org/stable/gallery/showcase/anatomy.html ).\n\n\n\n\nFigure 6.2: Matplotlib anatomy of a figure\n\n\n\nLet’s visualize the life expectancy of different countries with GDP per capita. We’ll read the data file gdp_lifeExpectancy.csv, which contains the GDP per capita and life expectancy of countries from 1952 to 2007.\n\nimport pandas as pd\nimport numpy as np\n\n\ngdp_data = pd.read_csv('./Datasets/gdp_lifeExpectancy.csv')\ngdp_data.head()\n\n\n\n\n\n  \n    \n      \n      country\n      continent\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      Asia\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1\n      Afghanistan\n      Asia\n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      2\n      Afghanistan\n      Asia\n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      3\n      Afghanistan\n      Asia\n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      4\n      Afghanistan\n      Asia\n      1972\n      36.088\n      13079460\n      739.981106\n    \n  \n\n\n\n\n\n\n6.1.2 Scatterplots and trendline with Matplotlib\nPurpose of scatterplots: Scatterplots (with or without a trendline) allow us to visualize the relationship between two numerical variables.\nWe’ll import the pyplot module of matplotlib to make plots. We’ll use the plot() function to make the scatter plot, and the functions xlabel() and ylabel() for labeling the plot axes.\n\nimport matplotlib.pyplot as plt\n\nQ: Make a scatterplot of Life expectancy vs GDP per capita.\nThere are two ways of plotting the figure:\n\nExplicitly creating figures and axes, and call methods on them (object-oriented style).\nLetting pyplot implicitly track the plot that it wants to reference. Simple functions are used to add plot elements (lines, images, text, etc.) to the current axes in the current figure (pyplot-style).\n\nWe’ll plot the figure in both ways.\n\n#Method 1: Object-oriented style\nfig, ax = plt.subplots() #Create a figure and an axes\nx = gdp_data.gdpPercap \ny = gdp_data.lifeExp\nax.plot(x,y,'o')   #Plot data on the axes\nax.set_xlabel('GDP per capita')    #Add an x-label to the axes\nax.set_ylabel('Life expectancy')   #Add a y-label to the axes\nax.set_title('Life expectancy vs GDP per capita from 1952 to 2007')\n\nText(0.5, 1.0, 'Life expectancy vs GDP per capita from 1952 to 2007')\n\n\n\n\n\n\n#Method 2: pyplot style\nx = gdp_data.gdpPercap\ny = gdp_data.lifeExp\nplt.plot(x,y,'o') #By default, the plot() function makes a lineplot. The 'o' arguments specifies a scatterplot\nplt.xlabel('GDP per capita')  #Labelling the horizontal X-axis\nplt.ylabel('Life expectancy') #Labelling the verical Y-axis\nplt.title('Life expectancy vs GDP per capita from 1952 to 2007')\n\nText(0.5, 1.0, 'Life expectancy vs GDP per capita from 1952 to 2007')\n\n\n\n\n\nBoth the plotting styles - object-oriented style and the pyplot style are perfectly valid and have their pros and cons.\n\nPyplot style is easier for simple plots\nObject-oriented style is slightly more complicated but more powerful as it allows for greater control over the axes in figure. This proves to be quite useful when we are dealing with a figure with multiple axes.\n\nFrom the above plot, we observe that life expectancy seems to be positively correlated with the GDP per capita of the country, as one may expect. However, there are a few outliers in the data - which are countries having extremely high GDP per capita, but not a correspondingly high life expectancy.\nSometimes it is difficult to get an idea of the overall trend (positive or negative correlation). In such cases, it may help to add a trendline to the scatter plot. In the plot below we add a trendline over the scatterplot showing that the life expectancy on an average increases with increasing GDP per capita. The trendline is actually a linear regression of life expectancy on GDP per capita. However, we’ll not discuss linear regression in this book.\nQ: Add a trendline over the scatterplot of life expectancy vs GDP per capita.\n\n#Making a scatterplot of Life expectancy vs GDP per capita\nx = gdp_data.gdpPercap\ny = gdp_data.lifeExp\nplt.plot(x,y,'o') #By default, the plot() function makes a lineplot. The 'o' arguments specifies a scatterplot\nplt.xlabel('GDP per capita')  #Labelling the horizontal X-axis\nplt.ylabel('Life expectancy') #Labelling the verical Y-axis\n\n#Plotting a trendline (linear regression) on the scatterplot\nslope_intercept_trendline = np.polyfit(x,y,1)   #Finding the slope and intercept for the trendline\ncompute_y_given_x = np.poly1d(slope_intercept_trendline) #Defining a function that computes the 'y' given 'x' for the trendline\nplt.plot(x,compute_y_given_x(x)) #Plotting the trendline\n\n\n\n\nThe above plot shows that our earlier intuition of a postive correlation between Life expectancy and GDP per capita was correct.\nWe used the NumPy function polyfit() to compute the slope and intercept of the trendline. Then, we defined an object compute_y_given_x of poly1d class and used it to compute the trendline.\n\n\n6.1.3 Subplots\nThere is often a need to make a few plots together to compare them. See the example below.\nQ: Make scatterplots of life expectancy vs GDP per capita separately for each of the 4 continents of Asia, Europe, Africa and America. Arrange the plots in a 2 x 2 grid.\n\n#Defining a 2x2 grid of subplots\nfig, axes = plt.subplots(2,2,figsize=(16,10))\nplt.subplots_adjust(wspace=0.2) #adjusting white space between individual plots\n\n#Making a scatterplot of Life expectancy vs GDP per capita for each continent\ncontinents = np.array([['Asia', 'Europe'], ['Africa', 'Americas']])\n\n#Looping over the 2x2 grid\nfor i in range(2):\n    for j in range(2):\n        \n        #Getting the GDP per capita and life expectancy of the countries of the (i,j)th continent\n        x = gdp_data.loc[gdp_data.continent==continents[i,j],:].gdpPercap\n        y = gdp_data.loc[gdp_data.continent==continents[i,j],:].lifeExp\n        \n        #Making the scatterplot\n        axes[i,j].plot(x,y,'o') \n        \n        #Setting limits on the 'x' and 'y' axes\n        axes[i,j].set_xlim([gdp_data.gdpPercap.min(), gdp_data.gdpPercap.max()])\n        axes[i,j].set_ylim([gdp_data.lifeExp.min(), gdp_data.lifeExp.max()])\n        \n        #Labelling the 'x' and 'y' axes\n        axes[i,j].set_xlabel('GDP per capita for '+ continents[i,j],fontsize = 14)\n        axes[i,j].set_ylabel('Life expectancy for '+ continents[i,j],fontsize = 14)\n        \n        #Putting a dollar sign, and thousand-comma separator on x-axis labels\n        axes[i,j].xaxis.set_major_formatter('${x:,.0f}')\n        \n        #Increasing font size of axis labels\n        axes[i,j].tick_params(axis = 'both',labelsize=14)\n\n\n\n\nWe observe that for each continent, except Africa, initially life expectancy increases rapidly with increasing GDP per capita. However, after a certain threshold of GDP per capita, life expectancy increases slowly. Several countries in Europe enjoy a relatively high GDP per capita as well as high life expectancy. Some countries in Asia have an extremely high GDP per capita, but a relatively low life expectancy. It will be interesting to see the proportion of GDP associated with healthcare for these outlying Asian countries, and European countries.\nWe used the subplot function of matplotlib to define the 2x2 grid of subplots. The function subplots_adjust() can be used to adjust white spaces around the plot. We used a for loop to iterate over each subplot. The axes object returned by the subplot() function was used to refer to individual subplots.\n\n\n6.1.4 Practice problem 1\nIs NU_GPA associated with parties_per_month? Analyze the association separately for Sophomores, Juniors, and Seniors (categories of the variable school_year).\nMake scatterplots of NU_GPA vs parties_per_month in a 1 x 3 grid, where each grid is for a distinct school_year. Plot the trendline as well for each scatterplot. Use the file survey_data_clean.csv.\nSolution:\n\nsurvey_data = pd.read_csv('./Datasets/survey_data_clean.csv')\n\ndef NU_GPA_vs_parties_per_month(data):\n    fig, axes = plt.subplots(1,3,figsize=(15,5))\n    plt.subplots_adjust(wspace=0.4) \n    \n    school_years = np.array(['Sophomore', 'Junior','Senior'])\n    for i in range(3):\n        x = data.loc[data.school_year==school_years[i],:].parties_per_month\n        y = data.loc[data.school_year==school_years[i],:].NU_GPA\n        \n        #The data has missing values. We can draw a trendline using only the non-missing value-pairs of NU_GPA and parties_per_month\n        #`idx_non_missing` will have the indices of the non-missing value-pairs of NU_GPA and parties_per_month\n        idx_non_missing = np.isfinite(x) & np.isfinite(y)\n        \n        axes[i].plot(x,y,'o',label = school_years[i]) \n        axes[i].set_xlim([data.parties_per_month.min(), data.parties_per_month.max()])\n        axes[i].set_ylim([data.NU_GPA.min(), data.NU_GPA.max()])\n        axes[i].set_xlabel('Parties per month',fontsize = 14)  \n        axes[i].set_ylabel('NU GPA',fontsize = 14) \n        axes[i].set_title(school_years[i],fontsize = 15)\n        slope_intercept_trendline = np.polyfit(x[idx_non_missing],y[idx_non_missing],1)   #Finding the slope and intercept for the trendline\n        compute_y_given_x = np.poly1d(slope_intercept_trendline) #Defining a function that computes the 'y' given 'x' for the trendline\n        axes[i].plot(x,compute_y_given_x(x)) #Plotting the trendline\n\nNU_GPA_vs_parties_per_month(survey_data)\n\n\n\n\nNote that the trendline in the above plots seems to be influenced by a few points having extreme values of parties_per_month. These points have a high leverage (a concept we’ll learn in a future course on linear regression) in influencing the trendline. So, we should visualize the trend by removing or capping these high-leverage points, to avoid the distortion of the trend by a few points.\nLet us cap the the values of parties_per_month to 30, and make the visualizations again.\n\nsurvey_data_parties_capped = survey_data.copy()\nsurvey_data_parties_capped.parties_per_month = survey_data.parties_per_month.apply(lambda x: min(30,x))\nNU_GPA_vs_parties_per_month(survey_data_parties_capped)\n\n\n\n\nWe see that the trend didn’t change much after removing the high leverage points. (Note that although the high leverage points have the leverage to influence the trendline, they need not necessarily influence it). From the visualization, NU_GPA doesn’t seem to be associated with parties_per_month for students of any of the school years.\n\n\n6.1.5 Overlapping plots with legend\nWe can also have the scatterplot of all the continents on the sample plot, with a distinct color for each continent. A legend will be required to identify the continent’s color.\n\ncontinents = np.array([['Asia', 'Europe'], ['Africa', 'Americas']])\nplt.rcParams[\"figure.figsize\"] = (9,6)\nfor i in range(2):\n    for j in range(2):\n        x = gdp_data.loc[gdp_data.continent==continents[i,j],:].gdpPercap\n        y = gdp_data.loc[gdp_data.continent==continents[i,j],:].lifeExp\n        plt.plot(x,y,'o',label = continents[i,j]) \n        plt.xlim([gdp_data.gdpPercap.min(), gdp_data.gdpPercap.max()])\n        plt.ylim([gdp_data.lifeExp.min(), gdp_data.lifeExp.max()])\n        plt.xlabel('GDP per capita')  \n        plt.ylabel('Life expectancy') \nplt.legend()\n\n<matplotlib.legend.Legend at 0x2320d6d70a0>\n\n\n\n\n\nNote that a disadvantage of the above plot is overplotting. The data points corresponding to the Americas are hiding the data points of other continents. However, if the data points corresponding to different categories are spread apart, then it may be convenient to visualize all the categories on the same plot."
  },
  {
    "objectID": "Data visualization.html#pandas",
    "href": "Data visualization.html#pandas",
    "title": "6  Data visualization",
    "section": "6.2 Pandas",
    "text": "6.2 Pandas\nMatplotlib is a low-level tool, in which different components of the plot, such as points, legend, axis titles, etc. need to be specified separately. The Pandas plot() function can be used directly with a DataFrame or Series to make plots.\n\n6.2.1 Scatterplots with Pandas\n\n#Plotting life expectancy vs GDP per capita using the Pandas plot() function\nax = gdp_data.plot(x = 'gdpPercap', y = 'lifeExp', kind = 'scatter',figsize=(10, 6),xlabel = 'GDP per capita', \n              ylabel = 'Life expectancy')\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n\n\n\n\n\nIn the above plot, note that:\n\nWith matplotlib, it will take 3 lines to make the same plot - one for the scatterplot, and two for the axis titles.\nThe object ax is of type matplotlib.axes._subplots.AxesSubplot (check the code below). This means we can use the attributes and methods associated with the axes object of Matplotlib. If you see the documentation of the Pandas plot() function, you will find that under the kwargs** argument, you have Options to pass to matplotlib plotting method. Thus, you get the convenience of using the Pandas plot() function, while also having the attributes and methods associated with Matplotlib.\n\n\ntype(ax)\n\nmatplotlib.axes._subplots.AxesSubplot\n\n\n\n\n6.2.2 Lineplots with Pandas\nPurpose of lineplots: Lineplots show the relationship between two numerical variables when the variable on the x-axis, also called the explanatory variable, is of a sequential nature; in other words there is an inherent ordering to the variable. The most common example of lineplots have some notion of time on the x-axis (or the horizontal axis): hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Lineplots that have some notion of time on the x-axis are also called time series plots. Lineplots should be avoided when there is not a clear sequential ordering to the variable on the x-axis.\nLet us re-arrange the data to show other benefits of the Pandas plot() function. Note that data resphaping is explained in Chapter 8 of the book, so you may ignore the code block below that uses the pivot_table() function.\n\n#You may ignore this code block until Chapter 8.\nmean_gdp_per_capita = gdp_data.pivot_table(index = 'year', columns = 'continent',values = 'gdpPercap')\nmean_gdp_per_capita.head()\n\n\n\n\n\n  \n    \n      continent\n      Africa\n      Americas\n      Asia\n      Europe\n      Oceania\n    \n    \n      year\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1952\n      1252.572466\n      4079.062552\n      5195.484004\n      5661.057435\n      10298.085650\n    \n    \n      1957\n      1385.236062\n      4616.043733\n      5787.732940\n      6963.012816\n      11598.522455\n    \n    \n      1962\n      1598.078825\n      4901.541870\n      5729.369625\n      8365.486814\n      12696.452430\n    \n    \n      1967\n      2050.363801\n      5668.253496\n      5971.173374\n      10143.823757\n      14495.021790\n    \n    \n      1972\n      2339.615674\n      6491.334139\n      8187.468699\n      12479.575246\n      16417.333380\n    \n  \n\n\n\n\nWe have reshaped the data to obtain the mean GDP per capita of each continent for each year.\nThe pandas plot() function can be directly used with this DataFrame to create line plots showing mean GDP per capita of each continent with year.\n\nax = mean_gdp_per_capita.plot(ylabel = 'GDP per capita',figsize = (10,6),marker='o')\nax.yaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nWe observe that the mean GDP per capita of of Europe and Oceania have increased rapidly, while that for Africa is increasing very slowly.\nThe above plot will take several lines of code if developed using only matplotlib. The pandas plot() function has a framework to conveniently make commonly used plots.\nNote that argument marker = ‘o’ puts a solid circle at each of the data points.\n\n\n6.2.3 Bar plots with Pandas\nPurpose of bar plots: Barplots are used to visualize any aggregate statistics of a continuous variable with respect to the categories or levels of a categorical variable. For example, we may visualize the average IMDB rating (aggregate statistics) of movies based on their genre (the categorical variable).\nBar plots can be made using the pandas bar function with the DataFrame or Series, just like the line plots and scatterplots.\nBelow, we are reading the dataset of noise complaints of type Loud music/Party received the police in New York City in 2016.\n\nnyc_party_complaints = pd.read_csv('./Datasets/party_nyc.csv')\nnyc_party_complaints.head()\n\n\n\n\n\n  \n    \n      \n      Created Date\n      Closed Date\n      Location Type\n      Incident Zip\n      City\n      Borough\n      Latitude\n      Longitude\n      Hour_of_the_day\n      Month_of_the_year\n    \n  \n  \n    \n      0\n      12/31/2015 0:01\n      12/31/2015 3:48\n      Store/Commercial\n      10034.0\n      NEW YORK\n      MANHATTAN\n      40.866183\n      -73.918930\n      0\n      12\n    \n    \n      1\n      12/31/2015 0:02\n      12/31/2015 4:36\n      Store/Commercial\n      10040.0\n      NEW YORK\n      MANHATTAN\n      40.859324\n      -73.931237\n      0\n      12\n    \n    \n      2\n      12/31/2015 0:03\n      12/31/2015 0:40\n      Residential Building/House\n      10026.0\n      NEW YORK\n      MANHATTAN\n      40.799415\n      -73.953371\n      0\n      12\n    \n    \n      3\n      12/31/2015 0:03\n      12/31/2015 1:53\n      Residential Building/House\n      11231.0\n      BROOKLYN\n      BROOKLYN\n      40.678285\n      -73.994668\n      0\n      12\n    \n    \n      4\n      12/31/2015 0:05\n      12/31/2015 3:49\n      Residential Building/House\n      10033.0\n      NEW YORK\n      MANHATTAN\n      40.850304\n      -73.938516\n      0\n      12\n    \n  \n\n\n\n\nLet us visualise the locations from where the the complaints are coming.\n\n#Using the pandas function bar() to create bar plot\nax = nyc_party_complaints['Location Type'].value_counts().plot.bar(ylabel = 'Number of complaints')\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\nFrom the above plot, we observe that most of the complaints come from residential buildings and houses, as one may expect.\nLet is visualize the time of the year when most complaints occur.\n\n#Using the pandas function bar() to create bar plot\nax = nyc_party_complaints['Month_of_the_year'].value_counts().sort_index().plot.bar(ylabel = 'Number of complaints',\n                                                                              xlabel = \"Month\")\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\nTry executing the code without sort_index() to figure out the purpose of using the function.\nFrom the above plot, we observe that most of the complaints occur during summer and early Fall.\nLet us create a stacked bar chart that combines both the above plots into a single plot. You may ignore the code used for re-shaping the data until Chapter 8. The purpose here is to show the utility of the pandas bar() function.\n\n#Reshaping the data to make it suitable for a stacked barplot - ignore this code until chapter 8\ncomplaints_location=pd.crosstab(nyc_party_complaints.Month_of_the_year, nyc_party_complaints['Location Type'])\ncomplaints_location.head()\n\n\n\n\n\n  \n    \n      Location Type\n      Club/Bar/Restaurant\n      House of Worship\n      Park/Playground\n      Residential Building/House\n      Store/Commercial\n      Street/Sidewalk\n    \n    \n      Month_of_the_year\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      748\n      24\n      17\n      9393\n      1157\n      832\n    \n    \n      2\n      570\n      29\n      16\n      8383\n      1197\n      782\n    \n    \n      3\n      747\n      39\n      90\n      9689\n      1480\n      1835\n    \n    \n      4\n      848\n      53\n      129\n      11984\n      1761\n      2943\n    \n    \n      5\n      2091\n      72\n      322\n      15676\n      1941\n      5090\n    \n  \n\n\n\n\n\n#Stacked bar plot showing number of complaints at different months of the year, and from different locations\nax = complaints_location.plot.bar(stacked=True,ylabel = 'Number of complaints',figsize=(15, 10), xlabel = 'Month')\nax.tick_params(axis = 'both',labelsize=15)\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\nFigure 6.3: Stacked bar plot with Pandas\n\n\n\n\nThe above plots gives the insights about location and day of the year simultaneously that were previously separately obtained by the individual plots.\nAn alternative to stacked barplots are side-by-side barplots, as shown below.\n\n#Side-by-side bar plot showing number of complaints at different months of the year, and from different locations\nax = complaints_location.plot.bar(ylabel = 'Number of complaints',figsize=(15, 10), xlabel = 'Month')\nax.tick_params(axis = 'both',labelsize=15)\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\n\\(\\color{red}{\\text{Q1}}\\) In which scenarios should we use a stacked barplot instead of a side-by-side barplot and vice-versa?"
  },
  {
    "objectID": "Data visualization.html#seaborn",
    "href": "Data visualization.html#seaborn",
    "title": "6  Data visualization",
    "section": "6.3 Seaborn",
    "text": "6.3 Seaborn\nSeaborn offers the flexibility of simultaneously visualizing multiple variables in a single plot, and offers several themes to develop plots.\n\n#Importing the seaborn library\nimport seaborn as sns\n\n\n6.3.1 Bar plots with confidence intervals with Seaborn\nWe’ll group the data to obtain the total complaints for each Location Type, Borough, Month_of_the_year, and Hour_of_the_day. Note that you’ll learn grouping data in Chapter 9, so you may ignore the next code block. The grouping is done to shape the data in a suitable form for visualization.\n\n#Grouping the data to make it suitable for visualization using Seaborn. Ignore this code block until learn chapter 9.\nnyc_complaints_grouped = nyc_party_complaints[['Location Type','Borough','Month_of_the_year','Latitude','Hour_of_the_day']].groupby(['Location Type','Borough','Month_of_the_year','Hour_of_the_day'])['Latitude'].agg([('complaints','count')]).reset_index()\nnyc_complaints_grouped.head()\n\n\n\n\n\n  \n    \n      \n      Location Type\n      Borough\n      Month_of_the_year\n      Hour_of_the_day\n      complaints\n    \n  \n  \n    \n      0\n      Club/Bar/Restaurant\n      BRONX\n      1\n      0\n      10\n    \n    \n      1\n      Club/Bar/Restaurant\n      BRONX\n      1\n      1\n      10\n    \n    \n      2\n      Club/Bar/Restaurant\n      BRONX\n      1\n      2\n      6\n    \n    \n      3\n      Club/Bar/Restaurant\n      BRONX\n      1\n      3\n      6\n    \n    \n      4\n      Club/Bar/Restaurant\n      BRONX\n      1\n      4\n      3\n    \n  \n\n\n\n\nLet us create a bar plot visualizing the average number of complaints with the time of the day.\n\nax = sns.barplot(x=\"Hour_of_the_day\", y = 'complaints',  data=nyc_complaints_grouped)\nax.figure.set_figwidth(15)\n\n\n\n\nFrom the above plot, we observe that most of the complaints are made around midnight. However, interestingly, there are some complaints at each hour of the day.\nNote that the above barplot shows the mean number of complaints in a month at each hour of the day. The black lines are the 95% confidence intervals of the mean number of complaints.\n\n\n6.3.2 Facetgrid: Multi-plot grid for plotting conditional relationships\nWith pandas, we simultaneously visualized the number of complaints with month of the year and location type in Figure 6.3. We’ll use Seaborn to add another variable - Borough to the visualization.\nQ: Visualize the mean number of complaints with Month_of_the_year, Location Type, and Borough.\nThe seaborn class FacetGrid is used to design the plot, i.e., specify the way the data will be divided in mutually exclusive subsets for visualization. Then the [map] function of the FacetGrid class is used to apply a plotting function to each subset of the data.\n\n#Visualizing the number of complaints with Month_of_the_year, Location Type, and Borough.\na = sns.FacetGrid(nyc_complaints_grouped, hue = 'Location Type', col = 'Borough',col_wrap=3,height=3.5,aspect = 1)\na.map(sns.lineplot,'Month_of_the_year','complaints')\na.set_axis_labels(\"Month of the year\", \"Complaints\")\na.add_legend()\n\n<seaborn.axisgrid.FacetGrid at 0x231afeafac0>\n\n\n\n\n\nFrom the above plot, we get a couple of interesting insights: 1. For Queens and Staten Island, most of the complaints occur in summer, for Manhattan and Bronx it is mostly during late spring, while Brooklyn has a spike of complaints in early Fall. 2. In most of the Boroughs, the majority complaints always occur in residential areas. However, for Manhattan, the number of street/sidewalk complaints in the summer are comparable to those from residential areas.\nWe have visualized 4 variables simultaneously in the above plot.\nLet us consider another example, where we will visualize the weather in a few cities of Australia. The file Australia_weather.csv consists of weather details of Sydney, Canberra, and Melbourne from 2007 to 2017.\n\naussie_weather = pd.read_csv('./Datasets/Australia_weather.csv')\naussie_weather.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Location\n      MinTemp\n      MaxTemp\n      Rainfall\n      Evaporation\n      Sunshine\n      WindGustDir\n      WindGustSpeed\n      WindDir9am\n      ...\n      Humidity3pm\n      Pressure9am\n      Pressure3pm\n      Cloud9am\n      Cloud3pm\n      Temp9am\n      Temp3pm\n      RainToday\n      RISK_MM\n      RainTomorrow\n    \n  \n  \n    \n      0\n      10/20/2010\n      Sydney\n      12.9\n      20.3\n      0.2\n      3.0\n      10.9\n      ENE\n      37\n      W\n      ...\n      57\n      1028.8\n      1025.6\n      3\n      1\n      16.9\n      19.8\n      No\n      0.0\n      No\n    \n    \n      1\n      10/21/2010\n      Sydney\n      13.3\n      21.5\n      0.0\n      6.6\n      11.0\n      ENE\n      41\n      W\n      ...\n      58\n      1025.9\n      1022.4\n      2\n      5\n      17.6\n      21.3\n      No\n      0.0\n      No\n    \n    \n      2\n      10/22/2010\n      Sydney\n      15.3\n      23.0\n      0.0\n      5.6\n      11.0\n      NNE\n      41\n      W\n      ...\n      63\n      1021.4\n      1017.8\n      1\n      4\n      19.0\n      22.2\n      No\n      0.0\n      No\n    \n    \n      3\n      10/26/2010\n      Sydney\n      12.9\n      26.7\n      0.2\n      3.8\n      12.1\n      NE\n      33\n      W\n      ...\n      56\n      1018.0\n      1015.0\n      1\n      5\n      17.8\n      22.5\n      No\n      0.0\n      No\n    \n    \n      4\n      10/27/2010\n      Sydney\n      14.8\n      23.8\n      0.0\n      6.8\n      9.6\n      SSE\n      54\n      SSE\n      ...\n      69\n      1016.0\n      1014.7\n      2\n      7\n      20.2\n      20.6\n      No\n      1.8\n      Yes\n    \n  \n\n5 rows × 24 columns\n\n\n\n\naussie_weather.shape\n\n(4666, 24)\n\n\nQ: Visualize if it rains the next day (RainTomorrow) given whether it has rained today (RainToday), the current day’s humidity (Humidity9am), maximum temperature (MaxTemp) and the city (Location).\n\na = sns.FacetGrid(aussie_weather,col='Location',row='RainToday',height = 4,aspect = 1,hue = 'RainTomorrow')\na.map(plt.scatter,'MaxTemp','Humidity9am')\na.set_axis_labels(\"Maximum temperature\", \"Humidity at 9 am\")\na.set_titles(col_template=\"{col_name}\", row_template=\"Rain today: {row_name}\")\na.add_legend()\n\n<seaborn.axisgrid.FacetGrid at 0x231b57e51c0>\n\n\n\n\n\nHumidity tends to be higher when it is going to rain the next day. However, the correlation is much more pronounced for Syndey. In case it is not raining on the current day, humidity seems to be slightly negatively correlated with temperature.\n\n\n6.3.3 Practice exercise 2\nHow does the expected marriage age of the people of STAT303-1 depend on their characteristics? We’ll use visualizations to answer this question. Use data from the file survey_data_clean.csv. Proceed as follows:\n\nMake a visualization that compares the mean expected_marriage_age of introverts and extroverts (use the variable introvert_extrovert). What insights do you obtain?\n\n\nplt.rcParams[\"figure.figsize\"] = (9,6)\nsns.barplot(x = 'introvert_extrovert' ,y = 'expected_marriage_age', data = survey_data)\nplt.xlabel('Personality', fontsize=16);\nplt.ylabel('Expected marriage age', fontsize=16);\nplt.xticks(fontsize=15);\nplt.yticks(fontsize=15);\n\n\n\n\nThe mean expected marriage age for introverts is about 2 years higher than that for extroverts. Also, there is a higher variation in the expected marriage age of introverts as compared to extroverts.\n\nDoes the mean expected_marriage_age of introverts and extroverts depend on whether they believe in love in first sight (variable name: love_first_sight)? Update the previous visualization to answer the question.\n\n\nsns.set(font_scale=1.25)\na = sns.FacetGrid(survey_data,col='love_first_sight',height = 5,aspect = 1)\na.map(sns.barplot, 'introvert_extrovert' , 'expected_marriage_age' ,order = ['Introvert', 'Extrovert'])\na.set_axis_labels(\"Personality\", \"Expected marriage age\")\na.add_legend()\n\n<seaborn.axisgrid.FacetGrid at 0x20c14eed760>\n\n\n\n\n\nYes, only those introverts who do not believe in love in first sight have a higher mean value of expected marriage age.\n\nIn addition to love_first_sight, does the mean expected_marriage_age of introverts and extroverts depend on whether they are a procrastinator (variable name: procrastinator)? Update the previous visualization to answer the question.\n\n\nsns.set(font_scale=1.25)\na = sns.FacetGrid(survey_data,col='love_first_sight',height = 5,aspect = 1, row = 'procrastinator')\na.map(sns.barplot, 'introvert_extrovert' , 'expected_marriage_age' ,order = ['Introvert', 'Extrovert'])\na.set_axis_labels(\"Personality\", \"Expected marriage age\")\na.add_legend()\n\n<seaborn.axisgrid.FacetGrid at 0x20c1578cb50>\n\n\n\n\n\nProcrastination does not seem to make much of a difference in the expected marriage age. The mean expected marriage age of procrastinating introverts seems to be only a little higher than the non-procrastinating introverts.\n\nIs there any critical information missing in the above visualizations that, if revealed, may cast doubts on the patterns observed in them?\n\nYes, we don’t know the number of observations corresponding to each bar of the bar plots. If there are a very few observations in any of the categories, then the trend shown by that category may not be reliable. For example, in the data (survey_data), there are only 8 introverts who are not procrastinators and believe in love in first sight, while there are 52 introverts who are procrastinators and do not believe in love in first sight.\nIf there are more introverts in the class of STAT303-1 who are not procrastinators and believe in love in first sight (may be they didn’t fill the survey), then they are under-represnted in the sample of people who filled the survey, and the trend observed for them may be less reliable than that for other people.\n\n#Code for finding the number of people in each category - you will understand this code later in chapter 9 on data aggregation\nsurvey_data[['introvert_extrovert','love_first_sight','procrastinator','Timestamp']].groupby(['introvert_extrovert',\n                                                                                  'love_first_sight','procrastinator']).count()\n\n\n\n\n\n  \n    \n      \n      \n      \n      Timestamp\n    \n    \n      introvert_extrovert\n      love_first_sight\n      procrastinator\n      \n    \n  \n  \n    \n      Extrovert\n      0\n      0\n      19\n    \n    \n      1\n      35\n    \n    \n      1\n      0\n      10\n    \n    \n      1\n      15\n    \n    \n      Introvert\n      0\n      0\n      32\n    \n    \n      1\n      52\n    \n    \n      1\n      0\n      8\n    \n    \n      1\n      21\n    \n  \n\n\n\n\n\n\n6.3.4 Histogram and density plots with Seaborn\nPurpose: Histogram and density plots visualize the distribution of a continuous variable.\nA histogram plots the number of observations occurring within discrete, evenly spaced bins of a random variable, to visualize the distribution of the variable. It may be considered a special case of a bar plot as bars are used to plot the observation counts.\nA density plot uses a kernel density estimate to approximate the distribution of random variable.\nWe can use the Seaborn displot() function to make both kinds of plots - histogram or density plot.\nExample: Make a histogram showing the distributions of maximum temperature in Sydney, Canberra and Melbourne.\n\nsns.set(font_scale = 1.4)\na = sns.displot(data = aussie_weather, x = 'MaxTemp',kind = 'hist',col='Location')\na.set_axis_labels(\"Maximum temperature\", \"Count\")\na.set_titles(\"{col_name}\")\n\n<seaborn.axisgrid.FacetGrid at 0x2319fbeabb0>\n\n\n\n\n\nFrom the above plot, we observe that: 1. Melbourne has a right skewed distribution with the median temperature being smaller than the mean. 2. Canberra seems to have the highest variation in the temperature.\nExample: Make a density plot showing the distributions of maximum temperature in Sydney, Canberra and Melbourne.\n\nsns.displot(data = aussie_weather, x = 'MaxTemp',kind = 'kde', col = 'Location')\n\n<seaborn.axisgrid.FacetGrid at 0x1d0e963f8e0>\n\n\n\n\n\nExample: Show the distributions of the maximum and minimum temperatures in a single plot.\n\nsns.histplot(data=aussie_weather, x=\"MaxTemp\", color=\"skyblue\", label=\"MaxTemp\", kde=True)\nsns.histplot(data=aussie_weather, x=\"MinTemp\", color=\"red\", label=\"MinTemp\", kde=True)\nplt.legend()\nplt.xlabel('Temperature')\n\nText(0.5, 0, 'Temperature')\n\n\n\n\n\nThe Seaborn function histplot() can be used to make a density plot overlapping on a histogram.\n\n\n6.3.5 Boxplots with Seaborn\nPurpose: Boxplots is a standardized way of visualizing the distribution of a continuous variable. They show five key metrics that describe the data distribution - median, 25th percentile value, 75th percentile value, minimum and maximum, as shown in the figure below. Note that the minimum and maximum exclude the outliers.\n\n\n\n\n\nExample: Make a boxplot comparing the distributions of maximum temperatures of Sydney, Canberra and Melbourne, given whether or not it has rained on the day.\n\nsns.boxplot(data = aussie_weather,x = 'Location', y = 'MaxTemp',hue = 'RainToday')\n\n<AxesSubplot:xlabel='Location', ylabel='MaxTemp'>\n\n\n\n\n\nFrom the above plot, we observe that: 1. The maximum temperature of the day, on an average, is lower if it rained on the day. 2. Sydney and Melbourne have some extremely high outlying values of maximum temperature.\nWe have used the Seaborn boxplot() function for the above plot.\n\n\n6.3.6 Scatterplots with Seaborn\nWe made scatterplots with Matplotlib and Pandas earlier. With Seaborn, the regplot() function allows us to plot a trendline over the scatterplot, along with a 95% confidence interval for the trendline. Note that this is much easier than making a trendline with Matplotlib.\n\n#Scatterplot and trendline with seaborn\nsns.regplot(x = 'gdpPercap', y = 'lifeExp', data = gdp_data)\n\n<AxesSubplot:xlabel='gdpPercap', ylabel='lifeExp'>\n\n\n\n\n\nNote that the confidence interval of the trendline broadens as we move farther away from most of the data points. In other words, there is more uncertainty about the trend as we move to a domain space farther away from the data.\n\n\n6.3.7 Heatmaps with Seaborn\nPurpose: Heatmaps help us visualize the correlation between all variable-pairs.\nBelow is a heatmap visualizing the pairwise correlation of all the numerical variables of survey_data_clean. With a heatmap it becomes easier to see strongly correlated variables.\n\nsns.set(rc={'figure.figsize':(12,10)})\nsns.heatmap(survey_data.corr())\n\n<AxesSubplot:>\n\n\n\n\n\nFrom the above map, we can see that:\n\nstudent athlete is strongly postively correlated with minutes_ex_per_week\nprocrastinator is strongly negatively correlated with NU_GPA\n\n\n\n6.3.8 Pairplots with Seaborn\nPurpose: Pairplots are used to visualize the association between all variable-pairs in the data. In other words, pairplots simultaneously visualize the scatterplots between all variable-pairs.\nLet us visualize the pair-wise association of nutrition variables in the starbucks drinks data.\n\nstarbucks_drinks = pd.read_csv('./Datasets/starbucks-menu-nutrition-drinks.csv')\nsns.pairplot(starbucks_drinks)\n\n<seaborn.axisgrid.PairGrid at 0x231831dc940>\n\n\n\n\n\nIn the above pairplot, note that:\n\nThe histograms on the diagonal of the grid show the distribution of each of the variables.\nInstead of a histogram, we can visualize the density plot with the argument kde = True.\nThe scatterplots in the rest of the grid are the pair-wise plots of all the variables.\n\nFrom the above plot, we observe that:\n\nAlmost all the variable pairs have a positive correlation, i.e., if one of the nutrients increase in a drink, others also are likely to increase.\nThe number of calories seem to be strongly positively correlated with the amount of carbs in the drink.\nFrom the density plots we can see that there is a lot of choice for consumers to buy a drink that has a zero value for any of the nutrients - fat, protein, fiber, or sodium."
  },
  {
    "objectID": "Data cleaning and preparation.html",
    "href": "Data cleaning and preparation.html",
    "title": "7  Data cleaning and preparation",
    "section": "",
    "text": "Missing values in a dataset can occur due to several reasons such as breakdown of measuring equipment, accidental removal of observations, lack of response by respondents, error on the part of the researcher, etc.\nLet us read the dataset GDP_missing_data.csv, in which we have randomly removed some values, or put missing values in some of the columns.\nWe’ll also read GDP_complete_data.csv, in which we have not removed any values. We’ll use this data later to assess the accuracy of our guess or estimate of missing values in GDP_missing_data.csv.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn as sk\nimport seaborn as sns\ngdp_missing_values_data = pd.read_csv('./Datasets/GDP_missing_data.csv')\ngdp_complete_data = pd.read_csv('./Datasets/GDP_complete_data.csv')\n\n\ngdp_missing_values_data.head()\n\n\n\n\n\n  \n    \n      \n      economicActivityFemale\n      country\n      lifeMale\n      infantMortality\n      gdpPerCapita\n      economicActivityMale\n      illiteracyMale\n      illiteracyFemale\n      lifeFemale\n      geographic_location\n      contraception\n      continent\n    \n  \n  \n    \n      0\n      7.2\n      Afghanistan\n      45.0\n      154.0\n      2474.0\n      87.5\n      NaN\n      85.0\n      46.0\n      Southern Asia\n      NaN\n      Asia\n    \n    \n      1\n      7.8\n      Algeria\n      67.5\n      44.0\n      11433.0\n      76.4\n      26.1\n      51.0\n      70.3\n      Northern Africa\n      NaN\n      Africa\n    \n    \n      2\n      41.3\n      Argentina\n      69.6\n      22.0\n      NaN\n      76.2\n      3.8\n      3.8\n      76.8\n      South America\n      NaN\n      South America\n    \n    \n      3\n      52.0\n      Armenia\n      67.2\n      25.0\n      13638.0\n      65.0\n      NaN\n      0.5\n      74.0\n      Western Asia\n      NaN\n      Asia\n    \n    \n      4\n      53.8\n      Australia\n      NaN\n      6.0\n      54891.0\n      NaN\n      1.0\n      1.0\n      81.2\n      Oceania\n      NaN\n      Oceania\n    \n  \n\n\n\n\nObserve that the gdp_missing_values_data dataset consists of some missing values shown as NaN (Not a Number).\n\n\nMissing values in a Pandas DataFrame can be identified with the isnull() method. The Pandas Series object also consists of the isnull() method. For finding the number of missing values in each column of gdp_missing_values_data, we will sum up the missing values in each column of the dataset:\n\ngdp_missing_values_data.isnull().sum()\n\neconomicActivityFemale    10\ncountry                    0\nlifeMale                  10\ninfantMortality           10\ngdpPerCapita              10\neconomicActivityMale      10\nilliteracyMale            10\nilliteracyFemale          10\nlifeFemale                10\ngeographic_location        0\ncontraception             71\ncontinent                  0\ndtype: int64\n\n\nNote that the descriptive statistics methods associated with Pandas objects ignore missing values by default. Consider the summary statistics of gdp_missing_values_data:\n\ngdp_missing_values_data.describe()\n\n\n\n\n\n  \n    \n      \n      economicActivityFemale\n      lifeMale\n      infantMortality\n      gdpPerCapita\n      economicActivityMale\n      illiteracyMale\n      illiteracyFemale\n      lifeFemale\n      contraception\n    \n  \n  \n    \n      count\n      145.000000\n      145.000000\n      145.000000\n      145.000000\n      145.000000\n      145.000000\n      145.000000\n      145.000000\n      84.000000\n    \n    \n      mean\n      45.935172\n      65.491724\n      37.158621\n      24193.482759\n      76.563448\n      13.570028\n      21.448897\n      70.615862\n      51.773810\n    \n    \n      std\n      16.875922\n      9.099256\n      34.465699\n      22748.764444\n      7.854730\n      16.497954\n      25.497045\n      9.923791\n      31.930026\n    \n    \n      min\n      1.900000\n      36.000000\n      3.000000\n      772.000000\n      51.200000\n      0.000000\n      0.000000\n      39.100000\n      0.000000\n    \n    \n      25%\n      35.500000\n      62.900000\n      10.000000\n      6837.000000\n      72.000000\n      1.000000\n      2.300000\n      67.500000\n      17.000000\n    \n    \n      50%\n      47.600000\n      67.800000\n      24.000000\n      15184.000000\n      77.300000\n      6.600000\n      9.720000\n      73.900000\n      65.000000\n    \n    \n      75%\n      55.900000\n      72.400000\n      54.000000\n      35957.000000\n      81.600000\n      19.500000\n      30.200000\n      78.100000\n      77.000000\n    \n    \n      max\n      90.600000\n      77.400000\n      169.000000\n      122740.000000\n      93.000000\n      70.500000\n      90.800000\n      82.900000\n      79.000000\n    \n  \n\n\n\n\nObserve that the count statistics report the number of non-missing values of each column in the data, as the number of rows in the data (see code below) is more than the number of non-missing values of all the variables in the above table. Similarly, for the rest of the statistics, such as mean, std, etc., the missing values are ignored.\n\n#The dataset gdp_missing_values_data has 155 rows\ngdp_missing_values_data.shape[0]\n\n155\n\n\n\n\n\nNow that we know how to identify missing values in the dataset, let us learn about the types of missing values that can be there. Rubin (1976) classified missing values in three categories.\n\n\nIf the probability of being missing is the same for all cases, then the data are said to be missing completely at random. An example of MCAR is a weighing scale that ran out of batteries. Some of the data will be missing simply because of bad luck.\n\n\n\nIf the probability of being missing is the same only within groups defined by the observed data, then the data are missing at random (MAR). MAR is a much broader class than MCAR. For example, when placed on a soft surface, a weighing scale may produce more missing values than when placed on a hard surface. Such data are thus not MCAR. If, however, we know surface type and if we can assume MCAR within the type of surface, then the data are MAR\n\n\n\nMNAR means that the probability of being missing varies for reasons that are unknown to us. For example, the weighing scale mechanism may wear out over time, producing more missing data as time progresses, but we may fail to note this. If the heavier objects are measured later in time, then we obtain a distribution of the measurements that will be distorted. MNAR includes the possibility that the scale produces more missing values for the heavier objects (as above), a situation that might be difficult to recognize and handle.\nSource: https://stefvanbuuren.name/fimd/sec-MCAR.html\n\n\n\n\n\n\nIn which of the above scenarios can we ignore the observations corresponding to missing values without the risk of skewing the analysis/trends in the data?\n\n\n\nIn which of the above scenarios will it be the more risky to impute or estimate missing values?\n\n\n\nFor the datset consisting of GDP per capita, think of hypothetical scenarios in which the missing values of GDP per capita can correspond to MCAR / MAR / MNAR.\n\n\n\n\nSometimes our analysis requires that there should be no missing values in the dataset. For example, while building statistical models, we may require the values of all the predictor variables. The quickest way is to use the dropna() method, which drops the observations that even have a single missing value, and leaves only complete observations in the data.\nLet us drop the rows containing even a single value from gdp_missing_values_data.\n\ngdp_no_missing_data = gdp_missing_values_data.dropna()\n\n\n#Shape of gdp_no_missing_data\ngdp_no_missing_data.shape\n\n(42, 12)\n\n\nDropping rows with even a single missing value has reduced the number of rows from 155 to 42! However, earlier we saw that all the columns except contraception had at most 10 missing values. Removing all rows / columns with even a single missing value results in loss of data that is non-missing in the respective rows/columns. Thus, it is typically a bad idea to drop observations with even a single missing value, except in cases where we have a very small number of missing-value observations.\nIf a few values of a column are missing, we can possibly estimate them using the rest of the data, so that we can (hopefully) maximize the information that can be extracted from the data. However, if most of the values of a column are missing, it may be harder to estimate its values.\nIn this case, we see that around 50% values of the contraception column is missing. Thus, we’ll drop the column as it may be hard to impute its values based on a relatively small number of non-missing values.\n\n#Deleting column with missing values in almost half of the observations\ngdp_missing_values_data.drop(['contraception'],axis=1,inplace=True)\ngdp_missing_values_data.shape\n\n(155, 11)\n\n\n\n\n\nThere are an unlimited number of ways to impute missing values. Some imputation methods are provided in the Pandas documentation.\nThe best way to impute them will depend on the problem, and the assumptions taken. Below are just a few examples.\n\n\nFilling the missing value of a column by copying the value of the previous non-missing observation.\n\n#Filling missing values: Method 1- Naive way\ngdp_imputed_data = gdp_missing_values_data.fillna(method = 'ffill')\n\n\n#Checking if any missing values are remaining\ngdp_imputed_data.isnull().sum()\n\neconomicActivityFemale    0\ncountry                   0\nlifeMale                  0\ninfantMortality           0\ngdpPerCapita              0\neconomicActivityMale      0\nilliteracyMale            1\nilliteracyFemale          0\nlifeFemale                0\ngeographic_location       0\ncontinent                 0\ndtype: int64\n\n\nAfter imputing missing values, note there is still one missing value for illiteracyMale. Can you guess why one missing value remained?\nLet us check how good is this method in imputing missing values. We’ll compare the imputed values of gdpPerCapita with the actual values. Recall that we had randomly put some missing values in gdp_missing_values_data, and we have the actual values in gdp_complete_data.\n\n#Index of rows with missing values for GDP per capita\nnull_ind_gdpPC = gdp_missing_values_data.index[gdp_missing_values_data.gdpPerCapita.isnull()]\n\n\n#Defining a function to plot the imputed values vs actual values \ndef plot_actual_vs_predicted():\n    fig, ax = plt.subplots(figsize=(8, 6))\n    plt.rc('xtick', labelsize=15) \n    plt.rc('ytick', labelsize=15) \n    x = gdp_complete_data.loc[null_ind_gdpPC,'gdpPerCapita']\n    y = gdp_imputed_data.loc[null_ind_gdpPC,'gdpPerCapita']\n    plt.scatter(x,y)\n    z=np.polyfit(x,y,1)\n    p=np.poly1d(z)\n    plt.plot(x,x,color='orange')\n    plt.xlabel('Actual GDP per capita',fontsize=20)\n    plt.ylabel('Imputed GDP per capita',fontsize=20)\n    ax.xaxis.set_major_formatter('${x:,.0f}')\n    ax.yaxis.set_major_formatter('${x:,.0f}')\n    rmse = np.sqrt(((x-y).pow(2)).mean())\n    print(\"RMSE=\",rmse)\n\n\n#Plot comparing imputed values with actual values, and computing the Root mean square error (RMSE) of the imputed values\nplot_actual_vs_predicted()\n\nRMSE= 34843.91091137732\n\n\n\n\n\nWe observe that the accuracy of imputation is poor as GDP per capita can vary a lot across countries, and the data is not sorted by GDP per capita. There is no reason why the GDP per capita of a country should be close to the GDP per capita of the country in the observation above it.\n\n\n\nLet us impute missing values in the column as the average of the non-missing values of the column. The sum of squared differences between actual values and the imputed values is likely to be smaller if we impute using the mean. However, this may not be true in cases other than MCAR (Missing completely at random).\n\n#Filling missing values: Method 2\ngdp_imputed_data = gdp_missing_values_data.fillna(gdp_missing_values_data.mean())\n\n\nplot_actual_vs_predicted()\n\nRMSE= 30793.549983587087\n\n\n\n\n\nAlthough this method of imputation doesn’t seem impressive, the RMSE of the estimates is lower than that of the naive method. Since we had introduced missing values randomly in gdp_missing_values_data, the mean GDP per capita will be the closest constant to the GDP per capita values, in terms of squared error.\n\n\n\nIf a variable is highly correlated with another variable in the dataset, we can approximate its missing values using the trendline with the highly correlated variable.\nLet us visualize the distribution of GDP per capita for different continents.\n\nplt.rcParams[\"figure.figsize\"] = (12,6)\nsns.boxplot(x = 'continent',y='gdpPerCapita',data = gdp_missing_values_data)\n\n<AxesSubplot:xlabel='continent', ylabel='gdpPerCapita'>\n\n\n\n\n\nWe observe that there is a distinct difference between the GDPs per capita of some of the contents. Let us impute the missing GDP per capita of a country as the mean GDP per capita of the corresponding continent. This imputation should be better than imputing the missing GDP per capita as the mean of all the non-missing values, as the GDP per capita of a country is likely to be closer to the mean GDP per capita of the continent, rather the mean GDP per capita of the whole world.\n\n#Finding the mean GDP per capita of the continent - please differ the understanding of this code to chapter 9.\navg_gdpPerCapita = gdp_missing_values_data['gdpPerCapita'].groupby(gdp_missing_values_data['continent']).mean()\navg_gdpPerCapita\n\ncontinent\nAfrica            7638.178571\nAsia             25922.750000\nEurope           45455.303030\nNorth America    19625.210526\nOceania          15385.857143\nSouth America    15360.909091\nName: gdpPerCapita, dtype: float64\n\n\n\n#Creating a copy of missing data to impute missing values\ngdp_imputed_data = gdp_missing_values_data.copy()\n\n\n#Replacing missing GDP per capita with the mean GDP per capita for the corresponding continent\nfor i in gdp_imputed_data.continent.unique():    \n    ind_miss = np.where(gdp_imputed_data.index.isin(null_ind_gdpPC) & gdp_imputed_data.continent.isin([i]))\n    gdp_imputed_data.iloc[ind_miss[0],4] = avg_gdpPerCapita[i]\ngdp_imputed_data\nplot_actual_vs_predicted()\n\nRMSE= 25473.20645170116\n\n\n\n\n\nNote that the imputed values are closer to the actual values, and the RMSE has further reduced as expected.\n\n\n\nFind the numeric variable most strongly correlated with GDP per capita, and use it to impute its missing values. Find the RMSE of the imputed values.\n\n\n\nIn this method, we’ll impute the missing value of the variable as the mean value of the \\(K\\)-nearest neighbors having non-missing values for that variable. The neighbors to a data-point are identified based on their Euclidean distance to the point in terms of the standardized values of rest of the variables in the data.\n\n#Toy example\nnan = np.nan\nX = np.array([[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]])\nX\n\narray([[ 1.,  2., nan],\n       [ 3.,  4.,  3.],\n       [nan,  6.,  5.],\n       [ 8.,  8.,  7.]])\n\n\n\nfrom sklearn import metrics\nfrom sklearn import impute\n\n\n#This is the distance matrix containng the distance of the ith observation from the jth observation at the (i,j) position in the matrix\nmetrics.pairwise.nan_euclidean_distances(X,X)\n\narray([[ 0.        ,  3.46410162,  6.92820323, 11.29158979],\n       [ 3.46410162,  0.        ,  3.46410162,  7.54983444],\n       [ 6.92820323,  3.46410162,  0.        ,  3.46410162],\n       [11.29158979,  7.54983444,  3.46410162,  0.        ]])\n\n\n\n#imputing missing values with 2 nearest neighbors, where the neighbors have equal weights\nimputer = impute.KNNImputer(n_neighbors=2)\nimputer.fit_transform(X)\n\narray([[1. , 2. , 4. ],\n       [3. , 4. , 3. ],\n       [5.5, 6. , 5. ],\n       [8. , 8. , 7. ]])\n\n\n\n#Considering numeric columns in the data to use KNN\nnum_cols = list(range(0,1))+list(range(2,9))\nnum_cols\n\n[0, 2, 3, 4, 5, 6, 7, 8]\n\n\n\n#Scaling data to compute equally weighted distances from the 'k' nearest neighbors\nscaler = sk.preprocessing.MinMaxScaler()\nscaled_data = pd.DataFrame(scaler.fit_transform(gdp_missing_values_data.iloc[:,num_cols]))\nscaled_data\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n    \n  \n  \n    \n      0\n      0.059752\n      0.217391\n      0.909639\n      0.013954\n      0.868421\n      NaN\n      0.936123\n      0.157534\n    \n    \n      1\n      0.066516\n      0.760870\n      0.246988\n      0.087408\n      0.602871\n      0.370213\n      0.561674\n      0.712329\n    \n    \n      2\n      0.444194\n      0.811594\n      0.114458\n      NaN\n      0.598086\n      0.053901\n      0.041850\n      0.860731\n    \n    \n      3\n      0.564825\n      0.753623\n      0.132530\n      0.105487\n      0.330144\n      NaN\n      0.005507\n      0.796804\n    \n    \n      4\n      0.585118\n      NaN\n      0.018072\n      0.443715\n      NaN\n      0.014184\n      0.011013\n      0.961187\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      150\n      0.872604\n      0.712560\n      0.210843\n      0.015365\n      0.894737\n      0.495234\n      0.510661\n      0.694064\n    \n    \n      151\n      0.443067\n      0.821256\n      0.108434\n      0.036124\n      0.739234\n      0.116312\n      0.106828\n      NaN\n    \n    \n      152\n      0.813980\n      NaN\n      0.204819\n      0.089409\n      0.727273\n      0.049645\n      0.096916\n      0.696347\n    \n    \n      153\n      0.000000\n      0.516908\n      0.463855\n      0.009445\n      0.703349\n      0.459660\n      0.765991\n      0.440639\n    \n    \n      154\n      0.505073\n      0.280193\n      0.391566\n      0.015791\n      0.633971\n      0.136170\n      0.221366\n      NaN\n    \n  \n\n155 rows × 8 columns\n\n\n\n\n#Imputed scaled array\nimputer = impute.KNNImputer(n_neighbors=3, weights=\"uniform\")\nimputed_arr = imputer.fit_transform(scaled_data)\n\n\n#Scaling back the scaled array to obtain the data at the actual scale\nunscaled_data = scaler.inverse_transform(imputed_arr)\n\n\n#Imputed missing values\ngdp_imputed_data = gdp_missing_values_data.copy()\ngdp_imputed_data.loc[:,'gdpPerCapita'] = unscaled_data[:,3]\n\n\nplot_actual_vs_predicted()\n\nRMSE= 16804.195967740387\n\n\n\n\n\nNote that the RMSE is the lowest in this method. It is because this method imputes missing values as the weighted average of the values of “similar” observations, which is smarter and more robust than the previous methods.\n\n\n\n\nRubin, Donald B. 1976. “Inference and Missing Data.” Biometrika 63 (3): 581–92."
  },
  {
    "objectID": "Data wrangling.html",
    "href": "Data wrangling.html",
    "title": "8  Data wrangling",
    "section": "",
    "text": "Data wrangling"
  },
  {
    "objectID": "Data aggregation.html",
    "href": "Data aggregation.html",
    "title": "9  Data aggregation",
    "section": "",
    "text": "Data aggregation"
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "10  Datasets",
    "section": "",
    "text": "Datasets used in the book can be found here"
  },
  {
    "objectID": "Assignment 4 (Data Visualization).html",
    "href": "Assignment 4 (Data Visualization).html",
    "title": "11  Assignment 4 (Data Visualization)",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 4th November 2022 at 11:59 pm.\nSome questions in this assignment do not have a single correct answer. As data visualization is subject to interpretation, any logically sound answer / explanation is acceptable.\nThere is a bonus question worth 30 points. However, there is no partial credit for the bonus question. You will get 30 or 0. If everything is correct, you can score 130 out of 100 in the assignment."
  },
  {
    "objectID": "Assignment 4 (Data Visualization).html#time-trend",
    "href": "Assignment 4 (Data Visualization).html#time-trend",
    "title": "11  Assignment 4 (Data Visualization)",
    "section": "11.1 Time trend",
    "text": "11.1 Time trend\nLet us analyze if the profitability of a movie is associated with the time of its release.\n\n11.1.1 Month of release\n\n11.1.1.1 \nMake an appropriate plot to visualize the mean profit of movies released each month.\nHint:\n\nUse the Pandas function to_datetime() to convert Release Date to a datetime datatype.\nUse the library datetime to extract the month from Release Date.\n\n(6 points)\n\n\n11.1.1.2 \nBased on the plot, which seasons have been the most and least profitable (on an average) for a movie release. Don’t worry about the exact start and end date of seasons. Don’t perform any computations. Just make comments based on the plot. You can use seasons such as early summer, late spring etc.\n(2 points)\n\n\n\n11.1.2 Month of release with number of movies in each genre\n\n11.1.2.1 \nNow that we know the most profitable season for releasing movies, let us visualize if some genres are more popular during certain seasons.\nUse the code below to create a new column called genre.\n\n\nCode\n#Combining Major Genre\nmovies_data['genre'] = movies_data['Major Genre'].apply(lambda x:'Comedy' if x!=None and 'Comedy' in x else 'Horror' if x!=None and 'Thriller' in x else 'Action/Adventure' if x!=None and ('Action' in x or 'Adventure' in x) else 'Musical/Western' if x!=None and ('Musical' in x or 'Western' in x or 'Concert' in x) else x)\n\n\nMake an appropriate plot to visualize the number of movies released for each genre in each calendar month.\n(8 points)\nHint:\n\nUse barplot() with estimator as len\nUse the hue argument\n\n\n\n11.1.2.2 \nBased on the above plot, which genre is the most popular during the most profitable season of release? And which genre is the most popular during the least profitable season of release?\n(2 points)\n\n\n\n11.1.3 Month of release with proportion of movies in each genre\n\n11.1.3.1 \nVisualize the proportion of movies in each genre for each month of release.\nUse the code below to re-arrange your data that will help with creating the visualization\n\n\nCode\ngenre_proportion_release_month = pd.crosstab(index=movies_data['release_month'],\n                             columns=movies_data['genre'],\n                             normalize=\"index\")\ngenre_proportion_release_month.head()\n\n\nHint:\n\nMake a 100% stacked barplot with the Pandas plot() function\nUse the argument bbox_to_anchor with the Matplotlib function legend() to place the legend outside the plot area.\n\n(8 points)\n\n\n11.1.3.2 \nWhich genre is the most popular during the month of May, and which one is the most popular during December?\n(2 points)\n\n\n\n11.1.4 Year of release with genre\n\n11.1.4.1 \nMake an appropriate figure to visualize the average profit of movies of each genre for each year. Consider only the movies released from 1991 to 2010. Also show the 95% confidence interval in the average profit.\nHint:\n\nUse the library datetime to extract year from Release Date.\nUse the Seaborn Facetgrid() object.\nA figure can have multiple subplots. Put the figure for each genre in a separate subplot.\n\n(6 points)\n\n\n11.1.4.2 \nBased on the figure above, which genre’s profitabiltiy seems to be increasing over the years, and which genre has the least uncertainty in profit for most of the years.\n(2 points)"
  },
  {
    "objectID": "Assignment 4 (Data Visualization).html#associations",
    "href": "Assignment 4 (Data Visualization).html#associations",
    "title": "11  Assignment 4 (Data Visualization)",
    "section": "11.2 Associations",
    "text": "11.2 Associations\n\n11.2.1 Pairplot / heatmap\n\n11.2.1.1 \nMake a pairplot and heatmap of all the continuous variables in the data.\n(8 points)\n\n\n11.2.1.2 \nAre there any trends that you can see in the pairplot, but not in the heatmap?\n(2 points)\n\n\n11.2.1.3 \nBased on the plots in 2(a)(i), which variables are associated with profit?\n(2 points)\n\n\n11.2.1.4 \nAmong the variables listed in 2(a)(iii), select a subset of variables such that none of them are highly associated with each other. The rest of the variables identified in 2(a)(iii) are redundant with regard to association with profit.\n(2 points)\n\n\n\n11.2.2 Nested associations\n\n11.2.2.1 \nUse the code below to create some new columns.\n\n\nCode\nmovies_data['screenplay'] = movies_data.Source.apply(lambda x:'Non-original' if x!='Original Screenplay' else x)\nmovies_data['rating'] = movies_data['MPAA Rating'].apply(lambda x:'R rated' if x=='R' else 'Not R rated')\nmovies_data['fiction'] = movies_data['Creative Type'].apply(lambda x:'Contemporary' if x=='Contemporary Fiction' else 'other')\n\n\nMake an appropriate figure to visualize the association of the number of IMDB votes with profit for each genre (use the variable genre). Which genre has the highest association between profit and IMDB votes?\n(8 points)\n\n\n11.2.2.2 \nMake an appropriate figure to visualize the association between the number of IMDB votes and profit, for each combination of the fiction type (use the variable fiction) and the movie rating (use the variable rating).\nFor which combination of fiction and rating categories do you observe the highest association between IMDB votes and profit?\n(8 points)\nHint: Use row and col attributes of the Seaborn Facetgrid() object.\n\n\n\n11.2.3 Profit based on movie director\n\n11.2.3.1 \nConsider the directors who have directed more than 10 movies (based on the dataset). Make a horizontal barplot that shows the mean profit of the movies of these directors along with the 95% confidence interval. Sort the bars of the barplot such that the director with the highest mean profit is at the top.\nIf the dataset director_with_more_than_10_movies has only those movies that correspond to directors with more than 10 movies, then the following code will give you the order in which the names of the directors must appear in the barplot:\n(8 points)\n\n\nCode\ndirector_with_more_than_10_movies[['Director','profit']].groupby('Director').mean().sort_values(by = 'profit',\n                                            ascending= False).index.to_list()\n\n\n\n\n11.2.3.2 \nBased on the above plot, which director has the highest mean profitability, and which one has the highest variation in profitability?\n(2 points)"
  },
  {
    "objectID": "Assignment 4 (Data Visualization).html#distributions",
    "href": "Assignment 4 (Data Visualization).html#distributions",
    "title": "11  Assignment 4 (Data Visualization)",
    "section": "11.3 Distributions",
    "text": "11.3 Distributions\n\n11.3.1 Distribution of profit based on genre (boxplots)\n\n11.3.1.1 \nMake boxplots to visualize the distribution of profit based on genre. Based on the plot, which genre has the most profitable movies?\n(6 points)\n\n\n11.3.1.2 \nWhich genre has the most variation in profit, and which one has the least?\n(2 points)\n\n\n\n11.3.2 Distribution of profit based on genre (density plots)\n\n11.3.2.1 \nMake density plots of profit based on genre. Adjust the limit on the horizonal axis, so that the plots are clearly visible.\n(6 points)\n\n\n11.3.2.2 \nWhat additional insight / trend can you seen in the above plot that you cannot see with the boxplots?\n(2 points)"
  },
  {
    "objectID": "Assignment 4 (Data Visualization).html#insights",
    "href": "Assignment 4 (Data Visualization).html#insights",
    "title": "11  Assignment 4 (Data Visualization)",
    "section": "11.4 Insights",
    "text": "11.4 Insights\nFrom all the visualizations above, describe the insights you get about the factors associated with the profitability of a movie.\nAlso, elaborate on the extent to which these trends can be generalized. For example, comment on whether these trends be generalized to the current time and all the Hollywood movies? If not, is there any time period or type of movie to which these trends can be applicable?\n(4+ 4 points)"
  },
  {
    "objectID": "Assignment 4 (Data Visualization).html#return-on-investment",
    "href": "Assignment 4 (Data Visualization).html#return-on-investment",
    "title": "11  Assignment 4 (Data Visualization)",
    "section": "11.5 Return on investment",
    "text": "11.5 Return on investment\n\n11.5.1 Impatient investor (daily)\nSuppose there is an investor who only holds the index for a single day (buy yesterday sell today).\nBased on the data,\n\n11.5.1.1 \nShow the histogram graph for all the possible returns.\n\n\n11.5.1.2 \nWhat is the expected return, risk and sharpe ratio?\n\n\n11.5.1.3 \nIs the return significantly greater than zero (a.k.a positive return) (use a threshold 0.01 for \\(p\\)-value) ?\nHINT: use scipy.stats.ttest_1samp to do one-sided mean test. (We ignore the fact that T-test requires the data are sampled from a population of normal distribution, which might not be true in this exercise)\n(6 points)\n\n\n\n11.5.2 Patient Investor (yearly)\nSuppose there is an investor who will hold the index for a year (suppose there are 250 trading days in a year). Do the same analysis as the above:\n\n11.5.2.1 \nShow the histogram graph for all the possible returns.\n\n\n11.5.2.2 \nWhat is the expected return, risk and sharpe ratio?\n\n\n11.5.2.3 \nIs the return significantly greater than zero (a.k.a positive return) (use a threshold 0.01 for \\(p\\)-value) ?\n(6 points)\n\n\n\n11.5.3 From daily to yearly\nExplore how the expected return/risk/shape ratio change as we increase our holding period from 1 day to 1 year(250 days).\nShow/answer:\n\nAt least how many days do you need to hold the index in order to make a significant positive return (threshold 0.01)?\nHow are the returns associated with the risks for different investment strategies?\n\nMake a graph as shown below:\n(18 points)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Rubin, Donald B. 1976. “Inference and Missing Data.”\nBiometrika 63 (3): 581–92."
  }
]