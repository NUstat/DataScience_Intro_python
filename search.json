[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science with Python",
    "section": "",
    "text": "This book is developed for the course STAT303-1 (Data Science with Python-1). The first two chapters of the book are a review of python, and will be covered very quickly. Students are expected to know the contents of these chapters beforehand, or be willing to learn it quickly. The core part of the course begins from the third chapter - Reading data.\nNote that this book is still being edited. Please let the instructors know in case of any typos/mistakes/general feedback."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html",
    "href": "Introduction to Python and Jupyter Notebooks.html",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "",
    "text": "This chapter is a very brief introduction to python and Jupyter notebooks. We only discuss the content relevant for applying python to analyze data."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#installation",
    "href": "Introduction to Python and Jupyter Notebooks.html#installation",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "1.1 Installation",
    "text": "1.1 Installation\nAnaconda: If you are new to python, we recommend downloading the Anaconda installer and following the instructions for installation. Once installed, we’ll use the Jupyter Notebook interface to write code.\nQuarto: We’ll use Quarto to publish the .ipynb file containing text, python code, and the output. Download and install Quarto from here."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#jupyter-notebook",
    "href": "Introduction to Python and Jupyter Notebooks.html#jupyter-notebook",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "1.2 Jupyter notebook",
    "text": "1.2 Jupyter notebook\n\n1.2.1 Introduction\nJupyter notebook is an interactive platform, where you can write code and text, and make visualizations. You can access Jupyter notebook from the Anaconda Navigator, or directly open the Jupyter Notebook application itself. It should automatically open up in your default browser. The figure below shows a Jupyter Notebook opened with Google Chrome. This page is called the landing page of the notebook.\n\n\n\n\n\nTo create a new notebook, click on the New button and select the Python 3 option. You should see a blank notebook as in the figure below.\n\n\n\n\n\n\n\n1.2.2 Writing and executing code\nCode cell: By default, a cell is of type Code, i.e., for typing code, as seen as the default choice in the dropdown menu below the Widgets tab. Try typing a line of python code (say, 2+3) in an empty code cell and execute it by pressing Shift+Enter. This should execute the code, and create an new code cell. Pressing Ctlr+Enter for Windows (or Cmd+Enter for Mac) will execute the code without creating a new cell.\nCommenting code in a code cell: Comments should be made while writing the code to explain the purpose of the code or a brief explanation of the tasks being performed by the code. A comment can be added in a code cell by preceding it with a # sign. For example, see the comment in the code below.\nWriting comments will help other users understand your code. It is also useful for the coder to keep track of the tasks being performed by their code.\n\n#This code adds 3 and 5\n3+5\n\n8\n\n\nMarkdown cell: Although a comment can be written in a code cell, a code cell cannot be used for writing headings/sub-headings, and is not appropriate for writing lengthy chunks of text. In such cases, change the cell type to Markdown from the dropdown menu below the Widgets tab. Use any markdown cheat sheet found online, for example, this one to format text in the markdown cells.\nGive a name to the notebook by clicking on the text, which says ‘Untitled’.\n\n\n1.2.3 Saving and loading notebooks\nSave the notebook by clicking on File, and selecting Save as, or clicking on the Save and Checkpoint icon (below the File tab). Your notebook will be saved as a file with an extension ipynb. This file will contain all the code as well as the outputs, and can be loaded and edited by a Jupyter user. To load an existing Jupyter notebook, navigate to the folder of the notebook on the landing page, and then click on the file to open it.\n\n\n1.2.4 Rendering notebook as HTML\nWe’ll use Quarto to print the **.ipynb* file as HTML. Check the procedure for rendering a notebook as HTML here. You have several options to format the file.\nYou will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#in-class-exercise",
    "href": "Introduction to Python and Jupyter Notebooks.html#in-class-exercise",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "In-class exercise",
    "text": "In-class exercise\n\nCreate a new notebook.\nSave the file as In_class_exercise1.\nGive a heading to the file - First HTML file.\nPrint Today is day 1 of class.\nCompute and print the number of hours of this course in the quarter (that will be 10 weeks x 2 classes per week x 1.33 hours per class).\n\nThe HTML file should look like the picture below."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#python-language-basics",
    "href": "Introduction to Python and Jupyter Notebooks.html#python-language-basics",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "1.3 Python language basics",
    "text": "1.3 Python language basics\n\n1.3.1 Object Oriented Programming\nPython is an object-oriented programming language. In layman terms, it means that every number, string, data structure, function, class, module, etc., exists in the python interpreter as a python object. An object may have attributes and methods associated with it. For example, let us define a variable that stores an integer:\n\nvar = 2\n\nThe variable var is an object that has attributes and methods associated with it. For example a couple of its attributes are real and imag, which store the real and imaginary parts respectively, of the object var:\n\nprint(\"Real part of 'var': \",var.real)\nprint(\"Real part of 'var': \",var.imag)\n\nReal part of 'var':  2\nReal part of 'var':  0\n\n\nAttribute: An attribute is a value associated with an object, defined within the class of the object.\nMethod: A method is a function associated with an object, defined within the class of the object, and has access to the attributes associated with the object.\nFor looking at attributes and methods associated with an object, say obj, press tab key after typing obj..\nConsider the example below of a class example_class:\n\nclass example_class:\n    class_name = 'My Class'\n    def my_method(self):\n        print('Hello World!')\n\ne = example_class()\n\nIn the above class, class_name is an attribute, while my_method is a method.\n\n\n1.3.2 Assigning variable name to object\nWhen an object is assigned to a variable name, the variable name serves as a reference to the object. For example, consider the following assignment:\n\nx = [5,3]\n\nThe variable name x is a reference to the memory location where the object [5, 3] is stored. Now, suppose we assign x to a new variable y:\n\ny = x\n\nIn the above statement the variable name y now refers to the same object [5,3]. The object [5,3] does not get copied to a new memory location referred by y. To prove this, let us add an element to y:\n\ny.append(4)\nprint(y)\n\n[5, 3, 4]\n\n\n\nprint(x)\n\n[5, 3, 4]\n\n\nWhen we changed y, note that x also changed to the same object, showing that x and y refer to the same object, instead of referring to different copies of the same object.\n\n\n1.3.3 Importing libraries\nThere are several built-in functions in python like print(), abs(), max(), sum() etc., which do not require importing any library. However, these functions will typically be insufficient for a analyzing data. Some of the popular libraries and their primary purposes are as follows:\n\nNumPy: Performing numerical operations and efficiently storing numerical data.\nPandas: Reading, cleaning and manipulating data.\nMatplotlib, Seaborn: Visualizing data.\nSciPy: Performing scientific computing such as solving differential equations, optimization, statistical tests, etc.\nScikit-learn: Data pre-processing and machine learning, with a focus on prediction.\nStatsmodels: Developing statistical models with a focus on inference\n\nA library can be imported using the import keyword. For example, a NumPy library can be imported as:\n\nimport numpy as np\n\nUsing the as keyboard, the NumPy library has been given the name np. All the functions and attributes of the library can be called using the ‘np.’ prefix. For example, let us generate a sequence of whole numbers upto 10 using the NumPy function arange():\n\nnp.arange(8)\n\narray([0, 1, 2, 3, 4, 5, 6, 7])\n\n\n\n\n1.3.4 Built-in objects\nThere are several built-in objects, modules and functions in python. Below are a few examples:\nScalar objects: Python has some built-in datatypes for handling scalar objects such as number, string, boolean values, and date/time. The built-in function type() function can be used to determine the datatype of an object:\n\nvar = 2.2\ntype(var)\n\nfloat\n\n\nrange(): The range() function returns a sequence of evenly-spaced integer values. It is commonly used in for loops to define the sequence of elements over which the iterations are performed.\nBelow is an example where the range() function is used to create a sequence of whole numbers upto 10:\n\nprint(list(range(1,10)))\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nDate time: Python as a built-in datetime module for handling date/time objects:\n\nimport datetime as dt\n\n\n#Defining a date-time object \ndt_object = dt.datetime(2022, 9, 20, 11,30,0)\n\nInformation about date and time can be accessed with the relevant attribute of the datetime object.\n\ndt_object.day\n\n20\n\n\n\ndt_object.year\n\n2022\n\n\nThe strftime method of the datetime module formats a datetime object as a string. There are several types of formats for representing date as a string:\n\ndt_object.strftime('%m/%d/%Y')\n\n'09/20/2022'\n\n\n\ndt_object.strftime('%m/%d/%y %H:%M')\n\n'09/20/22 11:30'\n\n\n\ndt_object.strftime('%h-%d-%Y')\n\n'Sep-20-2022'\n\n\n\n\n1.3.5 Control flow\nAs in other languages, python has built-in keywords that provide conditional flow of control in the code.\nIf-elif-else: The if-elif-else statement can check several conditions, and execute the code corresponding to the condition that is true. Note that there can be as many elif statements as required.\n\n#Example of if-elif-else\nx = 5\nif x>0:\n    print(\"x is positive\")\nelif x==0:\n    print(\"x is zero\")\nelse:\n    print(\"X is negative\")\n    print(\"This was the last condition checked\")\n\nx is positive\n\n\nfor loop: A for loop iterates over the elements of an object, and executes the statements within the loop in each iteration. For example, below is a for loop that prints odd natural numbers upto 10:\n\nfor i in range(10):\n    if i%2!=0:\n        print(i)\n\n1\n3\n5\n7\n9\n\n\nwhile loop: A while loop iterates over a set of statements while a condition is satisfied. For example, below is a while loop that prints odd numbers upto 10:\n\ni=0\nwhile i<10:\n    if i%2!=0:\n        print(i)\n    i=i+1\n\n1\n3\n5\n7\n9"
  },
  {
    "objectID": "data_structures.html",
    "href": "data_structures.html",
    "title": "2  Data structures",
    "section": "",
    "text": "In this chapter we’ll learn about the python data structures that are often used or appear while analyzing data."
  },
  {
    "objectID": "data_structures.html#tuple",
    "href": "data_structures.html#tuple",
    "title": "2  Data structures",
    "section": "2.1 Tuple",
    "text": "2.1 Tuple\nTuple is a sequence of python objects, with two key characteristics: (1) the number of objects are fixed, and (2) the objects are immutable, i.e., they cannot be changed.\nTuple can be defined as a sequence of python objects separated by commas, and enclosed in rounded brackets (). For example, below is a tuple containing three integers.\n\ntuple_example = (2,7,4)\n\nWe can check the data type of a python object using the in-built python function type(). Let us check the data type of the object tuple_example.\n\ntype(tuple_example)\n\ntuple\n\n\nElements of a tuple can be extracted using their index within square brackets. For example the second element of the tuple tuple_example can be extracted as follows:\n\ntuple_example[1]\n\n7\n\n\nNote that an element of a tuple cannot be modified. For example, consider the following attempt in changing the second element of the tuple tuple_example.\n\ntuple_example[1] = 8\n\nTypeError: 'tuple' object does not support item assignment\n\n\nThe above code results in an error as tuple elements cannot be modified.\n\n2.1.1 Concatenating tuples\nTuples can be concatenated using the + operator to produce a longer tuple:\n\n(2,7,4) + (\"another\", \"tuple\") + (\"mixed\",\"datatypes\",5)\n\n(2, 7, 4, 'another', 'tuple', 'mixed', 'datatypes', 5)\n\n\nMultiplying a tuple by an integer results in repetition of the tuple:\n\n(2,7,\"hi\") * 3\n\n(2, 7, 'hi', 2, 7, 'hi', 2, 7, 'hi')\n\n\n\n\n2.1.2 Unpacking tuples\nIf tuples are assigned to an expression containing multiple variables, the tuple will be unpacked and each variable will be assigned a value as per the order in which it appears. See the example below.\n\nx,y,z  = (4.5, \"this is a string\", ((\"Nested tuple\",5)))\n\n\nx\n\n4.5\n\n\n\ny\n\n'this is a string'\n\n\n\nz\n\n('Nested tuple', 5)\n\n\nIf we are interested in retrieving only some values of the tuple, the expression *_ can be used to discard the other values. Let’s say we are interested in retrieving only the first and the last two values of the tuple:\n\nx,*_,y,z  = (4.5, \"this is a string\", ((\"Nested tuple\",5)),\"99\",99)\n\n\nx\n\n4.5\n\n\n\ny\n\n'99'\n\n\n\nz\n\n99\n\n\n\n\n2.1.3 Tuple methods\nA couple of useful tuple methods are count, which counts the occurrences of an element in the tuple and index, which returns the position of the first occurrence of an element in the tuple:\n\ntuple_example = (2,5,64,7,2,2)\n\n\ntuple_example.count(2)\n\n3\n\n\n\ntuple_example.index(2)\n\n0\n\n\nNow that we have an idea about tuple, let us try to think where it can be used."
  },
  {
    "objectID": "data_structures.html#list",
    "href": "data_structures.html#list",
    "title": "2  Data structures",
    "section": "2.2 List",
    "text": "2.2 List\nList is a sequence of python objects, with two key characeterisics that differentiates it from tuple: (1) the number of objects are variable, i.e., objects can be added or removed from a list, and (2) the objects are mutable, i.e., they can be changed.\nList can be defined as a sequence of python objects separated by commas, and enclosed in square brackets []. For example, below is a list consisting of three integers.\n\nlist_example = [2,7,4]\n\n\n2.2.1 Adding and removing elements in a list\nWe can add elements at the end of the list using the append method. For example, we append the string ‘red’ to the list list_example below.\n\nlist_example.append('red')\n\n\nlist_example\n\n[2, 7, 4, 'red']\n\n\nNote that the objects of a list or a tuple can be of different datatypes.\nAn element can be added at a specific location of the list using the insert method. For example, if we wish to insert the number 2.32 as the second element of the list list_example, we can do it as follows:\n\nlist_example.insert(1,2.32)\n\n\nlist_example\n\n[2, 2.32, 7, 4, 'red']\n\n\nFor removing an element from the list, the pop and remove methods may be used. The pop method removes an element at a particular index, while the remove method removes the element’s first occurence in the list by its value. See the examples below.\nLet us say, we need to remove the third element of the list.\n\nlist_example.pop(2)\n\n7\n\n\n\nlist_example\n\n[2, 2.32, 4, 'red']\n\n\nLet us say, we need to remove the element ‘red’.\n\nlist_example.remove('red')\n\n\nlist_example\n\n[2, 2.32, 4]\n\n\n\n#If there are multiple occurences of an element in the list, the first occurence will be removed\nlist_example2 = [2,3,2,4,4]\nlist_example2.remove(2)\nlist_example2\n\n[3, 2, 4, 4]\n\n\nFor removing multiple elements in a list, either pop or remove can be used in a for loop, or a for loop can be used with a condition. See the examples below.\nLet’s say we need to remove intergers less than 100 from the following list.\n\nlist_example3 = list(range(95,106))\nlist_example3\n\n[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]\n\n\n\n#Method 1: For loop with remove\nlist_example3_filtered = list(list_example3) #\nfor element in list_example3:\n    if element<100:\n        list_example3_filtered.remove(element)\nprint(list_example3_filtered)\n\n[100, 101, 102, 103, 104, 105]\n\n\n\\(\\color{red}{\\text{Q1}}\\): What’s the need to define a new variable list\\_example3\\_filtered in the above code?\n\\(\\color{blue}{\\text{A1}}\\): Replace list_example3_filtered with list_example3 and identify the issue.\n\n#Method 2: For loop with condition\n[element for element in list_example3 if element>100]\n\n[101, 102, 103, 104, 105]\n\n\n\n\n2.2.2 List comprehensions\nList comprehension is a compact way to create new lists based on elements of an existing list.\nExample: Create a list that has squares of natural numbers from 5 to 15.\n\nsqrt_natural_no_5_15 = [(x**2) for x in range(5,16)]\nprint(sqrt_natural_no_5_15)\n\n[25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225]\n\n\nExample: Create a list of tuples, where each tuple consists of a natural number and its square, for natural numbers ranging from 5 to 15.\n\nsqrt_natural_no_5_15 = [(x,x**2) for x in range(5,16)]\nprint(sqrt_natural_no_5_15)\n\n[(5, 25), (6, 36), (7, 49), (8, 64), (9, 81), (10, 100), (11, 121), (12, 144), (13, 169), (14, 196), (15, 225)]\n\n\n\n\n2.2.3 Practice exercise 1\nBelow is a list consisting of responses to the question: “At what age do you think you will marry?” from students of the STAT303-1 Fall 2022 class.\n\nexp_marriage_age=['24','30','28','29','30','27','26','28','30+','26','28','30','30','30','probably never','30','25','25','30','28','30+ ','30','25','28','28','25','25','27','28','30','30','35','26','28','27','27','30','25','30','26','32','27','26','27','26','28','37','28','28','28','35','28','27','28','26','28','26','30','27','30','28','25','26','28','35','29','27','27','30','24','25','29','27','33','30','30','25','26','30','32','26','30','30','I wont','25','27','27','25','27','27','32','26','25','never','28','33','28','35','25','30','29','30','31','28','28','30','40','30','28','30','27','by 30','28','27','28','30-35','35','30','30','never','30','35','28','31','30','27','33','32','27','27','26','N/A','25','26','29','28','34','26','24','28','30','120','25','33','27','28','32','30','26','30','30','28','27','27','27','27','27','27','28','30','30','30','28','30','28','30','30','28','28','30','27','30','28','25','never','69','28','28','33','30','28','28','26','30','26','27','30','25','Never','27','27','25']\n\nUse list comprehension to:\n\n2.2.3.1 \nRemove the elements that are not integers - such as ‘probably never’, ‘30+’, etc. What is the length of the new list?\nHint: The built-in python function of the str class - isdigit() may be useful to check if the string contains only digits.\nSolution:\n\nexp_marriage_age_num = [x for x in exp_marriage_age if x.isdigit()==True]\nprint(\"Length of the new list = \",len(exp_marriage_age_num))\n\nLength of the new list =  181\n\n\n\n\n2.2.3.2 \nCap the values greater than 80 to 80, in the clean list obtained in (1). What is the mean age when people expect to marry in the new list?\n\nexp_marriage_age_capped = [min(int(x),80) for x in exp_marriage_age_num]\nprint(\"Mean age when people expect to marry = \", sum(exp_marriage_age_capped)/len(exp_marriage_age_capped))\n\nMean age when people expect to marry =  28.955801104972377\n\n\n\n\n2.2.3.3 \nDetermine the percentage of people who expect to marry at an age of 30 or more.\n\nprint(\"Percentage of people who expect to marry at an age of 30 or more =\", str(100*sum([1 for x in exp_marriage_age_capped if x>=30])/len(exp_marriage_age_capped)),\"%\")\n\nPercentage of people who expect to marry at an age of 30 or more = 37.01657458563536 %\n\n\n\n\n\n2.2.4 Concatenating lists\nAs in tuples, lists can be concatenated using the + operator:\n\nimport time as tm\n\n\nlist_example4 = [5,'hi',4] \nlist_example4 = list_example4 + [None,'7',9]\nlist_example4\n\n[5, 'hi', 4, None, '7', 9]\n\n\nFor adding elements to a list, the extend method is preferred over the + operator. This is because the + operator creates a new list, while the extend method adds elements to an existing list. Thus, the extend operator is more memory efficient.\n\nlist_example4 = [5,'hi',4]\nlist_example4.extend([None, '7', 9])\nlist_example4\n\n[5, 'hi', 4, None, '7', 9]\n\n\n\n\n2.2.5 Sorting a list\nA list can be sorted using the sort method:\n\nlist_example5 = [6,78,9]\nlist_example5.sort(reverse=True) #the reverse argument is used to specify if the sorting is in ascending or descending order\nlist_example5\n\n[78, 9, 6]\n\n\n\n\n2.2.6 Slicing a list\nWe may extract or update a section of the list by passing the starting index (say start) and the stopping index (say stop) as start:stop to the index operator []. This is called slicing a list. For example, see the following example.\n\nlist_example6 = [4,7,3,5,7,1,5,87,5]\n\nLet us extract a slice containing all the elements from the the 3rd position to the 7th position.\n\nlist_example6[2:7]\n\n[3, 5, 7, 1, 5]\n\n\nNote that while the element at the start index is included, the element with the stop index is excluded in the above slice.\nIf either the start or stop index is not mentioned, the slicing will be done from the beginning or until the end of the list, respectively.\n\nlist_example6[:7]\n\n[4, 7, 3, 5, 7, 1, 5]\n\n\n\nlist_example6[2:]\n\n[3, 5, 7, 1, 5, 87, 5]\n\n\nTo slice the list relative to the end, we can use negative indices:\n\nlist_example6[-4:]\n\n[1, 5, 87, 5]\n\n\n\nlist_example6[-4:-2:]\n\n[1, 5]\n\n\nAn extra colon (‘:’) can be used to slice every \\(n\\)th element of a list.\n\n#Selecting every 3rd element of a list\nlist_example6[::3]\n\n[4, 5, 5]\n\n\n\n#Selecting every 3rd element of a list from the end\nlist_example6[::-3]\n\n[5, 1, 3]\n\n\n\n#Selecting every element of a list from the end or reversing a list \nlist_example6[::-1]\n\n[5, 87, 5, 1, 7, 5, 3, 7, 4]\n\n\n\n\n2.2.7 Practice exercise 2\nStart with the list [8,9,10]. Do the following:\n\n2.2.7.1 \nSet the second entry (index 1) to 17\n\nL = [8,9,10]\nL[1]=17\n\n\n\n2.2.7.2 \nAdd 4, 5, and 6 to the end of the list\n\nL = L+[4,5,6]\n\n\n\n2.2.7.3 \nRemove the first entry from the list\n\nL.pop(0)\n\n8\n\n\n\n\n2.2.7.4 \nSort the list\n\nL.sort()\n\n\n\n2.2.7.5 \nDouble the list (concatenate the list to itself)\n\nL=L+L\n\n\n\n2.2.7.6 \nInsert 25 at index 3\nThe final list should equal [4,5,6,25,10,17,4,5,6,10,17]\n\nL.insert(3,25)\nL\n\n[4, 5, 6, 25, 10, 17, 4, 5, 6, 10, 17]\n\n\nNow that we have an idea about lists, let us try to think where it can be used.\n\n\n\n\n\n \n        \n\n\nNow that we have learned about lists and tuples, let us compare them.\n\\(\\color{red}{\\text{Q2}}\\): A list seems to be much more flexible than tuple, and can replace a tuple almost everywhere. Then why use tuple at all?\n\\(\\color{blue}{\\text{A2}}\\): The additional flexibility of a list comes at the cost of efficiency. Some of the advantages of a tuple over a list are as follows:\n\nSince a list can be extended, space is over-allocated when creating a list. A tuple takes less storage space as compared to a list of the same length.\nTuples are not copied. If a tuple is assigned to another tuple, both tuples point to the same memory location. However, if a list is assigned to another list, a new list is created consuming the same memory space as the original list.\nTuples refer to their element directly, while in a list, there is an extra layer of pointers that refers to their elements. Thus it is faster to retrieve elements from a tuple.\n\nThe examples below illustrate the above advantages of a tuple.\n\n#Example showing tuples take less storage space than lists for the same elements\ntuple_ex = (1, 2, 'Obama')\nlist_ex = [1, 2, 'Obama']\nprint(\"Space taken by tuple =\",tuple_ex.__sizeof__(),\" bytes\")\nprint(\"Space taken by list =\",list_ex.__sizeof__(),\" bytes\")\n\nSpace taken by tuple = 48  bytes\nSpace taken by list = 64  bytes\n\n\n\n#Examples showing that a tuples are not copied, while lists can be copied\ntuple_copy = tuple(tuple_ex)\nprint(\"Is tuple_copy same as tuple_ex?\", tuple_ex is tuple_copy)\nlist_copy = list(list_ex)\nprint(\"Is list_copy same as list_ex?\",list_ex is list_copy)\n\nIs tuple_copy same as tuple_ex? True\nIs list_copy same as list_ex? False\n\n\n\n#Examples showing tuples takes lesser time to retrieve elements\nimport time as tm\ntt = tm.time()\nlist_ex = list(range(1000000)) #List containinig whole numbers upto 1 million\na=(list_ex[::-2])\nprint(\"Time take to retrieve every 2nd element from a list = \", tm.time()-tt)\n\ntt = tm.time()\ntuple_ex = tuple(range(1000000)) #tuple containinig whole numbers upto 1 million\na=(tuple_ex[::-2])\nprint(\"Time take to retrieve every 2nd element from a tuple = \", tm.time()-tt)\n\nTime take to retrieve every 2nd element from a list =  0.03579902648925781\nTime take to retrieve every 2nd element from a tuple =  0.02684164047241211"
  },
  {
    "objectID": "data_structures.html#dictionary",
    "href": "data_structures.html#dictionary",
    "title": "2  Data structures",
    "section": "2.3 Dictionary",
    "text": "2.3 Dictionary\nA dictionary consists of key-value pairs, where the keys and values are python objects. While values can be any python object, keys need to be immutable python objects, like strings, intergers, tuples, etc. Thus, a list can be a value, but not a key, as elements of list can be changed. A dictionary is defined using the keyword dict along with curly braces, colons to separate keys and values, and commas to separate elements of a dictionary:\n\ndict_example = {'USA':'Joe Biden', 'India':'Narendra Modi', 'China':'Xi Jinping'}\n\nElements of a dictionary can be retrieved by using the corresponding key.\n\ndict_example['India']\n\n'Narendra Modi'\n\n\n\n2.3.1 Adding and removing elements in a dictionary\nNew elements can be added to a dictionary by defining a key in square brackets and assiging it to a value:\n\ndict_example['Japan'] = 'Fumio Kishida'\ndict_example['Countries'] = 4\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Japan': 'Fumio Kishida',\n 'Countries': 4}\n\n\nElements can be removed from the dictionary using the del method or the pop method:\n\n#Removing the element having key as 'Countries'\ndel dict_example['Countries']\n\n\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Japan': 'Fumio Kishida'}\n\n\n\n#Removing the element having key as 'USA'\ndict_example.pop('USA')\n\n'Joe Biden'\n\n\n\ndict_example\n\n{'India': 'Narendra Modi', 'China': 'Xi Jinping', 'Japan': 'Fumio Kishida'}\n\n\nNew elements can be added, and values of exisiting keys can be changed using the update method:\n\ndict_example = {'USA':'Joe Biden', 'India':'Narendra Modi', 'China':'Xi Jinping','Countries':3}\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Countries': 3}\n\n\n\ndict_example.update({'Countries':4, 'Japan':'Fumio Kishida'})\n\n\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Countries': 4,\n 'Japan': 'Fumio Kishida'}\n\n\n\n\n2.3.2 Iterating over elements of a dictionary\nThe items() attribute of a dictionary can be used to iterate over elements of a dictionary.\n\nfor key,value in dict_example.items():\n    print(\"The Head of State of\",key,\"is\",value)\n\nThe Head of State of USA is Joe Biden\nThe Head of State of India is Narendra Modi\nThe Head of State of China is Xi Jinping\nThe Head of State of Countries is 4\nThe Head of State of Japan is Fumio Kishida\n\n\n\n\n2.3.3 Practice exercise 3\nThe GDP per capita of USA for most years from 1960 to 2021 is given by the dictionary D given in the code cell below.\nFind:\n\nThe GDP per capita in 2015\nThe GDP per capita of 2014 is missing. Update the dictionary to include the GDP per capita of 2014 as the average of the GDP per capita of 2013 and 2015.\nImpute the GDP per capita of other missing years in the same manner as in (2), i.e., as the average GDP per capita of the previous year and the next year. Note that the GDP per capita is not missing for any two consecutive years.\nPrint the years and the imputed GDP per capita for the years having a missing value of GDP per capita in (3).\n\n\nD = {'1960':3007,'1961':3067,'1962':3244,'1963':3375,'1964':3574,'1965':3828,'1966':4146,'1967':4336,'1968':4696,'1970':5234,'1971':5609,'1972':6094,'1973':6726,'1974':7226,'1975':7801,'1976':8592,'1978':10565,'1979':11674, '1980':12575,'1981':13976,'1982':14434,'1983':15544,'1984':17121,'1985':18237,  '1986':19071,'1987':20039,'1988':21417,'1989':22857,'1990':23889,'1991':24342,  '1992':25419,'1993':26387,'1994':27695,'1995':28691,'1996':29968,'1997':31459,  '1998':32854,'2000':36330,'2001':37134,'2002':37998,'2003':39490,'2004':41725,  '2005':44123,'2006':46302,'2007':48050,'2008':48570,'2009':47195,'2010':48651,  '2011':50066,'2012':51784,'2013':53291,'2015':56763,'2016':57867,'2017':59915,'2018':62805, '2019':65095,'2020':63028,'2021':69288}\n\nSolution:\n\nprint(\"GDP per capita in 2015 =\", D['2015'])\nD['2014'] = (D['2013']+D['2015'])/2\nfor i in range(1960,2021):\n    if str(i) not in D.keys():    \n        D[str(i)] = (D[str(i-1)]+D[str(i+1)])/2\n        print(\"Imputed GDP per capita for the year\",i,\"is $\",D[str(i)])\n\nGDP per capita in 2015 = 56763\nImputed GDP per capita for the year 1969 is $ 4965.0\nImputed GDP per capita for the year 1977 is $ 9578.5\nImputed GDP per capita for the year 1999 is $ 34592.0"
  },
  {
    "objectID": "data_structures.html#functions",
    "href": "data_structures.html#functions",
    "title": "2  Data structures",
    "section": "2.4 Functions",
    "text": "2.4 Functions\nIf an algorithm or block of code is being used several times in a code, then it can be separately defined as a function. This makes the code more organized and readable. For example, let us define a function that prints prime numbers between a and b, and returns the number of prime numbers found.\n\n#Function definition\ndef prime_numbers (a,b=100):\n    num_prime_nos = 0\n    \n    #Iterating over all numbers between a and b\n    for i in range(a,b):\n        num_divisors=0\n        \n        #Checking if the ith number has any factors\n        for j in range(2, i):\n            if i%j == 0:\n                num_divisors=1;break;\n                \n        #If there are no factors, then printing and counting the number as prime        \n        if num_divisors==0:\n            print(i)\n            num_prime_nos = num_prime_nos+1\n            \n    #Return count of the number of prime numbers\n    return num_prime_nos\n\nIn the above function, the keyword def is used to define the function, prime_numbers is the name of the function, a and b are the arguments that the function uses to compute the output.\nLet us use the defined function to print and count the prime numbers between 40 and 60.\n\n#Printing prime numbers between 40 and 60\nnum_prime_nos_found = prime_numbers(40,60)\n\n41\n43\n47\n53\n59\n\n\n\nnum_prime_nos_found\n\n5\n\n\nIf the user calls the function without specifying the value of the argument b, then it will take the default value of 100, as mentioned in the function definition. However, for the argument a, the user will need to specify a value, as there is no value defined as a default value in the function definition.\n\n2.4.1 Global and local variables with respect to a function\nA variable defined within a function is local to that function, while a variable defined outside the function is global to that function. In case a variable with the same name is defined both outside and inside a function, it will refer to its global value outside the function and local value within the function.\nThe example below shows a variable with the name var referring to its local value when called within the function, and global value when called outside the function.\n\nvar = 5\ndef sample_function(var):    \n    print(\"Local value of 'var' within 'sample_function()'= \",var)\n\nsample_function(4)\nprint(\"Global value of 'var' outside 'sample_function()' = \",var)\n\nLocal value of 'var' within 'sample_function()'=  4\nGlobal value of 'var' outside 'sample_function()' =  5\n\n\n\n\n2.4.2 Practice exercise 4\nThe object deck defined below corresponds to a deck of cards. Estimate the probablity that a five card hand will be a flush, as follows:\n\nWrite a function that accepts a hand of 5 cards as argument, and returns whether the hand is a flush or not.\nRandomly pull a hand of 5 cards from the deck. Call the function developed in (1) to determine if the hand is a flush.\nRepeat (2) 10,000 times.\nEstimate the probability of the hand being a flush from the results of the 10,000 simulations.\n\nYou may use the function shuffle() from the random library to shuffle the deck everytime before pulling a hand of 5 cards.\n\ndeck = [{'value':i, 'suit':c}\nfor c in ['spades', 'clubs', 'hearts', 'diamonds']\nfor i in range(2,15)]\n\nSolution:\n\nimport random as rm\n\n#Function to check if a 5-card hand is a flush\ndef chck_flush(hands):  \n    \n    #Assuming that the hand is a flush, before checking the cards\n    yes_flush =1\n    \n    #Storing the suit of the first card in 'first_suit'\n    first_suit = hands[0]['suit']\n    \n    #Iterating over the remaining 4 cards of the hand\n    for j in range(1,len(hands)):\n        \n        #If the suit of any of the cards does not match the suit of the first card, the hand is not a flush\n        if first_suit!=hands[j]['suit']:\n            yes_flush = 0; \n            \n            #As soon as a card with a different suit is found, the hand is not a flush and there is no need to check other cards. So, we 'break' out of the loop\n            break;\n    return yes_flush\n\nflush=0\nfor i in range(10000):\n    \n    #Shuffling the deck\n    rm.shuffle(deck)\n    \n    #Picking out the first 5 cards of the deck as a hand and checking if they are a flush\n    #If the hand is a flush it is counted\n    flush=flush+chck_flush(deck[0:5])\n    \nprint(\"Probability of obtaining a flush=\", 100*(flush/10000),\"%\")\n\nProbability of obtaining a flush= 0.18 %"
  },
  {
    "objectID": "data_structures.html#practice-exercise-5",
    "href": "data_structures.html#practice-exercise-5",
    "title": "2  Data structures",
    "section": "2.5 Practice exercise 5",
    "text": "2.5 Practice exercise 5\nThe code cell below defines an object having the nutrition information of drinks in starbucks. Assume that the manner in which the information is structured is consistent throughout the object.\n\nstarbucks_drinks_nutrition={'Cool Lime Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 45}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 11}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Strawberry Acai Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 80}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 18}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Very Berry Hibiscus Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 14}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Evolution Fresh™ Organic Ginger Limeade': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Iced Coffee': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Iced Espresso Classics - Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 21}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 65}], 'Iced Espresso Classics - Caffe Mocha': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 23}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 90}], 'Iced Espresso Classics - Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 21}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 65}], 'Shaken Sweet Tea': [{'Nutrition_type': 'Calories', 'value': 80}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Berry Blossom White': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Black Mango': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 38}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Black with Lemon': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Brambleberry': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Giant Peach': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Iced Passion': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Lemon Ginger': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 31}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Black Lemonade': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Iced Black Tea': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Iced Green Tea': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 31}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Plum Pomegranate': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Tazoberry': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 38}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled White Cranberry': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Teavana® Shaken Iced Black Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Black Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Teavana® Shaken Iced Green Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Green Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Passion Tango™ Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Passion Tango™ Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 24}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Teavana® Shaken Iced Peach Green Tea': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks Refreshers™ Raspberry Pomegranate': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 27}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks Refreshers™ Strawberry Lemonade': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 27}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Doubleshot Protein Dark Chocolate': [{'Nutrition_type': 'Calories', 'value': 210}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 33}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Starbucks® Doubleshot Protein Vanilla': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 120}], 'Starbucks® Iced Coffee Caramel': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 13}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Iced Coffee Light Sweetened': [{'Nutrition_type': 'Calories', 'value': 50}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 11}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Iced Coffee Unsweetened': [{'Nutrition_type': 'Calories', 'value': 10}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 2}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Blonde Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Clover® Brewed Coffee': [{'Nutrition_type': 'Calories', 'value': 10}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Decaf Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Featured Dark Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Nariño 70 Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Nariño 70 Cold Brew with Milk': [{'Nutrition_type': 'Calories', 'value': 0}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Nitro Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Nitro Cold Brew with Sweet Cream': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 5.0}, {'Nutrition_type': 'Carb', 'value': 5}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 20}], 'Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Vanilla Sweet Cream Cold Brew': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 14}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 25}], 'Hot Chocolate': [{'Nutrition_type': 'Calories', 'value': 320}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 47}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 14}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Starbucks® Signature Hot Chocolate': [{'Nutrition_type': 'Calories', 'value': 430}, {'Nutrition_type': 'Fat', 'value': 26.0}, {'Nutrition_type': 'Carb', 'value': 45}, {'Nutrition_type': 'Fiber', 'value': 5}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 13}, {'Nutrition_type': 'Sodium', 'value': 170}], 'Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 290}, {'Nutrition_type': 'Fat', 'value': 8.0}, {'Nutrition_type': 'Carb', 'value': 42}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 13}, {'Nutrition_type': 'Sodium', 'value': 140}], 'Cappuccino': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 12}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 8}, {'Nutrition_type': 'Sodium', 'value': 100}], 'Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 260}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 40}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 11}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 32}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Flat White': [{'Nutrition_type': 'Calories', 'value': 180}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 18}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Iced Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 4.5}, {'Nutrition_type': 'Carb', 'value': 13}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 8}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Iced Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 230}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 36}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 9}, {'Nutrition_type': 'Sodium', 'value': 90}], 'Iced Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Iced Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 7}, {'Nutrition_type': 'Sodium', 'value': 95}], 'Iced Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories', 'value': 260}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 11}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Iced Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 30}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 7}, {'Nutrition_type': 'Sodium', 'value': 100}], 'Iced White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 300}, {'Nutrition_type': 'Fat', 'value': 8.0}, {'Nutrition_type': 'Carb', 'value': 47}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 190}], 'Latte Macchiato': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Starbucks Doubleshot® on Ice Beverage': [{'Nutrition_type': 'Calories', 'value': 45}, {'Nutrition_type': 'Fat', 'value': 1.0}, {'Nutrition_type': 'Carb', 'value': 5}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 3}, {'Nutrition_type': 'Sodium', 'value': 40}], 'Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 150}], 'White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 360}, {'Nutrition_type': 'Fat', 'value': 11.0}, {'Nutrition_type': 'Carb', 'value': 53}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 14}, {'Nutrition_type': 'Sodium', 'value': 240}], 'Cinnamon Dolce Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 350}, {'Nutrition_type': 'Fat', 'value': 4.5}, {'Nutrition_type': 'Carb', 'value': 64}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 15}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Coffee Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 24}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 3}, {'Nutrition_type': 'Sodium', 'value': 200}], 'Mocha Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 280}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 60}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 4}, {'Nutrition_type': 'Sodium', 'value': 220}], 'Mocha Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.5}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 4}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Cinnamon Dolce Crème': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 135}], 'Vanilla Crème': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 135}], 'Chocolate Smoothie': [{'Nutrition_type': 'Calories', 'value': 320}, {'Nutrition_type': 'Fat', 'value': 5.0}, {'Nutrition_type': 'Carb', 'value': 53}, {'Nutrition_type': 'Fiber', 'value': 8}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 170}], 'Strawberry Smoothie': [{'Nutrition_type': 'Calories', 'value': 300}, {'Nutrition_type': 'Fat', 'value': 2.0}, {'Nutrition_type': 'Carb', 'value': 60}, {'Nutrition_type': 'Fiber', 'value': 7}, {'Nutrition_type': 'Protein', 'value': 16}, {'Nutrition_type': 'Sodium', 'value': 130}]}\n\nUse the object above to answer the following questions:\n\n2.5.1 \nWhat is the datatype of the object?\n\nprint(\"Datatype=\",type(starbucks_drinks_nutrition)) \n\nDatatype= <class 'dict'>\n\n\n\n2.5.1.1 \nIf the object in (1) is a dictonary, what is the datatype of the values of the dictionary?\n\nprint(\"Datatype=\",type(starbucks_drinks_nutrition[list(starbucks_drinks_nutrition.keys())[0]]))\n\nDatatype= <class 'list'>\n\n\n\n\n2.5.1.2 \nIf the object in (1) is a dictonary, what is the datatype of the elements within the values of the dictionary?\n\nprint(\"Datatype=\",type(starbucks_drinks_nutrition[list(starbucks_drinks_nutrition.keys())[0]][0]))\n\nDatatype= <class 'dict'>\n\n\n\n\n2.5.1.3 \nHow many calories are there in Iced Coffee?\n\nprint(\"Calories = \",starbucks_drinks_nutrition['Iced Coffee'][0]['value'])\n\nCalories =  5\n\n\n\n\n2.5.1.4 \nWhich drink(s) have the highest amount of protein in them, and what is that protein amount?\n\n#Defining an empty dictionary that will be used to store the protein of each drink\nprotein={}\n\nfor key,value in starbucks_drinks_nutrition.items():\n    for nutrition in value:        \n        if nutrition['Nutrition_type']=='Protein':\n            protein[key]=(nutrition['value'])\n\n#Using dictionary comprehension to find the key-value pair having the maximum value in the dictionary\n{key:value for key, value in protein.items() if value == max(protein.values())}\n\n{'Starbucks® Doubleshot Protein Dark Chocolate': 20,\n 'Starbucks® Doubleshot Protein Vanilla': 20,\n 'Chocolate Smoothie': 20}\n\n\n\n\n2.5.1.5 \nWhich drink(s) have a fat content of more than 10g, and what is their fat content?\n\n#Defining an empty dictionary that will be used to store the fat of each drink\nfat={}\nfor key,value in starbucks_drinks_nutrition.items():\n    for nutrition in value:        \n        if nutrition['Nutrition_type']=='Fat':\n            fat[key]=(nutrition['value'])\n            \n#Using dictionary comprehension to find the key-value pair having the value more than 10\n{key:value for key, value in fat.items() if value>=10}\n\n{'Starbucks® Signature Hot Chocolate': 26.0, 'White Chocolate Mocha': 11.0}"
  },
  {
    "objectID": "Reading data.html",
    "href": "Reading data.html",
    "title": "3  Reading data",
    "section": "",
    "text": "Reading data is the first step to extract information from it. Data can exist broadly in two formats:\n\nStructured data, and\nUntructured data.\n\nStructured data is typically stored in a tabular form, where rows in the data correspond to “observations” and columns correspond to “variables”. For example, the following dataset contains 5 observations, where each observation (or row) consists of information about a movie. The variables (or columns) contain different pieces of information about a given movie. As all variables for a given row are related to the same movie, the data below is also called relational data.\n\n\n\n\n\n\n  \n    \n      \n      Title\n      US Gross\n      Production Budget\n      Release Date\n      Major Genre\n      Creative Type\n      Rotten Tomatoes Rating\n      IMDB Rating\n    \n  \n  \n    \n      0\n      The Shawshank Redemption\n      28241469\n      25000000\n      Sep 23 1994\n      Drama\n      Fiction\n      88\n      9.2\n    \n    \n      1\n      Inception\n      285630280\n      160000000\n      Jul 16 2010\n      Horror/Thriller\n      Fiction\n      87\n      9.1\n    \n    \n      2\n      One Flew Over the Cuckoo's Nest\n      108981275\n      4400000\n      Nov 19 1975\n      Comedy\n      Fiction\n      96\n      8.9\n    \n    \n      3\n      The Dark Knight\n      533345358\n      185000000\n      Jul 18 2008\n      Action/Adventure\n      Fiction\n      93\n      8.9\n    \n    \n      4\n      Schindler's List\n      96067179\n      25000000\n      Dec 15 1993\n      Drama\n      Non-Fiction\n      97\n      8.9\n    \n  \n\n\n\n\nUnstructured data is data that is not organized in any pre-defined manner. Examples of unstructured data can be text files, audio/video files, images, Internet of Things (IoT) data, etc. Unstructured data is relatively harder to analyze as most of the analytical methods and tools are oriented towards structured data. However, an unstructured data can be used to obtain structured data, which in turn can be analyzed. For example, an image can be converted to an array of pixels - which will be structured data. Machine learning algorithms can then be used on the array to classify the image as that of a dog or a cat.\nIn this course, we will focus on analyzing structured data."
  },
  {
    "objectID": "Reading data.html#reading-a-csv-file-with-pandas",
    "href": "Reading data.html#reading-a-csv-file-with-pandas",
    "title": "3  Reading data",
    "section": "3.2 Reading a csv file with Pandas",
    "text": "3.2 Reading a csv file with Pandas\nStructured data can be stored in a variety of formats. The most popular format is data_file_name.csv, where the extension csv stands for comma separated values. The variable values of each observation are separated by a comma in a .csv file. In other words, the delimiter is a comma in a csv file. However, the comma is not visible when a .csv file is opened with Microsoft Excel.\n\n3.2.1 Using the read_csv function\nWe will use functions from the Pandas library of Python to read data. Let us import Pandas to use its functions.\n\nimport pandas as pd\n\nNote that pd is the acronym that we will use to call a Pandas function. This acronym can be anything as desired by the user.\nThe function to read a csv file is read_csv(). It reads the dataset into an object of type Pandas DataFrame. Let us read the dataset movie_ratings.csv in Python.\n\nmovie_ratings = pd.read_csv('movie_ratings.csv')\n\nThe built-in python function type can be used to check the dataype of an object:\n\ntype(movie_ratings)\n\npandas.core.frame.DataFrame\n\n\nNote that the file movie_ratings.csv is stored at the same location as the python script containing the above code. If that is not the case, we’ll need to specify the location of the file as in the following code.\n\nmovie_ratings = pd.read_csv('D:/Books/DataScience_Intro_python/movie_ratings.csv')\n\nNote that forward slash is used instead of backslash while specifying the path of the data file. Another option is to use two consecutive backslashes instead of a single forward slash.\n\n\n3.2.2 Specifying the working directory\nIn case we need to read several datasets from a given location, it may be inconvenient to specify the path every time. In such a case we can change the current working directory to the location where the datasets are located.\nWe’ll use the os library of Python to view and/or change the current working directory.\n\nimport os #Importing the 'os' library\nos.getcwd() #Getting the path to the current working directory\n\n\n\nC:\\Users\\username\\STAT303-1\\Quarto Book\\DataScience_Intro_python\n\n\nThe function getcwd() stands for get current working directory.\nSuppose the dataset to be read is located at 'D:\\Books\\DataScience_Intro_python\\Datasets'. Then, we’ll use the function chdir to change the current working directory to this location.\n\nos.chdir('D:/Books/DataScience_Intro_python/Datasets')\n\nNow we can read the dataset from this location without mentioning the entire path as shown below.\n\nmovie_ratings = pd.read_csv('movie_ratings.csv')\n\n\n\n3.2.3 Data overview\nOnce the data has been read, we may want to see what the data looks like. We’ll use another Pandas function head() to view the first few rows of the data.\n\nmovie_ratings.head()\n\n\n\n\n\n  \n    \n      \n      Title\n      US Gross\n      Worldwide Gross\n      Production Budget\n      Release Date\n      MPAA Rating\n      Source\n      Major Genre\n      Creative Type\n      IMDB Rating\n      IMDB Votes\n    \n  \n  \n    \n      0\n      Opal Dreams\n      14443\n      14443\n      9000000\n      Nov 22 2006\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      6.5\n      468\n    \n    \n      1\n      Major Dundee\n      14873\n      14873\n      3800000\n      Apr 07 1965\n      PG/PG-13\n      Adapted screenplay\n      Western/Musical\n      Fiction\n      6.7\n      2588\n    \n    \n      2\n      The Informers\n      315000\n      315000\n      18000000\n      Apr 24 2009\n      R\n      Adapted screenplay\n      Horror/Thriller\n      Fiction\n      5.2\n      7595\n    \n    \n      3\n      Buffalo Soldiers\n      353743\n      353743\n      15000000\n      Jul 25 2003\n      R\n      Adapted screenplay\n      Comedy\n      Fiction\n      6.9\n      13510\n    \n    \n      4\n      The Last Sin Eater\n      388390\n      388390\n      2200000\n      Feb 09 2007\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      5.7\n      1012\n    \n  \n\n\n\n\n\n3.2.3.1 Row Indices and column names (axis labels)\nThe bold integers on the left are the indices of the DataFrame. Each index refers to a distinct row. For example, the index 2 correponds to the row of the movie The Informers. By default, the indices are integers starting from 0. However, they can be changed (to even non-integer values) if desired by the user.\nThe bold text on top of the DataFrame refers to column names. For example, the column US Gross consists of the gross revenue of a movie in the US.\nCollectively, the indices and column names are referred as axis labels.\n\n\n3.2.3.2 Shape of DataFrame\nFor finding the number of rows and columns in the data, you may use the shape() function.\n\n#Finding the shape of movie_ratings dataset\nmovie_ratings.shape\n\n(2228, 11)\n\n\nThe movie_ratings dataset contains 2,228 observations (or rows) and 11 variables (or columns).\n\n\n\n3.2.4 Summary statistics\n\n3.2.4.1 Numeric columns summary\nThe Pandas function of the DataFrame class, describe() can be used very conviniently to print the summary statistics of numeric columns of the data.\n\n#Finding summary statistics of movie_ratings dataset\nmovie_ratings.describe()\n\n\n\n\n\nTable 3.1:  Summary statistics of numeric variables \n  \n    \n      \n      US Gross\n      Worldwide Gross\n      Production Budget\n      IMDB Rating\n      IMDB Votes\n      Release Year\n    \n  \n  \n    \n      count\n      2.228000e+03\n      2.228000e+03\n      2.228000e+03\n      2228.000000\n      2228.000000\n      2228.000000\n    \n    \n      mean\n      5.076370e+07\n      1.019370e+08\n      3.816055e+07\n      6.239004\n      33585.154847\n      2002.005386\n    \n    \n      std\n      6.643081e+07\n      1.648589e+08\n      3.782604e+07\n      1.243285\n      47325.651561\n      5.524324\n    \n    \n      min\n      0.000000e+00\n      8.840000e+02\n      2.180000e+02\n      1.400000\n      18.000000\n      1953.000000\n    \n    \n      25%\n      9.646188e+06\n      1.320737e+07\n      1.200000e+07\n      5.500000\n      6659.250000\n      1999.000000\n    \n    \n      50%\n      2.838649e+07\n      4.266892e+07\n      2.600000e+07\n      6.400000\n      18169.000000\n      2002.000000\n    \n    \n      75%\n      6.453140e+07\n      1.200000e+08\n      5.300000e+07\n      7.100000\n      40092.750000\n      2006.000000\n    \n    \n      max\n      7.601676e+08\n      2.767891e+09\n      3.000000e+08\n      9.200000\n      519541.000000\n      2039.000000\n    \n  \n\n\n\n\n\nAnswer the following questions based on the above table.\n\n\n\n\n\n \n        \n\n\n\n\n\n\n\n \n        \n\n\n\n\n3.2.4.2 Summary statistics across rows/columns\nThe Pandas DataFrame class has functions such as sum() and mean() to compute sum over rows or columns of a DataFrame.\nLet us compute the mean of all the numeric columns of the data:\n\nmovie_ratings.mean(axis = 0)\n\nUS Gross             5.076370e+07\nWorldwide Gross      1.019370e+08\nProduction Budget    3.816055e+07\nIMDB Rating          6.239004e+00\nIMDB Votes           3.358515e+04\ndtype: float64\n\n\nThe argument axis=0 denotes that the mean is taken over all the rows of the DataFrame. For computing a statistic across column the argument axis=1 will be used.\nIf mean over a subset of columns is desired, then those column names can be subset from the data. For example, let us compute the mean IMDB rating, and mean IMDB votes of all the movies:\n\nmovie_ratings[['IMDB Rating','IMDB Votes']].mean(axis = 0)\n\nIMDB Rating        6.239004\nIMDB Votes     33585.154847\ndtype: float64\n\n\n\n\n\n3.2.5 Practice exercise 1\nRead the file Top 10 Albums By Year.csv. This file contains the top 10 albums for each year from 1990 to 2021. Each row corresponds to a unique album.\n\n3.2.5.1 \nPrint the first 5 rows of the data.\n\nalbum_data = pd.read_csv('./Datasets/Top 10 Albums By Year.csv')\nalbum_data.head()\n\n\n\n\n\n  \n    \n      \n      Year\n      Ranking\n      Artist\n      Album\n      Worldwide Sales\n      CDs\n      Tracks\n      Album Length\n      Hours\n      Minutes\n      Seconds\n      Genre\n    \n  \n  \n    \n      0\n      1990\n      8\n      Phil Collins\n      Serious Hits... Live!\n      9956520\n      1\n      15\n      1:16:53\n      1.28\n      76.88\n      4613\n      Rock\n    \n    \n      1\n      1990\n      1\n      Madonna\n      The Immaculate Collection\n      30000000\n      1\n      17\n      1:13:32\n      1.23\n      73.53\n      4412\n      Pop\n    \n    \n      2\n      1990\n      10\n      The Three Tenors\n      Carreras Domingo Pavarotti In Concert 1990\n      8533000\n      1\n      17\n      1:07:55\n      1.13\n      67.92\n      4075\n      Classical\n    \n    \n      3\n      1990\n      4\n      MC Hammer\n      Please Hammer Don't Hurt Em\n      18000000\n      1\n      13\n      0:59:04\n      0.98\n      59.07\n      3544\n      Hip Hop\n    \n    \n      4\n      1990\n      6\n      Movie Soundtrack\n      Aashiqui\n      15000000\n      1\n      12\n      0:58:13\n      0.97\n      58.22\n      3493\n      World\n    \n  \n\n\n\n\n\n\n3.2.5.2 \nHow many rows and columns are there in the data?\n\nalbum_data.shape\n\n(320, 12)\n\n\nThere are 320 rows and 12 columns in the data\n\n\n3.2.5.3 \nPrint the summary statistics of the data, and answer the following questions:\n\nWhat proportion of albums have 15 or lesser tracks? Mention a range for the proportion.\nWhat is the mean length of a track (in minutes)?\n\n\nalbum_data.describe()\n\n\n\n\n\n  \n    \n      \n      Year\n      Ranking\n      CDs\n      Tracks\n      Hours\n      Minutes\n      Seconds\n    \n  \n  \n    \n      count\n      320.000000\n      320.00000\n      320.000000\n      320.000000\n      320.000000\n      320.000000\n      320.000000\n    \n    \n      mean\n      2005.500000\n      5.50000\n      1.043750\n      14.306250\n      0.941406\n      56.478500\n      3388.715625\n    \n    \n      std\n      9.247553\n      2.87678\n      0.246528\n      5.868995\n      0.382895\n      22.970109\n      1378.209812\n    \n    \n      min\n      1990.000000\n      1.00000\n      1.000000\n      6.000000\n      0.320000\n      19.430000\n      1166.000000\n    \n    \n      25%\n      1997.750000\n      3.00000\n      1.000000\n      12.000000\n      0.740000\n      44.137500\n      2648.250000\n    \n    \n      50%\n      2005.500000\n      5.50000\n      1.000000\n      13.000000\n      0.860000\n      51.555000\n      3093.500000\n    \n    \n      75%\n      2013.250000\n      8.00000\n      1.000000\n      15.000000\n      1.090000\n      65.112500\n      3906.750000\n    \n    \n      max\n      2021.000000\n      10.00000\n      4.000000\n      67.000000\n      5.070000\n      304.030000\n      18242.000000\n    \n  \n\n\n\n\nAt least 75% of the albums have 15 tracks since the 75th percentile value of the number of tracks is 15. However, albums between those having 75th percentile value for the number of tracks and those having the maximum number of tracks can also have 15 tracks. Thus, the proportion of albums having 15 or lesser tracks = [75%-99.99%].\n\nprint(\"Mean length of a track =\",56.478500/14.306250, \"minutes\")\n\nMean length of a track = 3.9478200087374398 minutes\n\n\n\n\n\n3.2.6 Creating new columns from existing columns\nNew variables (or columns) can be created based on existing variables, or with external data (we’ll see adding external data later). For example, let us create a new variable ratio_wgross_by_budget, which is the ratio of Worldwide Gross and Production Budget for each movie:\n\nmovie_ratings['ratio_wgross_by_budget'] = movie_ratings['Worldwide Gross']/movie_ratings['Production Budget']\n\nThe new variable can be seen at the right end of the updated DataFrame as shown below.\n\nmovie_ratings.head()\n\n\n\n\n\n  \n    \n      \n      Title\n      US Gross\n      Worldwide Gross\n      Production Budget\n      Release Date\n      MPAA Rating\n      Source\n      Major Genre\n      Creative Type\n      IMDB Rating\n      IMDB Votes\n      ratio_wgross_by_budget\n    \n  \n  \n    \n      0\n      Opal Dreams\n      14443\n      14443\n      9000000\n      Nov 22 2006\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      6.5\n      468\n      0.001605\n    \n    \n      1\n      Major Dundee\n      14873\n      14873\n      3800000\n      Apr 07 1965\n      PG/PG-13\n      Adapted screenplay\n      Western/Musical\n      Fiction\n      6.7\n      2588\n      0.003914\n    \n    \n      2\n      The Informers\n      315000\n      315000\n      18000000\n      Apr 24 2009\n      R\n      Adapted screenplay\n      Horror/Thriller\n      Fiction\n      5.2\n      7595\n      0.017500\n    \n    \n      3\n      Buffalo Soldiers\n      353743\n      353743\n      15000000\n      Jul 25 2003\n      R\n      Adapted screenplay\n      Comedy\n      Fiction\n      6.9\n      13510\n      0.023583\n    \n    \n      4\n      The Last Sin Eater\n      388390\n      388390\n      2200000\n      Feb 09 2007\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      5.7\n      1012\n      0.176541\n    \n  \n\n\n\n\n\n\n3.2.7 Datatype of variables\nNote that in Table 3.1 (summary statistics), we don’t see Release Date. This is because the datatype of Release Data is not numeric.\nThe datatype of each variable can be seen using the dtypes() function of the DataFrame class.\n\n#Checking the datatypes of the variables \nmovie_ratings.dtypes\n\nTitle                 object\nUS Gross               int64\nWorldwide Gross        int64\nProduction Budget      int64\nRelease Date          object\nMPAA Rating           object\nSource                object\nMajor Genre           object\nCreative Type         object\nIMDB Rating          float64\nIMDB Votes             int64\ndtype: object\n\n\nOften, we wish to convert the datatypes of some of the variables to make them suitable for analysis. For example, the datatype of Release Date in the DataFrame movie_ratings is object. To perform numerical computations on this variable, we’ll need to convert it to a datatime format. We’ll use the Pandas function to_datatime() to covert it to a datatime format. Similar functions such as to_numeric(), to_string() etc., can be used for other conversions.\n\npd.to_datetime(movie_ratings['Release Date'])\n\n0      2006-11-22\n1      1965-04-07\n2      2009-04-24\n3      2003-07-25\n4      2007-02-09\n          ...    \n2223   2004-07-07\n2224   1998-06-19\n2225   2010-05-14\n2226   1991-06-14\n2227   1998-01-23\nName: Release Date, Length: 2228, dtype: datetime64[ns]\n\n\nWe can see above that the function to_datetime() converts Release Date to a datetime format.\nNow, we’ll update the variable Release Date in the DataFrame to be in the datetime format:\n\nmovie_ratings['Release Date'] = pd.to_datetime(movie_ratings['Release Date'])\n\n\nmovie_ratings.dtypes\n\nTitle                        object\nUS Gross                      int64\nWorldwide Gross               int64\nProduction Budget             int64\nRelease Date         datetime64[ns]\nMPAA Rating                  object\nSource                       object\nMajor Genre                  object\nCreative Type                object\nIMDB Rating                 float64\nIMDB Votes                    int64\ndtype: object\n\n\nWe can see that the datatype of Release Date has changed to datetime in the updated DataFrame, movie_ratings. Now we can perform computations on Release Date. Suppose we wish to create a new variable Release_year that consists of the year of release of the movie. We’ll use the attribute year of the datetime module to extract the year from Release Date:\n\n#Extracting year from Release Date\nmovie_ratings['Release Year'] = movie_ratings['Release Date'].dt.year\n\n\nmovie_ratings.head()\n\n\n\n\n\n  \n    \n      \n      Title\n      US Gross\n      Worldwide Gross\n      Production Budget\n      Release Date\n      MPAA Rating\n      Source\n      Major Genre\n      Creative Type\n      IMDB Rating\n      IMDB Votes\n      Release Year\n    \n  \n  \n    \n      0\n      Opal Dreams\n      14443\n      14443\n      9000000\n      2006-11-22\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      6.5\n      468\n      2006\n    \n    \n      1\n      Major Dundee\n      14873\n      14873\n      3800000\n      1965-04-07\n      PG/PG-13\n      Adapted screenplay\n      Western/Musical\n      Fiction\n      6.7\n      2588\n      1965\n    \n    \n      2\n      The Informers\n      315000\n      315000\n      18000000\n      2009-04-24\n      R\n      Adapted screenplay\n      Horror/Thriller\n      Fiction\n      5.2\n      7595\n      2009\n    \n    \n      3\n      Buffalo Soldiers\n      353743\n      353743\n      15000000\n      2003-07-25\n      R\n      Adapted screenplay\n      Comedy\n      Fiction\n      6.9\n      13510\n      2003\n    \n    \n      4\n      The Last Sin Eater\n      388390\n      388390\n      2200000\n      2007-02-09\n      PG/PG-13\n      Adapted screenplay\n      Drama\n      Fiction\n      5.7\n      1012\n      2007\n    \n  \n\n\n\n\nAs year is a numeric variable, it will appear in the numeric summary statistics with the describe() function, as shown below.\n\nmovie_ratings.describe()\n\n\n\n\n\n  \n    \n      \n      US Gross\n      Worldwide Gross\n      Production Budget\n      IMDB Rating\n      IMDB Votes\n      Release Year\n    \n  \n  \n    \n      count\n      2.228000e+03\n      2.228000e+03\n      2.228000e+03\n      2228.000000\n      2228.000000\n      2228.000000\n    \n    \n      mean\n      5.076370e+07\n      1.019370e+08\n      3.816055e+07\n      6.239004\n      33585.154847\n      2002.005386\n    \n    \n      std\n      6.643081e+07\n      1.648589e+08\n      3.782604e+07\n      1.243285\n      47325.651561\n      5.524324\n    \n    \n      min\n      0.000000e+00\n      8.840000e+02\n      2.180000e+02\n      1.400000\n      18.000000\n      1953.000000\n    \n    \n      25%\n      9.646188e+06\n      1.320737e+07\n      1.200000e+07\n      5.500000\n      6659.250000\n      1999.000000\n    \n    \n      50%\n      2.838649e+07\n      4.266892e+07\n      2.600000e+07\n      6.400000\n      18169.000000\n      2002.000000\n    \n    \n      75%\n      6.453140e+07\n      1.200000e+08\n      5.300000e+07\n      7.100000\n      40092.750000\n      2006.000000\n    \n    \n      max\n      7.601676e+08\n      2.767891e+09\n      3.000000e+08\n      9.200000\n      519541.000000\n      2039.000000\n    \n  \n\n\n\n\n\n\n3.2.8 Practice exercise 2\n\n3.2.8.1 \nWhy is Worldwide Sales not included in the summary statistics table printed in Practice exercise 1?\n\nalbum_data.dtypes\n\nYear                 int64\nRanking              int64\nArtist              object\nAlbum               object\nWorldwide Sales     object\nCDs                  int64\nTracks               int64\nAlbum Length        object\nHours              float64\nMinutes            float64\nSeconds              int64\nGenre               object\ndtype: object\n\n\nWorldwide Sales is not included in the summary statistics table printed in Practice exercise 1 because its data type is object and not int or float\n\n\n3.2.8.2 \nUpdate the DataFrame so that Worldwide Sales is included in the summary statistics table. Print the summary statistics table.\nHint: Sometimes it may not be possible to convert an object to numeric(). For example, the object ‘hi’ cannot be converted to a numeric() by the python compiler. To avoid getting an error, use the errors argument of to_numeric() to force such conversions to NaN (missing value).\n\nalbum_data['Worldwide Sales'] = pd.to_numeric(album_data['Worldwide Sales'], errors = 'coerce')\nalbum_data.describe()\n\n\n\n\n\n  \n    \n      \n      Year\n      Ranking\n      Worldwide Sales\n      CDs\n      Tracks\n      Hours\n      Minutes\n      Seconds\n    \n  \n  \n    \n      count\n      320.000000\n      320.00000\n      3.190000e+02\n      320.000000\n      320.000000\n      320.000000\n      320.000000\n      320.000000\n    \n    \n      mean\n      2005.500000\n      5.50000\n      1.071093e+07\n      1.043750\n      14.306250\n      0.941406\n      56.478500\n      3388.715625\n    \n    \n      std\n      9.247553\n      2.87678\n      7.566796e+06\n      0.246528\n      5.868995\n      0.382895\n      22.970109\n      1378.209812\n    \n    \n      min\n      1990.000000\n      1.00000\n      1.909009e+06\n      1.000000\n      6.000000\n      0.320000\n      19.430000\n      1166.000000\n    \n    \n      25%\n      1997.750000\n      3.00000\n      5.000000e+06\n      1.000000\n      12.000000\n      0.740000\n      44.137500\n      2648.250000\n    \n    \n      50%\n      2005.500000\n      5.50000\n      8.255866e+06\n      1.000000\n      13.000000\n      0.860000\n      51.555000\n      3093.500000\n    \n    \n      75%\n      2013.250000\n      8.00000\n      1.400000e+07\n      1.000000\n      15.000000\n      1.090000\n      65.112500\n      3906.750000\n    \n    \n      max\n      2021.000000\n      10.00000\n      4.500000e+07\n      4.000000\n      67.000000\n      5.070000\n      304.030000\n      18242.000000\n    \n  \n\n\n\n\n\n\n3.2.8.3 \nCreate a new column that computes the average worldwide sales per year for each album, assuming that the worldwide sales are as of 2022. Print the first 5 rows of the updated DataFrame.\n\nalbum_data['mean_sales_per_year'] = album_data['Worldwide Sales']/(2022-album_data['Year'])\nalbum_data.head()\n\n\n\n\n\n  \n    \n      \n      Year\n      Ranking\n      Artist\n      Album\n      Worldwide Sales\n      CDs\n      Tracks\n      Album Length\n      Hours\n      Minutes\n      Seconds\n      Genre\n      mean_sales_per_year\n    \n  \n  \n    \n      0\n      1990\n      8\n      Phil Collins\n      Serious Hits... Live!\n      9956520.0\n      1\n      15\n      1:16:53\n      1.28\n      76.88\n      4613\n      Rock\n      311141.25\n    \n    \n      1\n      1990\n      1\n      Madonna\n      The Immaculate Collection\n      30000000.0\n      1\n      17\n      1:13:32\n      1.23\n      73.53\n      4412\n      Pop\n      937500.00\n    \n    \n      2\n      1990\n      10\n      The Three Tenors\n      Carreras Domingo Pavarotti In Concert 1990\n      8533000.0\n      1\n      17\n      1:07:55\n      1.13\n      67.92\n      4075\n      Classical\n      266656.25\n    \n    \n      3\n      1990\n      4\n      MC Hammer\n      Please Hammer Don't Hurt Em\n      18000000.0\n      1\n      13\n      0:59:04\n      0.98\n      59.07\n      3544\n      Hip Hop\n      562500.00\n    \n    \n      4\n      1990\n      6\n      Movie Soundtrack\n      Aashiqui\n      15000000.0\n      1\n      12\n      0:58:13\n      0.97\n      58.22\n      3493\n      World\n      468750.00\n    \n  \n\n\n\n\n\n\n\n3.2.9 Reading a sub-set of data: loc and iloc\nSometimes we may be interested in working with a subset of rows and columns of the data, instead of working with the entire dataset. The indexing operators loc and iloc provide a convenient way of selecting a subset of desired rows and columns. The operator loc uses axis labels (row indices and column names) to subset the data, while iloc uses the position of rows or columns, where position has values 0,1,2,3,…and so on, for rows from top to bottom and columns from left to right. In other words, the first row has position 0, the second row has position 1, the third row has position 2, and so on. Similarly, the first column from left has position 0, the second column from left has position 1, the third column from left has position 2, and so on.\nLet us read the file movie_IMDBratings_sorted.csv, which has movies sorted in the descending order of their IMDB ratings.\n\nmovies_sorted = pd.read_csv('./Datasets/movie_IMDBratings_sorted.csv',index_col = 0)\n\nThe argument index_col=0 assigns the first column of the file as the row indices of the DataFrame.\n\nmovies_sorted.head()\n\n\n\n\n\n  \n    \n      \n      Title\n      US Gross\n      Worldwide Gross\n      Production Budget\n      Release Date\n      MPAA Rating\n      Source\n      Major Genre\n      Creative Type\n      IMDB Rating\n      IMDB Votes\n    \n    \n      Rank\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      The Shawshank Redemption\n      28241469\n      28241469\n      25000000\n      Sep 23 1994\n      R\n      Adapted screenplay\n      Drama\n      Fiction\n      9.2\n      519541\n    \n    \n      2\n      Inception\n      285630280\n      753830280\n      160000000\n      Jul 16 2010\n      PG/PG-13\n      Original Screenplay\n      Horror/Thriller\n      Fiction\n      9.1\n      188247\n    \n    \n      3\n      The Dark Knight\n      533345358\n      1022345358\n      185000000\n      Jul 18 2008\n      PG/PG-13\n      Adapted screenplay\n      Action/Adventure\n      Fiction\n      8.9\n      465000\n    \n    \n      4\n      Schindler's List\n      96067179\n      321200000\n      25000000\n      Dec 15 1993\n      R\n      Adapted screenplay\n      Drama\n      Non-Fiction\n      8.9\n      276283\n    \n    \n      5\n      Pulp Fiction\n      107928762\n      212928762\n      8000000\n      Oct 14 1994\n      R\n      Original Screenplay\n      Drama\n      Fiction\n      8.9\n      417703\n    \n  \n\n\n\n\nLet us say, we wish to subset the title, worldwide gross, production budget, and IMDB raring of top 3 movies.\n\n# Subsetting the DataFrame by loc - using axis labels\nmovies_subset = movies_sorted.loc[1:3,['Title','Worldwide Gross','Production Budget','IMDB Rating']]\nmovies_subset\n\n\n\n\n\n  \n    \n      \n      Title\n      Worldwide Gross\n      Production Budget\n      IMDB Rating\n    \n    \n      Rank\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      The Shawshank Redemption\n      28241469\n      25000000\n      9.2\n    \n    \n      2\n      Inception\n      753830280\n      160000000\n      9.1\n    \n    \n      3\n      The Dark Knight\n      1022345358\n      185000000\n      8.9\n    \n  \n\n\n\n\n\n# Subsetting the DataFrame by iloc - using index of the position of rows and columns\nmovies_subset = movies_sorted.iloc[0:3,[0,2,3,9]]\nmovies_subset\n\n\n\n\n\n  \n    \n      \n      Title\n      Worldwide Gross\n      Production Budget\n      IMDB Rating\n    \n    \n      Rank\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      The Shawshank Redemption\n      28241469\n      25000000\n      9.2\n    \n    \n      2\n      Inception\n      753830280\n      160000000\n      9.1\n    \n    \n      3\n      The Dark Knight\n      1022345358\n      185000000\n      8.9\n    \n  \n\n\n\n\nLet us find the movie with the maximum Worldwide Gross.\nWe will use the argmax() function of the Pandas Series class to find the position of the movie with the maximum worldwide gross, and then use the position to find the movie.\n\nposition_max_wgross = movies_sorted['Worldwide Gross'].argmax()\n\n\nmovies_sorted.iloc[position_max_wgross,:]\n\nTitle                             Avatar\nUS Gross                       760167650\nWorldwide Gross               2767891499\nProduction Budget              237000000\nRelease Date                 Dec 18 2009\nMPAA Rating                     PG/PG-13\nSource               Original Screenplay\nMajor Genre             Action/Adventure\nCreative Type                    Fiction\nIMDB Rating                          8.3\nIMDB Votes                        261439\nName: 59, dtype: object\n\n\nAvatar has the highest worldwide gross of all the movies. Note that the : indicates that all the columns of the DataFrame are selected.\n\n\n3.2.10 Practice exercise 3\n\n3.2.10.1 \nFind the album having the highest worldwide sales per year, and its artist.\n\nalbum_data.iloc[album_data['mean_sales_per_year'].argmax(),:]\n\nYear                        2021\nRanking                        1\nArtist                     Adele\nAlbum                         30\nWorldwide Sales        4485025.0\nCDs                            1\nTracks                        12\nAlbum Length             0:58:14\nHours                       0.97\nMinutes                    58.23\nSeconds                     3494\nGenre                        Pop\nmean_sales_per_year    4485025.0\nName: 312, dtype: object\n\n\n‘30’ has the highest worldwide sales and its artist is Adele.\n\n\n3.2.10.2 \nSubset the data to include only Hip-Hop albums. How many Hip_Hop albums are there?\n\nhiphop_albums = album_data.loc[album_data['Genre']=='Hip Hop',:]\nprint(\"There are\",hiphop_albums.shape[0], \"hip-hop albums\")\n\nThere are 42 hip-hop albums\n\n\n\n\n3.2.10.3 \nWhich album amongst hip-hop has the higest mean sales per year per track, and who is its artist?\n\nhiphop_albums.loc[:,'mean_sales_per_year_track'] = hiphop_albums.loc[:,'Worldwide Sales']/((2022-hiphop_albums.loc[:,'Year'])*(hiphop_albums.loc[:,'Tracks']))\nhiphop_albums.iloc[hiphop_albums['mean_sales_per_year_track'].argmax(),:]\n\nYear                                  2021\nRanking                                  6\nArtist                           Cai Xukun\nAlbum                                    迷\nWorldwide Sales                  3402981.0\nCDs                                      1\nTracks                                  11\nAlbum Length                       0:24:16\nHours                                  0.4\nMinutes                              24.27\nSeconds                               1456\nGenre                              Hip Hop\nmean_sales_per_year              3402981.0\nmean_sales_per_year_track    309361.909091\nName: 318, dtype: object\n\n\n迷 has the higest mean sales per year per track amongst hip-hop albumns, and its artist is Cai Xukun."
  },
  {
    "objectID": "Reading data.html#reading-other-data-formats---txt-html-json",
    "href": "Reading data.html#reading-other-data-formats---txt-html-json",
    "title": "3  Reading data",
    "section": "3.3 Reading other data formats - txt, html, json",
    "text": "3.3 Reading other data formats - txt, html, json\nAlthough csv is a very popular format for structured data, data is found in several other formats as well. Some of the other data formats are txt, html and json.\n\n3.3.1 Reading txt files\nThe txt format offers some additional flexibility as compared to the csv format. In the csv format, the delimiter is a comma (or the column values are separated by a comma). However, in a txt file, the delimiter can be anything as desired by the user. Let us read the file movie_ratings.txt, where the variable values are separated by a tab character.\n\nmovie_ratings_txt = pd.read_csv('movie_ratings.txt',sep='\\t')\n\nWe use the function read_csv to read a txt file. However, we mention the tab character (r”) as a separator of variable values.\nNote that there is no need to remember the argument name - sep for specifying the delimiter. You can always refer to the read_csv() documentation to find the relevant argument.\n\n\n3.3.2 Practice exercise 4\nRead the file bestseller_books.txt. It contains top 50 best-selling books on amazon from 2009 to 2019. Identify the delimiter without opening the file with Notepad or a text-editing software. How many rows and columns are there in the dataset?\nSolution:\n\n#Reading some lines with 'error_bad_lines=False' to identify the delimiter\nbestseller_books = pd.read_csv('./Datasets/bestseller_books.txt',error_bad_lines=False)\nbestseller_books.head()\n\nb'Skipping line 6: expected 1 fields, saw 2\\nSkipping line 10: expected 1 fields, saw 3\\nSkipping line 16: expected 1 fields, saw 5\\nSkipping line 17: expected 1 fields, saw 4\\nSkipping line 20: expected 1 fields, saw 3\\nSkipping line 29: expected 1 fields, saw 2\\nSkipping line 33: expected 1 fields, saw 2\\nSkipping line 40: expected 1 fields, saw 2\\nSkipping line 41: expected 1 fields, saw 2\\nSkipping line 42: expected 1 fields, saw 3\\nSkipping line 43: expected 1 fields, saw 3\\nSkipping line 44: expected 1 fields, saw 2\\nSkipping line 60: expected 1 fields, saw 4\\nSkipping line 61: expected 1 fields, saw 3\\nSkipping line 63: expected 1 fields, saw 2\\nSkipping line 64: expected 1 fields, saw 2\\nSkipping line 70: expected 1 fields, saw 3\\nSkipping line 71: expected 1 fields, saw 2\\nSkipping line 72: expected 1 fields, saw 2\\nSkipping line 73: expected 1 fields, saw 2\\nSkipping line 80: expected 1 fields, saw 4\\nSkipping line 82: expected 1 fields, saw 2\\nSkipping line 94: expected 1 fields, saw 4\\nSkipping line 95: expected 1 fields, saw 2\\nSkipping line 96: expected 1 fields, saw 2\\nSkipping line 101: expected 1 fields, saw 3\\nSkipping line 119: expected 1 fields, saw 3\\nSkipping line 130: expected 1 fields, saw 2\\nSkipping line 131: expected 1 fields, saw 2\\nSkipping line 132: expected 1 fields, saw 2\\nSkipping line 133: expected 1 fields, saw 2\\nSkipping line 148: expected 1 fields, saw 3\\nSkipping line 149: expected 1 fields, saw 3\\nSkipping line 150: expected 1 fields, saw 3\\nSkipping line 154: expected 1 fields, saw 3\\nSkipping line 155: expected 1 fields, saw 2\\nSkipping line 156: expected 1 fields, saw 3\\nSkipping line 157: expected 1 fields, saw 2\\nSkipping line 158: expected 1 fields, saw 2\\nSkipping line 159: expected 1 fields, saw 2\\nSkipping line 177: expected 1 fields, saw 4\\nSkipping line 178: expected 1 fields, saw 2\\nSkipping line 179: expected 1 fields, saw 2\\nSkipping line 183: expected 1 fields, saw 3\\nSkipping line 209: expected 1 fields, saw 2\\nSkipping line 215: expected 1 fields, saw 3\\nSkipping line 224: expected 1 fields, saw 3\\nSkipping line 230: expected 1 fields, saw 2\\nSkipping line 241: expected 1 fields, saw 2\\nSkipping line 247: expected 1 fields, saw 2\\nSkipping line 248: expected 1 fields, saw 2\\nSkipping line 249: expected 1 fields, saw 2\\nSkipping line 250: expected 1 fields, saw 2\\nSkipping line 251: expected 1 fields, saw 2\\nSkipping line 252: expected 1 fields, saw 2\\nSkipping line 253: expected 1 fields, saw 2\\nSkipping line 254: expected 1 fields, saw 2\\nSkipping line 259: expected 1 fields, saw 3\\nSkipping line 273: expected 1 fields, saw 2\\nSkipping line 274: expected 1 fields, saw 2\\nSkipping line 275: expected 1 fields, saw 2\\nSkipping line 276: expected 1 fields, saw 2\\nSkipping line 277: expected 1 fields, saw 2\\nSkipping line 278: expected 1 fields, saw 2\\nSkipping line 279: expected 1 fields, saw 2\\nSkipping line 280: expected 1 fields, saw 2\\nSkipping line 281: expected 1 fields, saw 2\\nSkipping line 282: expected 1 fields, saw 2\\nSkipping line 292: expected 1 fields, saw 4\\nSkipping line 293: expected 1 fields, saw 4\\nSkipping line 295: expected 1 fields, saw 7\\nSkipping line 296: expected 1 fields, saw 7\\nSkipping line 297: expected 1 fields, saw 2\\nSkipping line 302: expected 1 fields, saw 3\\nSkipping line 315: expected 1 fields, saw 3\\nSkipping line 321: expected 1 fields, saw 2\\nSkipping line 346: expected 1 fields, saw 3\\nSkipping line 347: expected 1 fields, saw 3\\nSkipping line 365: expected 1 fields, saw 2\\nSkipping line 408: expected 1 fields, saw 2\\nSkipping line 420: expected 1 fields, saw 2\\nSkipping line 421: expected 1 fields, saw 2\\nSkipping line 430: expected 1 fields, saw 2\\nSkipping line 434: expected 1 fields, saw 2\\nSkipping line 446: expected 1 fields, saw 2\\nSkipping line 448: expected 1 fields, saw 2\\nSkipping line 449: expected 1 fields, saw 4\\nSkipping line 451: expected 1 fields, saw 3\\nSkipping line 458: expected 1 fields, saw 2\\nSkipping line 459: expected 1 fields, saw 2\\nSkipping line 460: expected 1 fields, saw 2\\nSkipping line 465: expected 1 fields, saw 2\\nSkipping line 470: expected 1 fields, saw 2\\nSkipping line 471: expected 1 fields, saw 2\\nSkipping line 476: expected 1 fields, saw 2\\nSkipping line 495: expected 1 fields, saw 2\\nSkipping line 496: expected 1 fields, saw 2\\nSkipping line 497: expected 1 fields, saw 2\\nSkipping line 512: expected 1 fields, saw 5\\nSkipping line 513: expected 1 fields, saw 2\\nSkipping line 515: expected 1 fields, saw 2\\nSkipping line 517: expected 1 fields, saw 3\\nSkipping line 518: expected 1 fields, saw 3\\nSkipping line 519: expected 1 fields, saw 3\\nSkipping line 520: expected 1 fields, saw 3\\nSkipping line 521: expected 1 fields, saw 3\\nSkipping line 525: expected 1 fields, saw 3\\nSkipping line 533: expected 1 fields, saw 3\\nSkipping line 534: expected 1 fields, saw 3\\n'\n\n\n\n\n\n\n  \n    \n      \n      ;Unnamed: 0;Name;Author;User Rating;Reviews;Price;Year;Genre\n    \n  \n  \n    \n      0\n      0;0;10-Day Green Smoothie Cleanse;JJ Smith;4.7...\n    \n    \n      1\n      1;1;11/22/63: A Novel;Stephen King;4.6;2052;22...\n    \n    \n      2\n      2;2;12 Rules for Life: An Antidote to Chaos;Jo...\n    \n    \n      3\n      3;3;1984 (Signet Classics);George Orwell;4.7;2...\n    \n    \n      4\n      5;5;A Dance with Dragons (A Song of Ice and Fi...\n    \n  \n\n\n\n\n\n#The delimiter seems to be ';' based on the output of the above code\nbestseller_books = pd.read_csv('./Datasets/bestseller_books.txt',sep=';')\nbestseller_books.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Unnamed: 0.1\n      Name\n      Author\n      User Rating\n      Reviews\n      Price\n      Year\n      Genre\n    \n  \n  \n    \n      0\n      0\n      0\n      10-Day Green Smoothie Cleanse\n      JJ Smith\n      4.7\n      17350\n      8\n      2016\n      Non Fiction\n    \n    \n      1\n      1\n      1\n      11/22/63: A Novel\n      Stephen King\n      4.6\n      2052\n      22\n      2011\n      Fiction\n    \n    \n      2\n      2\n      2\n      12 Rules for Life: An Antidote to Chaos\n      Jordan B. Peterson\n      4.7\n      18979\n      15\n      2018\n      Non Fiction\n    \n    \n      3\n      3\n      3\n      1984 (Signet Classics)\n      George Orwell\n      4.7\n      21424\n      6\n      2017\n      Fiction\n    \n    \n      4\n      4\n      4\n      5,000 Awesome Facts (About Everything!) (Natio...\n      National Geographic Kids\n      4.8\n      7665\n      12\n      2019\n      Non Fiction\n    \n  \n\n\n\n\n\n#The file read with ';' as the delimited is correct\nprint(\"The file has\",bestseller_books.shape[0],\"rows and\",bestseller_books.shape[1],\"columns\")\n\nThe file has 550 rows and 9 columns\n\n\nAlternatively, you can use the argument sep = None, and engine = 'python'. The default engine is C. However, the ‘python’ engine has a ‘sniffer’ tool which may identify the delimiter automatically.\n\nbestseller_books = pd.read_csv('./Datasets/bestseller_books.txt',sep=None, engine = 'python')\nbestseller_books.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Unnamed: 0.1\n      Name\n      Author\n      User Rating\n      Reviews\n      Price\n      Year\n      Genre\n    \n  \n  \n    \n      0\n      0\n      0\n      10-Day Green Smoothie Cleanse\n      JJ Smith\n      4.7\n      17350\n      8\n      2016\n      Non Fiction\n    \n    \n      1\n      1\n      1\n      11/22/63: A Novel\n      Stephen King\n      4.6\n      2052\n      22\n      2011\n      Fiction\n    \n    \n      2\n      2\n      2\n      12 Rules for Life: An Antidote to Chaos\n      Jordan B. Peterson\n      4.7\n      18979\n      15\n      2018\n      Non Fiction\n    \n    \n      3\n      3\n      3\n      1984 (Signet Classics)\n      George Orwell\n      4.7\n      21424\n      6\n      2017\n      Fiction\n    \n    \n      4\n      4\n      4\n      5,000 Awesome Facts (About Everything!) (Natio...\n      National Geographic Kids\n      4.8\n      7665\n      12\n      2019\n      Non Fiction\n    \n  \n\n\n\n\n\n\n3.3.3 Reading HTML data\nThe Pandas function read_html searches for tabular data, i.e., data contained within the <table> tags of an html file. Let us read the tables in the GDP per capita page on Wikipedia.\n\n#Reading all the tables from the Wikipedia page on GDP per capita\ntables = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita')\n\nAll the tables will be read and stored in the variable named as tables. Let us find the datatype of the variable tables.\n\n#Finidng datatype of the variable - tables\ntype(tables)\n\nlist\n\n\nThe variable - tables is a list of all the tables read from the HTML data.\n\n#Number of tables read from the page\nlen(tables)\n\n6\n\n\nThe in-built function len can be used to find the length of the list - tables or the number of tables read from the Wikipedia page. Let us check out the first table.\n\n#Checking out the first table. Note that the index of the first table will be 0.\ntables[0]\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      .mw-parser-output .legend{page-break-inside:av...\n      $20,000 - $30,000 $10,000 - $20,000 $5,000 - $...\n      $1,000 - $2,500 $500 - $1,000 <$500 No data\n    \n  \n\n\n\n\nThe above table doesn’t seem to be useful. Let us check out the second table.\n\n#Checking out the second table. Note that the index of the first table will be 1.\ntables[1]\n\n\n\n\n\n  \n    \n      \n      Country/Territory\n      UN Region\n      IMF[4][5]\n      United Nations[6]\n      World Bank[7]\n    \n    \n      \n      Country/Territory\n      UN Region\n      Estimate\n      Year\n      Estimate\n      Year\n      Estimate\n      Year\n    \n  \n  \n    \n      0\n      Liechtenstein *\n      Europe\n      —\n      —\n      180227\n      2020\n      169049\n      2019\n    \n    \n      1\n      Monaco *\n      Europe\n      —\n      —\n      173696\n      2020\n      173688\n      2020\n    \n    \n      2\n      Luxembourg *\n      Europe\n      135046\n      2022\n      117182\n      2020\n      135683\n      2021\n    \n    \n      3\n      Bermuda *\n      Americas\n      —\n      —\n      123945\n      2020\n      110870\n      2021\n    \n    \n      4\n      Ireland *\n      Europe\n      101509\n      2022\n      86251\n      2020\n      85268\n      2020\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      212\n      Central AfricanRepublic *\n      Africa\n      527\n      2022\n      481\n      2020\n      477\n      2020\n    \n    \n      213\n      Sierra Leone *\n      Africa\n      513\n      2022\n      475\n      2020\n      485\n      2020\n    \n    \n      214\n      Madagascar *\n      Africa\n      504\n      2022\n      470\n      2020\n      496\n      2020\n    \n    \n      215\n      South Sudan *\n      Africa\n      393\n      2022\n      1421\n      2020\n      1120\n      2015\n    \n    \n      216\n      Burundi *\n      Africa\n      272\n      2022\n      286\n      2020\n      274\n      2020\n    \n  \n\n217 rows × 8 columns\n\n\n\nThe above table contains the estimated GDP per capita of all countries. This is the table that is likely to be relevant to a user interested in analyzing GDP per capita of countries. Instead of reading all tables of an HTML file, we can focus the search to tables containing certain relevant keywords. Let us try searching all table containing the word ‘Country’.\n\n#Reading all the tables from the Wikipedia page on GDP per capita, containing the word 'Country'\ntables = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita', match = 'Country')\n\nThe match argument can be used to specify the keywords to be present in the table to be read.\n\nlen(tables)\n\n1\n\n\nOnly one table contains the keyword - ‘Country’. Let us check out the table obtained.\n\n#Table having the keyword - 'Country' from the HTML page\ntables[0]\n\n\n\n\n\n  \n    \n      \n      Country/Territory\n      UN Region\n      IMF[4][5]\n      United Nations[6]\n      World Bank[7]\n    \n    \n      \n      Country/Territory\n      UN Region\n      Estimate\n      Year\n      Estimate\n      Year\n      Estimate\n      Year\n    \n  \n  \n    \n      0\n      Liechtenstein *\n      Europe\n      —\n      —\n      180227\n      2020\n      169049\n      2019\n    \n    \n      1\n      Monaco *\n      Europe\n      —\n      —\n      173696\n      2020\n      173688\n      2020\n    \n    \n      2\n      Luxembourg *\n      Europe\n      135046\n      2022\n      117182\n      2020\n      135683\n      2021\n    \n    \n      3\n      Bermuda *\n      Americas\n      —\n      —\n      123945\n      2020\n      110870\n      2021\n    \n    \n      4\n      Ireland *\n      Europe\n      101509\n      2022\n      86251\n      2020\n      85268\n      2020\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      212\n      Central AfricanRepublic *\n      Africa\n      527\n      2022\n      481\n      2020\n      477\n      2020\n    \n    \n      213\n      Sierra Leone *\n      Africa\n      513\n      2022\n      475\n      2020\n      485\n      2020\n    \n    \n      214\n      Madagascar *\n      Africa\n      504\n      2022\n      470\n      2020\n      496\n      2020\n    \n    \n      215\n      South Sudan *\n      Africa\n      393\n      2022\n      1421\n      2020\n      1120\n      2015\n    \n    \n      216\n      Burundi *\n      Africa\n      272\n      2022\n      286\n      2020\n      274\n      2020\n    \n  \n\n217 rows × 8 columns\n\n\n\nThe argument match helps with a more focussed search, and helps us discard irrelevant tables.\n\n\n3.3.4 Practice exercise 5\nRead the table(s) consisting of attendance of spectators in FIFA worlds cup from this page. Read only those table(s) that have the word ‘attendance’ in them. How many rows and columns are there in the table(s)?\n\ndfs = pd.read_html('https://en.wikipedia.org/wiki/FIFA_World_Cup',\n                       match='attendance')\nprint(len(dfs))\ndata = dfs[0]\nprint(\"Number of rows =\",data.shape[0], \"and number of columns=\",data.shape[1])\n\n1\nNumber of rows = 22 and number of columns= 9\n\n\n\n\n3.3.5 Reading JSON data\nJSON stands for JavaScript Object Notation, in which the data is stored and transmitted as plain text. A couple of benefits of the JSON format are:\n\nSince the format is text only, JSON data can easily be exchanged between web applications, and used by any programming language.\nUnlike the csv format, JSON supports a hierarchical data structure, and is easier to integrate with APIs.\n\nThe JSON format can support a hierachical data structure, as it is built on the following two data structures (Source: technical documentation):\n\nA collection of name/value pairs. In various languages, this is realized as an object, record, struct, dictionary, hash table, keyed list, or associative array.\nAn ordered list of values. In most languages, this is realized as an array, vector, list, or sequence.\n\nThese are universal data structures. Virtually all modern programming languages support them in one form or another. It makes sense that a data format that is interchangeable with programming languages also be based on these structures.\nThe Pandas function read_json converts a JSON string to a Pandas DataFrame. The function dumps() of the json library converts a Python object to a JSON string.\nLets read the JSON data on Ted Talks.\n\ntedtalks_data = pd.read_json('https://raw.githubusercontent.com/cwkenwaysun/TEDmap/master/data/TED_Talks.json')\n\n\ntedtalks_data.head()\n\n\n\n\n\n  \n    \n      \n      id\n      speaker\n      headline\n      URL\n      description\n      transcript_URL\n      month_filmed\n      year_filmed\n      event\n      duration\n      date_published\n      tags\n      newURL\n      date\n      views\n      rates\n    \n  \n  \n    \n      0\n      7\n      David Pogue\n      Simplicity sells\n      http://www.ted.com/talks/view/id/7\n      New York Times columnist David Pogue takes aim...\n      http://www.ted.com/talks/view/id/7/transcript?...\n      2\n      2006\n      TED2006\n      0:21:26\n      6/27/06\n      simplicity,computers,software,interface design...\n      https://www.ted.com/talks/david_pogue_says_sim...\n      2006-06-27\n      1646773\n      [{'id': 7, 'name': 'Funny', 'count': 968}, {'i...\n    \n    \n      1\n      6\n      Craig Venter\n      Sampling the ocean's DNA\n      http://www.ted.com/talks/view/id/6\n      Genomics pioneer Craig Venter takes a break fr...\n      http://www.ted.com/talks/view/id/6/transcript?...\n      7\n      2005\n      TEDGlobal 2005\n      0:16:51\n      2004/05/07\n      biotech,invention,oceans,genetics,DNA,biology,...\n      https://www.ted.com/talks/craig_venter_on_dna_...\n      2004-05-07\n      562625\n      [{'id': 3, 'name': 'Courageous', 'count': 21},...\n    \n    \n      2\n      4\n      Burt Rutan\n      The real future of space exploration\n      http://www.ted.com/talks/view/id/4\n      In this passionate talk, legendary spacecraft ...\n      http://www.ted.com/talks/view/id/4/transcript?...\n      2\n      2006\n      TED2006\n      0:19:37\n      10/25/06\n      aircraft,flight,industrial design,NASA,rocket ...\n      https://www.ted.com/talks/burt_rutan_sees_the_...\n      2006-10-25\n      2046869\n      [{'id': 3, 'name': 'Courageous', 'count': 169}...\n    \n    \n      3\n      3\n      Ashraf Ghani\n      How to rebuild a broken state\n      http://www.ted.com/talks/view/id/3\n      Ashraf Ghani's passionate and powerful 10-minu...\n      http://www.ted.com/talks/view/id/3/transcript?...\n      7\n      2005\n      TEDGlobal 2005\n      0:18:45\n      10/18/06\n      corruption,poverty,economics,investment,milita...\n      https://www.ted.com/talks/ashraf_ghani_on_rebu...\n      2006-10-18\n      814554\n      [{'id': 3, 'name': 'Courageous', 'count': 140}...\n    \n    \n      4\n      5\n      Chris Bangle\n      Great cars are great art\n      http://www.ted.com/talks/view/id/5\n      American designer Chris Bangle explains his ph...\n      http://www.ted.com/talks/view/id/5/transcript?...\n      2\n      2002\n      TED2002\n      0:20:04\n      2004/05/07\n      cars,industrial design,transportation,inventio...\n      https://www.ted.com/talks/chris_bangle_says_gr...\n      2004-05-07\n      870950\n      [{'id': 1, 'name': 'Beautiful', 'count': 89}, ...\n    \n  \n\n\n\n\n\n\n3.3.6 Practice exercise 6\nRead the movies dataset from here. How many rows and columns are there in the data?\n\nmovies_data = pd.read_json('https://raw.githubusercontent.com/vega/vega-datasets/master/data/movies.json')\nprint(\"Number of rows =\",movies_data.shape[0], \"and number of columns=\",movies_data.shape[1])\n\nNumber of rows = 3201 and number of columns= 16\n\n\n\n\n3.3.7 Reading data from web APIs\nAPI, an acronym for Application programming interface, is a way of transferring information between systems. Many websites have public APIs that provide data via JSON or other formats. For example, the IMDb-API is a web service for receiving movies, serial, and cast information. API results are in the JSON format and include items such as movie specifications, ratings, Wikipedia page content, etc. One of these APIs contains ratings of the top 250 movies on IMDB. Let us read this data using the IMDB API.\nWe’ll use the get function from the python library requests to request data from the API and obtain a response code. The response code will let us know if our request to pull data from this API was successful.\n\n#Importing the requests library\nimport requests as rq\n\n\n# Downloading imdb top 250 movie's data\nurl = 'https://imdb-api.com/en/API/Top250Movies/k_v6gf8ppf' #URL of the API containing top 250 movies based on IMDB ratings\nresponse = rq.get(url) #Requesting data from the API\nresponse\n\n<Response [200]>\n\n\nWe have a response code of 200, which indicates that the request was successful.\nThe response object’s JSON method will return a dictionary containing JSON parsed into native Python objects.\n\nmovie_data = response.json()\n\n\nmovie_data.keys()\n\ndict_keys(['items', 'errorMessage'])\n\n\nThe movie_data contains only two keys. The items key seems likely to contain information about the top 250 movies. Let us convert the values of the items key (which is list of dictionaries) to a dataframe, so that we can view it in a tabular form.\n\n#Converting a list of dictionaries to a dataframe\nmovie_data_df = pd.DataFrame(movie_data['items'])\n\n\n#Checking the movie data pulled using the API\nmovie_data_df.head()\n\n\n\n\n\n  \n    \n      \n      id\n      rank\n      title\n      fullTitle\n      year\n      image\n      crew\n      imDbRating\n      imDbRatingCount\n    \n  \n  \n    \n      0\n      tt0111161\n      1\n      The Shawshank Redemption\n      The Shawshank Redemption (1994)\n      1994\n      https://m.media-amazon.com/images/M/MV5BMDFkYT...\n      Frank Darabont (dir.), Tim Robbins, Morgan Fre...\n      9.2\n      2624065\n    \n    \n      1\n      tt0068646\n      2\n      The Godfather\n      The Godfather (1972)\n      1972\n      https://m.media-amazon.com/images/M/MV5BM2MyNj...\n      Francis Ford Coppola (dir.), Marlon Brando, Al...\n      9.2\n      1817542\n    \n    \n      2\n      tt0468569\n      3\n      The Dark Knight\n      The Dark Knight (2008)\n      2008\n      https://m.media-amazon.com/images/M/MV5BMTMxNT...\n      Christopher Nolan (dir.), Christian Bale, Heat...\n      9.0\n      2595637\n    \n    \n      3\n      tt0071562\n      4\n      The Godfather Part II\n      The Godfather Part II (1974)\n      1974\n      https://m.media-amazon.com/images/M/MV5BMWMwMG...\n      Francis Ford Coppola (dir.), Al Pacino, Robert...\n      9.0\n      1248050\n    \n    \n      4\n      tt0050083\n      5\n      12 Angry Men\n      12 Angry Men (1957)\n      1957\n      https://m.media-amazon.com/images/M/MV5BMWU4N2...\n      Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb\n      8.9\n      775140\n    \n  \n\n\n\n\n\n#Rows and columns of the movie data\nmovie_data_df.shape\n\n(250, 9)\n\n\nThis API provides the names of the top 250 movies along with the year of release, IMDB ratings, and cast information."
  },
  {
    "objectID": "Reading data.html#writing-data",
    "href": "Reading data.html#writing-data",
    "title": "3  Reading data",
    "section": "3.4 Writing data",
    "text": "3.4 Writing data\nThe Pandas function to_csv can be used to write (or export) data to a csv or txt file. Below are some examples.\nExample 1: Let us export the movies data of the top 250 movies to a csv file.\n\n#Exporting the data of the top 250 movies to a csv file\nmovie_data_df.to_csv('movie_data_exported.csv')\n\nThe file movie_data_exported.csv will appear in the working directory.\nExample 2: Let us export the movies data of the top 250 movies to a txt file with a semi-colon as the delimiter.\n\nmovie_data_df.to_csv('movie_data_exported.txt',sep=';')\n\nExample 3: Let us export the movies data of the top 250 movies to a JSON file.\n\nwith open('movie_data.json', 'w') as f:\n    json.dump(movie_data, f)"
  },
  {
    "objectID": "NumPy.html",
    "href": "NumPy.html",
    "title": "4  NumPy",
    "section": "",
    "text": "NumPy, short for Numerical Python is used to analyze numeric data with Python. NumPy arrays are primarily used to create homogeneous \\(n\\)-dimensional arrays (\\(n = 1,...,n\\)). Let us import the NumPy library to use its methods and functions, and the NumPy function array() to define a NumPy array.\nThe NumPy function array() creates an object of type numpy.ndarray."
  },
  {
    "objectID": "NumPy.html#why-do-we-need-numpy-arrays",
    "href": "NumPy.html#why-do-we-need-numpy-arrays",
    "title": "4  NumPy",
    "section": "4.1 Why do we need NumPy arrays?",
    "text": "4.1 Why do we need NumPy arrays?\nNumPy arrays can store data just like other data structures such as such as lists, tuples, and Pandas DataFrame. Computations performed using NumPy arrays can also be performed with data stored in the other data structures. However, NumPy is preferred for its efficiency, especially when working with large arrays of data.\n\n4.1.1 Numpy arrays are memory efficient\nA NumPy array is a collection of homogeneous data-types that are stored in contiguous memory locations. On the other hand, data structures such as lists are a collection of heterogeneous data types stored in non-contiguous memory locations. Homogenous data elements let the NumPy array be densely packed resulting in lesser memory consumption. The following example illustrates the smaller size of NumPy arrays as compared to other data structures.\n\n#Example showing NumPy arrays take less storage space than lists, tuples and Pandas DataFrame for the same elements\ntuple_ex = tuple(range(1000))\nlist_ex = list(range(1000))\nnumpy_ex = np.array([range(1000)])\npandas_df = pd.DataFrame(range(1000))\nprint(\"Space taken by tuple =\",tuple_ex.__sizeof__(),\" bytes\")\nprint(\"Space taken by list =\",list_ex.__sizeof__(),\" bytes\")\nprint(\"Space taken by Pandas DataFrame =\",pandas_df.__sizeof__(),\" bytes\")\nprint(\"Space taken by NumPy array =\",numpy_ex.__sizeof__(),\" bytes\")\n\nSpace taken by tuple = 8024  bytes\nSpace taken by list = 8040  bytes\nSpace taken by Pandas DataFrame = 8128  bytes\nSpace taken by NumPy array = 4120  bytes\n\n\nNote that NumPy arrays are memory efficient as long as they are homogenous. They will lose the memory efficiency if they are used to store elements of multiple data types.\nThe example below compares the size of a homogenous NumPy array to that of a similar heterogenous NumPy array to illustrate the point.\n\nnumpy_homogenous = np.array([[1,2],[3,3]])\nprint(\"Size of a homogenous numpy array = \",numpy_homogenous.__sizeof__(), \"bytes\")\n\nSize of homogenous numpy array =  136 bytes\n\n\nNow let us convert an element of the above array to a string, and check the size of the array.\n\nnumpy_homogenous = np.array([[1,'2'],[3,3]])\nprint(\"Size of a heterogenous numpy array = \",numpy_homogenous.__sizeof__(), \"bytes\")\n\nSize of a heterogenous numpy array =  296 bytes\n\n\nThe size of the homogenous NumPy array is much lesser than that of the one with heterogenous data. Thus, NumPy arrays are primarily used for storing homogenous data.\nOn the other hand, the size of other data structures, such as a list, does not depend on whether the elements in them are homogenous or heterogenous, as shown by the example below.\n\nlist_homogenous = list([1,2,3,4])\nprint(\"Size of a homogenous list = \",list_homogenous.__sizeof__(), \"bytes\")\nlist_heterogenous = list([1,'2',3,4])\nprint(\"Size of a heterogenous list = \",list_heterogenous.__sizeof__(), \"bytes\")\n\nSize of a homogenous list =  72 bytes\nSize of a heterogenous list =  72 bytes\n\n\nNote that the memory efficiency of NumPy arrays does not come into play with a very small amount of data. Thus, a list with four elements - 1,2,3 and 4, has a lesser size than a NumPy array with the same elements. However, with larger datasets, such as the one shown earlier (sequence of integers from 0 to 999), the memory efficiency of NumPy arrays can be seen.\nUnlike data structures such as lists, tuples, and dictionary, all elements of a NumPy array should be of same type to leverage the memory efficiency of NumPy arrays.\n\n\n4.1.2 NumPy arrays are fast\nWith NumPy arrays, mathematical computations can be performed faster, as compared to other data structures, due to the following reasons:\n\nAs the NumPy array is densely packed with homogenous data, it helps retrieve the data faster as well, thereby making computations faster.\nWith NumPy, vectorized computations can replace the relatively more expensive python for loops. The NumPy package breaks down the vectorized computations into multiple fragments and then processes all the fragments parallelly. However, with a for loop, computations will be one at a time.\nThe NumPy package integrates C, and C++ codes in Python. These programming languages have very little execution time as compared to Python.\n\nWe’ll see the faster speed on NumPy computations in the example below.\nExample: This example shows that computations using NumPy arrays are typically much faster than computations with other data structures.\nQ: Multiply whole numbers upto 1 million by an integer, say 2. Compare the time taken for the computation if the numbers are stored in a NumPy array vs a list.\nUse the numpy function arange() to define a one-dimensional NumPy array.\n\n#Examples showing NumPy arrays are more efficient for numerical computation\nimport time as tm\nstart_time = tm.time()\nlist_ex = list(range(1000000)) #List containinig whole numbers upto 1 million\na=(list_ex*2)\nprint(\"Time take to multiply numbers in a list = \", tm.time()-start_time)\n\nstart_time = tm.time()\ntuple_ex = tuple(range(1000000)) #Tuple containinig whole numbers upto 1 million\na=(tuple_ex*2)\nprint(\"Time take to multiply numbers in a tuple = \", tm.time()-start_time)\n\nstart_time = tm.time()\ndf_ex = pd.DataFrame(range(1000000)) #Pandas DataFrame containinig whole numbers upto 1 million\na=(df_ex*2)\nprint(\"Time take to multiply numbers in a Pandas DataFrame = \", tm.time()-start_time)\n\nstart_time = tm.time()\nnumpy_ex = np.arange(1000000) #NumPy array containinig whole numbers upto 1 million\na=(numpy_ex*2)\nprint(\"Time take to multiply numbers in a NumPy array = \", tm.time()-start_time)\n\nTime take to multiply numbers in a list =  0.023949384689331055\nTime take to multiply numbers in a tuple =  0.03192734718322754\nTime take to multiply numbers in a Pandas DataFrame =  0.047330617904663086\nTime take to multiply numbers in a NumPy array =  0.0"
  },
  {
    "objectID": "NumPy.html#numpy-array-basic-attributes",
    "href": "NumPy.html#numpy-array-basic-attributes",
    "title": "4  NumPy",
    "section": "4.2 NumPy array: Basic attributes",
    "text": "4.2 NumPy array: Basic attributes\nLet us define a NumPy array:\n\nnumpy_ex = np.array([[1,2,3],[4,5,6]])\nnumpy_ex\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\nThe attributes of numpy_ex can be seen by typing numpy_ex followed by a ., and then pressing the tab key.\nSome of the basic attributes of a NumPy array are the following:\n\n4.2.1 ndim\nShows the number of dimensions (or axes) of the array.\n\nnumpy_ex.ndim\n\n2\n\n\n\n\n4.2.2 shape\nThis is a tuple of integers indicating the size of the array in each dimension. For a matrix with n rows and m columns, the shape will be (n,m). The length of the shape tuple is therefore the rank, or the number of dimensions, ndim.\n\nnumpy_ex.shape\n\n(2, 3)\n\n\n\n\n4.2.3 size\nThis is the total number of elements of the array, which is the product of the elements of shape.\n\nnumpy_ex.size\n\n6\n\n\n\n\n4.2.4 dtype\nThis is an object describing the type of the elements in the array. One can create or specify dtype’s using standard Python types. NumPy provides many, for example bool_, character, int_, int8, int16, int32, int64, float_, float8, float16, float32, float64, complex_, complex64, object_.\n\nnumpy_ex.dtype\n\ndtype('int32')\n\n\n\n\n4.2.5 T\nThis attribute is used to transpose the NumPy array. This is often used to make matrices (2-dimensional arrays) compatible for multiplication.\nFor matrix multiplication, the columns of the first matrix must be equal to the rows of the second matrix. For example, consider the matrix below:\n\nmatrix_to_multiply = np.array([[1,2,1],[0,1,0]])\n\nSuppose we wish to multiply this matrix with numpy_ex. Note the shape of both the matrices below.\n\nmatrix_to_multiply.shape\n\n(2, 3)\n\n\n\nnumpy_ex.shape\n\n(2, 3)\n\n\nTo multiply the above matrices the number of columns of the one of the matrices must be the same as the number of rows of the other matrix. With the current matrices, this is not true as the number of columns of the first matrix is 3, and the the number of rows of the second matrix is 2 (no matter which matrix is considered to be the first one).\nHowever, if we transpose one of the matrices, their shapes will be compatible for multiplication. Let’s transpose matrix_to_multiply:\n\nmatrix_to_multiply_transpose = matrix_to_multiply.T\nmatrix_to_multiply_transpose\n\narray([[1, 0],\n       [2, 1],\n       [1, 0]])\n\n\nThe shape of matrix_to_multiply_transpose is:\n\nmatrix_to_multiply_transpose.shape\n\n(3, 2)\n\n\nThe matrices matrix_to_multiply_transpose and numpy_ex are compatible for matrix multiplication. However, the result will depend on the order in which the matrices are multiplied:\n\n#Matrix multiplication with matrix_to_multiply_transpose before numpy_ex\nmatrix_to_multiply_transpose.dot(numpy_ex)\n\narray([[ 1,  2,  3],\n       [ 6,  9, 12],\n       [ 1,  2,  3]])\n\n\n\n#Matrix multiplication with numpy_ex before matrix_to_multiply_transpose\nnumpy_ex.dot(matrix_to_multiply_transpose)\n\narray([[ 8,  2],\n       [20,  5]])\n\n\nThe shape of the resulting matrix is equal to the rows of the first matrix and the columns of the second matrix. The order of matrices must be decided as per the requirements of the problem."
  },
  {
    "objectID": "NumPy.html#arithmetic-operations",
    "href": "NumPy.html#arithmetic-operations",
    "title": "4  NumPy",
    "section": "4.3 Arithmetic operations",
    "text": "4.3 Arithmetic operations\nNumpy arrays support arithmetic operators like +, -, *, etc. We can perform an arithmetic operation on an array either with a single number (also called scalar) or with another array of the same shape. However, we cannot perform an arithmetic operation on an array with an array of a different shape.\nBelow are some examples of arithmetic operations on arrays.\n\n#Defining two arrays of the same shape\narr1 = np.array([[1, 2, 3, 4], \n                 [5, 6, 7, 8], \n                 [9, 1, 2, 3]])\narr2 = np.array([[11, 12, 13, 14], \n                 [15, 16, 17, 18], \n                 [19, 11, 12, 13]])\n\n\n#Element-wise summation of arrays\narr1 + arr2\n\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 12, 14, 16]])\n\n\n\n# Element-wise subtraction\narr2 - arr1\n\narray([[10, 10, 10, 10],\n       [10, 10, 10, 10],\n       [10, 10, 10, 10]])\n\n\n\n# Adding a scalar to an array adds the scalar to each element of the array\narr1 + 3\n\narray([[ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12,  4,  5,  6]])\n\n\n\n# Dividing an array by a scalar divides all elements of the array by the scalar\narr1 / 2\n\narray([[0.5, 1. , 1.5, 2. ],\n       [2.5, 3. , 3.5, 4. ],\n       [4.5, 0.5, 1. , 1.5]])\n\n\n\n# Element-wise multiplication\narr1 * arr2\n\narray([[ 11,  24,  39,  56],\n       [ 75,  96, 119, 144],\n       [171,  11,  24,  39]])\n\n\n\n# Modulus operator with scalar\narr1 % 4\n\narray([[1, 2, 3, 0],\n       [1, 2, 3, 0],\n       [1, 1, 2, 3]], dtype=int32)"
  },
  {
    "objectID": "NumPy.html#broadcasting",
    "href": "NumPy.html#broadcasting",
    "title": "4  NumPy",
    "section": "4.4 Broadcasting",
    "text": "4.4 Broadcasting\nBroadcasting allows arithmetic operations between two arrays with different numbers of dimensions but compatible shapes.\nThe Broadcasting documentation succinctly explains it as the following:\n“The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is broadcast across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations.”\nThe example below shows the broadcasting of two arrays.\n\narr1 = np.array([[1, 2, 3, 4], \n                 [5, 6, 7, 8], \n                 [9, 1, 2, 3]])\narr2 = np.array([4, 5, 6, 7])\n\n\narr1 + arr2\n\narray([[ 5,  7,  9, 11],\n       [ 9, 11, 13, 15],\n       [13,  6,  8, 10]])\n\n\nWhen the expression arr1 + arr2 is evaluated, arr2 (which has the shape (4,)) is replicated three times to match the shape (3, 4) of arr1. Numpy performs the replication without actually creating three copies of the smaller dimension array, thus improving performance and using lower memory.\nIn the above addition of arrays, arr2 was stretched or broadcast to the shape of arr1. However, this broadcasting was possible only because the right dimension of both the arrays is 4, and the left dimension of one of the arrays is 1.\nSee the broadcasting documentation to understand the rules for broadcasting:\n“When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when:\n\nthey are equal, or\none of them is 1”\n\nIf the rightmost dimension of arr2 is 3, broadcasting will not occur, as it is not equal to the rightmost dimension of arr1:\n\n#Defining arr2 as an array of shape (3,)\narr2 = np.array([4, 5, 6])\n\n\n#Broadcasting will not happen when the broadcasting rules are violated\narr1 + arr2\n\nValueError: operands could not be broadcast together with shapes (3,4) (3,)"
  },
  {
    "objectID": "NumPy.html#comparison",
    "href": "NumPy.html#comparison",
    "title": "4  NumPy",
    "section": "4.5 Comparison",
    "text": "4.5 Comparison\nNumpy arrays support comparison operations like ==, !=, > etc. The result is an array of booleans.\n\narr1 = np.array([[1, 2, 3], [3, 4, 5]])\narr2 = np.array([[2, 2, 3], [1, 2, 5]])\n\n\narr1 == arr2\n\narray([[False,  True,  True],\n       [False, False,  True]])\n\n\n\narr1 != arr2\n\narray([[ True, False, False],\n       [ True,  True, False]])\n\n\n\narr1 >= arr2\n\narray([[False,  True,  True],\n       [ True,  True,  True]])\n\n\n\narr1 < arr2\n\narray([[ True, False, False],\n       [False, False, False]])\n\n\nArray comparison is frequently used to count the number of equal elements in two arrays using the sum method. Remember that True evaluates to 1 and False evaluates to 0 when booleans are used in arithmetic operations.\n\n(arr1 == arr2).sum()\n\n3"
  },
  {
    "objectID": "NumPy.html#concatenating-arrays",
    "href": "NumPy.html#concatenating-arrays",
    "title": "4  NumPy",
    "section": "4.6 Concatenating arrays",
    "text": "4.6 Concatenating arrays\nArrays can be concatenated along an axis with NumPy’s concatenate function. The axis argument specifies the dimension for concatenation. The arrays should have the same number of dimensions, and the same length along each axis except the axis used for concatenation.\nThe examples below show concatenation of arrays.\n\narr1 = np.array([[1, 2, 3], [3, 4, 5]])\narr2 = np.array([[2, 2, 3], [1, 2, 5]])\nprint(\"Array 1:\\n\",arr1)\nprint(\"Array 2:\\n\",arr2)\n\nArray 1:\n [[1 2 3]\n [3 4 5]]\nArray 2:\n [[2 2 3]\n [1 2 5]]\n\n\n\n#Concatenating the arrays along the default axis: axis=0\nnp.concatenate((arr1,arr2))\n\narray([[1, 2, 3],\n       [3, 4, 5],\n       [2, 2, 3],\n       [1, 2, 5]])\n\n\n\n#Concatenating the arrays along axis = 1\nnp.concatenate((arr1,arr2),axis=1)\n\narray([[1, 2, 3, 2, 2, 3],\n       [3, 4, 5, 1, 2, 5]])\n\n\nSince the arrays need to have the same dimension only along the axis of concatenation, let us try concatenate the array below (arr3) with arr1, along axis = 0.\n\narr3 = np.array([2, 2, 3])\n\n\nnp.concatenate((arr1,arr3),axis=0)\n\nValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)\n\n\nNote the above error, which indicates that arr3 has only one dimension. Let us check the shape of arr3.\n\narr3.shape\n\n(3,)\n\n\nWe can reshape arr3 to a shape of (1,3) to make it compatible for concatenation with arr1 along axis = 0.\n\narr3_reshaped = arr3.reshape(1,3)\narr3_reshaped\n\narray([[2, 2, 3]])\n\n\nNow we can concatenate the reshaped arr3 with arr1 along axis = 0.\n\nnp.concatenate((arr1,arr3_reshaped),axis=0)\n\narray([[1, 2, 3],\n       [3, 4, 5],\n       [2, 2, 3]])"
  },
  {
    "objectID": "NumPy.html#practice-exercise-1",
    "href": "NumPy.html#practice-exercise-1",
    "title": "4  NumPy",
    "section": "4.7 Practice exercise 1",
    "text": "4.7 Practice exercise 1\n\n4.7.0.1 \nRead the coordinates of the capital cities of the world from http://techslides.com/list-of-countries-and-capitals . Use NumPy to print the name and coordinates of the capital city closest to the US capital - Washington DC.\nNote that:\n\nThe Country Name for US is given as United States in the data.\nThe ‘closeness’ of capital cities from the US capital is based on the Euclidean distance of their coordinates to those of the US capital.\n\nHints:\n\nUse read_html() from the Pandas library to read the table.\nUse the to_numpy() function of the Pandas DataFrame class to convert a DataFrame to a Numpy array\nUse broadcasting to compute the euclidean distance of capital cities from Washington DC.\n\nSolution:\n\nimport pandas as pd\ncapital_cities = pd.read_html('http://techslides.com/list-of-countries-and-capitals',header=0)[0]\ncoordinates_capital_cities = capital_cities.iloc[:,2:4].to_numpy()\nus_coordinates = capital_cities.loc[capital_cities['Country Name']=='United States',['Capital Latitude','Capital Longitude']].to_numpy()\n\n#Broadcasting\ndistance_from_DC = np.sqrt(np.sum((us_coordinates-coordinates_capital_cities)**2,axis=1))\n\n#Assigning a high value of distance to DC, otherwise it will itself be selected as being closest to DC\ndistance_from_DC[distance_from_DC==0]=9999\nclosest_capital_index = np.argmin(distance_from_DC)\nprint(\"Closest capital city is:\" ,capital_cities.loc[closest_capital_index,'Capital Name'])\nprint(\"Coordinates of the closest capital city are:\",coordinates_capital_cities[closest_capital_index,:])\n\nClosest capital city is: Ottawa\nCoordinates of the closest capital city are: [ 45.41666667 -75.7       ]\n\n\n\n\n4.7.0.2 \nUse NumPy to:\n\nPrint the names of the countries of the top 10 capital cities closest to the US capital - Washington DC.\nCreate and print a NumPy array containing the coordinates of the top 10 cities.\n\nHint: Use the concatenate() function from the NumPy library to stack the coordinates of the top 10 cities.\n\ntop10_cities_coordinates = coordinates_capital_cities[closest_capital_index,:].reshape(1,2)\nprint(\"Top 10 countries closest to Washington DC are:\\n Canada\")\nfor i in range(9):\n    distance_from_DC[closest_capital_index]=9999\n    closest_capital_index = np.argmin(distance_from_DC)\n    print(capital_cities.loc[closest_capital_index,'Country Name'])\n    top10_cities_coordinates=np.concatenate((top10_cities_coordinates,coordinates_capital_cities[closest_capital_index,:].reshape(1,2)))\nprint(\"Coordinates of the top 10 cities closest to US are: \\n\",top10_cities_coordinates)\n\nTop 10 countries closest to Washington DC are:\n Canada\nBahamas\nBermuda\nCuba\nTurks and Caicos Islands\nCayman Islands\nHaiti\nJamaica\nDominican Republic\nSaint Pierre and Miquelon\nCoordinates of the top 10 cities closest to US are: \n [[ 45.41666667 -75.7       ]\n [ 25.08333333 -77.35      ]\n [ 32.28333333 -64.783333  ]\n [ 23.11666667 -82.35      ]\n [ 21.46666667 -71.133333  ]\n [ 19.3        -81.383333  ]\n [ 18.53333333 -72.333333  ]\n [ 18.         -76.8       ]\n [ 18.46666667 -69.9       ]\n [ 46.76666667 -56.183333  ]]"
  },
  {
    "objectID": "NumPy.html#vectorized-computation-with-numpy",
    "href": "NumPy.html#vectorized-computation-with-numpy",
    "title": "4  NumPy",
    "section": "4.8 Vectorized computation with NumPy",
    "text": "4.8 Vectorized computation with NumPy\nSeveral matrix algebra operations such as multiplications, decompositions, determinants, etc. can be performed conveniently with NumPy. However, we’ll focus on matrix multiplication as it is very commonly used to avoid python for loops and make computations faster. The dot function is used to multiply matrices:\n\n#Defining a 2x2 matrix\na = np.array([[0,1],[3,4]])\na\n\narray([[0, 1],\n       [3, 4]])\n\n\n\n#Defining a 2x2 matrix\nb = np.array([[6,-1],[2,1]])\nb\n\narray([[ 6, -1],\n       [ 2,  1]])\n\n\n\n#Multiplying matrices 'a' and 'b' using the dot function\na.dot(b)\n\narray([[ 2,  1],\n       [26,  1]])\n\n\n\n#Note that * results in element-wise multiplication\na*b\n\narray([[ 0, -1],\n       [ 6,  4]])\n\n\nExample 2: This example will show vectorized computations with NumPy. Vectorized computations help perform computations more efficiently, and also make the code concise.\nQ: Read the (1) quantities of roll, bun, cake and bread required by 3 people - Ben, Barbara & Beth, from food_quantity.csv, (2) price of these food items in two shops - Target and Kroger, from price.csv. Find out which shop should each person go to minimize their expenses.\n\n#Reading the datasets on food quantity and price\nimport pandas as pd\nfood_qty = pd.read_csv('./Datasets/food_quantity.csv')\nprice = pd.read_csv('./Datasets/price.csv')\n\n\nfood_qty\n\n\n\n\n\n  \n    \n      \n      Person\n      roll\n      bun\n      cake\n      bread\n    \n  \n  \n    \n      0\n      Ben\n      6\n      5\n      3\n      1\n    \n    \n      1\n      Barbara\n      3\n      6\n      2\n      2\n    \n    \n      2\n      Beth\n      3\n      4\n      3\n      1\n    \n  \n\n\n\n\n\nprice\n\n\n\n\n\n  \n    \n      \n      Item\n      Target\n      Kroger\n    \n  \n  \n    \n      0\n      roll\n      1.5\n      1.0\n    \n    \n      1\n      bun\n      2.0\n      2.5\n    \n    \n      2\n      cake\n      5.0\n      4.5\n    \n    \n      3\n      bread\n      16.0\n      17.0\n    \n  \n\n\n\n\nFirst, let’s start from a simple problem. We’ll compute the expenses of Ben if he prefers to buy all food items from Target\n\n#Method 1: Using loop\nbens_target_expense = 0 #Initializing Ben's expenses to 0\nfor k in range(4):   #Iterating over all the four desired food items\n    bens_target_expense += food_qty.iloc[0,k+1]*price.iloc[k,1] #Total expenses on the kth item\nbens_target_expense    #Total expenses for Ben if he goes to Target\n\n50.0\n\n\n\n#Method 2: Using NumPy array\nfood_num = food_qty.iloc[0,1:].to_numpy()  #Converting food quantity (for Ben) dataframe to NumPy array\nprice_num = price.iloc[:,1].to_numpy()     #Converting price (for Target) dataframe to NumPy array\nfood_num.dot(price_num)   #Matrix multiplication of the quantity vector with the price vector directly yields the result\n\n50.0\n\n\nBen will spend $50 if he goes to Target\nNow, let’s add another layer of complication. We’ll compute Ben’s expenses for both stores - Target and Kroger\n\n#Method 1: Using loops\n\n#Initializing a Series of length two to store the expenses in Target and Kroger for Ben\nbens_store_expense = pd.Series(0.0,index=price.columns[1:3])\nfor j in range(2):      #Iterating over both the stores - Target and Kroger\n    for k in range(4):        #Iterating over all the four desired food items\n        bens_store_expense[j] += food_qty.iloc[0,k+1]*price.iloc[k,j+1]\nbens_store_expense\n\nTarget    50.0\nKroger    49.0\ndtype: float64\n\n\n\n#Method 2: Using NumPy array\nfood_num = food_qty.iloc[0,1:].to_numpy()  #Converting food quantity (for Ben) dataframe to NumPy array\nprice_num = price.iloc[:,1:].to_numpy()    #Converting price dataframe to NumPy array\nfood_num.dot(price_num)      #Matrix multiplication of the quantity vector with the price matrix directly yields the result\n\narray([50.0, 49.0], dtype=object)\n\n\nBen will spend \\$50 if he goes to Target, and $49 if he goes to Kroger. Thus, he should choose Kroger.\nNow, let’s add the final layer of complication, and solve the problem. We’ll compute everyone’s expenses for both stores - Target and Kroger\n\n#Method 1: Using loops\nstore_expense = pd.DataFrame(0.0,index=price.columns[1:3],columns = food_qty['Person'])\nfor i in range(3):    #Iterating over all the three people - Ben, Barbara, and Beth\n    for j in range(2):     #Iterating over both the stores - Target and Kroger\n        for k in range(4):        #Iterating over all the four desired food items\n            store_expense.iloc[j,i] += food_qty.iloc[i,k+1]*price.iloc[k,j+1]\nstore_expense\n\n\n\n\n\n  \n    \n      Person\n      Ben\n      Barbara\n      Beth\n    \n  \n  \n    \n      Target\n      50.0\n      58.5\n      43.5\n    \n    \n      Kroger\n      49.0\n      61.0\n      43.5\n    \n  \n\n\n\n\n\n#Method 2: Using NumPy array\nfood_num = food_qty.iloc[:,1:].to_numpy() #Converting food quantity dataframe to NumPy array\nprice_num = price.iloc[:,1:].to_numpy()  #Converting price dataframe to NumPy array\nfood_num.dot(price_num)  #Matrix multiplication of the quantity matrix with the price matrix directly yields the result\n\narray([[50. , 49. ],\n       [58.5, 61. ],\n       [43.5, 43.5]])\n\n\nBased on the above table, Ben should go to Kroger, Barbara to Target and Beth can go to either store.\nNote that, with each layer of complication, the number of for loops keep increasing, thereby increasing the complexity of Method 1, while the method with NumPy array does not change much. Vectorized computations with arrays are much more efficient.\n\n4.8.1 Practice exercise 2\nUse matrix multiplication to find the average IMDB rating and average Rotten tomatoes rating for each genre - comedy, action, drama and horror. Use the data: movies_cleaned.csv. Which is the most preferred genre for IMDB users, and which is the least preferred genre for Rotten Tomatoes users?\nHint: 1. Create two matrices - one containing the IMDB and Rotten Tomatoes ratings, and the other containing the genre flags (comedy/action/drama/horror).\n\nMultiply the two matrices created in 1.\nDivide each row/column of the resulting matrix by a vector having the number of ratings in each genre to get the average rating for the genre.\n\nSolution:\n\nimport pandas as pd\ndata = pd.read_csv('./Datasets/movies_cleaned.csv')\ndata.head()\n\n\n\n\n\n  \n    \n      \n      Title\n      IMDB Rating\n      Rotten Tomatoes Rating\n      Running Time min\n      Release Date\n      US Gross\n      Worldwide Gross\n      Production Budget\n      comedy\n      Action\n      drama\n      horror\n    \n  \n  \n    \n      0\n      Broken Arrow\n      5.8\n      55\n      108\n      Feb 09 1996\n      70645997\n      148345997\n      65000000\n      0\n      1\n      0\n      0\n    \n    \n      1\n      Brazil\n      8.0\n      98\n      136\n      Dec 18 1985\n      9929135\n      9929135\n      15000000\n      1\n      0\n      0\n      0\n    \n    \n      2\n      The Cable Guy\n      5.8\n      52\n      95\n      Jun 14 1996\n      60240295\n      102825796\n      47000000\n      1\n      0\n      0\n      0\n    \n    \n      3\n      Chain Reaction\n      5.2\n      13\n      106\n      Aug 02 1996\n      21226204\n      60209334\n      55000000\n      0\n      1\n      0\n      0\n    \n    \n      4\n      Clash of the Titans\n      5.9\n      65\n      108\n      Jun 12 1981\n      30000000\n      30000000\n      15000000\n      0\n      1\n      0\n      0\n    \n  \n\n\n\n\n\n# Getting ratings of all movies\ndrating = data[['IMDB Rating','Rotten Tomatoes Rating']]\ndrating_num = drating.to_numpy() #Converting the data to NumPy array\ndrating_num\n\narray([[ 5.8, 55. ],\n       [ 8. , 98. ],\n       [ 5.8, 52. ],\n       ...,\n       [ 7. , 65. ],\n       [ 5.7, 26. ],\n       [ 6.7, 82. ]])\n\n\n\n# Getting the matrix indicating the genre of all movies\ndgenre = data.iloc[:,8:12]\ndgenre_num = dgenre.to_numpy() #Converting the data to NumPy array\ndgenre_num\n\narray([[0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       ...,\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 1, 0, 0]], dtype=int64)\n\n\nWe’ll first find the total IMDB and Rotten tomatoes ratings for all movies of each genre, and then divide them by the number of movies of the corresponding genre to find the average rating for the genre.\nFor finding the total IMDB and Rotten tomatoes ratings, we’ll multiply drating_num with dgenre_num. However, before multiplying, we’ll check if their shapes are compatible for matrix multiplication.\n\n#Shape of drating_num\ndrating_num.shape\n\n(980, 2)\n\n\n\n#Shape of dgenre_num\ndgenre_num.shape\n\n(980, 4)\n\n\nNote that the above shapes are not compatible for matrix multiplication. We’ll transpose dgenre_num to make the shapes compatible.\n\n#Total IMDB and Rotten tomatoes ratings for each genre\nratings_sum_genre = drating_num.T.dot(dgenre_num)\nratings_sum_genre\n\narray([[ 1785.6,  1673.1,  1630.3,   946.2],\n       [14119. , 13725. , 14535. ,  6533. ]])\n\n\n\n#Number of movies in the data will be stored in 'rows', and number of columns stored in 'cols'\nrows, cols = data.shape\n\n\n#Getting number of movies in each genre\nmovies_count_genre = dgenre_num.T.dot(np.ones(rows))\nmovies_count_genre\n\narray([302., 264., 239., 154.])\n\n\n\n#Finding the average IMDB and average Rotten tomatoes ratings for each genre\nratings_sum_genre/movies_count_genre\n\narray([[ 5.91258278,  6.3375    ,  6.82133891,  6.14415584],\n       [46.75165563, 51.98863636, 60.81589958, 42.42207792]])\n\n\n\npd.DataFrame(ratings_sum_genre/movies_count_genre,columns = ['comedy','Action','drama','horror'],\n             index = ['IMDB Rating','Rotten Tomatoes Rating'])\n\n\n\n\n\n  \n    \n      \n      comedy\n      Action\n      drama\n      horror\n    \n  \n  \n    \n      IMDB Rating\n      5.912583\n      6.337500\n      6.821339\n      6.144156\n    \n    \n      Rotten Tomatoes Rating\n      46.751656\n      51.988636\n      60.815900\n      42.422078\n    \n  \n\n\n\n\nIMDB users prefer drama, and are amused the least by comedy movies, on an average. However, Rotten tomatoes critics would rather watch comedy than horror movies, on an average."
  },
  {
    "objectID": "NumPy.html#pseudorandom-number-generation",
    "href": "NumPy.html#pseudorandom-number-generation",
    "title": "4  NumPy",
    "section": "4.9 Pseudorandom number generation",
    "text": "4.9 Pseudorandom number generation\nRandom numbers often need to be generated to analyze processes or systems, especially in cases when these processes or systems are governed by known probability distrbutions. For example, the number of personnel required to answer calls at a call center can be analyzed by simulating occurence and duration of calls.\nNumPy’s random module can be used to generate arrays of random numbers from several different probability distributions. For example, a 3x5 array of uniformly distributed random numbers can be generated using the uniform function of the random module.\n\nnp.random.uniform(size = (3,5))\n\narray([[0.69256322, 0.69259973, 0.03515058, 0.45186048, 0.43513769],\n       [0.07373366, 0.07465425, 0.92195975, 0.72915895, 0.8906299 ],\n       [0.15816734, 0.88144978, 0.05954028, 0.81403832, 0.97725557]])\n\n\nRandom numbers can also be generated by Python’s built-in random module. However, it generates one random number at a time, which makes it much slower than NumPy’s random module.\nExample: Suppose 500 people eat at Food cart 1, and another 500 eat at Food cart 2, everyday.\nThe waiting time at Food cart 2 has a normal distribution with mean 8 minutes and standard deviation 3 minutes, while the waiting time at Food cart 1 has a uniform distribution with minimum 5 minutes and maximum 25 minutes.\nSimulate a dataset containing waiting times for 500 ppl for 30 days in each of the food joints. Assume that the waiting times are measured simultaneously at a certain time in both places, i.e., the observations are paired.\nOn how many days is the average waiting time at Food cart 2 higher than that at Food cart 1?\nWhat percentage of times the waiting time at Food cart 2 was higher than the waiting time at Food cart 1?\nTry both approaches: (1) Using loops to generate data, (2) numpy array to generate data. Compare the time taken in both approaches.\n\nimport time as tm\n\n\n#Method 1: Using loops\nstart_time = tm.time() #Current system time\n\n#Initializing waiting times for 500 ppl over 30 days\nwaiting_times_FoodCart1 = pd.DataFrame(0,index=range(500),columns=range(30)) #FoodCart1\nwaiting_times_FoodCart2 = pd.DataFrame(0,index=range(500),columns=range(30)) #FoodCart2\nimport random as rm\nfor i in range(500):  #Iterating over 500 ppl\n    for j in range(30): #Iterating over 30 days\n        waiting_times_FoodCart2.iloc[i,j] = rm.gauss(8,3) #Simulating waiting time in FoodCart2 for the ith person on jth day\n        waiting_times_FoodCart1.iloc[i,j] = rm.uniform(5,25) #Simulating waiting time in FoodCart1 for the ith person on jth day\ntime_diff = waiting_times_FoodCart2-waiting_times_FoodCart1\n\nprint(\"On \",sum(time_diff.mean()>0),\" days, the average waiting time at FoodCart2 higher than that at FoodCart1\")\nprint(\"Percentage of times waiting time at FoodCart2 was greater than that at FoodCart1 = \",100*(time_diff>0).sum().sum()/(30*500),\"%\")\nend_time = tm.time() #Current system time\nprint(\"Time taken = \", end_time-start_time)\n\nOn  0  days, the average waiting time at FoodCart2 higher than that at FoodCart1\nPercentage of times waiting time at FoodCart2 was greater than that at FoodCart1 =  16.226666666666667 %\nTime taken =  4.521248817443848\n\n\n\n#Method 2: Using NumPy arrays\nstart_time = tm.time()\nwaiting_time_FoodCart2 = np.random.normal(8,3,size = (500,30)) #Simultaneously generating the waiting times of 500 ppl over 30 days in FoodCart2\nwaiting_time_FoodCart1 = np.random.uniform(5,25,size = (500,30)) #Simultaneously generating the waiting times of 500 ppl over 30 days in FoodCart1\ntime_diff = waiting_time_FoodCart2-waiting_time_FoodCart1\nprint(\"On \",(time_diff.mean()>0).sum(),\" days, the average waiting time at FoodCart2 higher than that at FoodCart1\")\nprint(\"Percentage of times waiting time at FoodCart2 was greater than that at FoodCart1 = \",100*(time_diff>0).sum()/15000,\"%\")\nend_time = tm.time()\nprint(\"Time taken = \", end_time-start_time)\n\nOn  0  days, the average waiting time at FoodCart2 higher than that at FoodCart1\nPercentage of times waiting time at FoodCart2 was greater than that at FoodCart1 =  16.52 %\nTime taken =  0.008000850677490234\n\n\nThe approach with NumPy is much faster than the one with loops.\n\n4.9.1 Practice exercise 3\nBootstrapping: Find the 95% confidence interval of mean profit for ‘Action’ movies, using Bootstrapping.\nBootstrapping is a non-parametric method for obtaining confidence interval. Use the algorithm below to find the confidence interval:\n\nFind the profit for each of the ‘Action’ movies. Suppose there are N such movies. We will have a Profit column with N values.\n\nRandomly sample N values with replacement from the Profit column\n\nFind the mean of the N values obtained in (b)\n\nRepeat steps (b) and (c) M=1000 times\n\nThe 95% Confidence interval is the range between the 2.5% and 97.5% percentile values of the 1000 means obtained in (c)\nUse the movies_cleaned.csv dataset.\n\nSolution:\n\n#Reading data\nmovies = pd.read_csv('./Datasets/movies_cleaned.csv')\n\n#Filtering action movies\nmovies_action = movies.loc[movies['Action']==1,:]\n\n#Computing profit of movies\nmovies_action.loc[:,'Profit'] = movies_action.loc[:,'Worldwide Gross'] - movies_action.loc[:,'Production Budget']\n\n#Subsetting the profit column\nprofit_vec = movies_action['Profit']\n\n#Creating a matrix of 1000 samples with replacement from the profit column\nbootstrap_samples=np.random.choice(profit_vec,size = (1000,len(profit_vec)))\n\n#Computing the mean of each of the 1000 samples\nbootstrap_sample_means = bootstrap_samples.mean(axis=1)\n\n#The confidence interval is the 2.5th and 97.5th percentile of the mean of the 1000 samples\nprint(\"Confidence interval = [$\"+str(np.round(np.percentile(bootstrap_sample_means,2.5)/1e6,2))+\" million, $\"+str(np.round(np.percentile(bootstrap_sample_means,97.5)/1e6,2))+\" million]\")\n\nConfidence interval = [$132.53 million, $182.69 million]"
  },
  {
    "objectID": "Pandas.html",
    "href": "Pandas.html",
    "title": "5  Pandas",
    "section": "",
    "text": "The Pandas library contains several methods and functions for cleaning, manipulating and analyzing data. While NumPy is suited for working with homogenous numerical array data, Pandas is designed for working with tabular or heterogenous data.\nPandas is built on top of the NumPy package. Thus, there are some similarities between the two libraries. Like NumPy, Pandas provides the basic mathematical functionalities like addition, subtraction, conditional operations and broadcasting. However, unlike NumPy library which provides objects for multi-dimensional arrays, Pandas provides the 2D table object called Dataframe.\nData in pandas is often used to feed statistical analysis in SciPy, plotting functions from Matplotlib, and machine learning algorithms in Scikit-learn.\nTypically, the Pandas library is used for:\n\nCleaning the data by tasks such as removing missing values, filtering rows / columns, aggregating data, mutating data, etc.\nComputing summary statistics such as the mean, median, max, min, standard deviation, etc.\nComputing correlation among columns in the data\nComputing the data distribution\nVisualizing the data with help from the Matplotlib library\nWriting the cleaned and transformed data into a CSV file or other database formats\n\nLet’s import the Pandas library to use its methods and functions.\n\nimport pandas as pd"
  },
  {
    "objectID": "Pandas.html#pandas-data-structures---series-and-dataframe",
    "href": "Pandas.html#pandas-data-structures---series-and-dataframe",
    "title": "5  Pandas",
    "section": "5.2 Pandas data structures - Series and DataFrame",
    "text": "5.2 Pandas data structures - Series and DataFrame\nThere are two core components of the Pandas library - Series and DataFrame.\nA DataFrame is a two-dimensional object - comprising of tabular data organized in rows and columns, where individual columns can be of different value types (numeric / string / boolean etc.). A DataFrame has row labels (also called row indices) which refer to individual rows, and column labels (also called column names) that refer to individual columns. By default, the row indices are integers starting from zero. However, both the row indices and column names can be customized by the user.\nLet us read the spotify data - spotify_data.csv, using the Pandas function read_csv().\n\nspotify_data = pd.read_csv('./Datasets/spotify_data.csv')\nspotify_data.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      0\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      6\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n    \n    \n      2\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      7\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n    \n    \n      3\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      2\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n    \n    \n      4\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      8\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\nThe object spotify_data is a pandas DataFrame:\n\ntype(spotify_data)\n\npandas.core.frame.DataFrame\n\n\nA Series is a one-dimensional object, containing a sequence of values, where each value has an index. Each column of a DataFrame is Series as shown in the example below.\n\n#Extracting movie titles from the movie_ratings DataFrame\nspotify_songs = spotify_data['track_name']\nspotify_songs\n\n0                           All Girls Are The Same\n1                                     Lucid Dreams\n2                                  Hear Me Calling\n3                                          Robbery\n4                                      Big Stepper\n                            ...                   \n243185                                    Stardust\n243186             Knockin' A Jug - 78 rpm Version\n243187            When It's Sleepy Time Down South\n243188    On The Sunny Side Of The Street - Part 2\n243189                                    My Sweet\nName: track_name, Length: 243190, dtype: object\n\n\n\n#The object movie_titles is a Series\ntype(spotify_songs)\n\npandas.core.series.Series\n\n\nA Series is essentially a column, and a DataFrame is a two-dimensional table made up of a collection of Series"
  },
  {
    "objectID": "Pandas.html#creating-a-pandas-series-dataframe",
    "href": "Pandas.html#creating-a-pandas-series-dataframe",
    "title": "5  Pandas",
    "section": "5.3 Creating a Pandas Series / DataFrame",
    "text": "5.3 Creating a Pandas Series / DataFrame\n\n5.3.1 Specifying data within the Series() / DataFrame() functions\nA Pandas Series and DataFrame can be created by specifying the data within the Series() / DataFrame() function. Below are examples of defining a Pandas Series / DataFrame.\n\n#Defining a Pandas Series\nseries_example = pd.Series(['these','are','english','words'])\nseries_example\n\n0      these\n1        are\n2    english\n3      words\ndtype: object\n\n\nNote that the default row indices are integers starting from 0. However, the index can be specified with the index argument if desired by the user:\n\n#Defining a Pandas Series with custom row labels\nseries_example = pd.Series(['these','are','english','words'], index = range(101,105))\nseries_example\n\n101      these\n102        are\n103    english\n104      words\ndtype: object\n\n\n\n\n5.3.2 Transforming in-built data structures\nA Pandas DataFrame can be created by converting the in-built python data structures such as lists, dictionaries, and list of dictionaries to DataFrame. See the examples below.\n\n#List consisting of expected age to marry of students of the STAT303-1 Fall 2022 class\nexp_marriage_age_list=['24','30','28','29','30','27','26','28','30+','26','28','30','30','30','probably never','30','25','25','30','28','30+ ','30','25','28','28','25','25','27','28','30','30','35','26','28','27','27','30','25','30','26','32','27','26','27','26','28','37','28','28','28','35','28','27','28','26','28','26','30','27','30','28','25','26','28','35','29','27','27','30','24','25','29','27','33','30','30','25','26','30','32','26','30','30','I wont','25','27','27','25','27','27','32','26','25','never','28','33','28','35','25','30','29','30','31','28','28','30','40','30','28','30','27','by 30','28','27','28','30-35','35','30','30','never','30','35','28','31','30','27','33','32','27','27','26','N/A','25','26','29','28','34','26','24','28','30','120','25','33','27','28','32','30','26','30','30','28','27','27','27','27','27','27','28','30','30','30','28','30','28','30','30','28','28','30','27','30','28','25','never','69','28','28','33','30','28','28','26','30','26','27','30','25','Never','27','27','25']\n\n\n#Example 1: Creating a Pandas Series from a list\nexp_marriage_age_series=pd.Series(exp_marriage_age_list,name = 'expected_marriage_age')\nexp_marriage_age_series.head()\n\n0    24\n1    30\n2    28\n3    29\n4    30\nName: expected_marriage_age, dtype: object\n\n\n\n#Dictionary consisting of the GDP per capita of the US from 1960 to 2021 with some missing values\nGDP_per_capita_dict = {'1960':3007,'1961':3067,'1962':3244,'1963':3375,'1964':3574,'1965':3828,'1966':4146,'1967':4336,'1968':4696,'1970':5234,'1971':5609,'1972':6094,'1973':6726,'1974':7226,'1975':7801,'1976':8592,'1978':10565,'1979':11674, '1980':12575,'1981':13976,'1982':14434,'1983':15544,'1984':17121,'1985':18237,  '1986':19071,'1987':20039,'1988':21417,'1989':22857,'1990':23889,'1991':24342,  '1992':25419,'1993':26387,'1994':27695,'1995':28691,'1996':29968,'1997':31459,  '1998':32854,'2000':36330,'2001':37134,'2002':37998,'2003':39490,'2004':41725,  '2005':44123,'2006':46302,'2007':48050,'2008':48570,'2009':47195,'2010':48651,  '2011':50066,'2012':51784,'2013':53291,'2015':56763,'2016':57867,'2017':59915,'2018':62805, '2019':65095,'2020':63028,'2021':69288}\n\n\n#Example 2: Creating a Pandas Series from a Dictionary\nGDP_per_capita_series = pd.Series(GDP_per_capita_dict)\nGDP_per_capita_series.head()\n\n1960    3007\n1961    3067\n1962    3244\n1963    3375\n1964    3574\ndtype: int64\n\n\n\n#List of dictionary consisting of 52 playing cards of the deck\ndeck_list_of_dictionaries = [{'value':i, 'suit':c}\nfor c in ['spades', 'clubs', 'hearts', 'diamonds']\nfor i in range(2,15)]\n\n\n#Example 3: Creating a Pandas DataFrame from a List of dictionaries\ndeck_df = pd.DataFrame(deck_list_of_dictionaries)\ndeck_df.head()\n\n\n\n\n\n  \n    \n      \n      value\n      suit\n    \n  \n  \n    \n      0\n      2\n      spades\n    \n    \n      1\n      3\n      spades\n    \n    \n      2\n      4\n      spades\n    \n    \n      3\n      5\n      spades\n    \n    \n      4\n      6\n      spades\n    \n  \n\n\n\n\n\n\n5.3.3 Importing data from files\nIn the real world, a Pandas DataFrame will typically be created by loading the datasets from existing storage such as SQL Database, CSV file, Excel file, text files, HTML files, etc., as we learned in the third chapter of the book on Reading data."
  },
  {
    "objectID": "Pandas.html#attributes-and-methods-of-a-pandas-dataframe",
    "href": "Pandas.html#attributes-and-methods-of-a-pandas-dataframe",
    "title": "5  Pandas",
    "section": "5.4 Attributes and Methods of a Pandas DataFrame",
    "text": "5.4 Attributes and Methods of a Pandas DataFrame\nAll attributes and methods of a Pandas DataFrame object can be viewed with the python’s built-in dir() function.\n\n#List of attributes and methods of a Pandas DataFrame\n#This code is not executed as the list is too long\ndir(spotify_data)\n\nAlthough we’ll see examples of attributes and methods of a Pandas DataFrame, please note that most of these attributes and methods are also applicable to the Pandas Series object.\n\n5.4.1 Attributes of a Pandas DataFrame\nSome of the attributes of the Pandas DataFrame class are the following.\n\n5.4.1.1 dtypes\nThis attribute is a Series consisting the datatypes of columns of a Pandas DataFrame.\n\nspotify_data.dtypes\n\nartist_followers       int64\ngenres                object\nartist_name           object\nartist_popularity      int64\ntrack_name            object\ntrack_popularity       int64\nduration_ms            int64\nexplicit               int64\nrelease_year           int64\ndanceability         float64\nenergy               float64\nkey                    int64\nloudness             float64\nmode                   int64\nspeechiness          float64\nacousticness         float64\ninstrumentalness     float64\nliveness             float64\nvalence              float64\ntempo                float64\ntime_signature         int64\ndtype: object\n\n\nThe table below describes the datatypes of columns in a Pandas DataFrame.\n\n\n\nPandas Type\nNative Python Type\nDescription\n\n\n\n\nobject\nstring\nThe most general dtype. This datatype is assigned to a column if the column has mixed types (numbers and strings)\n\n\nint64\nint\nThis datatype is for integers from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807 or for integers having a maximum size of 64 bits\n\n\nfloat64\nfloat\nThis datatype is for real numbers. If a column contains integers and NaNs, Pandas will default to float64. This is because the missing values may be a real number\n\n\ndatetime64, timedelta[ns]\nN/A (but see the datetime module in Python’s standard library)\nValues meant to hold time data. This datatype is useful for time series analysis\n\n\n\n\n\n5.4.1.2 columns\nThis attribute consists of the column labels (or column names) of a Pandas DataFrame.\n\nspotify_data.columns\n\nIndex(['artist_followers', 'genres', 'artist_name', 'artist_popularity',\n       'track_name', 'track_popularity', 'duration_ms', 'explicit',\n       'release_year', 'danceability', 'energy', 'key', 'loudness', 'mode',\n       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n       'valence', 'tempo', 'time_signature'],\n      dtype='object')\n\n\n\n\n5.4.1.3 index\nThis attribute consists of the row lables (or row indices) of a Pandas DataFrame.\n\nspotify_data.index\n\nRangeIndex(start=0, stop=243190, step=1)\n\n\n\n\n5.4.1.4 axes\nThis is a list of length two, where the first element is the row labels, and the second element is the columns labels. In other words, this attribute combines the information in the attributes - index and columns.\n\nspotify_data.axes\n\n[RangeIndex(start=0, stop=243190, step=1),\n Index(['artist_followers', 'genres', 'artist_name', 'artist_popularity',\n        'track_name', 'track_popularity', 'duration_ms', 'explicit',\n        'release_year', 'danceability', 'energy', 'key', 'loudness', 'mode',\n        'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n        'valence', 'tempo', 'time_signature'],\n       dtype='object')]\n\n\n\n\n5.4.1.5 ndim\nAs in NumPy, this attribute specifies the number of dimensions. However, unlike NumPy, a Pandas DataFrame has a fixed dimenstion of 2, and a Pandas Series has a fixed dimesion of 1.\n\nspotify_data.ndim\n\n2\n\n\n\n\n5.4.1.6 size\nThis attribute specifies the number of elements in a DataFrame. Its value is the product of the number of rows and columns.\n\nspotify_data.size\n\n5106990\n\n\n\n\n5.4.1.7 shape\nThis is a tuple consisting of the number of rows and columns in a Pandas DataFrame.\n\nspotify_data.shape\n\n(243190, 21)\n\n\n\n\n5.4.1.8 values\nThis provides a NumPy representation of a Pandas DataFrame.\n\nspotify_data.values\n\narray([[16996777, 'rap', 'Juice WRLD', ..., 0.203, 161.991, 4],\n       [16996777, 'rap', 'Juice WRLD', ..., 0.218, 83.903, 4],\n       [16996777, 'rap', 'Juice WRLD', ..., 0.499, 88.933, 4],\n       ...,\n       [2256652, 'jazz', 'Louis Armstrong', ..., 0.37, 105.093, 4],\n       [2256652, 'jazz', 'Louis Armstrong', ..., 0.576, 101.279, 4],\n       [2256652, 'jazz', 'Louis Armstrong', ..., 0.816, 105.84, 4]],\n      dtype=object)\n\n\n\n\n\n5.4.2 Methods of a Pandas DataFrame\nSome of the commonly used methods of the Pandas DataFrame class are the following.\n\n5.4.2.1 head()\nPrints the first n rows of a DataFrame.\n\nspotify_data.head(2)\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      0\n      -7.226\n      1\n      0.306\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      6\n      -7.230\n      0\n      0.200\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n    \n  \n\n2 rows × 21 columns\n\n\n\n\n\n5.4.2.2 tail()\nPrints the last n rows of a DataFrame.\n\nspotify_data.tail(3)\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      243187\n      2256652\n      jazz\n      Louis Armstrong\n      74\n      When It's Sleepy Time Down South\n      4\n      200200\n      0\n      1923\n      0.527\n      ...\n      3\n      -14.814\n      1\n      0.0793\n      0.989\n      0.00001\n      0.1040\n      0.370\n      105.093\n      4\n    \n    \n      243188\n      2256652\n      jazz\n      Louis Armstrong\n      74\n      On The Sunny Side Of The Street - Part 2\n      4\n      185973\n      0\n      1923\n      0.559\n      ...\n      0\n      -9.804\n      1\n      0.0512\n      0.989\n      0.84700\n      0.4480\n      0.576\n      101.279\n      4\n    \n    \n      243189\n      2256652\n      jazz\n      Louis Armstrong\n      74\n      My Sweet\n      4\n      195960\n      0\n      1923\n      0.741\n      ...\n      3\n      -10.406\n      1\n      0.0505\n      0.927\n      0.07880\n      0.0633\n      0.816\n      105.840\n      4\n    \n  \n\n3 rows × 21 columns\n\n\n\n\n\n5.4.2.3 describe()\nPrint summary statistics of a Pandas DataFrame, as seen in chapter 3 on Reading Data.\n\nspotify_data.describe()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      artist_popularity\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      count\n      2.431900e+05\n      243190.000000\n      243190.000000\n      2.431900e+05\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n    \n    \n      mean\n      1.960931e+06\n      65.342633\n      36.080772\n      2.263209e+05\n      0.050039\n      1992.475258\n      0.568357\n      0.580633\n      5.240326\n      -9.432548\n      0.670928\n      0.111984\n      0.383938\n      0.071169\n      0.223756\n      0.552302\n      119.335060\n      3.884177\n    \n    \n      std\n      5.028746e+06\n      10.289182\n      16.476836\n      9.973214e+04\n      0.218026\n      18.481463\n      0.159444\n      0.236631\n      3.532546\n      4.449731\n      0.469877\n      0.198068\n      0.321142\n      0.209555\n      0.198076\n      0.250017\n      29.864219\n      0.458082\n    \n    \n      min\n      2.300000e+01\n      51.000000\n      0.000000\n      3.344000e+03\n      0.000000\n      1923.000000\n      0.000000\n      0.000000\n      0.000000\n      -60.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      1.832620e+05\n      57.000000\n      25.000000\n      1.776670e+05\n      0.000000\n      1980.000000\n      0.462000\n      0.405000\n      2.000000\n      -11.990000\n      0.000000\n      0.033200\n      0.070000\n      0.000000\n      0.098100\n      0.353000\n      96.099250\n      4.000000\n    \n    \n      50%\n      5.352520e+05\n      64.000000\n      36.000000\n      2.188670e+05\n      0.000000\n      1994.000000\n      0.579000\n      0.591000\n      5.000000\n      -8.645000\n      1.000000\n      0.043100\n      0.325000\n      0.000011\n      0.141000\n      0.560000\n      118.002000\n      4.000000\n    \n    \n      75%\n      1.587332e+06\n      72.000000\n      48.000000\n      2.645465e+05\n      0.000000\n      2008.000000\n      0.685000\n      0.776000\n      8.000000\n      -6.131000\n      1.000000\n      0.075300\n      0.671000\n      0.002220\n      0.292000\n      0.760000\n      137.929000\n      4.000000\n    \n    \n      max\n      7.890023e+07\n      100.000000\n      99.000000\n      4.995083e+06\n      1.000000\n      2021.000000\n      0.988000\n      1.000000\n      11.000000\n      3.744000\n      1.000000\n      0.969000\n      0.996000\n      1.000000\n      1.000000\n      1.000000\n      243.507000\n      5.000000\n    \n  \n\n\n\n\n\n\n5.4.2.4 max()/min()\nReturns the max/min values of numeric columns. If the function is applied on non-numeric columns, it will return the maximum/minimum value based on the order of the alphabet.\n\n#The max() method applied on a Series\nspotify_data['artist_popularity'].max()\n\n100\n\n\n\n#The max() method applied on a DataFrame\nspotify_data.max()\n\nartist_followers                    78900234\ngenres                                  rock\nartist_name                          高爾宣 OSN\nartist_popularity                        100\ntrack_name           행복했던 날들이었다 days gone by\ntrack_popularity                          99\nduration_ms                          4995083\nexplicit                                   1\nrelease_year                            2021\ndanceability                           0.988\nenergy                                   1.0\nkey                                       11\nloudness                               3.744\nmode                                       1\nspeechiness                            0.969\nacousticness                           0.996\ninstrumentalness                         1.0\nliveness                                 1.0\nvalence                                  1.0\ntempo                                243.507\ntime_signature                             5\ndtype: object\n\n\n\n\n5.4.2.5 mean()/median()\nReturns the mean/median values of numeric columns.\n\nspotify_data.median()\n\nartist_followers     535252.000000\nartist_popularity        64.000000\ntrack_popularity         36.000000\nduration_ms          218867.000000\nexplicit                  0.000000\nrelease_year           1994.000000\ndanceability              0.579000\nenergy                    0.591000\nkey                       5.000000\nloudness                 -8.645000\nmode                      1.000000\nspeechiness               0.043100\nacousticness              0.325000\ninstrumentalness          0.000011\nliveness                  0.141000\nvalence                   0.560000\ntempo                   118.002000\ntime_signature            4.000000\ndtype: float64\n\n\n\n\n5.4.2.6 std()\nReturns the standard deviation of numeric columns.\n\nspotify_data.std()\n\nartist_followers     5.028746e+06\nartist_popularity    1.028918e+01\ntrack_popularity     1.647684e+01\nduration_ms          9.973214e+04\nexplicit             2.180260e-01\nrelease_year         1.848146e+01\ndanceability         1.594436e-01\nenergy               2.366309e-01\nkey                  3.532546e+00\nloudness             4.449731e+00\nmode                 4.698771e-01\nspeechiness          1.980684e-01\nacousticness         3.211417e-01\ninstrumentalness     2.095551e-01\nliveness             1.980759e-01\nvalence              2.500172e-01\ntempo                2.986422e+01\ntime_signature       4.580822e-01\ndtype: float64\n\n\n\n\n5.4.2.7 sample(n)\nReturns n random observations from a Pandas DataFrame.\n\nspotify_data.sample(4)\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      42809\n      385756\n      rock\n      Saxon\n      56\n      Never Surrender - 2009 Remastered Version\n      2\n      195933\n      0\n      2012\n      0.506\n      ...\n      6\n      -7.847\n      1\n      0.0633\n      0.0535\n      0.00001\n      0.3330\n      0.536\n      90.989\n      4\n    \n    \n      25730\n      810526\n      hip hop\n      Froid\n      68\n      Pseudosocial\n      54\n      135963\n      0\n      2016\n      0.644\n      ...\n      7\n      -9.098\n      0\n      0.3280\n      0.8270\n      0.00001\n      0.1630\n      0.886\n      117.170\n      4\n    \n    \n      147392\n      479209\n      jazz\n      Sarah Vaughan\n      59\n      Love Dance\n      14\n      204400\n      0\n      1982\n      0.386\n      ...\n      0\n      -23.819\n      1\n      0.0372\n      0.8970\n      0.00000\n      0.0943\n      0.102\n      110.981\n      3\n    \n    \n      233189\n      1201905\n      rock\n      Grateful Dead\n      72\n      Cold Rain and Snow - 2013 Remaster\n      29\n      151702\n      0\n      1967\n      0.412\n      ...\n      6\n      -10.476\n      0\n      0.0487\n      0.4090\n      0.58300\n      0.1630\n      0.875\n      168.803\n      4\n    \n  \n\n4 rows × 21 columns\n\n\n\n\n\n5.4.2.8 dropna()\nDrops all observations with at least one missing value.\n\n#This code is not executed to avoid prining a large table\nspotify_data.dropna()\n\n\n\n5.4.2.9 apply()\nThis method is used to apply a function over all columns or rows of a Pandas DataFrame. For example, let us find the range of values of artist_followers, artist_popularity and release_year.\n\n#Defining the function to compute range of values of a columns\ndef range_of_values(x):\n    return x.max()-x.min()\n\n#Applying the function to three coluumns for which we wish to find the range of values\nspotify_data[['artist_followers','artist_popularity','release_year']].apply(range_of_values, axis=0)\n\nartist_followers     78900211\nartist_popularity          49\nrelease_year               98\ndtype: int64\n\n\nThe apply() method is often used with the one line function known as lambda function in python. These functions do not require a name, and can be defined using the keyword lambda. The above block of code can be concisely written as:\n\nspotify_data[['artist_followers','artist_popularity','release_year']].apply(lambda x:x.max()-x.min(), axis=0)\n\nartist_followers     78900211\nartist_popularity          49\nrelease_year               98\ndtype: int64\n\n\nNote that the Series object also has an apply() method associated with it. The method can be used to apply a function to each value of a Series.\n\n\n5.4.2.10 map()\nThe function is used to map distinct values of a Pandas Series to another set of corresponding values.\nFor example, suppose we wish to create a new column in the spotify dataset which indicates the modality of the song - major (mode = 1) or minor (mode = 0). We’ll map the values of the mode column to the categories major and minor:\n\n#Creating a dictionary that maps the values 0 and 1 to minor and major respectively\nmap_mode = {0:'minor', 1:'major'}\n\n#The map() function requires a dictionary object, and maps the 'values' of the 'keys' in the dictionary\nspotify_data['modality'] = spotify_data['mode'].map(map_mode)\n\nWe can see the variable modality in the updated DataFrame.\n\nspotify_data.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n      modality\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n      major\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n      minor\n    \n    \n      2\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n      minor\n    \n    \n      3\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n      major\n    \n    \n      4\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n      major\n    \n  \n\n5 rows × 22 columns\n\n\n\n\n\n5.4.2.11 drop()\nThis function is used to drop rows/columns from a DataFrame.\nFor example, let us drop the columns mode from the spotify dataset:\n\n#Dropping the column 'mode'\nspotify_data_new = spotify_data.drop('mode',axis=1)\nspotify_data_new.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n      modality\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      0\n      -7.226\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n      major\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      6\n      -7.230\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n      minor\n    \n    \n      2\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      7\n      -3.997\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n      minor\n    \n    \n      3\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      2\n      -5.181\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n      major\n    \n    \n      4\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      8\n      -8.469\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n      major\n    \n  \n\n5 rows × 21 columns\n\n\n\nNote that if multiple columns or rows are to be dropped, they must be enclosed in box brackets.\n\n\n5.4.2.12 unique()\nThis functions provides the unique values of a Series. For example, let us find the number of unique genres of songs in the spotify dataset:\n\nspotify_data.genres.unique()\n\narray(['rap', 'pop', 'miscellaneous', 'metal', 'hip hop', 'rock',\n       'pop & rock', 'hoerspiel', 'folk', 'electronic', 'jazz', 'country',\n       'latin'], dtype=object)\n\n\n\n\n5.4.2.13 value_counts()\nThis function provides the number of observations of each value of a Series. For example, let us find the number of songs of each genre in the spotify dataset:\n\nspotify_data.genres.value_counts()\n\npop              70441\nrock             49785\npop & rock       43437\nmiscellaneous    35848\njazz             13363\nhoerspiel        12514\nhip hop           7373\nfolk              2821\nlatin             2125\nrap               1798\nmetal             1659\ncountry           1236\nelectronic         790\nName: genres, dtype: int64\n\n\nMore than half the songs in the dataset are pop, rock or pop & rock.\n\n\n5.4.2.14 isin()\nThis function provides a boolean Series indicating the position of certain values in a Series. The function is helpful in sub-setting data. For example, let us subset the songs that are either latin, rap, or metal:\n\nlatin_rap_metal_songs = spotify_data.loc[spotify_data.genres.isin(['latin','rap','metal']),:]\nlatin_rap_metal_songs.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      0\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      6\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n    \n    \n      2\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      7\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n    \n    \n      3\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      2\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n    \n    \n      4\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      8\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n    \n  \n\n5 rows × 21 columns"
  },
  {
    "objectID": "Pandas.html#data-manipulations-with-pandas",
    "href": "Pandas.html#data-manipulations-with-pandas",
    "title": "5  Pandas",
    "section": "5.5 Data manipulations with Pandas",
    "text": "5.5 Data manipulations with Pandas\n\n5.5.1 Sub-setting data\n\n5.5.1.1 loc and iloc with the original row / column index\nSubsetting observations: In the chapter on reading data, we learned about operators loc and iloc that can be used to subset data based on axis labels and position of rows/columns respectively. However, usually we are not aware of the relevant row indices, and we may want to subset data based on some condition(s). For example, suppose we wish to analyze only those songs whose track popularity is higher than 50.\nQ: Do we need to subset rows or columns in this case?\nA: Rows, as songs correspond to rows, while features of songs correspond to columns.\nAs we need to subset rows, the filter must be applied at the starting index, i.e., the index before the ,. As we don’t need to subset any specific features of the songs, there is no subsetting to be done on the columns. A : at the ending index means that all columns need to selected.\n\n#Subsetting spotify songs that have track popularity score of more than 50\npopular_songs = spotify_data.loc[spotify_data.track_popularity>=50,:]\npopular_songs.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      181\n      1277325\n      hip hop\n      Dave\n      77\n      Titanium\n      69\n      127750\n      1\n      2021\n      0.959\n      ...\n      0\n      -8.687\n      0\n      0.4370\n      0.152000\n      0.000001\n      0.1050\n      0.510\n      121.008\n      4\n    \n    \n      191\n      1123869\n      rap\n      Jay Wheeler\n      85\n      Viendo el Techo\n      64\n      188955\n      0\n      2021\n      0.741\n      ...\n      11\n      -6.029\n      0\n      0.2290\n      0.306000\n      0.000327\n      0.1000\n      0.265\n      179.972\n      4\n    \n    \n      208\n      3657199\n      rap\n      Polo G\n      91\n      RAPSTAR\n      89\n      165926\n      1\n      2021\n      0.789\n      ...\n      6\n      -6.862\n      1\n      0.2420\n      0.410000\n      0.000000\n      0.1290\n      0.437\n      81.039\n      4\n    \n    \n      263\n      1461700\n      pop & rock\n      Teoman\n      67\n      Gecenin Sonuna Yolculuk\n      52\n      280600\n      0\n      2021\n      0.686\n      ...\n      11\n      -7.457\n      0\n      0.0268\n      0.119000\n      0.000386\n      0.1080\n      0.560\n      100.932\n      4\n    \n    \n      293\n      299746\n      pop & rock\n      Lars Winnerbäck\n      62\n      Själ och hjärta\n      55\n      271675\n      0\n      2021\n      0.492\n      ...\n      2\n      -6.005\n      0\n      0.0349\n      0.000735\n      0.000207\n      0.0953\n      0.603\n      142.042\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\nSubsetting columns: Suppose we wish to analyze only track_name, release year and track_popularity of songs. Then, we can subset the revelant columns:\n\nrelevant_columns = spotify_data.loc[:,['track_name','release_year','track_popularity']]\nrelevant_columns.head()\n\n\n\n\n\n  \n    \n      \n      track_name\n      release_year\n      track_popularity\n    \n  \n  \n    \n      0\n      All Girls Are The Same\n      2021\n      0\n    \n    \n      1\n      Lucid Dreams\n      2021\n      0\n    \n    \n      2\n      Hear Me Calling\n      2021\n      0\n    \n    \n      3\n      Robbery\n      2021\n      0\n    \n    \n      4\n      Big Stepper\n      2021\n      0\n    \n  \n\n\n\n\nNote that when multiple columns are subset with loc they are enclosed in a box bracket, unlike the case with a single column. Similarly if multiple observations are selected using the row labels, the row labels must be enclosed in box brackets.\n\n\n5.5.1.2 Re-indexing rows followed by loc / iloc\nSuppose we wish to subset data based on the genres. If we want to subset hiphop songs, we may subset as:\n\n#Subsetting hiphop songs\nhiphop_songs = spotify_data.loc[spotify_data['genres']=='hip hop',:]\nhiphop_songs.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      64\n      6485079\n      hip hop\n      DaBaby\n      93\n      FIND MY WAY\n      0\n      139890\n      1\n      2021\n      0.836\n      ...\n      4\n      -6.750\n      0\n      0.1840\n      0.1870\n      0.00000\n      0.1380\n      0.7000\n      103.000\n      4\n    \n    \n      80\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      Hula Hoop\n      0\n      236493\n      0\n      2021\n      0.799\n      ...\n      6\n      -4.628\n      1\n      0.0801\n      0.1130\n      0.00315\n      0.0942\n      0.9510\n      175.998\n      4\n    \n    \n      81\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      Gasolina - Live\n      0\n      306720\n      0\n      2021\n      0.669\n      ...\n      1\n      -4.251\n      1\n      0.2700\n      0.1530\n      0.00000\n      0.1540\n      0.0814\n      96.007\n      4\n    \n    \n      87\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      La Nueva Y La Ex\n      0\n      197053\n      0\n      2021\n      0.639\n      ...\n      5\n      -3.542\n      1\n      0.1360\n      0.0462\n      0.00000\n      0.1410\n      0.6390\n      198.051\n      4\n    \n    \n      88\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      Que Tire Pa Lante\n      0\n      210520\n      0\n      2021\n      0.659\n      ...\n      7\n      -2.814\n      1\n      0.0358\n      0.0478\n      0.00000\n      0.1480\n      0.7040\n      93.979\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\nHowever, if we need to subset data by genres frequently in our analysis, and we don’t need the current row labels, we may replace the row labels as genres to shorten the code for filtering the observations based on genres.\nWe use the set_index() function to re-index the rows based on existing column(s) of the DataFrame.\n\n#Defining row labels as the values of the column `genres`\nspotify_data_reindexed = spotify_data.set_index(keys=spotify_data['genres'])\nspotify_data_reindexed.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n    \n      genres\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      rap\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      0\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n    \n    \n      rap\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      6\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n    \n    \n      rap\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      7\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n    \n    \n      rap\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      2\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n    \n    \n      rap\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      8\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\nNow, we can subset hiphop songs using the row label of the data:\n\n#Subsetting hiphop songs using row labels\nhiphop_songs = spotify_data_reindexed.loc['hip hop',:]\nhiphop_songs.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n    \n      genres\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      hip hop\n      6485079\n      hip hop\n      DaBaby\n      93\n      FIND MY WAY\n      0\n      139890\n      1\n      2021\n      0.836\n      ...\n      4\n      -6.750\n      0\n      0.1840\n      0.1870\n      0.00000\n      0.1380\n      0.7000\n      103.000\n      4\n    \n    \n      hip hop\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      Hula Hoop\n      0\n      236493\n      0\n      2021\n      0.799\n      ...\n      6\n      -4.628\n      1\n      0.0801\n      0.1130\n      0.00315\n      0.0942\n      0.9510\n      175.998\n      4\n    \n    \n      hip hop\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      Gasolina - Live\n      0\n      306720\n      0\n      2021\n      0.669\n      ...\n      1\n      -4.251\n      1\n      0.2700\n      0.1530\n      0.00000\n      0.1540\n      0.0814\n      96.007\n      4\n    \n    \n      hip hop\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      La Nueva Y La Ex\n      0\n      197053\n      0\n      2021\n      0.639\n      ...\n      5\n      -3.542\n      1\n      0.1360\n      0.0462\n      0.00000\n      0.1410\n      0.6390\n      198.051\n      4\n    \n    \n      hip hop\n      22831280\n      hip hop\n      Daddy Yankee\n      91\n      Que Tire Pa Lante\n      0\n      210520\n      0\n      2021\n      0.659\n      ...\n      7\n      -2.814\n      1\n      0.0358\n      0.0478\n      0.00000\n      0.1480\n      0.7040\n      93.979\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\n\n\n\n5.5.2 Sorting data\nSorting dataset is a very common operation. The sort_values() function of Pandas can be used to sort a Pandas DataFrame or Series. Let us sort the spotify data in decreasing order of track_popularity:\n\nspotify_sorted = spotify_data.sort_values(by = 'track_popularity', ascending = False)\nspotify_sorted.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      2398\n      1444702\n      pop\n      Olivia Rodrigo\n      88\n      drivers license\n      99\n      242014\n      1\n      2021\n      0.585\n      ...\n      10\n      -8.761\n      1\n      0.0601\n      0.72100\n      0.000013\n      0.1050\n      0.132\n      143.874\n      4\n    \n    \n      2442\n      177401\n      hip hop\n      Masked Wolf\n      85\n      Astronaut In The Ocean\n      98\n      132780\n      0\n      2021\n      0.778\n      ...\n      4\n      -6.865\n      0\n      0.0913\n      0.17500\n      0.000000\n      0.1500\n      0.472\n      149.996\n      4\n    \n    \n      3133\n      1698014\n      pop\n      Kali Uchis\n      88\n      telepatía\n      97\n      160191\n      0\n      2020\n      0.653\n      ...\n      11\n      -9.016\n      0\n      0.0502\n      0.11200\n      0.000000\n      0.2030\n      0.553\n      83.970\n      4\n    \n    \n      6702\n      31308207\n      pop\n      The Weeknd\n      96\n      Save Your Tears\n      97\n      215627\n      1\n      2020\n      0.680\n      ...\n      0\n      -5.487\n      1\n      0.0309\n      0.02120\n      0.000012\n      0.5430\n      0.644\n      118.051\n      4\n    \n    \n      6703\n      31308207\n      pop\n      The Weeknd\n      96\n      Blinding Lights\n      96\n      200040\n      0\n      2020\n      0.514\n      ...\n      1\n      -5.934\n      1\n      0.0598\n      0.00146\n      0.000095\n      0.0897\n      0.334\n      171.005\n      4\n    \n  \n\n5 rows × 21 columns\n\n\n\nDrivers license is the most popular song!\n\n\n\n\n\n \n        \n\n\n\n\n5.5.3 Ranking data\nWith the rank() function, we can rank the observations.\nFor example, let us add a new column to the spotify data that provides the rank of the track_popularity column:\n\nspotify_ranked = spotify_data.copy()\nspotify_ranked['track_popularity_rank']=spotify_sorted['track_popularity'].rank()\nspotify_ranked.head()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n      track_popularity_rank\n    \n  \n  \n    \n      0\n      16996777\n      rap\n      Juice WRLD\n      96\n      All Girls Are The Same\n      0\n      165820\n      1\n      2021\n      0.673\n      ...\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n      963.5\n    \n    \n      1\n      16996777\n      rap\n      Juice WRLD\n      96\n      Lucid Dreams\n      0\n      239836\n      1\n      2021\n      0.511\n      ...\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n      963.5\n    \n    \n      2\n      16996777\n      rap\n      Juice WRLD\n      96\n      Hear Me Calling\n      0\n      189977\n      1\n      2021\n      0.699\n      ...\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n      963.5\n    \n    \n      3\n      16996777\n      rap\n      Juice WRLD\n      96\n      Robbery\n      0\n      240527\n      1\n      2021\n      0.708\n      ...\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n      963.5\n    \n    \n      4\n      5988689\n      rap\n      Roddy Ricch\n      88\n      Big Stepper\n      0\n      175170\n      0\n      2021\n      0.753\n      ...\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n      963.5\n    \n  \n\n5 rows × 22 columns\n\n\n\nNote the column track_popularity_rank. Why does it contain floating point numbers? Check the rank() documentation to find out!\n\n\n5.5.4 Practice exercise 1\n\n5.5.4.1 \nRead the file STAT303-1 survey for data analysis.csv.\n\nsurvey_data = pd.read_csv('./Datasets/STAT303-1 survey for data analysis.csv')\n\n\n\n5.5.4.2 \nHow many observations and variables are there in the data?\n\nprint(\"The data has \",survey_data.shape[0],\"observations, and\", survey_data.shape[1], \"columns\")\n\nThe data has  192 observations, and 51 columns\n\n\n\n\n5.5.4.3 \nRename all the columns of the data, except the first two columns, with the shorter names in the list new_col_names given below. The order of column names in the list is the same as the order in which the columns are to be renamed starting with the third column from the left.\n\nnew_col_names = ['parties_per_month', 'do_you_smoke', 'weed', 'are_you_an_introvert_or_extrovert', 'love_first_sight', 'learning_style', 'left_right_brained', 'personality_type', 'social_media', 'num_insta_followers', 'streaming_platforms', 'expected_marriage_age', 'expected_starting_salary', 'fav_sport', 'minutes_ex_per_week', 'sleep_hours_per_day', 'how_happy', 'farthest_distance_travelled', 'fav_number', 'fav_letter', 'internet_hours_per_day', 'only_child', 'birthdate_odd_even', 'birth_month', 'fav_season', 'living_location_on_campus', 'major', 'num_majors_minors', 'high_school_GPA', 'NU_GPA', 'age', 'height', 'height_father', 'height_mother', 'school_year', 'procrastinator', 'num_clubs', 'student_athlete', 'AP_stats', 'used_python_before', 'dominant_hand', 'childhood_in_US', 'gender', 'region_of_residence', 'political_affliation', 'cant_change_math_ability', 'can_change_math_ability', 'math_is_genetic', 'much_effort_is_lack_of_talent']\n\n\nsurvey_data.columns = list(survey_data.columns[0:2])+new_col_names\n\n\n\n5.5.4.4 \nRename the following columns again:\n\nRename do_you_smoke to smoke.\nRename are_you_an_introvert_or_extrovert to introvert_extrovert.\n\nHint: Use the function rename()\n\nsurvey_data.rename(columns={'do_you_smoke':'smoke','are_you_an_introvert_or_extrovert':'introvert_extrovert'},inplace=True)\n\n\n\n5.5.4.5 \nFind the proportion of people going to more than 4 parties per month. Use the variable parties_per_month.\n\nsurvey_data['parties_per_month']=pd.to_numeric(survey_data.parties_per_month,errors='coerce')\nsurvey_data.loc[survey_data['parties_per_month']>4,:].shape[0]/survey_data.shape[0]\n\n0.3385416666666667\n\n\n\n\n5.5.4.6 \nAmong the people who go to more than 4 parties a month, what proportion of them are introverts?\n\nsurvey_data.loc[((survey_data['parties_per_month']>4) & (survey_data.introvert_extrovert=='Introvert')),:].shape[0]/survey_data.loc[survey_data['parties_per_month']>4,:].shape[0]\n\n0.5076923076923077\n\n\n\n\n5.5.4.7 \nFind the proportion of people in each category of the variable how_happy.\n\nsurvey_data.how_happy.value_counts()/survey_data.shape[0]\n\nPretty happy     0.703125\nVery happy       0.151042\nNot too happy    0.088542\nDon't know       0.057292\nName: how_happy, dtype: float64\n\n\n\n\n5.5.4.8 \nAmong the people who go to more than 4 parties a month, what proportion of them are either Pretty happy or Very happy?\n\nsurvey_data.loc[((survey_data['parties_per_month']>4) & (survey_data.how_happy.isin(['Pretty happy','Very happy'])))].shape[0]/survey_data.loc[survey_data['parties_per_month']>4,:].shape[0]\n\n0.9076923076923077\n\n\n\n\n5.5.4.9 \nExamine the column num_insta_followers. Some numbers in the column contain a comma(,) or a tilde(~). Remove both these characters from the numbers in the column.\nHint: You may use the function str.replace() of the Pandas Series class.\n\nsurvey_data_insta = survey_data.copy()\nsurvey_data_insta['num_insta_followers']=survey_data_insta['num_insta_followers'].str.replace(',','')\nsurvey_data_insta['num_insta_followers']=survey_data_insta['num_insta_followers'].str.replace('~','')\n\n\n\n5.5.4.10 \nConvert the column num_insta_followers to numeric. Coerce the errors.\n\nsurvey_data_insta.num_insta_followers = pd.to_numeric(survey_data_insta.num_insta_followers,errors='coerce')\n\n\n\n5.5.4.11 \nDrop the observations consisting of missing values for num_insta_followers. Report the number of observations dropped.\n\nsurvey_data.num_insta_followers.isna().sum()\n\n3\n\n\nThere are 3 missing values of num_insta_followers.\n\n#Dropping observations with missing values of num_insta_followers\nsurvey_data=survey_data[~survey_data.num_insta_followers.isna()]\n\n\n\n5.5.4.12 \nWhat is the mean internet_hours_per_day for the top 46 people in terms of number of instagram followers?\n\nsurvey_data_insta.sort_values(by = 'num_insta_followers',ascending=False, inplace=True)\ntop_insta = survey_data_insta.iloc[:46,:]\ntop_insta.internet_hours_per_day = pd.to_numeric(top_insta.internet_hours_per_day,errors = 'coerce')\ntop_insta.internet_hours_per_day.mean()\n\n5.088888888888889\n\n\n\n\n5.5.4.13 \nWhat is the mean internet_hours_per_day for the remaining people?\n\nlow_insta = survey_data_insta.iloc[46:,:]\nlow_insta.internet_hours_per_day = pd.to_numeric(low_insta.internet_hours_per_day,errors = 'coerce')\nlow_insta.internet_hours_per_day.mean()\n\n13.118881118881118"
  },
  {
    "objectID": "Pandas.html#arithematic-operations",
    "href": "Pandas.html#arithematic-operations",
    "title": "5  Pandas",
    "section": "5.6 Arithematic operations",
    "text": "5.6 Arithematic operations\n\n5.6.1 Arithematic operations between DataFrames\nLet us create two toy DataFrames:\n\n#Creating two toy DataFrames\ntoy_df1 = pd.DataFrame([(1,2),(3,4),(5,6)], columns=['a','b'])\ntoy_df2 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'])\n\n\n#DataFrame 1\ntoy_df1\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      1\n      2\n    \n    \n      1\n      3\n      4\n    \n    \n      2\n      5\n      6\n    \n  \n\n\n\n\n\n#DataFrame 2\ntoy_df2\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      100\n      200\n    \n    \n      1\n      300\n      400\n    \n    \n      2\n      500\n      600\n    \n  \n\n\n\n\nElement by element operations between two DataFrames can be performed with the operators +, -, *,/,**, and %. Below is an example of element-by-element addition of two DataFrames:\n\n# Element-by-element arithmetic addition of the two DataFrames\ntoy_df1 + toy_df2\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      101\n      202\n    \n    \n      1\n      303\n      404\n    \n    \n      2\n      505\n      606\n    \n  \n\n\n\n\nNote that these operations create problems when the row indices and/or column names of the two DataFrames do not match. See the example below:\n\n#Creating another toy example of a DataFrame\ntoy_df3 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'], index=[1,2,3])\ntoy_df3\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      1\n      100\n      200\n    \n    \n      2\n      300\n      400\n    \n    \n      3\n      500\n      600\n    \n  \n\n\n\n\n\n#Adding DataFrames with some unmatching row indices\ntoy_df1 + toy_df3\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n    \n    \n      1\n      103.0\n      204.0\n    \n    \n      2\n      305.0\n      406.0\n    \n    \n      3\n      NaN\n      NaN\n    \n  \n\n\n\n\nNote that the rows whose indices match between the two DataFrames are added up. The rest of the values are missing (or NaN) because only one of the DataFrames has that index.\nAs in the case of row indices, missing values will also appear in the case of unmatching column names, as shown in the example below.\n\ntoy_df4 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['b','c'])\ntoy_df4\n\n\n\n\n\n  \n    \n      \n      b\n      c\n    \n  \n  \n    \n      0\n      100\n      200\n    \n    \n      1\n      300\n      400\n    \n    \n      2\n      500\n      600\n    \n  \n\n\n\n\n\n#Adding DataFrames with some unmatching column names\ntoy_df1 + toy_df4\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      NaN\n      102\n      NaN\n    \n    \n      1\n      NaN\n      304\n      NaN\n    \n    \n      2\n      NaN\n      506\n      NaN\n    \n  \n\n\n\n\n\n\n5.6.2 Arithematic operations between a Series and a DataFrame\nBroadcasting: As in NumPy, we can broadcast a Series to match the shape of another DataFrame:\n\n# Broadcasting: The row [1,2] (a Series) is added on every row in df2 \ntoy_df1.iloc[0,:] + toy_df2\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      101\n      202\n    \n    \n      1\n      301\n      402\n    \n    \n      2\n      501\n      602\n    \n  \n\n\n\n\nNote that the + operator is used to add values of a Series to a DataFrame based on column names. For adding a Series to a DataFrame based on row indices, we cannot use the + operator. Instead, we’ll need to use the add() function as explained below.\nBroadcasting based on row/column labels: We can use the add() function to broadcast a Series to a DataFrame. By default the Series adds based on column names, as in the case of the + operator.\n\n# Add the first row of df1 (a Series) to every row in df2 \ntoy_df2.add(toy_df1.iloc[0,:])\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      101\n      202\n    \n    \n      1\n      301\n      402\n    \n    \n      2\n      501\n      602\n    \n  \n\n\n\n\nFor broadcasting based on row indices, we use the axis argument of the add() function.\n\n# The second column of df1 (a Series) is added to every col in df2\ntoy_df2.add(toy_df1.iloc[:,1],axis='index')\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      102\n      202\n    \n    \n      1\n      304\n      404\n    \n    \n      2\n      506\n      606\n    \n  \n\n\n\n\n\n\n5.6.3 Case study\nTo see the application of arithematic operations on DataFrames, let us see the case study below.\nSong recommendation: Spotify recommends songs based on songs listened by the user. Suppose you have listened to the song drivers license. Spotify intends to recommend you 5 songs that are similar to drivers license. Which songs should it recommend?\nLet us see the available song information that can help us identify songs similar to drivers license. The columns attribute of DataFrame will display all the columns names. The description of some of the column names relating to audio features is here.\n\nspotify_data.columns\n\nIndex(['artist_followers', 'genres', 'artist_name', 'artist_popularity',\n       'track_name', 'track_popularity', 'duration_ms', 'explicit',\n       'release_year', 'danceability', 'energy', 'key', 'loudness', 'mode',\n       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n       'valence', 'tempo', 'time_signature'],\n      dtype='object')\n\n\nSolution approach: We have several features of a song. Let us find songs similar to drivers license in terms of danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, time_signature and tempo. Note that we are considering only audio features for simplicity.\nTo find the songs most similar to drivers license, we need to define a measure that quantifies the similarity. Let us define similarity of a song with drivers license as the Euclidean distance of the song from drivers license, where the coordinates of a song are: (danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, time_signature, tempo). Thus, similarity can be formulated as:\n\\[Similarity_{DL-S} = \\sqrt{(danceability_{DL}-danceability_{S})^2+(energy_{DL}-energy_{S})^2 +...+ (tempo_{DL}-tempo_{S})^2)},\\]\nwhere the subscript DL stands for drivers license and S stands for any song. The top 5 songs with the least value of \\(Similarity_{DL-S}\\) will be the most similar to drivers lincense and should be recommended.\nLet us subset the columns that we need to use to compute the Euclidean distance.\n\naudio_features = spotify_data[['danceability', 'energy', 'key', 'loudness','mode','speechiness',\n                               'acousticness', 'instrumentalness', 'liveness','valence', 'tempo', 'time_signature']]\n\n\naudio_features.head()\n\n\n\n\n\n  \n    \n      \n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      0.673\n      0.529\n      0\n      -7.226\n      1\n      0.3060\n      0.0769\n      0.000338\n      0.0856\n      0.203\n      161.991\n      4\n    \n    \n      1\n      0.511\n      0.566\n      6\n      -7.230\n      0\n      0.2000\n      0.3490\n      0.000000\n      0.3400\n      0.218\n      83.903\n      4\n    \n    \n      2\n      0.699\n      0.687\n      7\n      -3.997\n      0\n      0.1060\n      0.3080\n      0.000036\n      0.1210\n      0.499\n      88.933\n      4\n    \n    \n      3\n      0.708\n      0.690\n      2\n      -5.181\n      1\n      0.0442\n      0.3480\n      0.000000\n      0.2220\n      0.543\n      79.993\n      4\n    \n    \n      4\n      0.753\n      0.597\n      8\n      -8.469\n      1\n      0.2920\n      0.0477\n      0.000000\n      0.1970\n      0.616\n      76.997\n      4\n    \n  \n\n\n\n\n\n#Distribution of values of audio_features\naudio_features.describe()\n\n\n\n\n\n  \n    \n      \n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      count\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n    \n    \n      mean\n      0.568357\n      0.580633\n      5.240326\n      -9.432548\n      0.670928\n      0.111984\n      0.383938\n      0.071169\n      0.223756\n      0.552302\n      119.335060\n      3.884177\n    \n    \n      std\n      0.159444\n      0.236631\n      3.532546\n      4.449731\n      0.469877\n      0.198068\n      0.321142\n      0.209555\n      0.198076\n      0.250017\n      29.864219\n      0.458082\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      -60.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.462000\n      0.405000\n      2.000000\n      -11.990000\n      0.000000\n      0.033200\n      0.070000\n      0.000000\n      0.098100\n      0.353000\n      96.099250\n      4.000000\n    \n    \n      50%\n      0.579000\n      0.591000\n      5.000000\n      -8.645000\n      1.000000\n      0.043100\n      0.325000\n      0.000011\n      0.141000\n      0.560000\n      118.002000\n      4.000000\n    \n    \n      75%\n      0.685000\n      0.776000\n      8.000000\n      -6.131000\n      1.000000\n      0.075300\n      0.671000\n      0.002220\n      0.292000\n      0.760000\n      137.929000\n      4.000000\n    \n    \n      max\n      0.988000\n      1.000000\n      11.000000\n      3.744000\n      1.000000\n      0.969000\n      0.996000\n      1.000000\n      1.000000\n      1.000000\n      243.507000\n      5.000000\n    \n  \n\n\n\n\nNote that the audio features differ in terms of scale. Some features like key have a wide range of [0,11], while others like danceability have a very narrow range of [0,0.988]. If we use them directly, features like danceability will have a much higher influence on \\(Similarity_{DL-S}\\) as compared to features like key. Assuming we wish all the features to have equal weight in quantifying a song’s similarity to drivers license, we should scale the features, so that their values are comparable.\nLet us scale the value of each column to a standard uniform distribution: \\(U[0,1]\\).\nFor scaling the values of a column to \\(U[0,1]\\), we need to subtract the minimum value of the column from each value, and divide by the range of values of the column. For example, danceability can be standardized as follows:\n\n#Scaling danceability to U[0,1]\ndanceability_value_range = audio_features.danceability.max()-audio_features.danceability.min()\ndanceability_std = (audio_features.danceability-audio_features.danceability.min())/danceability_value_range\ndanceability_std\n\n0         0.681174\n1         0.517206\n2         0.707490\n3         0.716599\n4         0.762146\n            ...   \n243185    0.621457\n243186    0.797571\n243187    0.533401\n243188    0.565789\n243189    0.750000\nName: danceability, Length: 243190, dtype: float64\n\n\nHowever, it will be cumbersome to repeat the above code for each audio feature. We can instead write a function that scales values of a column to \\(U[0,1]\\), and apply the function on all the audio features.\n\n#Function to scale a column to U[0,1]\ndef scale_uniform(x):\n    return (x-x.min())/(x.max()-x.min())\n\nWe will use the Pandas function apply() to apply the above function to the DataFrame audio_features.\n\n#Scaling all audio features to U[0,1]\naudio_features_scaled = audio_features.apply(scale_uniform)\n\nThe above two blocks of code can be concisely written with the lambda function as:\n\naudio_features_scaled = audio_features.apply(lambda x: (x-x.min())/(x.max()-x.min()))\n\n\n#All the audio features are scaled to U[0,1]\naudio_features_scaled.describe()\n\n\n\n\n\n  \n    \n      \n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      count\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n      243190.000000\n    \n    \n      mean\n      0.575260\n      0.580633\n      0.476393\n      0.793290\n      0.670928\n      0.115566\n      0.385480\n      0.071169\n      0.223756\n      0.552302\n      0.490068\n      0.776835\n    \n    \n      std\n      0.161380\n      0.236631\n      0.321141\n      0.069806\n      0.469877\n      0.204405\n      0.322431\n      0.209555\n      0.198076\n      0.250017\n      0.122642\n      0.091616\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.467611\n      0.405000\n      0.181818\n      0.753169\n      0.000000\n      0.034262\n      0.070281\n      0.000000\n      0.098100\n      0.353000\n      0.394647\n      0.800000\n    \n    \n      50%\n      0.586032\n      0.591000\n      0.454545\n      0.805644\n      1.000000\n      0.044479\n      0.326305\n      0.000011\n      0.141000\n      0.560000\n      0.484594\n      0.800000\n    \n    \n      75%\n      0.693320\n      0.776000\n      0.727273\n      0.845083\n      1.000000\n      0.077709\n      0.673695\n      0.002220\n      0.292000\n      0.760000\n      0.566427\n      0.800000\n    \n    \n      max\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n    \n  \n\n\n\n\nSince we need to find the Euclidean distance from the song drivers license, let us find the index of the row containing features of drivers license.\n\n#Index of the row consisting of drivers license can be found with the index attribute\ndrivers_license_index = spotify_data[spotify_data.track_name=='drivers license'].index[0]\n\nNote that the object returned by the index attribute is of type pandas.core.indexes.numeric.Int64Index. The elements of this object can be retrieved like the elements of a python list. That is why the object is sliced with [0] to return the first element of the object. As there is only one observation with the track_name as drivers license, we sliced the first element. If there were multiple observations with track_name as drivers license, we will obtain the indices of all those observations with the index attribute.\nNow, we’ll subtract the audio features of drivers license from all other songs:\n\n#Audio features of drivers license are being subtracted from audio features of all songs by broadcasting\nsongs_minus_DL = audio_features_scaled-audio_features_scaled.loc[drivers_license_index,:]\n\nNow, let us square the difference computed above. We’ll use the in-built python function pow() to square the difference:\n\nsongs_minus_DL_sq = songs_minus_DL.pow(2)\nsongs_minus_DL_sq.head()\n\n\n\n\n\n  \n    \n      \n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      0\n      0.007933\n      0.008649\n      0.826446\n      0.000580\n      0.0\n      0.064398\n      0.418204\n      1.055600e-07\n      0.000376\n      0.005041\n      0.005535\n      0.0\n    \n    \n      1\n      0.005610\n      0.016900\n      0.132231\n      0.000577\n      1.0\n      0.020844\n      0.139498\n      1.716100e-10\n      0.055225\n      0.007396\n      0.060654\n      0.0\n    \n    \n      2\n      0.013314\n      0.063001\n      0.074380\n      0.005586\n      1.0\n      0.002244\n      0.171942\n      5.382400e-10\n      0.000256\n      0.134689\n      0.050906\n      0.0\n    \n    \n      3\n      0.015499\n      0.064516\n      0.528926\n      0.003154\n      0.0\n      0.000269\n      0.140249\n      1.716100e-10\n      0.013689\n      0.168921\n      0.068821\n      0.0\n    \n    \n      4\n      0.028914\n      0.025921\n      0.033058\n      0.000021\n      0.0\n      0.057274\n      0.456981\n      1.716100e-10\n      0.008464\n      0.234256\n      0.075428\n      0.0\n    \n  \n\n\n\n\nNow, we’ll sum the squares of differences from all audio features to compute the similarity of all songs to drivers license.\n\ndistance_squared = songs_minus_DL_sq.sum(axis = 1)\ndistance_squared.head()\n\n0    1.337163\n1    1.438935\n2    1.516317\n3    1.004043\n4    0.920316\ndtype: float64\n\n\nNow, we’ll sort these distances to find the top 5 songs closest to drivers’s license.\n\ndistances_sorted = distance_squared.sort_values()\ndistances_sorted.head()\n\n2398      0.000000\n81844     0.008633\n4397      0.011160\n130789    0.015018\n143744    0.015058\ndtype: float64\n\n\nUsing the indices of the top 5 distances, we will identify the top 5 songs most similar to drivers license:\n\nspotify_data.loc[distances_sorted.index[0:6],:]\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      genres\n      artist_name\n      artist_popularity\n      track_name\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      ...\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      2398\n      1444702\n      pop\n      Olivia Rodrigo\n      88\n      drivers license\n      99\n      242014\n      1\n      2021\n      0.585\n      ...\n      10\n      -8.761\n      1\n      0.0601\n      0.721\n      0.000013\n      0.105\n      0.132\n      143.874\n      4\n    \n    \n      81844\n      2264501\n      pop\n      Jay Chou\n      74\n      安靜\n      49\n      334240\n      0\n      2001\n      0.513\n      ...\n      10\n      -7.853\n      1\n      0.0281\n      0.688\n      0.000008\n      0.116\n      0.123\n      143.924\n      4\n    \n    \n      4397\n      25457\n      pop\n      Terence Lam\n      60\n      拼命無恙 in Bb major\n      52\n      241062\n      0\n      2020\n      0.532\n      ...\n      10\n      -9.690\n      1\n      0.0269\n      0.674\n      0.000000\n      0.117\n      0.190\n      151.996\n      4\n    \n    \n      130789\n      176266\n      pop\n      Alan Tam\n      54\n      從後趕上\n      8\n      258427\n      0\n      1988\n      0.584\n      ...\n      10\n      -11.889\n      1\n      0.0282\n      0.707\n      0.000002\n      0.107\n      0.124\n      140.147\n      4\n    \n    \n      143744\n      396326\n      pop & rock\n      Laura Branigan\n      64\n      How Am I Supposed to Live Without You\n      40\n      263320\n      0\n      1983\n      0.559\n      ...\n      10\n      -8.260\n      1\n      0.0355\n      0.813\n      0.000083\n      0.134\n      0.185\n      139.079\n      4\n    \n    \n      35627\n      1600562\n      pop\n      Tiziano Ferro\n      68\n      Non Me Lo So Spiegare\n      44\n      240040\n      0\n      2014\n      0.609\n      ...\n      11\n      -7.087\n      1\n      0.0352\n      0.706\n      0.000000\n      0.130\n      0.207\n      146.078\n      4\n    \n  \n\n6 rows × 21 columns\n\n\n\nWe can see the top 5 songs most similar to drivers license in the track_name column above. Interestingly, three of the five songs are Asian! These songs indeed sound similar to drivers license!"
  },
  {
    "objectID": "Pandas.html#correlation",
    "href": "Pandas.html#correlation",
    "title": "5  Pandas",
    "section": "5.7 Correlation",
    "text": "5.7 Correlation\nCorrelation may refer to any kind of association between two random variables. However, in this book, we will always consider correlation as the linear association between two random variables, or the Pearson’s correlation coefficient. Note that correlation does not imply causality and vice-versa.\nThe Pandas function corr() provides the pairwise correlation between all columns of a DataFrame, or between two Series. The function corrwith() provides the pairwise correlation of a DataFrame with another DataFrame or Series.\n\n#Pairwise correlation amongst all columns\nspotify_data.corr()\n\n\n\n\n\n  \n    \n      \n      artist_followers\n      artist_popularity\n      track_popularity\n      duration_ms\n      explicit\n      release_year\n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      time_signature\n    \n  \n  \n    \n      artist_followers\n      1.000000\n      0.577861\n      0.197426\n      0.040435\n      0.082857\n      0.098589\n      -0.010120\n      0.080085\n      -0.000119\n      0.123771\n      0.004313\n      -0.059933\n      -0.107475\n      -0.033986\n      0.002425\n      -0.053317\n      0.016524\n      0.030826\n    \n    \n      artist_popularity\n      0.577861\n      1.000000\n      0.285565\n      -0.097996\n      0.092147\n      0.062007\n      0.038784\n      0.039583\n      -0.011005\n      0.045165\n      0.018758\n      0.236942\n      -0.075715\n      -0.066679\n      0.099678\n      -0.034501\n      -0.032036\n      -0.033423\n    \n    \n      track_popularity\n      0.197426\n      0.285565\n      1.000000\n      0.060474\n      0.193685\n      0.568329\n      0.158507\n      0.217342\n      0.013369\n      0.296350\n      -0.022486\n      -0.056537\n      -0.284433\n      -0.124283\n      -0.090479\n      -0.038859\n      0.058408\n      0.071741\n    \n    \n      duration_ms\n      0.040435\n      -0.097996\n      0.060474\n      1.000000\n      -0.024226\n      0.067665\n      -0.145779\n      0.075990\n      0.007710\n      0.078586\n      -0.034818\n      -0.332585\n      -0.133960\n      0.067055\n      -0.034631\n      -0.155354\n      0.051046\n      0.085015\n    \n    \n      explicit\n      0.082857\n      0.092147\n      0.193685\n      -0.024226\n      1.000000\n      0.215656\n      0.138522\n      0.104734\n      0.011818\n      0.124410\n      -0.060350\n      0.077268\n      -0.129363\n      -0.039472\n      -0.024283\n      -0.032549\n      0.006585\n      0.043538\n    \n    \n      release_year\n      0.098589\n      0.062007\n      0.568329\n      0.067665\n      0.215656\n      1.000000\n      0.204743\n      0.338096\n      0.021497\n      0.430054\n      -0.071338\n      -0.032968\n      -0.369038\n      -0.149644\n      -0.045160\n      -0.070025\n      0.079382\n      0.089485\n    \n    \n      danceability\n      -0.010120\n      0.038784\n      0.158507\n      -0.145779\n      0.138522\n      0.204743\n      1.000000\n      0.137615\n      0.020128\n      0.142239\n      -0.051130\n      0.198509\n      -0.143936\n      -0.179213\n      -0.114999\n      0.505350\n      -0.125061\n      0.111015\n    \n    \n      energy\n      0.080085\n      0.039583\n      0.217342\n      0.075990\n      0.104734\n      0.338096\n      0.137615\n      1.000000\n      0.030824\n      0.747829\n      -0.053374\n      -0.043377\n      -0.678745\n      -0.131269\n      0.126050\n      0.348158\n      0.205960\n      0.170854\n    \n    \n      key\n      -0.000119\n      -0.011005\n      0.013369\n      0.007710\n      0.011818\n      0.021497\n      0.020128\n      0.030824\n      1.000000\n      0.024674\n      -0.139688\n      -0.003533\n      -0.023179\n      -0.006600\n      -0.011566\n      0.024206\n      0.008336\n      0.007738\n    \n    \n      loudness\n      0.123771\n      0.045165\n      0.296350\n      0.078586\n      0.124410\n      0.430054\n      0.142239\n      0.747829\n      0.024674\n      1.000000\n      -0.028151\n      -0.173444\n      -0.493020\n      -0.269008\n      0.002959\n      0.209588\n      0.171926\n      0.146030\n    \n    \n      mode\n      0.004313\n      0.018758\n      -0.022486\n      -0.034818\n      -0.060350\n      -0.071338\n      -0.051130\n      -0.053374\n      -0.139688\n      -0.028151\n      1.000000\n      -0.037237\n      0.043773\n      -0.024695\n      0.005657\n      0.010305\n      0.015399\n      -0.015225\n    \n    \n      speechiness\n      -0.059933\n      0.236942\n      -0.056537\n      -0.332585\n      0.077268\n      -0.032968\n      0.198509\n      -0.043377\n      -0.003533\n      -0.173444\n      -0.037237\n      1.000000\n      0.112061\n      -0.094796\n      0.263630\n      0.052171\n      -0.127945\n      -0.150350\n    \n    \n      acousticness\n      -0.107475\n      -0.075715\n      -0.284433\n      -0.133960\n      -0.129363\n      -0.369038\n      -0.143936\n      -0.678745\n      -0.023179\n      -0.493020\n      0.043773\n      0.112061\n      1.000000\n      0.112107\n      0.007415\n      -0.175674\n      -0.173152\n      -0.163243\n    \n    \n      instrumentalness\n      -0.033986\n      -0.066679\n      -0.124283\n      0.067055\n      -0.039472\n      -0.149644\n      -0.179213\n      -0.131269\n      -0.006600\n      -0.269008\n      -0.024695\n      -0.094796\n      0.112107\n      1.000000\n      -0.031301\n      -0.150172\n      -0.027369\n      -0.022034\n    \n    \n      liveness\n      0.002425\n      0.099678\n      -0.090479\n      -0.034631\n      -0.024283\n      -0.045160\n      -0.114999\n      0.126050\n      -0.011566\n      0.002959\n      0.005657\n      0.263630\n      0.007415\n      -0.031301\n      1.000000\n      -0.011137\n      -0.027716\n      -0.040789\n    \n    \n      valence\n      -0.053317\n      -0.034501\n      -0.038859\n      -0.155354\n      -0.032549\n      -0.070025\n      0.505350\n      0.348158\n      0.024206\n      0.209588\n      0.010305\n      0.052171\n      -0.175674\n      -0.150172\n      -0.011137\n      1.000000\n      0.100947\n      0.084783\n    \n    \n      tempo\n      0.016524\n      -0.032036\n      0.058408\n      0.051046\n      0.006585\n      0.079382\n      -0.125061\n      0.205960\n      0.008336\n      0.171926\n      0.015399\n      -0.127945\n      -0.173152\n      -0.027369\n      -0.027716\n      0.100947\n      1.000000\n      0.017423\n    \n    \n      time_signature\n      0.030826\n      -0.033423\n      0.071741\n      0.085015\n      0.043538\n      0.089485\n      0.111015\n      0.170854\n      0.007738\n      0.146030\n      -0.015225\n      -0.150350\n      -0.163243\n      -0.022034\n      -0.040789\n      0.084783\n      0.017423\n      1.000000\n    \n  \n\n\n\n\nQ: Which audio feature is the most correlated with track_popularity?\n\nspotify_data.corrwith(spotify_data.track_popularity).sort_values(ascending = False)\n\ntrack_popularity     1.000000\nrelease_year         0.568329\nloudness             0.296350\nartist_popularity    0.285565\nenergy               0.217342\nartist_followers     0.197426\nexplicit             0.193685\ndanceability         0.158507\ntime_signature       0.071741\nduration_ms          0.060474\ntempo                0.058408\nkey                  0.013369\nmode                -0.022486\nvalence             -0.038859\nspeechiness         -0.056537\nliveness            -0.090479\ninstrumentalness    -0.124283\nacousticness        -0.284433\ndtype: float64\n\n\nLoudness is the audio feature having the highest correlation with track_popularity.\nQ: Which audio feature is the most weakly correlated with track_popularity?\n\n5.7.1 Practice exercise 2\n\n5.7.1.1 \nUse the updated dataset from Practice exercise 1.\nThe last four variables in the dataset are:\n\ncant_change_math_ability\ncan_change_math_ability\nmath_is_genetic\nmuch_effort_is_lack_of_talent\n\nEach of the above variables has values - Agree / Disagree. Replace Agree with 1 and Disagree with 0.\nHint : You can do it with any one of the following methods:\n\nUse the map() function\nUse the apply() function with the lambda function\nUse the replace() function\nUse the applymap() function\n\nTwo of the above methods avoid a for-loop. Which ones?\nSolution:\n\n#Making a copy of data to avoid changing the original data. \nsurvey_data_copy = survey_data.copy()\n\n#Using the map function\nfor i in range(47,51):\n    survey_data_copy.iloc[:,i] = survey_data_copy.iloc[:,i].map({'Agree':1,'Disagree':0})\nsurvey_data_copy.iloc[:,47:51].head()\n\n\n\n\n\n  \n    \n      \n      cant_change_math_ability\n      can_change_math_ability\n      math_is_genetic\n      much_effort_is_lack_of_talent\n    \n  \n  \n    \n      0\n      0\n      1\n      0\n      0\n    \n    \n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      0\n      1\n      0\n      0\n    \n    \n      3\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      0\n      0\n      0\n    \n  \n\n\n\n\n\n#Making a copy of data to avoid changing the original data. \nsurvey_data_copy = survey_data.copy()\n\n#Using the lambda function with apply()\nfor i in range(47,51):\n    survey_data_copy.iloc[:,i] = survey_data_copy.iloc[:,i].apply(lambda x: 1 if x=='Agree' else 0)\nsurvey_data_copy.iloc[:,47:51].head()\n\n\n\n\n\n  \n    \n      \n      cant_change_math_ability\n      can_change_math_ability\n      math_is_genetic\n      much_effort_is_lack_of_talent\n    \n  \n  \n    \n      0\n      0\n      1\n      0\n      0\n    \n    \n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      0\n      1\n      0\n      0\n    \n    \n      3\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      0\n      0\n      0\n    \n  \n\n\n\n\n\n#Making a copy of data to avoid changing the original data. \nsurvey_data_copy = survey_data.copy()\n\n#Using the replace() function\nsurvey_data_copy.iloc[:,47:51] = survey_data_copy.iloc[:,47:51].replace('Agree','1')\nsurvey_data_copy.iloc[:,47:51] = survey_data_copy.iloc[:,47:51].replace('Disagree','0')\nsurvey_data_copy.iloc[:,47:51].head()\n\n\n\n\n\n  \n    \n      \n      cant_change_math_ability\n      can_change_math_ability\n      math_is_genetic\n      much_effort_is_lack_of_talent\n    \n  \n  \n    \n      0\n      0\n      1\n      0\n      0\n    \n    \n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      0\n      1\n      0\n      0\n    \n    \n      3\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      0\n      0\n      0\n    \n  \n\n\n\n\n\n#Making a copy of data to avoid changing the original data. \nsurvey_data_copy = survey_data.copy()\n\n#Using the lambda function with applymap()\nsurvey_data_copy.iloc[:,47:51] = survey_data_copy.iloc[:,47:51].applymap(lambda x: 1 if x=='Agree' else 0)\nsurvey_data_copy.iloc[:,47:51].head()\n\n\n\n\n\n  \n    \n      \n      cant_change_math_ability\n      can_change_math_ability\n      math_is_genetic\n      much_effort_is_lack_of_talent\n    \n  \n  \n    \n      0\n      0\n      1\n      0\n      0\n    \n    \n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      0\n      1\n      0\n      0\n    \n    \n      3\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      0\n      0\n      0\n    \n  \n\n\n\n\n\n\n5.7.1.2 \nAmong the four variables, which one is the most negatively correlated with math_is_genetic?\n\n#Computing correlation\nsurvey_data_copy.iloc[:,47:51].corrwith(survey_data_copy.math_is_genetic)\n\ncant_change_math_ability         0.294544\ncan_change_math_ability         -0.361546\nmath_is_genetic                  1.000000\nmuch_effort_is_lack_of_talent    0.154083\ndtype: float64\n\n\nThe variable can_change_math_ability is the most negatively correlated wtih math_is_genetic."
  },
  {
    "objectID": "Data visualization.html",
    "href": "Data visualization.html",
    "title": "6  Data visualization",
    "section": "",
    "text": "“One picture is worth a thousand words” - Fred R. Barnard\nVisual perception offers the highest bandwidth channel, as we acquire much more information through visual perception than with all of the other channels combined, as billions of our neurons are dedicated to this task. Moreover, the processing of visual information is, at its first stages, a highly parallel process. Thus, it is generally easier for humans to comprehend information with plots, diagrams and pictures, rather than with text and numbers. This makes data visualizations a vital part of data science. Some of the key purposes of data visualization are:\nWe’ll use a couple of libraries for making data visualizations - matplotlib and seaborn. Matplotlib is mostly used for creating relatively simple two-dimensional plots. Its plotting interface that is similar to the plot() function in MATLAB, so those who have used MATLAB should find it familiar. Seaborn is a recently developed data visualization library based on matplotlib. It is more oriented towards visualizing data with Pandas DataFrame and NumPy arrays. While matplotlib may also be used to create complex plots, seaborn has some built-in themes that may make it more convenient to make complex plots. Seaborn also has color schemes and plot styles that improve the readability and aesthetics of malplotlib plots. However, preferences depend on the user and their coding style, and it is perfectly fine to use either library for making the same visualization."
  },
  {
    "objectID": "Data visualization.html#matplotlib",
    "href": "Data visualization.html#matplotlib",
    "title": "6  Data visualization",
    "section": "6.1 Matplotlib",
    "text": "6.1 Matplotlib\nMatplotlib is:\n\na low-level graph plotting library in python that strives to emulate MATLAB,\ncan be used in Python scripts, Python and IPython shells, Jupyter notebooks and web application servers.\nis mostly written in python, a few segments are written in C, Objective-C and Javascript for Platform compatibility.\n\nConceptual model: Plotting requires action on a range of levels, ranging from the size of the figure to the text object in the plot. Matplotlib provides object-oriented interface in the hierarchical fashion to provide complete control over the plot. The user generates and keeps track of the figure and axes objects. These axes objects are then used for most plotting actions.\n\n6.1.1 Matplotlib: Object hierarchy\nA hierarchy means that there is a tree-like structure of matplotlib objects underlying each plot.\nA Figure object is the outermost container for a matplotlib graphic, which can contain multiple Axes objects. Note that an Axes actually translates into what we think of as an individual plot or graph (rather than the plural of axis as we might expect).\nThe Figure object is a box-like container holding one or more Axes (actual plots), as shown in Figure 6.1. Below the Axes in the hierarchy are smaller objects such as tick marks, individual lines, legends, and text boxes. Almost every element of a chart is its own manipulable Python object, all the way down to the ticks and labels.\n\n\n\n\nFigure 6.1: Matplotlib Object hierarchy\n\n\n\nHowever, Matplotlib presents this as a figure anatomy, rather than an explicit hierarchy. Figure 6.2 shows the components of a figure that can be customized with Matplotlib. (Source: https://matplotlib.org/stable/gallery/showcase/anatomy.html ).\n\n\n\n\nFigure 6.2: Matplotlib anatomy of a figure\n\n\n\nLet’s visualize the life expectancy of different countries with GDP per capita. We’ll read the data file gdp_lifeExpectancy.csv, which contains the GDP per capita and life expectancy of countries from 1952 to 2007.\n\nimport pandas as pd\nimport numpy as np\n\n\ngdp_data = pd.read_csv('./Datasets/gdp_lifeExpectancy.csv')\ngdp_data.head()\n\n\n\n\n\n  \n    \n      \n      country\n      continent\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      Asia\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1\n      Afghanistan\n      Asia\n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      2\n      Afghanistan\n      Asia\n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      3\n      Afghanistan\n      Asia\n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      4\n      Afghanistan\n      Asia\n      1972\n      36.088\n      13079460\n      739.981106\n    \n  \n\n\n\n\n\n\n6.1.2 Scatterplots and trendline with Matplotlib\nPurpose of scatterplots: Scatterplots (with or without a trendline) allow us to visualize the relationship between two numerical variables.\nWe’ll import the pyplot module of matplotlib to make plots. We’ll use the plot() function to make the scatter plot, and the functions xlabel() and ylabel() for labeling the plot axes.\n\nimport matplotlib.pyplot as plt\n\nQ: Make a scatterplot of Life expectancy vs GDP per capita.\nThere are two ways of plotting the figure:\n\nExplicitly creating figures and axes, and call methods on them (object-oriented style).\nLetting pyplot implicitly track the plot that it wants to reference. Simple functions are used to add plot elements (lines, images, text, etc.) to the current axes in the current figure (pyplot-style).\n\nWe’ll plot the figure in both ways.\n\n#Method 1: Object-oriented style\nfig, ax = plt.subplots() #Create a figure and an axes\nx = gdp_data.gdpPercap \ny = gdp_data.lifeExp\nax.plot(x,y,'o')   #Plot data on the axes\nax.set_xlabel('GDP per capita')    #Add an x-label to the axes\nax.set_ylabel('Life expectancy')   #Add a y-label to the axes\nax.set_title('Life expectancy vs GDP per capita from 1952 to 2007')\n\nText(0.5, 1.0, 'Life expectancy vs GDP per capita from 1952 to 2007')\n\n\n\n\n\n\n#Method 2: pyplot style\nx = gdp_data.gdpPercap\ny = gdp_data.lifeExp\nplt.plot(x,y,'o') #By default, the plot() function makes a lineplot. The 'o' arguments specifies a scatterplot\nplt.xlabel('GDP per capita')  #Labelling the horizontal X-axis\nplt.ylabel('Life expectancy') #Labelling the verical Y-axis\nplt.title('Life expectancy vs GDP per capita from 1952 to 2007')\n\nText(0.5, 1.0, 'Life expectancy vs GDP per capita from 1952 to 2007')\n\n\n\n\n\nBoth the plotting styles - object-oriented style and the pyplot style are perfectly valid and have their pros and cons.\n\nPyplot style is easier for simple plots\nObject-oriented style is slightly more complicated but more powerful as it allows for greater control over the axes in figure. This proves to be quite useful when we are dealing with a figure with multiple axes.\n\nFrom the above plot, we observe that life expectancy seems to be positively correlated with the GDP per capita of the country, as one may expect. However, there are a few outliers in the data - which are countries having extremely high GDP per capita, but not a correspondingly high life expectancy.\nSometimes it is difficult to get an idea of the overall trend (positive or negative correlation). In such cases, it may help to add a trendline to the scatter plot. In the plot below we add a trendline over the scatterplot showing that the life expectancy on an average increases with increasing GDP per capita. The trendline is actually a linear regression of life expectancy on GDP per capita. However, we’ll not discuss linear regression in this book.\nQ: Add a trendline over the scatterplot of life expectancy vs GDP per capita.\n\n#Making a scatterplot of Life expectancy vs GDP per capita\nx = gdp_data.gdpPercap\ny = gdp_data.lifeExp\nplt.plot(x,y,'o') #By default, the plot() function makes a lineplot. The 'o' arguments specifies a scatterplot\nplt.xlabel('GDP per capita')  #Labelling the horizontal X-axis\nplt.ylabel('Life expectancy') #Labelling the verical Y-axis\n\n#Plotting a trendline (linear regression) on the scatterplot\nslope_intercept_trendline = np.polyfit(x,y,1)   #Finding the slope and intercept for the trendline\ncompute_y_given_x = np.poly1d(slope_intercept_trendline) #Defining a function that computes the 'y' given 'x' for the trendline\nplt.plot(x,compute_y_given_x(x)) #Plotting the trendline\n\n\n\n\nThe above plot shows that our earlier intuition of a postive correlation between Life expectancy and GDP per capita was correct.\nWe used the NumPy function polyfit() to compute the slope and intercept of the trendline. Then, we defined an object compute_y_given_x of poly1d class and used it to compute the trendline.\n\n\n6.1.3 Subplots\nThere is often a need to make a few plots together to compare them. See the example below.\nQ: Make scatterplots of life expectancy vs GDP per capita separately for each of the 4 continents of Asia, Europe, Africa and America. Arrange the plots in a 2 x 2 grid.\n\n#Defining a 2x2 grid of subplots\nfig, axes = plt.subplots(2,2,figsize=(16,10))\nplt.subplots_adjust(wspace=0.2) #adjusting white space between individual plots\n\n#Making a scatterplot of Life expectancy vs GDP per capita for each continent\ncontinents = np.array([['Asia', 'Europe'], ['Africa', 'Americas']])\n\n#Looping over the 2x2 grid\nfor i in range(2):\n    for j in range(2):\n        \n        #Getting the GDP per capita and life expectancy of the countries of the (i,j)th continent\n        x = gdp_data.loc[gdp_data.continent==continents[i,j],:].gdpPercap\n        y = gdp_data.loc[gdp_data.continent==continents[i,j],:].lifeExp\n        \n        #Making the scatterplot\n        axes[i,j].plot(x,y,'o') \n        \n        #Setting limits on the 'x' and 'y' axes\n        axes[i,j].set_xlim([gdp_data.gdpPercap.min(), gdp_data.gdpPercap.max()])\n        axes[i,j].set_ylim([gdp_data.lifeExp.min(), gdp_data.lifeExp.max()])\n        \n        #Labelling the 'x' and 'y' axes\n        axes[i,j].set_xlabel('GDP per capita for '+ continents[i,j],fontsize = 14)\n        axes[i,j].set_ylabel('Life expectancy for '+ continents[i,j],fontsize = 14)\n        \n        #Putting a dollar sign, and thousand-comma separator on x-axis labels\n        axes[i,j].xaxis.set_major_formatter('${x:,.0f}')\n        \n        #Increasing font size of axis labels\n        axes[i,j].tick_params(axis = 'both',labelsize=14)\n\n\n\n\nWe observe that for each continent, except Africa, initially life expectancy increases rapidly with increasing GDP per capita. However, after a certain threshold of GDP per capita, life expectancy increases slowly. Several countries in Europe enjoy a relatively high GDP per capita as well as high life expectancy. Some countries in Asia have an extremely high GDP per capita, but a relatively low life expectancy. It will be interesting to see the proportion of GDP associated with healthcare for these outlying Asian countries, and European countries.\nWe used the subplot function of matplotlib to define the 2x2 grid of subplots. The function subplots_adjust() can be used to adjust white spaces around the plot. We used a for loop to iterate over each subplot. The axes object returned by the subplot() function was used to refer to individual subplots.\n\n\n6.1.4 Practice problem 1\nIs NU_GPA associated with parties_per_month? Analyze the association separately for Sophomores, Juniors, and Seniors (categories of the variable school_year).\nMake scatterplots of NU_GPA vs parties_per_month in a 1 x 3 grid, where each grid is for a distinct school_year. Plot the trendline as well for each scatterplot. Use the file survey_data_clean.csv.\nSolution:\n\nsurvey_data = pd.read_csv('./Datasets/survey_data_clean.csv')\n\ndef NU_GPA_vs_parties_per_month(data):\n    fig, axes = plt.subplots(1,3,figsize=(15,5))\n    plt.subplots_adjust(wspace=0.4) \n    \n    school_years = np.array(['Sophomore', 'Junior','Senior'])\n    for i in range(3):\n        x = data.loc[data.school_year==school_years[i],:].parties_per_month\n        y = data.loc[data.school_year==school_years[i],:].NU_GPA\n        \n        #The data has missing values. We can draw a trendline using only the non-missing value-pairs of NU_GPA and parties_per_month\n        #`idx_non_missing` will have the indices of the non-missing value-pairs of NU_GPA and parties_per_month\n        idx_non_missing = np.isfinite(x) & np.isfinite(y)\n        \n        axes[i].plot(x,y,'o',label = school_years[i]) \n        axes[i].set_xlim([data.parties_per_month.min(), data.parties_per_month.max()])\n        axes[i].set_ylim([data.NU_GPA.min(), data.NU_GPA.max()])\n        axes[i].set_xlabel('Parties per month',fontsize = 14)  \n        axes[i].set_ylabel('NU GPA',fontsize = 14) \n        axes[i].set_title(school_years[i],fontsize = 15)\n        slope_intercept_trendline = np.polyfit(x[idx_non_missing],y[idx_non_missing],1)   #Finding the slope and intercept for the trendline\n        compute_y_given_x = np.poly1d(slope_intercept_trendline) #Defining a function that computes the 'y' given 'x' for the trendline\n        axes[i].plot(x,compute_y_given_x(x)) #Plotting the trendline\n\nNU_GPA_vs_parties_per_month(survey_data)\n\n\n\n\nNote that the trendline in the above plots seems to be influenced by a few points having extreme values of parties_per_month. These points have a high leverage (a concept we’ll learn in a future course on linear regression) in influencing the trendline. So, we should visualize the trend by removing or capping these high-leverage points, to avoid the distortion of the trend by a few points.\nLet us cap the the values of parties_per_month to 30, and make the visualizations again.\n\nsurvey_data_parties_capped = survey_data.copy()\nsurvey_data_parties_capped.parties_per_month = survey_data.parties_per_month.apply(lambda x: min(30,x))\nNU_GPA_vs_parties_per_month(survey_data_parties_capped)\n\n\n\n\nWe see that the trend didn’t change much after removing the high leverage points. (Note that although the high leverage points have the leverage to influence the trendline, they need not necessarily influence it). From the visualization, NU_GPA doesn’t seem to be associated with parties_per_month for students of any of the school years.\n\n\n6.1.5 Overlapping plots with legend\nWe can also have the scatterplot of all the continents on the sample plot, with a distinct color for each continent. A legend will be required to identify the continent’s color.\n\ncontinents = np.array([['Asia', 'Europe'], ['Africa', 'Americas']])\nplt.rcParams[\"figure.figsize\"] = (9,6)\nfor i in range(2):\n    for j in range(2):\n        x = gdp_data.loc[gdp_data.continent==continents[i,j],:].gdpPercap\n        y = gdp_data.loc[gdp_data.continent==continents[i,j],:].lifeExp\n        plt.plot(x,y,'o',label = continents[i,j]) \n        plt.xlim([gdp_data.gdpPercap.min(), gdp_data.gdpPercap.max()])\n        plt.ylim([gdp_data.lifeExp.min(), gdp_data.lifeExp.max()])\n        plt.xlabel('GDP per capita')  \n        plt.ylabel('Life expectancy') \nplt.legend()\n\n<matplotlib.legend.Legend at 0x2320d6d70a0>\n\n\n\n\n\nNote that a disadvantage of the above plot is overplotting. The data points corresponding to the Americas are hiding the data points of other continents. However, if the data points corresponding to different categories are spread apart, then it may be convenient to visualize all the categories on the same plot."
  },
  {
    "objectID": "Data visualization.html#pandas",
    "href": "Data visualization.html#pandas",
    "title": "6  Data visualization",
    "section": "6.2 Pandas",
    "text": "6.2 Pandas\nMatplotlib is a low-level tool, in which different components of the plot, such as points, legend, axis titles, etc. need to be specified separately. The Pandas plot() function can be used directly with a DataFrame or Series to make plots.\n\n6.2.1 Scatterplots with Pandas\n\n#Plotting life expectancy vs GDP per capita using the Pandas plot() function\nax = gdp_data.plot(x = 'gdpPercap', y = 'lifeExp', kind = 'scatter',figsize=(10, 6),xlabel = 'GDP per capita', \n              ylabel = 'Life expectancy')\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n\n\n\n\n\nIn the above plot, note that:\n\nWith matplotlib, it will take 3 lines to make the same plot - one for the scatterplot, and two for the axis titles.\nThe object ax is of type matplotlib.axes._subplots.AxesSubplot (check the code below). This means we can use the attributes and methods associated with the axes object of Matplotlib. If you see the documentation of the Pandas plot() function, you will find that under the kwargs** argument, you have Options to pass to matplotlib plotting method. Thus, you get the convenience of using the Pandas plot() function, while also having the attributes and methods associated with Matplotlib.\n\n\ntype(ax)\n\nmatplotlib.axes._subplots.AxesSubplot\n\n\n\n\n6.2.2 Lineplots with Pandas\nPurpose of lineplots: Lineplots show the relationship between two numerical variables when the variable on the x-axis, also called the explanatory variable, is of a sequential nature; in other words there is an inherent ordering to the variable. The most common example of lineplots have some notion of time on the x-axis (or the horizontal axis): hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Lineplots that have some notion of time on the x-axis are also called time series plots. Lineplots should be avoided when there is not a clear sequential ordering to the variable on the x-axis.\nLet us re-arrange the data to show other benefits of the Pandas plot() function. Note that data resphaping is explained in Chapter 8 of the book, so you may ignore the code block below that uses the pivot_table() function.\n\n#You may ignore this code block until Chapter 8.\nmean_gdp_per_capita = gdp_data.pivot_table(index = 'year', columns = 'continent',values = 'gdpPercap')\nmean_gdp_per_capita.head()\n\n\n\n\n\n  \n    \n      continent\n      Africa\n      Americas\n      Asia\n      Europe\n      Oceania\n    \n    \n      year\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1952\n      1252.572466\n      4079.062552\n      5195.484004\n      5661.057435\n      10298.085650\n    \n    \n      1957\n      1385.236062\n      4616.043733\n      5787.732940\n      6963.012816\n      11598.522455\n    \n    \n      1962\n      1598.078825\n      4901.541870\n      5729.369625\n      8365.486814\n      12696.452430\n    \n    \n      1967\n      2050.363801\n      5668.253496\n      5971.173374\n      10143.823757\n      14495.021790\n    \n    \n      1972\n      2339.615674\n      6491.334139\n      8187.468699\n      12479.575246\n      16417.333380\n    \n  \n\n\n\n\nWe have reshaped the data to obtain the mean GDP per capita of each continent for each year.\nThe pandas plot() function can be directly used with this DataFrame to create line plots showing mean GDP per capita of each continent with year.\n\nax = mean_gdp_per_capita.plot(ylabel = 'GDP per capita',figsize = (10,6),marker='o')\nax.yaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nWe observe that the mean GDP per capita of of Europe and Oceania have increased rapidly, while that for Africa is increasing very slowly.\nThe above plot will take several lines of code if developed using only matplotlib. The pandas plot() function has a framework to conveniently make commonly used plots.\nNote that argument marker = ‘o’ puts a solid circle at each of the data points.\n\n\n6.2.3 Bar plots with Pandas\nPurpose of bar plots: Barplots are used to visualize any aggregate statistics of a continuous variable with respect to the categories or levels of a categorical variable. For example, we may visualize the average IMDB rating (aggregate statistics) of movies based on their genre (the categorical variable).\nBar plots can be made using the pandas bar function with the DataFrame or Series, just like the line plots and scatterplots.\nBelow, we are reading the dataset of noise complaints of type Loud music/Party received the police in New York City in 2016.\n\nnyc_party_complaints = pd.read_csv('./Datasets/party_nyc.csv')\nnyc_party_complaints.head()\n\n\n\n\n\n  \n    \n      \n      Created Date\n      Closed Date\n      Location Type\n      Incident Zip\n      City\n      Borough\n      Latitude\n      Longitude\n      Hour_of_the_day\n      Month_of_the_year\n    \n  \n  \n    \n      0\n      12/31/2015 0:01\n      12/31/2015 3:48\n      Store/Commercial\n      10034.0\n      NEW YORK\n      MANHATTAN\n      40.866183\n      -73.918930\n      0\n      12\n    \n    \n      1\n      12/31/2015 0:02\n      12/31/2015 4:36\n      Store/Commercial\n      10040.0\n      NEW YORK\n      MANHATTAN\n      40.859324\n      -73.931237\n      0\n      12\n    \n    \n      2\n      12/31/2015 0:03\n      12/31/2015 0:40\n      Residential Building/House\n      10026.0\n      NEW YORK\n      MANHATTAN\n      40.799415\n      -73.953371\n      0\n      12\n    \n    \n      3\n      12/31/2015 0:03\n      12/31/2015 1:53\n      Residential Building/House\n      11231.0\n      BROOKLYN\n      BROOKLYN\n      40.678285\n      -73.994668\n      0\n      12\n    \n    \n      4\n      12/31/2015 0:05\n      12/31/2015 3:49\n      Residential Building/House\n      10033.0\n      NEW YORK\n      MANHATTAN\n      40.850304\n      -73.938516\n      0\n      12\n    \n  \n\n\n\n\nLet us visualise the locations from where the the complaints are coming.\n\n#Using the pandas function bar() to create bar plot\nax = nyc_party_complaints['Location Type'].value_counts().plot.bar(ylabel = 'Number of complaints')\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\nFrom the above plot, we observe that most of the complaints come from residential buildings and houses, as one may expect.\nLet is visualize the time of the year when most complaints occur.\n\n#Using the pandas function bar() to create bar plot\nax = nyc_party_complaints['Month_of_the_year'].value_counts().sort_index().plot.bar(ylabel = 'Number of complaints',\n                                                                              xlabel = \"Month\")\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\nTry executing the code without sort_index() to figure out the purpose of using the function.\nFrom the above plot, we observe that most of the complaints occur during summer and early Fall.\nLet us create a stacked bar chart that combines both the above plots into a single plot. You may ignore the code used for re-shaping the data until Chapter 8. The purpose here is to show the utility of the pandas bar() function.\n\n#Reshaping the data to make it suitable for a stacked barplot - ignore this code until chapter 8\ncomplaints_location=pd.crosstab(nyc_party_complaints.Month_of_the_year, nyc_party_complaints['Location Type'])\ncomplaints_location.head()\n\n\n\n\n\n  \n    \n      Location Type\n      Club/Bar/Restaurant\n      House of Worship\n      Park/Playground\n      Residential Building/House\n      Store/Commercial\n      Street/Sidewalk\n    \n    \n      Month_of_the_year\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      748\n      24\n      17\n      9393\n      1157\n      832\n    \n    \n      2\n      570\n      29\n      16\n      8383\n      1197\n      782\n    \n    \n      3\n      747\n      39\n      90\n      9689\n      1480\n      1835\n    \n    \n      4\n      848\n      53\n      129\n      11984\n      1761\n      2943\n    \n    \n      5\n      2091\n      72\n      322\n      15676\n      1941\n      5090\n    \n  \n\n\n\n\n\n#Stacked bar plot showing number of complaints at different months of the year, and from different locations\nax = complaints_location.plot.bar(stacked=True,ylabel = 'Number of complaints',figsize=(15, 10), xlabel = 'Month')\nax.tick_params(axis = 'both',labelsize=15)\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\nFigure 6.3: Stacked bar plot with Pandas\n\n\n\n\nThe above plots gives the insights about location and day of the year simultaneously that were previously separately obtained by the individual plots.\nAn alternative to stacked barplots are side-by-side barplots, as shown below.\n\n#Side-by-side bar plot showing number of complaints at different months of the year, and from different locations\nax = complaints_location.plot.bar(ylabel = 'Number of complaints',figsize=(15, 10), xlabel = 'Month')\nax.tick_params(axis = 'both',labelsize=15)\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\n\\(\\color{red}{\\text{Q1}}\\) In which scenarios should we use a stacked barplot instead of a side-by-side barplot and vice-versa?"
  },
  {
    "objectID": "Data visualization.html#seaborn",
    "href": "Data visualization.html#seaborn",
    "title": "6  Data visualization",
    "section": "6.3 Seaborn",
    "text": "6.3 Seaborn\nSeaborn offers the flexibility of simultaneously visualizing multiple variables in a single plot, and offers several themes to develop plots.\n\n#Importing the seaborn library\nimport seaborn as sns\n\n\n6.3.1 Bar plots with confidence intervals with Seaborn\nWe’ll group the data to obtain the total complaints for each Location Type, Borough, Month_of_the_year, and Hour_of_the_day. Note that you’ll learn grouping data in Chapter 9, so you may ignore the next code block. The grouping is done to shape the data in a suitable form for visualization.\n\n#Grouping the data to make it suitable for visualization using Seaborn. Ignore this code block until learn chapter 9.\nnyc_complaints_grouped = nyc_party_complaints[['Location Type','Borough','Month_of_the_year','Latitude','Hour_of_the_day']].groupby(['Location Type','Borough','Month_of_the_year','Hour_of_the_day'])['Latitude'].agg([('complaints','count')]).reset_index()\nnyc_complaints_grouped.head()\n\n\n\n\n\n  \n    \n      \n      Location Type\n      Borough\n      Month_of_the_year\n      Hour_of_the_day\n      complaints\n    \n  \n  \n    \n      0\n      Club/Bar/Restaurant\n      BRONX\n      1\n      0\n      10\n    \n    \n      1\n      Club/Bar/Restaurant\n      BRONX\n      1\n      1\n      10\n    \n    \n      2\n      Club/Bar/Restaurant\n      BRONX\n      1\n      2\n      6\n    \n    \n      3\n      Club/Bar/Restaurant\n      BRONX\n      1\n      3\n      6\n    \n    \n      4\n      Club/Bar/Restaurant\n      BRONX\n      1\n      4\n      3\n    \n  \n\n\n\n\nLet us create a bar plot visualizing the average number of complaints with the time of the day.\n\nax = sns.barplot(x=\"Hour_of_the_day\", y = 'complaints',  data=nyc_complaints_grouped)\nax.figure.set_figwidth(15)\n\n\n\n\nFrom the above plot, we observe that most of the complaints are made around midnight. However, interestingly, there are some complaints at each hour of the day.\nNote that the above barplot shows the mean number of complaints in a month at each hour of the day. The black lines are the 95% confidence intervals of the mean number of complaints.\n\n\n6.3.2 Facetgrid: Multi-plot grid for plotting conditional relationships\nWith pandas, we simultaneously visualized the number of complaints with month of the year and location type in Figure 6.3. We’ll use Seaborn to add another variable - Borough to the visualization.\nQ: Visualize the mean number of complaints with Month_of_the_year, Location Type, and Borough.\nThe seaborn class FacetGrid is used to design the plot, i.e., specify the way the data will be divided in mutually exclusive subsets for visualization. Then the [map] function of the FacetGrid class is used to apply a plotting function to each subset of the data.\n\n#Visualizing the number of complaints with Month_of_the_year, Location Type, and Borough.\na = sns.FacetGrid(nyc_complaints_grouped, hue = 'Location Type', col = 'Borough',col_wrap=3,height=3.5,aspect = 1)\na.map(sns.lineplot,'Month_of_the_year','complaints')\na.set_axis_labels(\"Month of the year\", \"Complaints\")\na.add_legend()\n\n<seaborn.axisgrid.FacetGrid at 0x231afeafac0>\n\n\n\n\n\nFrom the above plot, we get a couple of interesting insights: 1. For Queens and Staten Island, most of the complaints occur in summer, for Manhattan and Bronx it is mostly during late spring, while Brooklyn has a spike of complaints in early Fall. 2. In most of the Boroughs, the majority complaints always occur in residential areas. However, for Manhattan, the number of street/sidewalk complaints in the summer are comparable to those from residential areas.\nWe have visualized 4 variables simultaneously in the above plot.\nLet us consider another example, where we will visualize the weather in a few cities of Australia. The file Australia_weather.csv consists of weather details of Sydney, Canberra, and Melbourne from 2007 to 2017.\n\naussie_weather = pd.read_csv('./Datasets/Australia_weather.csv')\naussie_weather.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Location\n      MinTemp\n      MaxTemp\n      Rainfall\n      Evaporation\n      Sunshine\n      WindGustDir\n      WindGustSpeed\n      WindDir9am\n      ...\n      Humidity3pm\n      Pressure9am\n      Pressure3pm\n      Cloud9am\n      Cloud3pm\n      Temp9am\n      Temp3pm\n      RainToday\n      RISK_MM\n      RainTomorrow\n    \n  \n  \n    \n      0\n      10/20/2010\n      Sydney\n      12.9\n      20.3\n      0.2\n      3.0\n      10.9\n      ENE\n      37\n      W\n      ...\n      57\n      1028.8\n      1025.6\n      3\n      1\n      16.9\n      19.8\n      No\n      0.0\n      No\n    \n    \n      1\n      10/21/2010\n      Sydney\n      13.3\n      21.5\n      0.0\n      6.6\n      11.0\n      ENE\n      41\n      W\n      ...\n      58\n      1025.9\n      1022.4\n      2\n      5\n      17.6\n      21.3\n      No\n      0.0\n      No\n    \n    \n      2\n      10/22/2010\n      Sydney\n      15.3\n      23.0\n      0.0\n      5.6\n      11.0\n      NNE\n      41\n      W\n      ...\n      63\n      1021.4\n      1017.8\n      1\n      4\n      19.0\n      22.2\n      No\n      0.0\n      No\n    \n    \n      3\n      10/26/2010\n      Sydney\n      12.9\n      26.7\n      0.2\n      3.8\n      12.1\n      NE\n      33\n      W\n      ...\n      56\n      1018.0\n      1015.0\n      1\n      5\n      17.8\n      22.5\n      No\n      0.0\n      No\n    \n    \n      4\n      10/27/2010\n      Sydney\n      14.8\n      23.8\n      0.0\n      6.8\n      9.6\n      SSE\n      54\n      SSE\n      ...\n      69\n      1016.0\n      1014.7\n      2\n      7\n      20.2\n      20.6\n      No\n      1.8\n      Yes\n    \n  \n\n5 rows × 24 columns\n\n\n\n\naussie_weather.shape\n\n(4666, 24)\n\n\nQ: Visualize if it rains the next day (RainTomorrow) given whether it has rained today (RainToday), the current day’s humidity (Humidity9am), maximum temperature (MaxTemp) and the city (Location).\n\na = sns.FacetGrid(aussie_weather,col='Location',row='RainToday',height = 4,aspect = 1,hue = 'RainTomorrow')\na.map(plt.scatter,'MaxTemp','Humidity9am')\na.set_axis_labels(\"Maximum temperature\", \"Humidity at 9 am\")\na.set_titles(col_template=\"{col_name}\", row_template=\"Rain today: {row_name}\")\na.add_legend()\n\n<seaborn.axisgrid.FacetGrid at 0x231b57e51c0>\n\n\n\n\n\nHumidity tends to be higher when it is going to rain the next day. However, the correlation is much more pronounced for Syndey. In case it is not raining on the current day, humidity seems to be slightly negatively correlated with temperature.\n\n\n6.3.3 Practice exercise 2\nHow does the expected marriage age of the people of STAT303-1 depend on their characteristics? We’ll use visualizations to answer this question. Use data from the file survey_data_clean.csv. Proceed as follows:\n\nMake a visualization that compares the mean expected_marriage_age of introverts and extroverts (use the variable introvert_extrovert). What insights do you obtain?\n\n\nplt.rcParams[\"figure.figsize\"] = (9,6)\nsns.barplot(x = 'introvert_extrovert' ,y = 'expected_marriage_age', data = survey_data)\nplt.xlabel('Personality', fontsize=16);\nplt.ylabel('Expected marriage age', fontsize=16);\nplt.xticks(fontsize=15);\nplt.yticks(fontsize=15);\n\n\n\n\nThe mean expected marriage age for introverts is about 2 years higher than that for extroverts. Also, there is a higher variation in the expected marriage age of introverts as compared to extroverts.\n\nDoes the mean expected_marriage_age of introverts and extroverts depend on whether they believe in love in first sight (variable name: love_first_sight)? Update the previous visualization to answer the question.\n\n\nsns.set(font_scale=1.25)\na = sns.FacetGrid(survey_data,col='love_first_sight',height = 5,aspect = 1)\na.map(sns.barplot, 'introvert_extrovert' , 'expected_marriage_age' ,order = ['Introvert', 'Extrovert'])\na.set_axis_labels(\"Personality\", \"Expected marriage age\")\na.add_legend()\n\n<seaborn.axisgrid.FacetGrid at 0x20c14eed760>\n\n\n\n\n\nYes, only those introverts who do not believe in love in first sight have a higher mean value of expected marriage age.\n\nIn addition to love_first_sight, does the mean expected_marriage_age of introverts and extroverts depend on whether they are a procrastinator (variable name: procrastinator)? Update the previous visualization to answer the question.\n\n\nsns.set(font_scale=1.25)\na = sns.FacetGrid(survey_data,col='love_first_sight',height = 5,aspect = 1, row = 'procrastinator')\na.map(sns.barplot, 'introvert_extrovert' , 'expected_marriage_age' ,order = ['Introvert', 'Extrovert'])\na.set_axis_labels(\"Personality\", \"Expected marriage age\")\na.add_legend()\n\n<seaborn.axisgrid.FacetGrid at 0x20c1578cb50>\n\n\n\n\n\nProcrastination does not seem to make much of a difference in the expected marriage age. The mean expected marriage age of procrastinating introverts seems to be only a little higher than the non-procrastinating introverts.\n\nIs there any critical information missing in the above visualizations that, if revealed, may cast doubts on the patterns observed in them?\n\nYes, we don’t know the number of observations corresponding to each bar of the bar plots. If there are a very few observations in any of the categories, then the trend shown by that category may not be reliable. For example, in the data (survey_data), there are only 8 introverts who are not procrastinators and believe in love in first sight, while there are 52 introverts who are procrastinators and do not believe in love in first sight.\nIf there are more introverts in the class of STAT303-1 who are not procrastinators and believe in love at first sight (may be they didn’t fill the survey), then they are under-represented in the sample of people who filled the survey, and the trend observed for them may be less reliable than that for other people.\n\n#Code for finding the number of people in each category - you will understand this code later in chapter 9 on data aggregation\nsurvey_data[['introvert_extrovert','love_first_sight','procrastinator','Timestamp']].groupby(['introvert_extrovert',\n                                                                                  'love_first_sight','procrastinator']).count()\n\n\n\n\n\n  \n    \n      \n      \n      \n      Timestamp\n    \n    \n      introvert_extrovert\n      love_first_sight\n      procrastinator\n      \n    \n  \n  \n    \n      Extrovert\n      0\n      0\n      19\n    \n    \n      1\n      35\n    \n    \n      1\n      0\n      10\n    \n    \n      1\n      15\n    \n    \n      Introvert\n      0\n      0\n      32\n    \n    \n      1\n      52\n    \n    \n      1\n      0\n      8\n    \n    \n      1\n      21\n    \n  \n\n\n\n\n\n\n6.3.4 Histogram and density plots with Seaborn\nPurpose: Histogram and density plots visualize the distribution of a continuous variable.\nA histogram plots the number of observations occurring within discrete, evenly spaced bins of a random variable, to visualize the distribution of the variable. It may be considered a special case of a bar plot as bars are used to plot the observation counts.\nA density plot uses a kernel density estimate to approximate the distribution of random variable.\nWe can use the Seaborn displot() function to make both kinds of plots - histogram or density plot.\nExample: Make a histogram showing the distributions of maximum temperature in Sydney, Canberra and Melbourne.\n\nsns.set(font_scale = 1.4)\na = sns.displot(data = aussie_weather, x = 'MaxTemp',kind = 'hist',col='Location')\na.set_axis_labels(\"Maximum temperature\", \"Count\")\na.set_titles(\"{col_name}\")\n\n<seaborn.axisgrid.FacetGrid at 0x2319fbeabb0>\n\n\n\n\n\nFrom the above plot, we observe that: 1. Melbourne has a right skewed distribution with the median temperature being smaller than the mean. 2. Canberra seems to have the highest variation in the temperature.\nExample: Make a density plot showing the distributions of maximum temperature in Sydney, Canberra and Melbourne.\n\nsns.displot(data = aussie_weather, x = 'MaxTemp',kind = 'kde', col = 'Location')\n\n<seaborn.axisgrid.FacetGrid at 0x1d0e963f8e0>\n\n\n\n\n\nExample: Show the distributions of the maximum and minimum temperatures in a single plot.\n\nsns.histplot(data=aussie_weather, x=\"MaxTemp\", color=\"skyblue\", label=\"MaxTemp\", kde=True)\nsns.histplot(data=aussie_weather, x=\"MinTemp\", color=\"red\", label=\"MinTemp\", kde=True)\nplt.legend()\nplt.xlabel('Temperature')\n\nText(0.5, 0, 'Temperature')\n\n\n\n\n\nThe Seaborn function histplot() can be used to make a density plot overlapping on a histogram.\n\n\n6.3.5 Boxplots with Seaborn\nPurpose: Boxplots is a standardized way of visualizing the distribution of a continuous variable. They show five key metrics that describe the data distribution - median, 25th percentile value, 75th percentile value, minimum and maximum, as shown in the figure below. Note that the minimum and maximum exclude the outliers.\n\n\n\n\n\nExample: Make a boxplot comparing the distributions of maximum temperatures of Sydney, Canberra and Melbourne, given whether or not it has rained on the day.\n\nsns.boxplot(data = aussie_weather,x = 'Location', y = 'MaxTemp',hue = 'RainToday')\n\n<AxesSubplot:xlabel='Location', ylabel='MaxTemp'>\n\n\n\n\n\nFrom the above plot, we observe that: 1. The maximum temperature of the day, on an average, is lower if it rained on the day. 2. Sydney and Melbourne have some extremely high outlying values of maximum temperature.\nWe have used the Seaborn boxplot() function for the above plot.\n\n\n6.3.6 Scatterplots with Seaborn\nWe made scatterplots with Matplotlib and Pandas earlier. With Seaborn, the regplot() function allows us to plot a trendline over the scatterplot, along with a 95% confidence interval for the trendline. Note that this is much easier than making a trendline with Matplotlib.\n\n#Scatterplot and trendline with seaborn\nplt.rcParams[\"figure.figsize\"] = (9,6)\nsns.set(font_scale=1.5)\nax=sns.regplot(x = 'gdpPercap', y = 'lifeExp', data = gdp_data,scatter_kws={\"color\": \"orange\"}, line_kws={\"color\": \"blue\"})\nax.xaxis.set_major_formatter('${x:,.0f}')\nax.set_xlabel('GDP per capita')\nax.set_ylabel('Life expectancy')\n\nText(0, 0.5, 'Life expectancy')\n\n\n\n\n\nNote that the confidence interval of the trendline broadens as we move farther away from most of the data points. In other words, there is more uncertainty about the trend as we move to a domain space farther away from the data.\n\n\n6.3.7 Heatmaps with Seaborn\nPurpose: Heatmaps help us visualize the correlation between all variable-pairs.\nBelow is a heatmap visualizing the pairwise correlation of all the numerical variables of survey_data_clean. With a heatmap it becomes easier to see strongly correlated variables.\n\nsns.set(rc={'figure.figsize':(12,10)})\nsns.heatmap(survey_data.corr())\n\n<AxesSubplot:>\n\n\n\n\n\nFrom the above map, we can see that:\n\nstudent athlete is strongly postively correlated with minutes_ex_per_week\nprocrastinator is strongly negatively correlated with NU_GPA\n\n\n\n6.3.8 Pairplots with Seaborn\nPurpose: Pairplots are used to visualize the association between all variable-pairs in the data. In other words, pairplots simultaneously visualize the scatterplots between all variable-pairs.\nLet us visualize the pair-wise association of nutrition variables in the starbucks drinks data.\n\nstarbucks_drinks = pd.read_csv('./Datasets/starbucks-menu-nutrition-drinks.csv')\nsns.pairplot(starbucks_drinks)\n\n<seaborn.axisgrid.PairGrid at 0x162423db9a0>\n\n\n\n\n\nIn the above pairplot, note that:\n\nThe histograms on the diagonal of the grid show the distribution of each of the variables.\nInstead of a histogram, we can visualize the density plot with the argument kde = True.\nThe scatterplots in the rest of the grid are the pair-wise plots of all the variables.\n\nFrom the above plot, we observe that:\n\nAlmost all the variable pairs have a positive correlation, i.e., if one of the nutrients increase in a drink, others also are likely to increase.\nThe number of calories seem to be strongly positively correlated with the amount of carbs in the drink.\nFrom the density plots we can see that there is a lot of choice for consumers to buy a drink that has a zero value for any of the nutrients - fat, protein, fiber, or sodium."
  },
  {
    "objectID": "Data cleaning and preparation.html",
    "href": "Data cleaning and preparation.html",
    "title": "7  Data cleaning and preparation",
    "section": "",
    "text": "Missing values in a dataset can occur due to several reasons such as breakdown of measuring equipment, accidental removal of observations, lack of response by respondents, error on the part of the researcher, etc.\nLet us read the dataset GDP_missing_data.csv, in which we have randomly removed some values, or put missing values in some of the columns.\nWe’ll also read GDP_complete_data.csv, in which we have not removed any values. We’ll use this data later to assess the accuracy of our guess or estimate of missing values in GDP_missing_data.csv.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn as sk\nimport seaborn as sns\ngdp_missing_values_data = pd.read_csv('./Datasets/GDP_missing_data.csv')\ngdp_complete_data = pd.read_csv('./Datasets/GDP_complete_data.csv')\n\n\ngdp_missing_values_data.head()\n\n\n\n\n\n  \n    \n      \n      economicActivityFemale\n      country\n      lifeMale\n      infantMortality\n      gdpPerCapita\n      economicActivityMale\n      illiteracyMale\n      illiteracyFemale\n      lifeFemale\n      geographic_location\n      contraception\n      continent\n    \n  \n  \n    \n      0\n      7.2\n      Afghanistan\n      45.0\n      154.0\n      2474.0\n      87.5\n      NaN\n      85.0\n      46.0\n      Southern Asia\n      NaN\n      Asia\n    \n    \n      1\n      7.8\n      Algeria\n      67.5\n      44.0\n      11433.0\n      76.4\n      26.1\n      51.0\n      70.3\n      Northern Africa\n      NaN\n      Africa\n    \n    \n      2\n      41.3\n      Argentina\n      69.6\n      22.0\n      NaN\n      76.2\n      3.8\n      3.8\n      76.8\n      South America\n      NaN\n      South America\n    \n    \n      3\n      52.0\n      Armenia\n      67.2\n      25.0\n      13638.0\n      65.0\n      NaN\n      0.5\n      74.0\n      Western Asia\n      NaN\n      Asia\n    \n    \n      4\n      53.8\n      Australia\n      NaN\n      6.0\n      54891.0\n      NaN\n      1.0\n      1.0\n      81.2\n      Oceania\n      NaN\n      Oceania\n    \n  \n\n\n\n\nObserve that the gdp_missing_values_data dataset consists of some missing values shown as NaN (Not a Number).\n\n\nMissing values in a Pandas DataFrame can be identified with the isnull() method. The Pandas Series object also consists of the isnull() method. For finding the number of missing values in each column of gdp_missing_values_data, we will sum up the missing values in each column of the dataset:\n\ngdp_missing_values_data.isnull().sum()\n\neconomicActivityFemale    10\ncountry                    0\nlifeMale                  10\ninfantMortality           10\ngdpPerCapita              10\neconomicActivityMale      10\nilliteracyMale            10\nilliteracyFemale          10\nlifeFemale                10\ngeographic_location        0\ncontraception             71\ncontinent                  0\ndtype: int64\n\n\nNote that the descriptive statistics methods associated with Pandas objects ignore missing values by default. Consider the summary statistics of gdp_missing_values_data:\n\ngdp_missing_values_data.describe()\n\n\n\n\n\n  \n    \n      \n      economicActivityFemale\n      lifeMale\n      infantMortality\n      gdpPerCapita\n      economicActivityMale\n      illiteracyMale\n      illiteracyFemale\n      lifeFemale\n      contraception\n    \n  \n  \n    \n      count\n      145.000000\n      145.000000\n      145.000000\n      145.000000\n      145.000000\n      145.000000\n      145.000000\n      145.000000\n      84.000000\n    \n    \n      mean\n      45.935172\n      65.491724\n      37.158621\n      24193.482759\n      76.563448\n      13.570028\n      21.448897\n      70.615862\n      51.773810\n    \n    \n      std\n      16.875922\n      9.099256\n      34.465699\n      22748.764444\n      7.854730\n      16.497954\n      25.497045\n      9.923791\n      31.930026\n    \n    \n      min\n      1.900000\n      36.000000\n      3.000000\n      772.000000\n      51.200000\n      0.000000\n      0.000000\n      39.100000\n      0.000000\n    \n    \n      25%\n      35.500000\n      62.900000\n      10.000000\n      6837.000000\n      72.000000\n      1.000000\n      2.300000\n      67.500000\n      17.000000\n    \n    \n      50%\n      47.600000\n      67.800000\n      24.000000\n      15184.000000\n      77.300000\n      6.600000\n      9.720000\n      73.900000\n      65.000000\n    \n    \n      75%\n      55.900000\n      72.400000\n      54.000000\n      35957.000000\n      81.600000\n      19.500000\n      30.200000\n      78.100000\n      77.000000\n    \n    \n      max\n      90.600000\n      77.400000\n      169.000000\n      122740.000000\n      93.000000\n      70.500000\n      90.800000\n      82.900000\n      79.000000\n    \n  \n\n\n\n\nObserve that the count statistics report the number of non-missing values of each column in the data, as the number of rows in the data (see code below) is more than the number of non-missing values of all the variables in the above table. Similarly, for the rest of the statistics, such as mean, std, etc., the missing values are ignored.\n\n#The dataset gdp_missing_values_data has 155 rows\ngdp_missing_values_data.shape[0]\n\n155\n\n\n\n\n\nNow that we know how to identify missing values in the dataset, let us learn about the types of missing values that can be there. Rubin (1976) classified missing values in three categories.\n\n\nIf the probability of being missing is the same for all cases, then the data are said to be missing completely at random. An example of MCAR is a weighing scale that ran out of batteries. Some of the data will be missing simply because of bad luck.\n\n\n\nIf the probability of being missing is the same only within groups defined by the observed data, then the data are missing at random (MAR). MAR is a much broader class than MCAR. For example, when placed on a soft surface, a weighing scale may produce more missing values than when placed on a hard surface. Such data are thus not MCAR. If, however, we know surface type and if we can assume MCAR within the type of surface, then the data are MAR\n\n\n\nMNAR means that the probability of being missing varies for reasons that are unknown to us. For example, the weighing scale mechanism may wear out over time, producing more missing data as time progresses, but we may fail to note this. If the heavier objects are measured later in time, then we obtain a distribution of the measurements that will be distorted. MNAR includes the possibility that the scale produces more missing values for the heavier objects (as above), a situation that might be difficult to recognize and handle.\nSource: https://stefvanbuuren.name/fimd/sec-MCAR.html\n\n\n\n\n\n\nIn which of the above scenarios can we ignore the observations corresponding to missing values without the risk of skewing the analysis/trends in the data?\n\n\n\nIn which of the above scenarios will it be the more risky to impute or estimate missing values?\n\n\n\nFor the datset consisting of GDP per capita, think of hypothetical scenarios in which the missing values of GDP per capita can correspond to MCAR / MAR / MNAR.\n\n\n\n\nSometimes our analysis requires that there should be no missing values in the dataset. For example, while building statistical models, we may require the values of all the predictor variables. The quickest way is to use the dropna() method, which drops the observations that even have a single missing value, and leaves only complete observations in the data.\nLet us drop the rows containing even a single value from gdp_missing_values_data.\n\ngdp_no_missing_data = gdp_missing_values_data.dropna()\n\n\n#Shape of gdp_no_missing_data\ngdp_no_missing_data.shape\n\n(42, 12)\n\n\nDropping rows with even a single missing value has reduced the number of rows from 155 to 42! However, earlier we saw that all the columns except contraception had at most 10 missing values. Removing all rows / columns with even a single missing value results in loss of data that is non-missing in the respective rows/columns. Thus, it is typically a bad idea to drop observations with even a single missing value, except in cases where we have a very small number of missing-value observations.\nIf a few values of a column are missing, we can possibly estimate them using the rest of the data, so that we can (hopefully) maximize the information that can be extracted from the data. However, if most of the values of a column are missing, it may be harder to estimate its values.\nIn this case, we see that around 50% values of the contraception column is missing. Thus, we’ll drop the column as it may be hard to impute its values based on a relatively small number of non-missing values.\n\n#Deleting column with missing values in almost half of the observations\ngdp_missing_values_data.drop(['contraception'],axis=1,inplace=True)\ngdp_missing_values_data.shape\n\n(155, 11)\n\n\n\n\n\nThere are an unlimited number of ways to impute missing values. Some imputation methods are provided in the Pandas documentation.\nThe best way to impute them will depend on the problem, and the assumptions taken. Below are just a few examples.\n\n\nFilling the missing value of a column by copying the value of the previous non-missing observation.\n\n#Filling missing values: Method 1- Naive way\ngdp_imputed_data = gdp_missing_values_data.fillna(method = 'ffill')\n\n\n#Checking if any missing values are remaining\ngdp_imputed_data.isnull().sum()\n\neconomicActivityFemale    0\ncountry                   0\nlifeMale                  0\ninfantMortality           0\ngdpPerCapita              0\neconomicActivityMale      0\nilliteracyMale            1\nilliteracyFemale          0\nlifeFemale                0\ngeographic_location       0\ncontinent                 0\ndtype: int64\n\n\nAfter imputing missing values, note there is still one missing value for illiteracyMale. Can you guess why one missing value remained?\nLet us check how good is this method in imputing missing values. We’ll compare the imputed values of gdpPerCapita with the actual values. Recall that we had randomly put some missing values in gdp_missing_values_data, and we have the actual values in gdp_complete_data.\n\n#Index of rows with missing values for GDP per capita\nnull_ind_gdpPC = gdp_missing_values_data.index[gdp_missing_values_data.gdpPerCapita.isnull()]\n\n\n#Defining a function to plot the imputed values vs actual values \ndef plot_actual_vs_predicted():\n    fig, ax = plt.subplots(figsize=(8, 6))\n    plt.rc('xtick', labelsize=15) \n    plt.rc('ytick', labelsize=15) \n    x = gdp_complete_data.loc[null_ind_gdpPC,'gdpPerCapita']\n    y = gdp_imputed_data.loc[null_ind_gdpPC,'gdpPerCapita']\n    plt.scatter(x,y)\n    z=np.polyfit(x,y,1)\n    p=np.poly1d(z)\n    plt.plot(x,x,color='orange')\n    plt.xlabel('Actual GDP per capita',fontsize=20)\n    plt.ylabel('Imputed GDP per capita',fontsize=20)\n    ax.xaxis.set_major_formatter('${x:,.0f}')\n    ax.yaxis.set_major_formatter('${x:,.0f}')\n    rmse = np.sqrt(((x-y).pow(2)).mean())\n    print(\"RMSE=\",rmse)\n\n\n#Plot comparing imputed values with actual values, and computing the Root mean square error (RMSE) of the imputed values\nplot_actual_vs_predicted()\n\nRMSE= 34843.91091137732\n\n\n\n\n\nWe observe that the accuracy of imputation is poor as GDP per capita can vary a lot across countries, and the data is not sorted by GDP per capita. There is no reason why the GDP per capita of a country should be close to the GDP per capita of the country in the observation above it.\n\n\n\nLet us impute missing values in the column as the average of the non-missing values of the column. The sum of squared differences between actual values and the imputed values is likely to be smaller if we impute using the mean. However, this may not be true in cases other than MCAR (Missing completely at random).\n\n#Filling missing values: Method 2\ngdp_imputed_data = gdp_missing_values_data.fillna(gdp_missing_values_data.mean())\n\n\nplot_actual_vs_predicted()\n\nRMSE= 30793.549983587087\n\n\n\n\n\nAlthough this method of imputation doesn’t seem impressive, the RMSE of the estimates is lower than that of the naive method. Since we had introduced missing values randomly in gdp_missing_values_data, the mean GDP per capita will be the closest constant to the GDP per capita values, in terms of squared error.\n\n\n\nIf a variable is highly correlated with another variable in the dataset, we can approximate its missing values using the trendline with the highly correlated variable.\nLet us visualize the distribution of GDP per capita for different continents.\n\nplt.rcParams[\"figure.figsize\"] = (12,6)\nsns.boxplot(x = 'continent',y='gdpPerCapita',data = gdp_missing_values_data)\n\n<AxesSubplot:xlabel='continent', ylabel='gdpPerCapita'>\n\n\n\n\n\nWe observe that there is a distinct difference between the GDPs per capita of some of the contents. Let us impute the missing GDP per capita of a country as the mean GDP per capita of the corresponding continent. This imputation should be better than imputing the missing GDP per capita as the mean of all the non-missing values, as the GDP per capita of a country is likely to be closer to the mean GDP per capita of the continent, rather the mean GDP per capita of the whole world.\n\n#Finding the mean GDP per capita of the continent - please defer the understanding of this code to chapter 9.\navg_gdpPerCapita = gdp_missing_values_data['gdpPerCapita'].groupby(gdp_missing_values_data['continent']).mean()\navg_gdpPerCapita\n\ncontinent\nAfrica            7638.178571\nAsia             25922.750000\nEurope           45455.303030\nNorth America    19625.210526\nOceania          15385.857143\nSouth America    15360.909091\nName: gdpPerCapita, dtype: float64\n\n\n\n#Creating a copy of missing data to impute missing values\ngdp_imputed_data = gdp_missing_values_data.copy()\n\n\n#Replacing missing GDP per capita with the mean GDP per capita for the corresponding continent\nfor cont in avg_gdpPerCapita.index:\n    gdp_imputed_data.loc[(gdp_imputed_data.continent==cont) & (gdp_imputed_data.gdpPerCapita.isnull()),\n                     'gdpPerCapita']=avg_gdpPerCapita[cont]\nplot_actual_vs_predicted()\n\nRMSE= 25473.20645170116\n\n\n\n\n\nNote that the imputed values are closer to the actual values, and the RMSE has further reduced as expected.\n\n\n\nFind the numeric variable most strongly correlated with GDP per capita, and use it to impute its missing values. Find the RMSE of the imputed values.\nSolution:\n\n#Let us identify the variable highly correlated with GDP per capita.\ngdp_missing_values_data.corrwith(gdp_missing_values_data.gdpPerCapita)\n\neconomicActivityFemale    0.078332\nlifeMale                  0.579850\ninfantMortality          -0.572201\ngdpPerCapita              1.000000\neconomicActivityMale     -0.134108\nilliteracyMale           -0.479143\nilliteracyFemale         -0.448273\nlifeFemale                0.615954\ncontraception             0.057923\ndtype: float64\n\n\n\n#The variable *lifeFemale* has the strongest correlation with GDP per capita. Let us use it to impute missing values of GDP per capita.\nx = gdp_missing_values_data.lifeFemale\ny = gdp_missing_values_data.gdpPerCapita\nidx_non_missing = np.isfinite(x) & np.isfinite(y)\nslope_intercept_trendline = np.polyfit(x[idx_non_missing],y[idx_non_missing],1)   #Finding the slope and intercept for the trendline\ncompute_y_given_x = np.poly1d(slope_intercept_trendline)\n#Creating a copy of missing data to impute missing values\ngdp_imputed_data = gdp_missing_values_data.copy()\ngdp_imputed_data.loc[null_ind_gdpPC,'gdpPerCapita']=compute_y_given_x(gdp_missing_values_data.loc[null_ind_gdpPC,'lifeFemale'])\nplot_actual_vs_predicted()\n\nRMSE= 25570.361516956993\n\n\n\n\n\n\n\n\nIn this method, we’ll impute the missing value of the variable as the mean value of the \\(K\\)-nearest neighbors having non-missing values for that variable. The neighbors to a data-point are identified based on their Euclidean distance to the point in terms of the standardized values of rest of the variables in the data.\nLet’s consider a toy example to understand missing value imputation by KNN. Suppose we have to impute missing values in a toy dataset, named as toy_data having 4 observations and 3 variables.\n\n#Toy example - A 4x3 array with missing values\nnan = np.nan\ntoy_data = np.array([[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]])\ntoy_data\n\narray([[ 1.,  2., nan],\n       [ 3.,  4.,  3.],\n       [nan,  6.,  5.],\n       [ 8.,  8.,  7.]])\n\n\nWe’ll use some functions from the sklearn library to perform the KNN imputation. It is much easier to directly use the algorithm from sklearn, instead of coding it from scratch.\n\n#Library to compute pair-wise Euclidean distance between all observations in the data\nfrom sklearn import metrics\n\n#Library to impute missing values with the KNN algorithm\nfrom sklearn import impute\n\nWe’ll use the sklearn function nan_euclidean_distances() to compute the Euclidean distance between all pairs of observations in the data.\n\n#This is the distance matrix containing the distance of the ith observation from the jth observation at the (i,j) position in the matrix\nmetrics.pairwise.nan_euclidean_distances(toy_data,toy_data)\n\narray([[ 0.        ,  3.46410162,  6.92820323, 11.29158979],\n       [ 3.46410162,  0.        ,  3.46410162,  7.54983444],\n       [ 6.92820323,  3.46410162,  0.        ,  3.46410162],\n       [11.29158979,  7.54983444,  3.46410162,  0.        ]])\n\n\nNote that the size of the above matrix is 4x4. This is because the \\((i,j)^{th}\\) element of the matrix is the distance of the \\(i^{th}\\) observation from the \\(j^{th}\\) observation. The matrix is symmetric because the distance of \\(i^{th}\\) observation to the \\(j^{th}\\) observation is the same as the distance of the \\(j^{th}\\) observation to the \\(i^{th}\\) observation.\nWe’ll use the sklearn function KNNImputer() to impute the missing value of a column in toy_data as the mean of the values of the \\(K\\) nearest neighbors to the observation that have non-missing values for that column.\nLet us impute the missing values in toy_data using the values of \\(K=2\\) nearest neighbors from the corresponding observation.\n\n#imputing missing values with 2 nearest neighbors, where the neighbors have equal weights\n\n#Define an object of type KNNImputer\nimputer = impute.KNNImputer(n_neighbors=2)\n\n#Use the object method 'fit_transform' to impute missing values\nimputer.fit_transform(toy_data)\n\narray([[1. , 2. , 4. ],\n       [3. , 4. , 3. ],\n       [5.5, 6. , 5. ],\n       [8. , 8. , 7. ]])\n\n\nThe third observation was the closest to the \\(2nd\\) and \\(4th\\) observations based on the Euclidean distance matrix. Thus, the missing value in the \\(3rd\\) row of the toy_data has been imputed as the mean of the values in the \\(2nd\\) and \\(4th\\) observations for the corresponding column. Similarly, the \\(1st\\) observation is the closest to the \\(2nd\\) and \\(3rd\\) observations. Thus the missing value in the \\(1st\\) row of toy_data has been imputed as the mean of the values in the \\(1st\\) and \\(2nd\\) observations for the corresponding column.\nLet us use KNN to impute the missing values of gdpPerCapita in gdp_missing_values_data. We’ll use only the numeric columns of the data in imputing the missing values. Also, we’ll ignore contraception as it has a lot of missing values, and thus may not be useful.\n\n#Considering numeric columns in the data to use KNN\nnum_cols = list(range(0,1))+list(range(2,9))\nnum_cols\n\n[0, 2, 3, 4, 5, 6, 7, 8]\n\n\nBefore computing the pair-wise Euclidean distance of observations, we must standardize the data so that all columns are at the same scale. This will avoid columns with a higher magnitude of values having a higher weight in determining the Euclidean distance. Unless there is a reason to give a higher weight to a column, we assume all columns to have the same weight in the Euclidean distance computation.\nWe can use the code below to scale the data. However, after imputing the missing values, the data is to be scaled back to the original scale, so that each variable is in the same units as in the original dataset. However, if the code below is used, we’ll lose the orginal scale of each of the columns.\n\n#Scaling data to compute equally weighted distances from the 'k' nearest neighbors\nscaled_data = gdp_missing_values_data.iloc[:,num_cols].apply(lambda x:(x-x.min())/(x.max()-x.min()))\n\nTo alleviate the problem of losing the orignial scale of the data, we’ll use the MinMaxScaler object of the sklearn library. The object will store the original scale of the data, which will help transform the data back to the original scale once the missing values have been imputed in the standardized data.\n\n# Scaling data - using sklearn\n\n#Create an object of type MinMaxScaler\nscaler = sk.preprocessing.MinMaxScaler()\n\n#Use the object method 'fit_transform' to scale the values to a standard uniform distribution\nscaled_data = pd.DataFrame(scaler.fit_transform(gdp_missing_values_data.iloc[:,num_cols]))\n\n\n#Imputing missing values with KNNImputer\n\n#Define an object of type KNNImputer\nimputer = impute.KNNImputer(n_neighbors=3, weights=\"uniform\")\n\n#Use the object method 'fit_transform' to impute missing values\nimputed_arr = imputer.fit_transform(scaled_data)\n\n\n#Scaling back the scaled array to obtain the data at the original scale\n\n#Use the object method 'inverse_transform' to scale back the values to the original scale of the data\nunscaled_data = scaler.inverse_transform(imputed_arr)\n\n\n#Note the method imputes the missing value of all the columns\n#However, we are interested in imputing the missing values of only the 'gdpPerCapita' column\ngdp_imputed_data = gdp_missing_values_data.copy()\ngdp_imputed_data.loc[:,'gdpPerCapita'] = unscaled_data[:,3]\n\n\n#Visualizing the accuracy of missing value imputation with KNN\nplot_actual_vs_predicted()\n\nRMSE= 16804.195967740387\n\n\n\n\n\nNote that the RMSE is the lowest in this method. It is because this method imputes missing values as the average of the values of “similar” observations, which is smarter and more robust than the previous methods.\nWe chose \\(K=3\\) in the missing value imputation for GDP per capita. However, the value of \\(K\\) is typically chosen using a method known as cross validation. We’ll learn about cross-validation in the next course of the sequence."
  },
  {
    "objectID": "Data cleaning and preparation.html#data-binning",
    "href": "Data cleaning and preparation.html#data-binning",
    "title": "7  Data cleaning and preparation",
    "section": "7.2 Data binning",
    "text": "7.2 Data binning\nData binning is a method to group values of a continuous / categorical variable into bins (or categories). Binning may help with\n\nBetter intepretation of data\n\nMaking better recommendations\n\nSmooth data, reduce noise\n\nExamples:\nBinning to better interpret data\n\nThe number of flu cases everyday may be binned to seasons such as fall, spring, winter and summer, to understand the effect of season on flu.\n\nBinning to make recommendations:\n\nA doctor may like to group patient age into bins. Grouping patient ages into categories such as Age <=12, 12<Age<=18, 18<Age<=65, Age>65 may help recommend the kind/doses of covid vaccine a patient needs.\nA credit card company may want to bin customers based on their spend, as “High spenders”, “Medium spenders” and “Low spenders”. Binning will help them design customized marketing campaigns for each bin, thereby increasing customer response (or revenue). On the other hand, they use the same campaign for customers withing the same bin, thus minimizng marketing costs.\n\nBinning to smooth data, and reduce noise\n\nA sales company may want to bin their total sales to a weekly / monthly / yearly level to reduce the noise in day-to-day sales.\n\nExample: The dataset College.csv contains information about US universities. The description of variables of the dataset can be found on page 54 of this book. Let’s see if we can apply binning to better interpret the association of instructional expenditure per student (Expend) with graduation rate (Grad.Rate) for US universities, and make recommendations.\n\ncollege = pd.read_csv('./Datasets/College.csv')\ncollege.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Private\n      Apps\n      Accept\n      Enroll\n      Top10perc\n      Top25perc\n      F.Undergrad\n      P.Undergrad\n      Outstate\n      Room.Board\n      Books\n      Personal\n      PhD\n      Terminal\n      S.F.Ratio\n      perc.alumni\n      Expend\n      Grad.Rate\n    \n  \n  \n    \n      0\n      Abilene Christian University\n      Yes\n      1660\n      1232\n      721\n      23\n      52\n      2885\n      537\n      7440\n      3300\n      450\n      2200\n      70\n      78\n      18.1\n      12\n      7041\n      60\n    \n    \n      1\n      Adelphi University\n      Yes\n      2186\n      1924\n      512\n      16\n      29\n      2683\n      1227\n      12280\n      6450\n      750\n      1500\n      29\n      30\n      12.2\n      16\n      10527\n      56\n    \n    \n      2\n      Adrian College\n      Yes\n      1428\n      1097\n      336\n      22\n      50\n      1036\n      99\n      11250\n      3750\n      400\n      1165\n      53\n      66\n      12.9\n      30\n      8735\n      54\n    \n    \n      3\n      Agnes Scott College\n      Yes\n      417\n      349\n      137\n      60\n      89\n      510\n      63\n      12960\n      5450\n      450\n      875\n      92\n      97\n      7.7\n      37\n      19016\n      59\n    \n    \n      4\n      Alaska Pacific University\n      Yes\n      193\n      146\n      55\n      16\n      44\n      249\n      869\n      7560\n      4120\n      800\n      1500\n      76\n      72\n      11.9\n      2\n      10922\n      15\n    \n  \n\n\n\n\nTo visualize the association between two numeric variables, we typically make a scatterplot. Let us make a scatterplot of graduation rate with expenditure per student, with a trendline.\n\n#Let's make a scatterplot of 'Grad.Rate' vs 'Expend' with a trendline, to visualize any trend(s).\nsns.set(font_scale=1.5)\nax=sns.regplot(data = college, x = \"Expend\", y = \"Grad.Rate\",scatter_kws={\"color\": \"orange\"}, line_kws={\"color\": \"blue\"})\nax.xaxis.set_major_formatter('${x:,.0f}')\nax.set_xlabel('Expenditure per student')\nax.set_ylabel('Graduation rate')\n\nText(0, 0.5, 'Graduation rate')\n\n\n\n\n\nThe trendline indicates a positive correlation between Expend and Grad.Rate. However, there seems to be a lot of noise and presence of outliers in the data, which makes it hard to interpret the overall trend.\nWe’ll bin Expend to see if we can better analyze its association with Grad.Rate. However, let us first visualize the distribution of Expend.\n\n#Visualizing the distribution of expend\nax=sns.histplot(data = college, x= 'Expend')\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nThe distribution of Extend is right skewed with potentially some extremely high outlying values.\n\n7.2.1 Binning with equal width bins\nWe’ll use the Pandas function cut() to bin Expend. This function creates bins such that all bins have the same width.\n\n#Using the cut() function in Pandas to bin \"Expend\"\nBinned_expend = pd.cut(college['Expend'],3,retbins = True)\nBinned_expend\n\n(0      (3132.953, 20868.333]\n 1      (3132.953, 20868.333]\n 2      (3132.953, 20868.333]\n 3      (3132.953, 20868.333]\n 4      (3132.953, 20868.333]\n                ...          \n 772    (3132.953, 20868.333]\n 773    (3132.953, 20868.333]\n 774    (3132.953, 20868.333]\n 775     (38550.667, 56233.0]\n 776    (3132.953, 20868.333]\n Name: Expend, Length: 777, dtype: category\n Categories (3, interval[float64]): [(3132.953, 20868.333] < (20868.333, 38550.667] < (38550.667, 56233.0]],\n array([ 3132.953     , 20868.33333333, 38550.66666667, 56233.        ]))\n\n\nThe cut() function returns a tuple of length 2. The first element of the tuple are the bins, while the second element is an array containing the cut-off values for the bins.\n\ntype(Binned_expend)\n\ntuple\n\n\n\nlen(Binned_expend)\n\n2\n\n\nOnce the bins are obtained, we’ll add a column in the dataset that indicates the bin for Expend.\n\n#Creating a categorical variable to store the level of expenditure on a student\ncollege['Expend_bin'] = Binned_expend[0]\ncollege.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Private\n      Apps\n      Accept\n      Enroll\n      Top10perc\n      Top25perc\n      F.Undergrad\n      P.Undergrad\n      Outstate\n      Room.Board\n      Books\n      Personal\n      PhD\n      Terminal\n      S.F.Ratio\n      perc.alumni\n      Expend\n      Grad.Rate\n      Expend_bin\n    \n  \n  \n    \n      0\n      Abilene Christian University\n      Yes\n      1660\n      1232\n      721\n      23\n      52\n      2885\n      537\n      7440\n      3300\n      450\n      2200\n      70\n      78\n      18.1\n      12\n      7041\n      60\n      (3132.953, 20868.333]\n    \n    \n      1\n      Adelphi University\n      Yes\n      2186\n      1924\n      512\n      16\n      29\n      2683\n      1227\n      12280\n      6450\n      750\n      1500\n      29\n      30\n      12.2\n      16\n      10527\n      56\n      (3132.953, 20868.333]\n    \n    \n      2\n      Adrian College\n      Yes\n      1428\n      1097\n      336\n      22\n      50\n      1036\n      99\n      11250\n      3750\n      400\n      1165\n      53\n      66\n      12.9\n      30\n      8735\n      54\n      (3132.953, 20868.333]\n    \n    \n      3\n      Agnes Scott College\n      Yes\n      417\n      349\n      137\n      60\n      89\n      510\n      63\n      12960\n      5450\n      450\n      875\n      92\n      97\n      7.7\n      37\n      19016\n      59\n      (3132.953, 20868.333]\n    \n    \n      4\n      Alaska Pacific University\n      Yes\n      193\n      146\n      55\n      16\n      44\n      249\n      869\n      7560\n      4120\n      800\n      1500\n      76\n      72\n      11.9\n      2\n      10922\n      15\n      (3132.953, 20868.333]\n    \n  \n\n\n\n\nSee the variable Expend_bin in the above dataset.\nLet us visualize the Expend bins over the distribution of the Expend variable.\n\n#Visualizing the bins for instructional expediture on a student\nsns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nax=sns.histplot(data = college, x= 'Expend')\nplt.vlines(Binned_expend[1], 0,100,color='red')\nplt.xlabel('Expenditure per student');\nplt.ylabel('Graduation rate');\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nBy default, the bins created have equal width. They are created by dividing the range between the maximum and minimum value of Expend into the desired number of equal-width intervals. We can label the bins as well as follows.\n\ncollege['Expend_bin'] = pd.cut(college['Expend'],3,labels = ['Low expend','Med expend','High expend'])\ncollege['Expend_bin']\n\n0       Low expend\n1       Low expend\n2       Low expend\n3       Low expend\n4       Low expend\n          ...     \n772     Low expend\n773     Low expend\n774     Low expend\n775    High expend\n776     Low expend\nName: Expend_bin, Length: 777, dtype: category\nCategories (3, object): ['Low expend' < 'Med expend' < 'High expend']\n\n\nNow that we have binned the variable Expend, let us see if we can better visualize the association of graduation rate with expenditure per student using Expened_bin.\n\n#Visualizing average graduation rate vs categories of instructional expenditure per student\nsns.barplot(x = 'Expend_bin', y = 'Grad.Rate', data = college)\n\n<AxesSubplot:xlabel='Expend_bin', ylabel='Grad.Rate'>\n\n\n\n\n\nIt seems that the graduation rate is the highest for universities with medium level of expenditure per student. This is different from the trend we saw earlier in the scatter plot. Let us investigate.\nLet us find the number of universities in each bin.\n\npd.value_counts(college['Expend_bin'])\n\nLow expend     751\nMed expend      21\nHigh expend      5\nName: Expend_bin, dtype: int64\n\n\nThe bin High expend consists of only 5 universities, or 0.6% of all the universities in the dataset. These universities may be outliers that are skewing the trend (as also evident in the histogram above).\nIn such cases, we should bin observations such that all bins are of equal size, i.e., they have the same number of observations.\n\n\n7.2.2 Binning with equal sized bins\nLet us bin the variable Expend such that each bin consists of the same number of observations.\nWe’ll use the Pandas function qcut() to make equal-sized bins (in contrast to equal-width bins in the previous section).\n\n#Using the Pandas function qcut() to create bins with the same number of observations\nBinned_expend = pd.qcut(college['Expend'],3,retbins = True)\ncollege['Expend_bin'] = Binned_expend[0]\n\nEach bin has the same number of observations with qcut():\n\npd.value_counts(college['Expend_bin'])\n\n(3185.999, 7334.333]    259\n(7334.333, 9682.333]    259\n(9682.333, 56233.0]     259\nName: Expend_bin, dtype: int64\n\n\nLet us visualize the Expend bins over the distribution of the Expend variable.\n\n#Visualizing the bins for instructional expediture on a student\nsns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nax=sns.histplot(data = college, x= 'Expend')\nplt.vlines(Binned_expend[1], 0,100,color='red')\nplt.xlabel('Expenditure per student');\nplt.ylabel('Graduation rate');\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nNote that the bin-widths have been adjusted to have the same number of observations in each bin. The bins are narrower in domains of high density, and wider in domains of sparse density.\nLet us again make the barplot visualizing the average graduate rate with level of instructional expenditure per student.\n\ncollege['Expend_bin'] = pd.qcut(college['Expend'],3,labels = ['Low expend','Med expend','High expend'])\na=sns.barplot(x = 'Expend_bin', y = 'Grad.Rate', data = college)\n\n\n\n\nNow we see the same trend that we saw in the scatterplot, but without the noise. We have smoothed the data. Note that making equal-sized bins helps reduce the effect of outliers in the overall trend.\nSuppose this analysis was done to provide recommendations to universities for increasing their graduation rate. With binning, we can can provide one recommendation to ‘Low expend’ universities, and another one to ‘Med expend’ universities. For example, the recommendations can be:\n\n‘Low expend’ universities can expect an increase of 9 percentage points in Grad.Rate, if they migrate to the ‘Med expend’ category.\n‘Med expend’ universities can expect an increase of 7 percentage points in Grad.Rate, if they migrate to the ‘High expend’ category.\n\nThe numbers in the above recommendations are based on the table below.\n\ncollege['Grad.Rate'].groupby(college.Expend_bin).mean()\n\nExpend_bin\nLow expend     57.343629\nMed expend     66.057915\nHigh expend    72.988417\nName: Grad.Rate, dtype: float64\n\n\nWe can also make recommendations based on the confidence intervals of mean Grad.Rate. Confidence intervals are computed below. We are finding confidence intervals based on a method known as bootstrapping. Refer https://en.wikipedia.org/wiki/Bootstrapping_(statistics) for a detailed description of Bootstrapping.\n\n#Bootstrapping to find 95% confidence intervals of Graduation Rate of US universities based on average expenditure per student\nfor expend_bin in college.Expend_bin.unique():\n    data_sub = college.loc[college.Expend_bin==expend_bin,:]\n    samples = np.random.choice(data_sub['Grad.Rate'], size=(10000,data_sub.shape[0]))\n    print(\"95% Confidence interval of Grad.Rate for \"+expend_bin+\" univeristies = [\"+str(np.round(np.percentile(samples.mean(axis=1),2.5),2))+\",\"+str(np.round(np.percentile(samples.mean(axis=1),97.5),2))+\"]\")\n\n95% Confidence interval of Grad.Rate for Low expend univeristies = [55.34,59.34]\n95% Confidence interval of Grad.Rate for High expend univeristies = [71.01,74.92]\n95% Confidence interval of Grad.Rate for Med expend univeristies = [64.22,67.93]\n\n\nApart from equal-width and equal-sized bins, custom bins can be created using the bins argument. Suppose, bins are to be created for Expend with cutoffs \\(\\$10,000, \\$20,000, \\$30,000... \\$60,000\\). Then, we can use the bins argument as in the code below:\n\n\n7.2.3 Binning with custom bins\n\nBinned_expend = pd.cut(college.Expend,bins = list(range(0,70000,10000)),retbins=True)\n\n\n#Visualizing the bins for instructional expediture on a student\nax=sns.histplot(data = college, x= 'Expend')\nplt.vlines(Binned_expend[1], 0,100,color='red')\nplt.xlabel('Expenditure per student');\nplt.ylabel('Graduation rate');\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nAs custom bin-cutoffs can be specified with the cut() function, custom bin quantiles can be specified with the qcut() function."
  },
  {
    "objectID": "Data cleaning and preparation.html#dummy-indicator-variables",
    "href": "Data cleaning and preparation.html#dummy-indicator-variables",
    "title": "7  Data cleaning and preparation",
    "section": "7.3 Dummy / Indicator variables",
    "text": "7.3 Dummy / Indicator variables\nDummy variables (or indicator variables) take only the values of 0 and 1 to indicate the presence or absence of a catagorical effect. They are particularly useful in regression modeling to help explain the dependent variable.\nIf a column in a DataFrame has \\(k\\) distinct values, we will get a DataFrame with \\(k\\) columns containing 0s and 1s with the Pandas get_dummies() function.\nLet us make dummy variables with the equal-sized bins we created for the average instruction expenditure per student.\n\n#Using the Pandas function qcut() to create bins with the same number of observations\nBinned_expend = pd.qcut(college['Expend'],3,retbins = True,labels = ['Low_expend','Med_expend','High_expend'])\ncollege['Expend_bin'] = Binned_expend[0]\n\n\n#Making dummy variables based on the levels (categories) of the 'Expend_bin' variable\ndummy_Expend = pd.get_dummies(college['Expend_bin'])\n\nThe dummy data dummy_Expend has a value of \\(1\\) if the observation corresponds to the category referenced by the column name.\n\ndummy_Expend.head()\n\n\n\n\n\n  \n    \n      \n      Low_expend\n      Med_expend\n      High_expend\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n    \n    \n      1\n      0\n      0\n      1\n    \n    \n      2\n      0\n      1\n      0\n    \n    \n      3\n      0\n      0\n      1\n    \n    \n      4\n      0\n      0\n      1\n    \n  \n\n\n\n\nWe can find the correlation between the dummy variables and graduation rate to identify if any of the dummy variables will be useful to estimate graduation rate (Grad.Rate).\n\n#Finding if dummy variables will be useful to estimate 'Grad.Rate'\ndummy_Expend.corrwith(college['Grad.Rate'])\n\nLow_expend    -0.334456\nMed_expend     0.024492\nHigh_expend    0.309964\ndtype: float64\n\n\nThe dummy variables Low expend and High expend may contribute in explaining Grad.Rate in a regression model.\n\n7.3.1 Practice exercise 3\nRead survey_data_clean.csv. Split the columns of the dataset, such that all columns with categorical values transform into dummy variables with each category corresponding to a column of 0s and 1s. Leave the Timestamp column.\nAs all categorical columns are transformed to dummy variables, all columns have numeric values.\nWhat is the total number of columns in the transformed data? What is the total number of columns of the original data?\nFind the:\n\nTop 5 variables having the highest positive correlation with NU_GPA.\nTop 5 variables having the highest negative correlation with NU_GPA.\n\nSolution:\n\nsurvey_data = pd.read_csv('./Datasets/survey_data_clean.csv')\nsurvey_dummies=pd.get_dummies(survey_data.iloc[:,1:])\nprint(\"The total number of columns in the transformed data are\", survey_dummies.shape[1])\nprint(\"The total number of columns in the original data are\", survey_dummies.shape[0])\n\nThe total number of columns in the transformed data are 308\nThe total number of columns in the original data are 192\n\n\nBelow are the top 5 variables having the highest positive correlation with NU_GPA:\n\nsurvey_dummies.corrwith(survey_dummies.NU_GPA).drop(index = 'NU_GPA').sort_values(ascending=False)[0:5]\n\nfav_letter_o                                                                              0.367140\nfav_sport_Dance!                                                                          0.367140\nmajor_Humanities / Communications;Physical Sciences / Natural Sciences / Engineering      0.271019\nfav_alcohol_I don't drink                                                                 0.213118\nlearning_style_Reading/Writing (learn best through words often note-taking or reading)    0.207451\ndtype: float64\n\n\nBelow are the top 5 variables having the highest negative correlation with NU_GPA:\n\nsurvey_dummies.corrwith(survey_dummies.NU_GPA).drop(index = 'NU_GPA').sort_values(ascending=True)[0:5]\n\nfav_number                                         -0.307656\nprocrastinator                                     -0.269552\nfav_sport_Underwater Basketweaving                 -0.224237\nbirth_month_February                               -0.222141\nstreaming_platforms_Netflix;Amazon Prime;HBO Max   -0.221099\ndtype: float64"
  },
  {
    "objectID": "Data cleaning and preparation.html#outlier-detection",
    "href": "Data cleaning and preparation.html#outlier-detection",
    "title": "7  Data cleaning and preparation",
    "section": "7.4 Outlier detection",
    "text": "7.4 Outlier detection\nAn outlier is an observation that is significantly different from the rest of the data. Detection of outliers is important as they may distort the general trends in data.\nLet us visualize outliers in average instructional expenditure per student given by the variable Expend.\n\nax=college.boxplot(column = 'Expend')\nax.yaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nThere are several outliers (shown as circles in the above boxplot), which correspond to high values of average instructional expenditure per student. Boxplot identifies outliers based on the Tukey’s fences criterion:\nTukey’s fences: John Tukey proposed that observations outside the range \\([Q1 - 1.5(Q3-Q1), Q3+1.5(Q3-Q1)]\\) are outliers, where \\(Q1\\) and \\(Q3\\) are the lower \\((25\\%)\\) and upper \\((75\\%)\\) quartiles respectively. Let us detect outliers based on Tukey’s fences.\n\n#Finding upper and lower quartiles and interquartile range\nq1 = np.percentile(college['Expend'],25)\nq3 = np.percentile(college['Expend'],75)\nintQ_range = q3-q1\n\n\n#Tukey's fences\nLower_fence = q1 - 1.5*intQ_range\nUpper_fence = q3 + 1.5*intQ_range\n\n\n#These are the outlying observations - those outside of Tukey's fences\nOutlying_obs = college[(college.Expend<Lower_fence) | (college.Expend>Upper_fence)]\n\n\n#Data without outliers\ncollege_data_without_outliers = college[((college.Expend>=Lower_fence) & (college.Expend<=Upper_fence))]\n\nEarlier, the trend was distorted by outliers when we created bins of equal width. Let us see if we get the correct trend with the outliers removed from the data.\n\nBinned_data = pd.cut(college_data_without_outliers['Expend'],3,labels = ['Low expend','Med expend','High expend'],retbins = True)\ncollege_data_without_outliers.loc[:,'Expend_bin'] = Binned_data[0]\n\n\nsns.barplot(x = 'Expend_bin', y = 'Grad.Rate', data = college_data_without_outliers)\n\n<AxesSubplot:xlabel='Expend_bin', ylabel='Grad.Rate'>\n\n\n\n\n\nWith the outliers removed, we obtain the correct overall trend, even in the case of equal-width bins. Note that these bins have unequal number of observations as shown below.\n\nsns.set(font_scale=1.35)\nax=sns.histplot(data = college_data_without_outliers, x= 'Expend')\nfor i in range(4):\n    plt.axvline(Binned_data[1][i], 0,100,color='red')\nplt.xlabel('Expenditure per student');\nplt.ylabel('Graduation rate');\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nNote that the right tail of the histogram has disappered since we removed outliers.\n\ncollege_data_without_outliers['Expend_bin'].value_counts()\n\nMed expend     327\nLow expend     314\nHigh expend     88\nName: Expend_bin, dtype: int64\n\n\n\n7.4.1 Practice exercise 4\nConsider the dataset created for survey_data_clean.csv in Practice exercise 3, which includes dummy variables for all the categorical variables. Find the number of outliers in each column of the dataset based on the Tukey’s fences criterion. Do not use a for loop.\nWhich column(s) have the maximum number of outliers?\nDo you think the outlying observations identified with the Tukey’s fences criterion for those columns(s) should be considered as outliers? If not, then which type of columns should be considered when finding outliers?\nSolution:\n\n#Function to identify outliers based on Tukey's fences\ndef rem_outliers(x):\n    q1 =x.quantile(0.25)\n    q3 = x.quantile(0.75)\n    intQ_range = q3-q1\n\n    #Tukey's fences\n    Lower_fence = q1 - 1.5*intQ_range\n    Upper_fence = q3 + 1.5*intQ_range\n    \n    #The object returned will be a data frame with bool values - True or False. 'True' will indicate that the value is an outlier\n    return ((x<Lower_fence) | (x>Upper_fence))\n\nsurvey_dummies.apply(rem_outliers).sum().sort_values(ascending=False)\n\nlearning_style_Reading/Writing (learn best through words often note-taking or reading)    48\nmuch_effort_is_lack_of_talent                                                             48\nleft_right_brained_Left-brained (logic, science, critical thinking, numbers)              47\nleft_right_brained_Right-brained (creative, art, imaginative, intuitive)                  47\nfav_season_Spring                                                                         47\n                                                                                          ..\nlove_first_sight                                                                           0\nbirthdate_odd_even_Odd                                                                     0\nprocrastinator                                                                             0\nbirthdate_odd_even_Even                                                                    0\nhow_happy_Pretty happy                                                                     0\nLength: 308, dtype: int64\n\n\nUsing Tukey’s criterion, the variables learning_style_Reading/Writing (learn best through words often note-taking or reading) and much_effort_is_lack_of_talent have the most number of outliers.\nHowever, these two variables only have 0s and 1s. For instance, let us consider learning_style_Reading/Writing (learn best through words often note-taking or reading).\n\nsurvey_dummies['learning_style_Reading/Writing (learn best through words often note-taking or reading)'].value_counts()\n\n0    144\n1     48\nName: learning_style_Reading/Writing (learn best through words often note-taking or reading), dtype: int64\n\n\nAs the percentage of 1s are \\(\\frac{48}{48+144}=25\\%\\), the \\(75^{th}\\) percentile value is 0.25, and the upper Tukey’s fence is \\(0.25+0.25*1.25 = 0.625\\), which makes the value \\(1\\) an outlier. However, we should not consider this value as an outlier, as a considerable fraction of the data (25%) has the value \\(1\\) for this variable.\nFurthermore, Tukey’s fences are developed for continuous variables. However, the variable learning_style_Reading/Writing (learn best through words often note-taking or reading) is discrete with only two levels. Thus, while finding outliers we must consider only continuous variables.\n\n#Finding continuous variables: Assuming numeric variables that have more than 2 distinct values are continuous\ncontinuous_variables = [x for x in list(survey_data.apply(lambda x: x.name if ((len(x.value_counts())>2) & (x.dtype!='O')) else '')) if x!='']\n\n\n#Finding number of outliers for only continuous variables\nsurvey_data.loc[:,continuous_variables].apply(rem_outliers).sum().sort_values(ascending=False)\n\nnum_clubs                      22\nfav_number                     19\nparties_per_month              12\ninternet_hours_per_day         11\nsleep_hours_per_day            10\nexpected_marriage_age          10\nexpected_starting_salary        8\nhigh_school_GPA                 8\nheight_mother                   7\nnum_insta_followers             6\nheight_father                   6\nNU_GPA                          5\nminutes_ex_per_week             4\nheight                          3\nfarthest_distance_travelled     2\nage                             1\nnum_majors_minors               0\ndtype: int64\n\n\nThe variable num_clubs has the maximum number of outliers.\n\n#Finding how many clubs makes a person an outlier\nq3_num_clubs = survey_data.num_clubs.quantile(0.75)\nq1_num_clubs = survey_data.num_clubs.quantile(0.25)\nprint(\"Tukeys fences = [\",q1_num_clubs-1.5*(q3_num_clubs-q1_num_clubs),q3_num_clubs+1.5*(q3_num_clubs-q1_num_clubs),\"]\")\n\nTukeys fences = [ 0.5 4.5 ]\n\n\nPeople joining no club, or more than 4 clubs are outliers.\n\n\n\n\nRubin, Donald B. 1976. “Inference and Missing Data.” Biometrika 63 (3): 581–92."
  },
  {
    "objectID": "Data wrangling.html",
    "href": "Data wrangling.html",
    "title": "8  Data wrangling",
    "section": "",
    "text": "Data wrangling refers to combining, transforming, and re-arranging data to make it suitable for further analysis. We’ll use Pandas for all data wrangling operations."
  },
  {
    "objectID": "Data wrangling.html#hierarchical-indexing",
    "href": "Data wrangling.html#hierarchical-indexing",
    "title": "8  Data wrangling",
    "section": "8.1 Hierarchical indexing",
    "text": "8.1 Hierarchical indexing\nUntil now we have seen only a single level of indexing in the rows and columns of a Pandas DataFrame. Hierarchical indexing refers to having multiple index levels on an axis (row / column) of a Pandas DataFrame. It helps us to work with a higher dimensional data in a lower dimensional form.\n\n8.1.1 Hierarchical indexing in Pandas Series\nLet us define Pandas Series as we defined in Chapter 5:\n\n#Defining a Pandas Series\nseries_example = pd.Series(['these','are','english','words','estas','son','palabras','en','español',\n                            'ce','sont','des','françai','mots'])\nseries_example\n\n0        these\n1          are\n2      english\n3        words\n4        estas\n5          son\n6     palabras\n7           en\n8      español\n9           ce\n10        sont\n11         des\n12     françai\n13        mots\ndtype: object\n\n\nLet us use the attribute nlevels to find the number of levels of the row indices of this Series:\n\nseries_example.index.nlevels\n\n1\n\n\nThe Series series_example has only one level of row indices.\nLet us introduce another level of row indices while defining the Series:\n\n#Defining a Pandas Series with multiple levels of row indices\nseries_example = pd.Series(['these','are','english','words','estas','son','palabras','en','español',\n                           'ce','sont','des','françai','mots'], \n                          index=[['English']*4+['Spanish']*5+['French']*5,list(range(1,5))+list(range(1,6))*2])\nseries_example\n\nEnglish  1       these\n         2         are\n         3     english\n         4       words\nSpanish  1       estas\n         2         son\n         3    palabras\n         4          en\n         5     español\nFrench   1          ce\n         2        sont\n         3         des\n         4     françai\n         5        mots\ndtype: object\n\n\nIn the above Series, there are two levels of row indices:\n\nseries_example.index.nlevels\n\n2\n\n\n\n\n8.1.2 Hierarchical indexing in Pandas DataFrame\nIn a Pandas DataFrame, both the rows and the columns can have hierarchical indexing. For example, consider the DataFrame below:\n\ndata=np.array([[771517,2697000,815201,3849000],[4.2,5.6,2.8,4.6],\n             [7.8,234.5,46.9,502],[6749, 597, 52, 305]])\ndf_example = pd.DataFrame(data,index = [['Demographics']*2+['Geography']*2,\n                                      ['Population','Unemployment (%)','Area (mile-sq)','Elevation (feet)']],\n                    columns = [['Illinois']*2+['California']*2,['Evanston','Chicago','San Francisco','Los Angeles']])\ndf_example\n\n\n\n\n\n  \n    \n      \n      \n      Illinois\n      California\n    \n    \n      \n      \n      Evanston\n      Chicago\n      San Francisco\n      Los Angeles\n    \n  \n  \n    \n      Demographics\n      Population\n      771517.0\n      2697000.0\n      815201.0\n      3849000.0\n    \n    \n      Unemployment (%)\n      4.2\n      5.6\n      2.8\n      4.6\n    \n    \n      Geography\n      Area (mile-sq)\n      7.8\n      234.5\n      46.9\n      502.0\n    \n    \n      Elevation (feet)\n      6749.0\n      597.0\n      52.0\n      305.0\n    \n  \n\n\n\n\nIn the above DataFrame, both the rows and columns have 2 levels of indexing. The number of levels of column indices can be found using the attribute nlevels:\n\ndf_example.columns.nlevels\n\n2\n\n\nThe columns attribute will now have a MultiIndex datatype in contrast to the Index datatype with single level of indexing. The same holds for row indices.\n\ntype(df_example.columns)\n\npandas.core.indexes.multi.MultiIndex\n\n\n\ndf_example.columns\n\nMultiIndex([(  'Illinois',      'Evanston'),\n            (  'Illinois',       'Chicago'),\n            ('California', 'San Francisco'),\n            ('California',   'Los Angeles')],\n           )\n\n\nThe hierarchical levels can have names. Let us assign names to the each level of the row and column labels:\n\n#Naming the row indices levels\ndf_example.index.names=['Information type', 'Statistic']\n\n#Naming the column indices levels\ndf_example.columns.names=['State', 'City']\n\n#Viewing the DataFrame\ndf_example\n\n\n\n\n\n  \n    \n      \n      State\n      Illinois\n      California\n    \n    \n      \n      City\n      Evanston\n      Chicago\n      San Francisco\n      Los Angeles\n    \n    \n      Information type\n      Statistic\n      \n      \n      \n      \n    \n  \n  \n    \n      Demographics\n      Population\n      771517.0\n      2697000.0\n      815201.0\n      3849000.0\n    \n    \n      Unemployment (%)\n      4.2\n      5.6\n      2.8\n      4.6\n    \n    \n      Geography\n      Area (mile-sq)\n      7.8\n      234.5\n      46.9\n      502.0\n    \n    \n      Elevation (feet)\n      6749.0\n      597.0\n      52.0\n      305.0\n    \n  \n\n\n\n\nObserve that the names of the row and column labels appear when we view the DataFrame.\n\n8.1.2.1 get_level_values()\nThe names of the column levels can be obtained using the function get_level_values(). The outer-most level corresponds to the level = 0, and it increases as we go to the inner levels.\n\n#Column levels at level 0 (the outer level)\ndf_example.columns.get_level_values(0)\n\nIndex(['Illinois', 'Illinois', 'California', 'California'], dtype='object', name='State')\n\n\n\n#Column levels at level 1 (the inner level)\ndf_example.columns.get_level_values(1)\n\nIndex(['Evanston', 'Chicago', 'San Francisco', 'Los Angeles'], dtype='object', name='City')\n\n\n\n\n\n8.1.3 Subsetting data\nWe can use the indices at the outer levels to concisely subset a Series / DataFrame.\nThe first four observations of the Series series_example correspond to the outer row index English, while the last 5 rows correspond to the outer row index Spanish. Let us subset all the observations corresponding to the outer row index English:\n\n#Subsetting data by row-index\nseries_example['English']\n\n1      these\n2        are\n3    english\n4      words\ndtype: object\n\n\nJust like in the case of single level indices, if we wish to subset corresponding to multiple outer-level indices, we put the indices within an additional box bracket []. For example, let us subset all the observations corresponding to the row-indices English and French:\n\n#Subsetting data by multiple row-indices\nseries_example[['English','French']]\n\nEnglish  1      these\n         2        are\n         3    english\n         4      words\nFrench   1         ce\n         2       sont\n         3        des\n         4    françai\n         5       mots\ndtype: object\n\n\nWe can also subset data using the inner row index. However, we will need to put a : sign to indicate that the row label at the inner level is being used.\n\n#Subsetting data by row-index\nseries_example[:,2]\n\nEnglish     are\nSpanish     son\nFrench     sont\ndtype: object\n\n\n\n#Subsetting data by multiple row-indices\nseries_example.loc[:,[1,2]]\n\nEnglish  1    these\nSpanish  1    estas\nFrench   1       ce\nEnglish  2      are\nSpanish  2      son\nFrench   2     sont\ndtype: object\n\n\nAs in Series, we can concisely subset rows / columns in a DataFrame based on the index at the outer levels.\n\ndf_example['Illinois']\n\n\n\n\n\n  \n    \n      \n      City\n      Evanston\n      Chicago\n    \n    \n      Information type\n      Statistic\n      \n      \n    \n  \n  \n    \n      Demographics\n      Population\n      771517.0\n      2697000.0\n    \n    \n      Unemployment (%)\n      4.2\n      5.6\n    \n    \n      Geography\n      Area (mile-sq)\n      7.8\n      234.5\n    \n    \n      Elevation (feet)\n      6749.0\n      597.0\n    \n  \n\n\n\n\nNote that the dataype of each column name is a tuple. For example, let us find the datatype of the \\(1^{st}\\) column name:\n\n#First column name\ndf_example.columns[0]\n\n('Illinois', 'Evanston')\n\n\n\n#Datatype of first column name\ntype(df_example.columns[0])\n\ntuple\n\n\nThus columns at the inner levels can be accessed by specifying the name as a tuple. For example, let us subset the column Evanston:\n\n#Subsetting the column 'Evanston'\ndf_example[('Illinois','Evanston')]\n\nInformation type  Statistic       \nDemographics      Population          771517.0\n                  Unemployment (%)         4.2\nGeography         Area (mile-sq)           7.8\n                  Elevation (feet)      6749.0\nName: (Illinois, Evanston), dtype: float64\n\n\n\n#Subsetting the columns 'Evanston' and 'Chicago' of the outer column level 'Illinois'\ndf_example.loc[:,('Illinois',['Evanston','Chicago'])]\n\n\n\n\n\n  \n    \n      \n      State\n      Illinois\n    \n    \n      \n      City\n      Evanston\n      Chicago\n    \n    \n      Information type\n      Statistic\n      \n      \n    \n  \n  \n    \n      Demographics\n      Population\n      771517.0\n      2697000.0\n    \n    \n      Unemployment (%)\n      4.2\n      5.6\n    \n    \n      Geography\n      Area (mile-sq)\n      7.8\n      234.5\n    \n    \n      Elevation (feet)\n      6749.0\n      597.0\n    \n  \n\n\n\n\n\n\n8.1.4 Practice exercise 1\nRead the table consisting of GDP per capita of countries from the webpage: https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita .\nTo only read the relevant table, read the tables that contain the word ‘Country’.\n\n8.1.4.1 \nHow many levels of indexing are there in the rows and columns?\n\ndfs = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita', match = 'Country')\ngdp_per_capita = dfs[0]\ngdp_per_capita.head()\n\n\n\n\n\n  \n    \n      \n      Country/Territory\n      UN Region\n      IMF[4]\n      World Bank[5]\n      United Nations[6]\n    \n    \n      \n      Country/Territory\n      UN Region\n      Estimate\n      Year\n      Estimate\n      Year\n      Estimate\n      Year\n    \n  \n  \n    \n      0\n      Liechtenstein *\n      Europe\n      —\n      —\n      169049\n      2019\n      180227\n      2020\n    \n    \n      1\n      Monaco *\n      Europe\n      —\n      —\n      173688\n      2020\n      173696\n      2020\n    \n    \n      2\n      Luxembourg *\n      Europe\n      127673\n      2022\n      135683\n      2021\n      117182\n      2020\n    \n    \n      3\n      Bermuda *\n      Americas\n      —\n      —\n      110870\n      2021\n      123945\n      2020\n    \n    \n      4\n      Ireland *\n      Europe\n      102217\n      2022\n      99152\n      2021\n      86251\n      2020\n    \n  \n\n\n\n\nJust by looking at the DataFrame, it seems as if there are two levels of indexing for columns and one level of indexing for rows. However, let us confirm it with the nlevels attribute.\n\ngdp_per_capita.columns.nlevels\n\n2\n\n\nYes, there are 2 levels of indexing for columns.\n\ngdp_per_capita.index.nlevels\n\n1\n\n\nThere is one level of indexing for rows.\n\n\n8.1.4.2 \nSubset a DataFrame that selects the country, and the United Nations’ estimates of GDP per capita with the corresponding year.\n\ngdp_per_capita.loc[:,['Country/Territory','United Nations[6]']]\n\n\n\n\n\n  \n    \n      \n      Country/Territory\n      United Nations[6]\n    \n    \n      \n      Country/Territory\n      Estimate\n      Year\n    \n  \n  \n    \n      0\n      Liechtenstein *\n      180227\n      2020\n    \n    \n      1\n      Monaco *\n      173696\n      2020\n    \n    \n      2\n      Luxembourg *\n      117182\n      2020\n    \n    \n      3\n      Bermuda *\n      123945\n      2020\n    \n    \n      4\n      Ireland *\n      86251\n      2020\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      217\n      Madagascar *\n      470\n      2020\n    \n    \n      218\n      Central African Republic *\n      481\n      2020\n    \n    \n      219\n      Sierra Leone *\n      475\n      2020\n    \n    \n      220\n      South Sudan *\n      1421\n      2020\n    \n    \n      221\n      Burundi *\n      286\n      2020\n    \n  \n\n222 rows × 3 columns\n\n\n\n\n\n8.1.4.3 \nSubset a DataFrame that selects only the World Bank and United Nations’ estimates of GDP per capita without the corresponding year or country.\n\ngdp_per_capita.loc[:,(['World Bank[5]','United Nations[6]'],'Estimate')]\n\n\n\n\n\n  \n    \n      \n      World Bank[5]\n      United Nations[6]\n    \n    \n      \n      Estimate\n      Estimate\n    \n  \n  \n    \n      0\n      169049\n      180227\n    \n    \n      1\n      173688\n      173696\n    \n    \n      2\n      135683\n      117182\n    \n    \n      3\n      110870\n      123945\n    \n    \n      4\n      99152\n      86251\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      217\n      515\n      470\n    \n    \n      218\n      512\n      481\n    \n    \n      219\n      516\n      475\n    \n    \n      220\n      1120\n      1421\n    \n    \n      221\n      237\n      286\n    \n  \n\n222 rows × 2 columns\n\n\n\n\n\n8.1.4.4 \nSubset a DataFrame that selects the country and only the World Bank and United Nations’ estimates of GDP per capita without the corresponding year or country.\n\ngdp_per_capita.loc[:,[('Country/Territory','Country/Territory'),('United Nations[6]','Estimate'),('World Bank[5]','Estimate')]]\n\n\n\n\n\n  \n    \n      \n      Country/Territory\n      United Nations[6]\n      World Bank[5]\n    \n    \n      \n      Country/Territory\n      Estimate\n      Estimate\n    \n  \n  \n    \n      0\n      Liechtenstein *\n      180227\n      169049\n    \n    \n      1\n      Monaco *\n      173696\n      173688\n    \n    \n      2\n      Luxembourg *\n      117182\n      135683\n    \n    \n      3\n      Bermuda *\n      123945\n      110870\n    \n    \n      4\n      Ireland *\n      86251\n      99152\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      217\n      Madagascar *\n      470\n      515\n    \n    \n      218\n      Central African Republic *\n      481\n      512\n    \n    \n      219\n      Sierra Leone *\n      475\n      516\n    \n    \n      220\n      South Sudan *\n      1421\n      1120\n    \n    \n      221\n      Burundi *\n      286\n      237\n    \n  \n\n222 rows × 3 columns\n\n\n\n\n\n8.1.4.5 \nDrop all columns consisting of years. Use the level argument of the drop() method.\n\ngdp_per_capita = gdp_per_capita.drop(columns='Year',level=1)\ngdp_per_capita\n\n\n\n\n\n  \n    \n      \n      Country/Territory\n      UN Region\n      IMF[4]\n      World Bank[5]\n      United Nations[6]\n    \n    \n      \n      Country/Territory\n      UN Region\n      Estimate\n      Estimate\n      Estimate\n    \n  \n  \n    \n      0\n      Liechtenstein *\n      Europe\n      —\n      169049\n      180227\n    \n    \n      1\n      Monaco *\n      Europe\n      —\n      173688\n      173696\n    \n    \n      2\n      Luxembourg *\n      Europe\n      127673\n      135683\n      117182\n    \n    \n      3\n      Bermuda *\n      Americas\n      —\n      110870\n      123945\n    \n    \n      4\n      Ireland *\n      Europe\n      102217\n      99152\n      86251\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      217\n      Madagascar *\n      Africa\n      522\n      515\n      470\n    \n    \n      218\n      Central African Republic *\n      Africa\n      496\n      512\n      481\n    \n    \n      219\n      Sierra Leone *\n      Africa\n      494\n      516\n      475\n    \n    \n      220\n      South Sudan *\n      Africa\n      328\n      1120\n      1421\n    \n    \n      221\n      Burundi *\n      Africa\n      293\n      237\n      286\n    \n  \n\n222 rows × 5 columns\n\n\n\n\n\n8.1.4.6 \nIn the dataset obtained above, drop the outer level of the column labels. Use the droplevel() method.\n\ngdp_per_capita = gdp_per_capita.droplevel(1,axis=1)\ngdp_per_capita\n\n\n\n\n\n  \n    \n      \n      Country/Territory\n      UN Region\n      IMF[4]\n      World Bank[5]\n      United Nations[6]\n    \n  \n  \n    \n      0\n      Liechtenstein *\n      Europe\n      —\n      169049\n      180227\n    \n    \n      1\n      Monaco *\n      Europe\n      —\n      173688\n      173696\n    \n    \n      2\n      Luxembourg *\n      Europe\n      127673\n      135683\n      117182\n    \n    \n      3\n      Bermuda *\n      Americas\n      —\n      110870\n      123945\n    \n    \n      4\n      Ireland *\n      Europe\n      102217\n      99152\n      86251\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      217\n      Madagascar *\n      Africa\n      522\n      515\n      470\n    \n    \n      218\n      Central African Republic *\n      Africa\n      496\n      512\n      481\n    \n    \n      219\n      Sierra Leone *\n      Africa\n      494\n      516\n      475\n    \n    \n      220\n      South Sudan *\n      Africa\n      328\n      1120\n      1421\n    \n    \n      221\n      Burundi *\n      Africa\n      293\n      237\n      286\n    \n  \n\n222 rows × 5 columns\n\n\n\n\n\n\n8.1.5 Practice exercise 2\nRecall problem 2(e) from assignment 3 on Pandas, where we needed to find the African country that is the closest to country \\(G\\) (Luxembourg) with regard to social indicators.\nWe will solve the question with the regular way in which we use single level of indexing (as you probably did during this assignment), and see if it is easier to do with hierarchical indexing.\nExecute the code below that we used to pre-process data to make it suitable for answering this question.\n\n#Pre-processing data - execute this code\nsocial_indicator = pd.read_csv(\"./Datasets/social_indicator.txt\",sep=\"\\t\",index_col = 0)\nsocial_indicator.geographic_location = social_indicator.geographic_location.apply(lambda x: 'Asia' if 'Asia' in x else 'Europe' if 'Europe' in x else 'Africa' if 'Africa' in x else x)\nsocial_indicator.rename(columns={'geographic_location':'continent'},inplace=True)\nsocial_indicator = social_indicator.sort_index(axis=1)\nsocial_indicator.drop(columns=['region','contraception'],inplace=True)\n\nBelow is the code to find the African country that is the closest to country \\(G\\) (Luxembourg) using single level of indexing. Your code in the assignment is probably similar to the one below:\n\n#Finding the index of the country G (Luxembourg) that has the maximum GDP per capita\ncountry_max_gdp_position = social_indicator.gdpPerCapita.argmax()\n\n#Scaling the social indicator dataset\nsocial_indicator_scaled = social_indicator.iloc[:,2:].apply(lambda x: (x-x.mean())/(x.std()))\n\n#Computing the Manhattan distances of all countries from country G (Luxembourg)\nmanhattan_distances = (social_indicator_scaled-social_indicator_scaled.iloc[country_max_gdp_position,:]).abs().sum(axis=1)\n\n#Finding the indices of African countries\nafrican_countries_indices = social_indicator.loc[social_indicator.continent=='Africa',:].index\n\n#Filtering the Manhattan distances of African countries from country G (Luxembourg)\nmanhattan_distances_African = manhattan_distances[african_countries_indices]\n\n#Finding the country among African countries that has the least Manhattan distance to country G (Luxembourg)\nsocial_indicator.loc[manhattan_distances_African.idxmin(),'country']\n\n'Reunion'\n\n\n\n8.1.5.1 \nUse the method set_index() to set continent and country as hierarchical indices of rows. Find the African country that is the closest to country \\(G\\) (Luxembourg) using this hierarchically indexed data. How many lines will be eliminated from the code above? Which lines will be eliminated?\nHint: Since continent and country are row indices, you don’t need to explicitly find:\n\nThe row index of country \\(G\\) (Luxembourg),\nThe row indices of African countries.\nThe Manhattan distances for African countries.\n\n\nsocial_indicator.set_index(['continent','country'],inplace = True)\nsocial_indicator_scaled = social_indicator.apply(lambda x: (x-x.mean())/(x.std()))\nmanhattan_distances = (social_indicator_scaled-social_indicator_scaled.loc[('Europe','Luxembourg'),:]).abs().sum(axis=1)\nmanhattan_distances['Africa'].idxmin()\n\n'Reunion'\n\n\nAs we have converted the columns continent and country to row indices, all the lines of code where we were keeping track of the index of country \\(G\\), African countries, and Manhattan distances of African countries are eliminated. Three lines of code are eliminated.\nHierarchical indexing relieves us from keeping track of indices, if we set indices that are relatable to our analysis.\n\n\n8.1.5.2 \nUse the Pandas DataFrame method mean() with the level argument to find the mean value of all social indicators for each continent.\n\nsocial_indicator.mean(level=0)\n\n\n\n\n\n  \n    \n      \n      economicActivityFemale\n      economicActivityMale\n      gdpPerCapita\n      illiteracyFemale\n      illiteracyMale\n      infantMortality\n      lifeFemale\n      lifeMale\n      totalfertilityrate\n    \n    \n      continent\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Asia\n      41.592683\n      79.282927\n      27796.390244\n      23.635951\n      13.780878\n      39.853659\n      70.724390\n      66.575610\n      3.482927\n    \n    \n      Africa\n      46.732258\n      79.445161\n      7127.483871\n      52.907226\n      33.673548\n      77.967742\n      56.841935\n      53.367742\n      4.889677\n    \n    \n      Oceania\n      51.280000\n      77.953333\n      14525.666667\n      9.666667\n      6.585200\n      23.666667\n      72.406667\n      67.813333\n      3.509333\n    \n    \n      North America\n      45.238095\n      77.166667\n      18609.047619\n      17.390286\n      14.609905\n      22.904762\n      75.457143\n      70.161905\n      2.804286\n    \n    \n      South America\n      42.008333\n      75.575000\n      15925.916667\n      9.991667\n      6.750000\n      34.750000\n      72.691667\n      66.975000\n      2.872500\n    \n    \n      Europe\n      52.060000\n      70.291429\n      45438.200000\n      2.308343\n      1.413543\n      10.571429\n      77.757143\n      70.374286\n      1.581714\n    \n  \n\n\n\n\n\n\n\n8.1.6 Practice exercise 3\nLet us try to find the areas where NU students lack in diversity. Read survey_data_clean.csv. Use hierarchical indexing to classify the columns as follows:\nClassify the following variables as lifestyle:\n\nlifestyle = ['fav_alcohol', 'parties_per_month', 'smoke', 'weed','streaming_platforms', 'minutes_ex_per_week',\n       'sleep_hours_per_day', 'internet_hours_per_day', 'procrastinator', 'num_clubs','student_athlete','social_media']\n\nClassify the following variables as personality:\n\npersonality = ['introvert_extrovert', 'left_right_brained', 'personality_type', \n       'num_insta_followers', 'fav_sport','learning_style','dominant_hand']\n\nClassify the following variables as opinion:\n\nopinion = ['love_first_sight', 'expected_marriage_age',  'expected_starting_salary', 'how_happy', \n       'fav_number', 'fav_letter', 'fav_season',   'political_affliation', 'cant_change_math_ability',\n       'can_change_math_ability', 'math_is_genetic', 'much_effort_is_lack_of_talent']\n\nClassify the following variables as academic information:\n\nacademic_info = ['major', 'num_majors_minors',\n       'high_school_GPA', 'NU_GPA', 'school_year','AP_stats', 'used_python_before']\n\nClassify the following variables as demographics:\n\ndemographics = [ 'only_child','birth_month', \n       'living_location_on_campus', 'age', 'height', 'height_father',\n       'height_mother',  'childhood_in_US', 'gender', 'region_of_residence']\n\nWrite a function that finds the number of variables having outliers in a dataset. Apply the function to each of the 5 categories of variables in the dataset. Our hypothesis is that the category that has the maximum number of variables with outliers has the least amount of diversity.\n\n\n8.1.7 Reshaping data\nApart from ease in subsetting data, hierarchical indexing also plays a role in reshaping data.\n\n8.1.7.1 unstack() (Pandas Series method)\nThe Pandas Series method unstack() pivots the desired level of row indices to columns, thereby creating a DataFrame. By default, the inner-most level of the row labels is pivoted.\n\n#Pivoting the inner-most Series row index to column labels\nseries_example_unstack = series_example.unstack()\nseries_example_unstack\n\n\n\n\n\n  \n    \n      \n      1\n      2\n      3\n      4\n      5\n    \n  \n  \n    \n      English\n      these\n      are\n      english\n      words\n      NaN\n    \n    \n      French\n      ce\n      sont\n      des\n      françai\n      mots\n    \n    \n      Spanish\n      estas\n      son\n      palabras\n      en\n      español\n    \n  \n\n\n\n\nWe can pivot the outer level of the row labels by specifying it in the level argument:\n\n#Pivoting the outer row indices to column labels\nseries_example_unstack = series_example.unstack(level=0)\nseries_example_unstack\n\n\n\n\n\n  \n    \n      \n      English\n      French\n      Spanish\n    \n  \n  \n    \n      1\n      these\n      ce\n      estas\n    \n    \n      2\n      are\n      sont\n      son\n    \n    \n      3\n      english\n      des\n      palabras\n    \n    \n      4\n      words\n      françai\n      en\n    \n    \n      5\n      NaN\n      mots\n      español\n    \n  \n\n\n\n\n\n\n8.1.7.2 unstack() (Pandas DataFrame method)\nThe Pandas DataFrame method unstack() pivots the specified level of row indices to the new inner-most level of column labels. By default, the inner-most level of the row labels is pivoted.\n\n#Pivoting the inner level of row labels to the inner-most level of column labels\ndf_example.unstack()\n\n\n\n\n\n  \n    \n      State\n      Illinois\n      California\n    \n    \n      City\n      Evanston\n      Chicago\n      San Francisco\n      Los Angeles\n    \n    \n      Statistic\n      Area (mile-sq)\n      Elevation (feet)\n      Population\n      Unemployement (%)\n      Area (mile-sq)\n      Elevation (feet)\n      Population\n      Unemployement (%)\n      Area (mile-sq)\n      Elevation (feet)\n      Population\n      Unemployement (%)\n      Area (mile-sq)\n      Elevation (feet)\n      Population\n      Unemployement (%)\n    \n    \n      Information type\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Demographics\n      NaN\n      NaN\n      771517.0\n      4.2\n      NaN\n      NaN\n      2697000.0\n      5.6\n      NaN\n      NaN\n      815201.0\n      2.8\n      NaN\n      NaN\n      3849000.0\n      4.6\n    \n    \n      Geography\n      7.8\n      6749.0\n      NaN\n      NaN\n      234.5\n      597.0\n      NaN\n      NaN\n      46.9\n      52.0\n      NaN\n      NaN\n      502.0\n      305.0\n      NaN\n      NaN\n    \n  \n\n\n\n\nAs with Series, we can pivot the outer level of the row labels by specifying it in the level argument:\n\n#Pivoting the outer level (level = 0) of row labels to the inner-most level of column labels\ndf_example.unstack(level=0)\n\n\n\n\n\n  \n    \n      State\n      Illinois\n      California\n    \n    \n      City\n      Evanston\n      Chicago\n      San Francisco\n      Los Angeles\n    \n    \n      Information type\n      Demographics\n      Geography\n      Demographics\n      Geography\n      Demographics\n      Geography\n      Demographics\n      Geography\n    \n    \n      Statistic\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Area (mile-sq)\n      NaN\n      7.8\n      NaN\n      234.5\n      NaN\n      46.9\n      NaN\n      502.0\n    \n    \n      Elevation (feet)\n      NaN\n      6749.0\n      NaN\n      597.0\n      NaN\n      52.0\n      NaN\n      305.0\n    \n    \n      Population\n      771517.0\n      NaN\n      2697000.0\n      NaN\n      815201.0\n      NaN\n      3849000.0\n      NaN\n    \n    \n      Unemployement (%)\n      4.2\n      NaN\n      5.6\n      NaN\n      2.8\n      NaN\n      4.6\n      NaN\n    \n  \n\n\n\n\n\n\n8.1.7.3 stack()\nThe inverse of unstack() is the stack() method, which creates the inner-most level of row indices by pivoting the column labels of the prescribed level.\nNote that if the column labels have only one level, we don’t need to specify a level.\n\n#Stacking the columns of a DataFrame\nseries_example_unstack.stack()\n\nEnglish  1       these\n         2         are\n         3     english\n         4       words\nFrench   1          ce\n         2        sont\n         3         des\n         4     françai\n         5        mots\nSpanish  1       estas\n         2         son\n         3    palabras\n         4          en\n         5     español\ndtype: object\n\n\nHowever, if the columns have multiple levels, we can specify the level to stack as the inner-most row level. By default, the inner-most column level is stacked.\n\n#Stacking the inner-most column labels inner-most row indices\ndf_example.stack()\n\n\n\n\n\n  \n    \n      \n      \n      State\n      California\n      Illinois\n    \n    \n      Information type\n      Statistic\n      City\n      \n      \n    \n  \n  \n    \n      Demographics\n      Population\n      Chicago\n      NaN\n      2697000.0\n    \n    \n      Evanston\n      NaN\n      771517.0\n    \n    \n      Los Angeles\n      3849000.0\n      NaN\n    \n    \n      San Francisco\n      815201.0\n      NaN\n    \n    \n      Unemployement (%)\n      Chicago\n      NaN\n      5.6\n    \n    \n      Evanston\n      NaN\n      4.2\n    \n    \n      Los Angeles\n      4.6\n      NaN\n    \n    \n      San Francisco\n      2.8\n      NaN\n    \n    \n      Geography\n      Area (mile-sq)\n      Chicago\n      NaN\n      234.5\n    \n    \n      Evanston\n      NaN\n      7.8\n    \n    \n      Los Angeles\n      502.0\n      NaN\n    \n    \n      San Francisco\n      46.9\n      NaN\n    \n    \n      Elevation (feet)\n      Chicago\n      NaN\n      597.0\n    \n    \n      Evanston\n      NaN\n      6749.0\n    \n    \n      Los Angeles\n      305.0\n      NaN\n    \n    \n      San Francisco\n      52.0\n      NaN\n    \n  \n\n\n\n\n\n#Stacking the outer column labels inner-most row indices\ndf_example.stack(level=0)\n\n\n\n\n\n  \n    \n      \n      \n      City\n      Chicago\n      Evanston\n      Los Angeles\n      San Francisco\n    \n    \n      Information type\n      Statistic\n      State\n      \n      \n      \n      \n    \n  \n  \n    \n      Demographics\n      Population\n      California\n      NaN\n      NaN\n      3849000.0\n      815201.0\n    \n    \n      Illinois\n      2697000.0\n      771517.0\n      NaN\n      NaN\n    \n    \n      Unemployement (%)\n      California\n      NaN\n      NaN\n      4.6\n      2.8\n    \n    \n      Illinois\n      5.6\n      4.2\n      NaN\n      NaN\n    \n    \n      Geography\n      Area (mile-sq)\n      California\n      NaN\n      NaN\n      502.0\n      46.9\n    \n    \n      Illinois\n      234.5\n      7.8\n      NaN\n      NaN\n    \n    \n      Elevation (feet)\n      California\n      NaN\n      NaN\n      305.0\n      52.0\n    \n    \n      Illinois\n      597.0\n      6749.0\n      NaN\n      NaN"
  },
  {
    "objectID": "Data wrangling.html#merging-data",
    "href": "Data wrangling.html#merging-data",
    "title": "8  Data wrangling",
    "section": "8.2 Merging data",
    "text": "8.2 Merging data\nThe Pandas DataFrame method merge() uses columns defined as key column(s) to merge two datasets. In case the key column(s) are not defined, the overlapping column(s) are considered as the key columns.\n\n8.2.1 Join types\nWhen a dataset is merged with another based on key column(s), one of the following four types of join will occur depending on the repetition of the values of the key(s) in the datasets.\n\nOne-to-one, (ii) Many-to-one, (iii) One-to-Many, and (iv) Many-to-many\n\nThe type of join may sometimes determine the number of rows to be obtained in the merged dataset. If we don’t get the expected number of rows in the merged dataset, an investigation of the datsets may be neccessary to identify and resolve the issue. There may be several possible issues, for example, the dataset may not be arranged in a way that we have assumed it to be arranged.\nWe’ll use toy datasets to understand the above types of joins. The .csv files with the prefix student consist of the names of a few students along with their majors, and the files with the prefix skills consist of the names of majors along with the skills imparted by the respective majors.\n\ndata_student = pd.read_csv('./Datasets/student_one.csv')\ndata_skill = pd.read_csv('./Datasets/skills_one.csv')\n\n\n8.2.1.1 One-to-one join\nEach row in one dataset is linked (or related) to a single row in another dataset based on the key column(s).\n\ndata_student\n\n\n\n\n\n  \n    \n      \n      Student\n      Major\n    \n  \n  \n    \n      0\n      Kitana\n      Statistics\n    \n    \n      1\n      Jax\n      Computer Science\n    \n    \n      2\n      Sonya\n      Material Science\n    \n    \n      3\n      Johnny\n      Music\n    \n  \n\n\n\n\n\ndata_skill\n\n\n\n\n\n  \n    \n      \n      Major\n      Skills\n    \n  \n  \n    \n      0\n      Statistics\n      Inference\n    \n    \n      1\n      Computer Science\n      Machine learning\n    \n    \n      2\n      Material Science\n      Structure prediction\n    \n    \n      3\n      Music\n      Opera\n    \n  \n\n\n\n\n\npd.merge(data_student,data_skill)\n\n\n\n\n\n  \n    \n      \n      Student\n      Major\n      Skills\n    \n  \n  \n    \n      0\n      Kitana\n      Statistics\n      Inference\n    \n    \n      1\n      Jax\n      Computer Science\n      Machine learning\n    \n    \n      2\n      Sonya\n      Material Science\n      Structure prediction\n    \n    \n      3\n      Johnny\n      Music\n      Opera\n    \n  \n\n\n\n\n\n\n8.2.1.2 Many-to-one join\nOne or more rows in one dataset is linked (or related) to a single row in another dataset based on the key column(s).\n\ndata_student = pd.read_csv('./Datasets/student_many.csv')\ndata_skill = pd.read_csv('./Datasets/skills_one.csv')\n\n\ndata_student\n\n\n\n\n\n  \n    \n      \n      Student\n      Major\n    \n  \n  \n    \n      0\n      Kitana\n      Statistics\n    \n    \n      1\n      Kitana\n      Computer Science\n    \n    \n      2\n      Jax\n      Computer Science\n    \n    \n      3\n      Sonya\n      Material Science\n    \n    \n      4\n      Johnny\n      Music\n    \n    \n      5\n      Johnny\n      Statistics\n    \n  \n\n\n\n\n\ndata_skill\n\n\n\n\n\n  \n    \n      \n      Major\n      Skills\n    \n  \n  \n    \n      0\n      Statistics\n      Inference\n    \n    \n      1\n      Computer Science\n      Machine learning\n    \n    \n      2\n      Material Science\n      Structure prediction\n    \n    \n      3\n      Music\n      Opera\n    \n  \n\n\n\n\n\npd.merge(data_student,data_skill)\n\n\n\n\n\n  \n    \n      \n      Student\n      Major\n      Skills\n    \n  \n  \n    \n      0\n      Kitana\n      Statistics\n      Inference\n    \n    \n      1\n      Johnny\n      Statistics\n      Inference\n    \n    \n      2\n      Kitana\n      Computer Science\n      Machine learning\n    \n    \n      3\n      Jax\n      Computer Science\n      Machine learning\n    \n    \n      4\n      Sonya\n      Material Science\n      Structure prediction\n    \n    \n      5\n      Johnny\n      Music\n      Opera\n    \n  \n\n\n\n\n\n\n8.2.1.3 One-to-many join\nEach row in one dataset is linked (or related) to one, or more rows in another dataset based on the key column(s).\n\ndata_student = pd.read_csv('./Datasets/student_one.csv')\ndata_skill = pd.read_csv('./Datasets/skills_many.csv')\n\n\ndata_student\n\n\n\n\n\n  \n    \n      \n      Student\n      Major\n    \n  \n  \n    \n      0\n      Kitana\n      Statistics\n    \n    \n      1\n      Jax\n      Computer Science\n    \n    \n      2\n      Sonya\n      Material Science\n    \n    \n      3\n      Johnny\n      Music\n    \n  \n\n\n\n\n\ndata_skill\n\n\n\n\n\n  \n    \n      \n      Major\n      Skills\n    \n  \n  \n    \n      0\n      Statistics\n      Inference\n    \n    \n      1\n      Statistics\n      Modeling\n    \n    \n      2\n      Computer Science\n      Machine learning\n    \n    \n      3\n      Computer Science\n      Computing\n    \n    \n      4\n      Material Science\n      Structure prediction\n    \n    \n      5\n      Music\n      Opera\n    \n    \n      6\n      Music\n      Pop\n    \n    \n      7\n      Music\n      Classical\n    \n  \n\n\n\n\n\npd.merge(data_student,data_skill)\n\n\n\n\n\n  \n    \n      \n      Student\n      Major\n      Skills\n    \n  \n  \n    \n      0\n      Kitana\n      Statistics\n      Inference\n    \n    \n      1\n      Kitana\n      Statistics\n      Modeling\n    \n    \n      2\n      Jax\n      Computer Science\n      Machine learning\n    \n    \n      3\n      Jax\n      Computer Science\n      Computing\n    \n    \n      4\n      Sonya\n      Material Science\n      Structure prediction\n    \n    \n      5\n      Johnny\n      Music\n      Opera\n    \n    \n      6\n      Johnny\n      Music\n      Pop\n    \n    \n      7\n      Johnny\n      Music\n      Classical\n    \n  \n\n\n\n\n\n\n8.2.1.4 Many-to-many join\nOne, or more, rows in one dataset is linked (or related) to one, or more, rows in another dataset using the key column(s).\n\ndata_student = pd.read_csv('./Datasets/student_many.csv')\ndata_skill = pd.read_csv('./Datasets/skills_many.csv')\n\n\ndata_student\n\n\n\n\n\n  \n    \n      \n      Student\n      Major\n    \n  \n  \n    \n      0\n      Kitana\n      Statistics\n    \n    \n      1\n      Kitana\n      Computer Science\n    \n    \n      2\n      Jax\n      Computer Science\n    \n    \n      3\n      Sonya\n      Material Science\n    \n    \n      4\n      Johnny\n      Music\n    \n    \n      5\n      Johnny\n      Statistics\n    \n  \n\n\n\n\n\ndata_skill\n\n\n\n\n\n  \n    \n      \n      Major\n      Skills\n    \n  \n  \n    \n      0\n      Statistics\n      Inference\n    \n    \n      1\n      Statistics\n      Modeling\n    \n    \n      2\n      Computer Science\n      Machine learning\n    \n    \n      3\n      Computer Science\n      Computing\n    \n    \n      4\n      Material Science\n      Structure prediction\n    \n    \n      5\n      Music\n      Opera\n    \n    \n      6\n      Music\n      Pop\n    \n    \n      7\n      Music\n      Classical\n    \n  \n\n\n\n\n\npd.merge(data_student,data_skill)\n\n\n\n\n\n  \n    \n      \n      Student\n      Major\n      Skills\n    \n  \n  \n    \n      0\n      Kitana\n      Statistics\n      Inference\n    \n    \n      1\n      Kitana\n      Statistics\n      Modeling\n    \n    \n      2\n      Johnny\n      Statistics\n      Inference\n    \n    \n      3\n      Johnny\n      Statistics\n      Modeling\n    \n    \n      4\n      Kitana\n      Computer Science\n      Machine learning\n    \n    \n      5\n      Kitana\n      Computer Science\n      Computing\n    \n    \n      6\n      Jax\n      Computer Science\n      Machine learning\n    \n    \n      7\n      Jax\n      Computer Science\n      Computing\n    \n    \n      8\n      Sonya\n      Material Science\n      Structure prediction\n    \n    \n      9\n      Johnny\n      Music\n      Opera\n    \n    \n      10\n      Johnny\n      Music\n      Pop\n    \n    \n      11\n      Johnny\n      Music\n      Classical\n    \n  \n\n\n\n\nNote that there are two ‘Statistics’ rows in data_student, and two ‘Statistics’ rows in data_skill, resulting in 2x2 = 4 ‘Statistics’ rows in the merged data. The same is true for the ‘Computer Science’ Major.\n\n\n\n8.2.2 Join types with how argument\nThe above mentioned types of join (one-to-one, many-to-one, etc.) occur depening on the structure of the datasets being merged. We don’t have control over the type of join. However, we can control how the joins are occurring. We can merge (or join) two datasets in one of the following four ways:\n\ninner join, (ii) left join, (iii) right join, (iv) outer join\n\n\n8.2.2.1 inner join\nThis is the join that occurs by default, i.e., without specifying the how argument in the merge() function. In inner join, only those observations are merged that have the same value(s) in the key column(s) of both the datasets.\n\ndata_student = pd.read_csv('./Datasets/student_how.csv')\ndata_skill = pd.read_csv('./Datasets/skills_how.csv')\n\n\ndata_student\n\n\n\n\n\n  \n    \n      \n      Student\n      Major\n    \n  \n  \n    \n      0\n      Kitana\n      Statistics\n    \n    \n      1\n      Jax\n      Computer Science\n    \n    \n      2\n      Sonya\n      Material Science\n    \n  \n\n\n\n\n\ndata_skill\n\n\n\n\n\n  \n    \n      \n      Major\n      Skills\n    \n  \n  \n    \n      0\n      Statistics\n      Inference\n    \n    \n      1\n      Computer Science\n      Machine learning\n    \n    \n      2\n      Music\n      Opera\n    \n  \n\n\n\n\n\npd.merge(data_student,data_skill)\n\n\n\n\n\n  \n    \n      \n      Student\n      Major\n      Skills\n    \n  \n  \n    \n      0\n      Kitana\n      Statistics\n      Inference\n    \n    \n      1\n      Jax\n      Computer Science\n      Machine learning\n    \n  \n\n\n\n\nWhen you may use inner join? You should use inner join when you cannot carry out the analysis unless the observation corresponding to the key column(s) is present in both the tables.\nExample: Suppose you wish to analyze the association between vaccinations and covid infection rate based on country-level data. In one of the datasets, you have the infection rate for each country, while in the other one you have the number of vaccinations in each country. The countries which have either the vaccination or the infection rate missing, cannot help analyze the association. In such as case you may be interested only in countries that have values for both the variables. Thus, you will use inner join to discard the countries with either value missing.\n\n\n8.2.2.2 left join\nIn left join, the merged dataset will have all the rows of the dataset that is specified first in the merge() function. Only those observations of the other dataset will be merged whose value(s) in the key column(s) exist in the dataset specified first in the merge() function.\n\npd.merge(data_student,data_skill,how='left')\n\n\n\n\n\n  \n    \n      \n      Student\n      Major\n      Skills\n    \n  \n  \n    \n      0\n      Kitana\n      Statistics\n      Inference\n    \n    \n      1\n      Jax\n      Computer Science\n      Machine learning\n    \n    \n      2\n      Sonya\n      Material Science\n      NaN\n    \n  \n\n\n\n\nWhen you may use left join? You should use left join when the primary variable(s) of interest are present in the one of the datasets, and whose missing values cannot be imputed. The variable(s) in the other dataset may not be as important or it may be possible to reasonably impute their values, if missing corresponding to the observation in the primary dataset.\nExamples:\n\nSuppose you wish to analyze the association between the covid infection rate and the government effectiveness score (a metric used to determine the effectiveness of the government in implementing policies, upholding law and order etc.) based on the data of all countries. Let us say that one of the datasets contains the covid infection rate, while the other one contains the government effectiveness score for each country. If the infection rate for a country is missing, it might be hard to impute. However, the government effectiveness score may be easier to impute based on GDP per capita, crime rate etc. - information that is easily available online. In such a case, you may wish to use a left join where you keep all the countries for which the infection rate is known.\nSuppose you wish to analyze the association between demographics such as age, income etc. and the amount of credit card spend. Let us say one of the datasets contains the demographic information of each customer, while the other one contains the credit card spend for the customers who made at least one purchase. In such as case, you may want to do a left join as customers not making any purchase might be absent in the card spend data. Their spend can be imputed as zero after merging the datasets.\n\n\n\n8.2.2.3 right join\nIn right join, the merged dataset will have all the rows of the dataset that is specified second in the merge() function. Only those observations of the other dataset will be merged whose value(s) in the key column(s) exist in the dataset specified second in the merge() function.\n\npd.merge(data_student,data_skill,how='right')\n\n\n\n\n\n  \n    \n      \n      Student\n      Major\n      Skills\n    \n  \n  \n    \n      0\n      Kitana\n      Statistics\n      Inference\n    \n    \n      1\n      Jax\n      Computer Science\n      Machine learning\n    \n    \n      2\n      NaN\n      Music\n      Opera\n    \n  \n\n\n\n\nWhen you may use right join? You can always use a left join instead of a right join. Their purpose is the same.\n\n\n8.2.2.4 outer join\nIn outer join, the merged dataset will have all the rows of both the datasets being merged.\n\npd.merge(data_student,data_skill,how='outer')\n\n\n\n\n\n  \n    \n      \n      Student\n      Major\n      Skills\n    \n  \n  \n    \n      0\n      Kitana\n      Statistics\n      Inference\n    \n    \n      1\n      Jax\n      Computer Science\n      Machine learning\n    \n    \n      2\n      Sonya\n      Material Science\n      NaN\n    \n    \n      3\n      NaN\n      Music\n      Opera\n    \n  \n\n\n\n\nWhen you may use outer join? You should use an outer join when you cannot afford to lose data present in either of the tables. All the other joins may result in loss of data.\nExample: Suppose I took two course surveys for this course. If I need to analyze student sentiment during the course, I will take an outer join of both the surveys. Assume that each survey is a dataset, where each row corresponds to a unique student. Even if a student has answered one of the two surverys, it will be indicative of the sentiment, and will be useful to keep in the merged dataset."
  },
  {
    "objectID": "Data wrangling.html#concatenating-datasets",
    "href": "Data wrangling.html#concatenating-datasets",
    "title": "8  Data wrangling",
    "section": "8.3 Concatenating datasets",
    "text": "8.3 Concatenating datasets\nThe Pandas DataFrame method concat() is used to stack datasets along an axis. The method is similar to NumPy’s concatenate() method.\nExample: You are given the life expectancy data of each continent as a separate *.csv file. Visualize the change of life expectancy over time for different continents.\n\ndata_asia = pd.read_csv('./Datasets/gdp_lifeExpec_Asia.csv')\ndata_europe = pd.read_csv('./Datasets/gdp_lifeExpec_Europe.csv')\ndata_africa = pd.read_csv('./Datasets/gdp_lifeExpec_Africa.csv')\ndata_oceania = pd.read_csv('./Datasets/gdp_lifeExpec_Oceania.csv')\ndata_americas = pd.read_csv('./Datasets/gdp_lifeExpec_Americas.csv')\n\n\n#Appending all the data files, i.e., stacking them on top of each other\ndata_all_continents = pd.concat([data_asia,data_europe,data_africa,data_oceania,data_americas],keys = ['Asia','Europe','Africa','Oceania','Americas'])\ndata_all_continents\n\n\n\n\n\n  \n    \n      \n      \n      country\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      Asia\n      0\n      Afghanistan\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      2\n      Afghanistan\n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      3\n      Afghanistan\n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      4\n      Afghanistan\n      1972\n      36.088\n      13079460\n      739.981106\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Americas\n      295\n      Venezuela\n      1987\n      70.190\n      17910182\n      9883.584648\n    \n    \n      296\n      Venezuela\n      1992\n      71.150\n      20265563\n      10733.926310\n    \n    \n      297\n      Venezuela\n      1997\n      72.146\n      22374398\n      10165.495180\n    \n    \n      298\n      Venezuela\n      2002\n      72.766\n      24287670\n      8605.047831\n    \n    \n      299\n      Venezuela\n      2007\n      73.747\n      26084662\n      11415.805690\n    \n  \n\n1704 rows × 5 columns\n\n\n\nLet’s have the continent as a column as we need to use that in the visualization.\n\ndata_all_continents.reset_index(inplace = True)\n\n\ndata_all_continents.head()\n\n\n\n\n\n  \n    \n      \n      level_0\n      level_1\n      country\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Asia\n      0\n      Afghanistan\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1\n      Asia\n      1\n      Afghanistan\n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      2\n      Asia\n      2\n      Afghanistan\n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      3\n      Asia\n      3\n      Afghanistan\n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      4\n      Asia\n      4\n      Afghanistan\n      1972\n      36.088\n      13079460\n      739.981106\n    \n  \n\n\n\n\n\ndata_all_continents.drop(columns = 'level_1',inplace = True)\ndata_all_continents.rename(columns = {'level_0':'continent'},inplace = True)\ndata_all_continents.head()\n\n\n\n\n\n  \n    \n      \n      continent\n      country\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Asia\n      Afghanistan\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1\n      Asia\n      Afghanistan\n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      2\n      Asia\n      Afghanistan\n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      3\n      Asia\n      Afghanistan\n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      4\n      Asia\n      Afghanistan\n      1972\n      36.088\n      13079460\n      739.981106\n    \n  \n\n\n\n\n\n#change of life expectancy over time for different continents\na = sns.FacetGrid(data_all_continents,col = 'continent',col_wrap = 3,height = 4.5,aspect = 1)#height = 3,aspect = 0.8)\na.map(sns.lineplot,'year','lifeExp')\na.add_legend()\n\n<seaborn.axisgrid.FacetGrid at 0x2a4103fa130>\n\n\n\n\n\nIn the above example, datasets were appended (or stacked on top of each other).\nDatasets can also be concatenated side-by-side (by providing the argument axis = 1 with the concat() function) as we saw with the merge function.\n\n8.3.1 Practice exercise 4\nRead the documentations of the Pandas DataFrame methods merge() and concat(), and identify the differences. Mention examples when you can use (i) either, (ii) only concat(), (iii) only merge()"
  },
  {
    "objectID": "Data wrangling.html#reshaping-data-1",
    "href": "Data wrangling.html#reshaping-data-1",
    "title": "8  Data wrangling",
    "section": "8.4 Reshaping data",
    "text": "8.4 Reshaping data\nData often needs to be re-arranged to ease analysis.\n\n8.4.1 Pivoting “long” to “wide” format\n\npivot()\nThis function helps re-arrange data from the ‘long’ form to a ‘wide’ form.\nExample: Let us consider the dataset data_all_continents obtained in the previous section after concatenating the data of all the continents.\n\ndata_all_continents.head()\n\n\n\n\n\n  \n    \n      \n      continent\n      country\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Asia\n      Afghanistan\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1\n      Asia\n      Afghanistan\n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      2\n      Asia\n      Afghanistan\n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      3\n      Asia\n      Afghanistan\n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      4\n      Asia\n      Afghanistan\n      1972\n      36.088\n      13079460\n      739.981106\n    \n  \n\n\n\n\n\n\n8.4.1.1 Pivoting a single column\nFor visualizing life expectancy in 2007 against life expectancy in 1957, we will need to filter the data, and then make the plot. Everytime that we need to compare a metric for a year against another year, we will need to filter the data.\nIf we need to often compare metrics of a year against another year, it will be easier to have each year as a separate column, instead of having all years in a single column.\nAs we are increasing the number of columns and deceasing the number of rows, we are re-arranging the data from long-form to wide-form.\n\ndata_wide = data_all_continents.pivot(index = ['continent','country'],columns = 'year',values = 'lifeExp')\n\n\ndata_wide.head()\n\n\n\n\n\n  \n    \n      \n      year\n      1952\n      1957\n      1962\n      1967\n      1972\n      1977\n      1982\n      1987\n      1992\n      1997\n      2002\n      2007\n    \n    \n      continent\n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Africa\n      Algeria\n      43.077\n      45.685\n      48.303\n      51.407\n      54.518\n      58.014\n      61.368\n      65.799\n      67.744\n      69.152\n      70.994\n      72.301\n    \n    \n      Angola\n      30.015\n      31.999\n      34.000\n      35.985\n      37.928\n      39.483\n      39.942\n      39.906\n      40.647\n      40.963\n      41.003\n      42.731\n    \n    \n      Benin\n      38.223\n      40.358\n      42.618\n      44.885\n      47.014\n      49.190\n      50.904\n      52.337\n      53.919\n      54.777\n      54.406\n      56.728\n    \n    \n      Botswana\n      47.622\n      49.618\n      51.520\n      53.298\n      56.024\n      59.319\n      61.484\n      63.622\n      62.745\n      52.556\n      46.634\n      50.728\n    \n    \n      Burkina Faso\n      31.975\n      34.906\n      37.814\n      40.697\n      43.591\n      46.137\n      48.122\n      49.557\n      50.260\n      50.324\n      50.650\n      52.295\n    \n  \n\n\n\n\nWith values of year as columns, it is easy to compare any metric for different years.\n\n#visualizing the change in life expectancy of all countries in 2007 as compared to that in 1957, i.e., the overall change in life expectancy in 50 years. \nsns.scatterplot(data = data_wide, x = 1957,y=2007,hue = 'continent')\nsns.lineplot(data = data_wide, x = 1957,y = 1957)\n\n<AxesSubplot:xlabel='1957', ylabel='2007'>\n\n\n\n\n\nObserve that for some African countries, the life expectancy has decreased after 50 years. It is worth investigating these countries to identify factors associated with the decrease.\n\n\n8.4.1.2 Pivoting multiple columns\nIn the above transformation, we retained only lifeExp in the ‘wide’ dataset. Suppose, we are also interested in visualizing GDP per capita of countries in one year against another year. In that case, we must have gdpPercap in the ’wide’-form data as well.\nLet us create a dataset named as data_wide_lifeExp_gdpPercap that will contain both lifeExp and gdpPercap for each year in a separate column. We will specify the columns to pivot in the values argument of the pivot() function.\n\ndata_wide_lifeExp_gdpPercap = data_all_continents.pivot(index = ['continent','country'],columns = 'year',values = ['lifeExp','gdpPercap'])\ndata_wide_lifeExp_gdpPercap.head()\n\n\n\n\n\n  \n    \n      \n      \n      lifeExp\n      ...\n      gdpPercap\n    \n    \n      \n      year\n      1952\n      1957\n      1962\n      1967\n      1972\n      1977\n      1982\n      1987\n      1992\n      1997\n      ...\n      1962\n      1967\n      1972\n      1977\n      1982\n      1987\n      1992\n      1997\n      2002\n      2007\n    \n    \n      continent\n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Africa\n      Algeria\n      43.077\n      45.685\n      48.303\n      51.407\n      54.518\n      58.014\n      61.368\n      65.799\n      67.744\n      69.152\n      ...\n      2550.816880\n      3246.991771\n      4182.663766\n      4910.416756\n      5745.160213\n      5681.358539\n      5023.216647\n      4797.295051\n      5288.040382\n      6223.367465\n    \n    \n      Angola\n      30.015\n      31.999\n      34.000\n      35.985\n      37.928\n      39.483\n      39.942\n      39.906\n      40.647\n      40.963\n      ...\n      4269.276742\n      5522.776375\n      5473.288005\n      3008.647355\n      2756.953672\n      2430.208311\n      2627.845685\n      2277.140884\n      2773.287312\n      4797.231267\n    \n    \n      Benin\n      38.223\n      40.358\n      42.618\n      44.885\n      47.014\n      49.190\n      50.904\n      52.337\n      53.919\n      54.777\n      ...\n      949.499064\n      1035.831411\n      1085.796879\n      1029.161251\n      1277.897616\n      1225.856010\n      1191.207681\n      1232.975292\n      1372.877931\n      1441.284873\n    \n    \n      Botswana\n      47.622\n      49.618\n      51.520\n      53.298\n      56.024\n      59.319\n      61.484\n      63.622\n      62.745\n      52.556\n      ...\n      983.653976\n      1214.709294\n      2263.611114\n      3214.857818\n      4551.142150\n      6205.883850\n      7954.111645\n      8647.142313\n      11003.605080\n      12569.851770\n    \n    \n      Burkina Faso\n      31.975\n      34.906\n      37.814\n      40.697\n      43.591\n      46.137\n      48.122\n      49.557\n      50.260\n      50.324\n      ...\n      722.512021\n      794.826560\n      854.735976\n      743.387037\n      807.198586\n      912.063142\n      931.752773\n      946.294962\n      1037.645221\n      1217.032994\n    \n  \n\n5 rows × 24 columns\n\n\n\nThe metric for each year is now in a separate column, and can be visualized directly. Note that re-arranging the dataset from the ‘long’-form to ‘wide-form’ leads to hierarchical indexing of columns when multiple ‘values’ need to be re-arranged. In this case, the multiple ‘values’ that need to be re-arranged are lifeExp and gdpPercap.\n\n\n\n8.4.2 Melting “wide” to “long” format\n\nmelt()\nThis function is used to re-arrange the dataset from the ‘wide’ form to the ‘long’ form.\n\n\n8.4.2.1 Melting columns with a single type of value\nLet us consider data_wide created in the previous section.\n\ndata_wide.head()\n\n\n\n\n\n  \n    \n      \n      year\n      1952\n      1957\n      1962\n      1967\n      1972\n      1977\n      1982\n      1987\n      1992\n      1997\n      2002\n      2007\n    \n    \n      continent\n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Africa\n      Algeria\n      43.077\n      45.685\n      48.303\n      51.407\n      54.518\n      58.014\n      61.368\n      65.799\n      67.744\n      69.152\n      70.994\n      72.301\n    \n    \n      Angola\n      30.015\n      31.999\n      34.000\n      35.985\n      37.928\n      39.483\n      39.942\n      39.906\n      40.647\n      40.963\n      41.003\n      42.731\n    \n    \n      Benin\n      38.223\n      40.358\n      42.618\n      44.885\n      47.014\n      49.190\n      50.904\n      52.337\n      53.919\n      54.777\n      54.406\n      56.728\n    \n    \n      Botswana\n      47.622\n      49.618\n      51.520\n      53.298\n      56.024\n      59.319\n      61.484\n      63.622\n      62.745\n      52.556\n      46.634\n      50.728\n    \n    \n      Burkina Faso\n      31.975\n      34.906\n      37.814\n      40.697\n      43.591\n      46.137\n      48.122\n      49.557\n      50.260\n      50.324\n      50.650\n      52.295\n    \n  \n\n\n\n\nSuppose, we wish to visualize the change of life expectancy over time for different continents, as we did in section 8.3. For plotting lifeExp against year, all the years must be in a single column. Thus, we need to melt the columns of data_wide to a single column and call it year.\nBut before melting the columns in the above dataset, we will convert continent to a column, as we need to make subplots based on continent.\nThe Pandas DataFrame method reset_index() can be used to remove one or more levels of indexing from the DataFrame.\n\n#Making 'continent' a column instead of row-index at level 0\ndata_wide.reset_index(inplace=True,level=0)\ndata_wide.head()\n\n\n\n\n\n  \n    \n      year\n      continent\n      1952\n      1957\n      1962\n      1967\n      1972\n      1977\n      1982\n      1987\n      1992\n      1997\n      2002\n      2007\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Algeria\n      Africa\n      43.077\n      45.685\n      48.303\n      51.407\n      54.518\n      58.014\n      61.368\n      65.799\n      67.744\n      69.152\n      70.994\n      72.301\n    \n    \n      Angola\n      Africa\n      30.015\n      31.999\n      34.000\n      35.985\n      37.928\n      39.483\n      39.942\n      39.906\n      40.647\n      40.963\n      41.003\n      42.731\n    \n    \n      Benin\n      Africa\n      38.223\n      40.358\n      42.618\n      44.885\n      47.014\n      49.190\n      50.904\n      52.337\n      53.919\n      54.777\n      54.406\n      56.728\n    \n    \n      Botswana\n      Africa\n      47.622\n      49.618\n      51.520\n      53.298\n      56.024\n      59.319\n      61.484\n      63.622\n      62.745\n      52.556\n      46.634\n      50.728\n    \n    \n      Burkina Faso\n      Africa\n      31.975\n      34.906\n      37.814\n      40.697\n      43.591\n      46.137\n      48.122\n      49.557\n      50.260\n      50.324\n      50.650\n      52.295\n    \n  \n\n\n\n\n\ndata_melted=pd.melt(data_wide,id_vars = ['continent'],var_name = 'Year',value_name = 'LifeExp')\ndata_melted.head()\n\n\n\n\n\n  \n    \n      \n      continent\n      Year\n      LifeExp\n    \n  \n  \n    \n      0\n      Africa\n      1952\n      43.077\n    \n    \n      1\n      Africa\n      1952\n      30.015\n    \n    \n      2\n      Africa\n      1952\n      38.223\n    \n    \n      3\n      Africa\n      1952\n      47.622\n    \n    \n      4\n      Africa\n      1952\n      31.975\n    \n  \n\n\n\n\nWith the above DataFrame, we can visualize the mean life expectancy against year separately for each continent.\nIf we wish to have country also in the above data, we can keep it while resetting the index:\n\n#Creating 'data_wide' again\ndata_wide = data_all_continents.pivot(index = ['continent','country'],columns = 'year',values = 'lifeExp')\n\n#Resetting the row-indices to default values\ndata_wide.reset_index(inplace=True)\ndata_wide.head()\n\n\n\n\n\n  \n    \n      year\n      continent\n      country\n      1952\n      1957\n      1962\n      1967\n      1972\n      1977\n      1982\n      1987\n      1992\n      1997\n      2002\n      2007\n    \n  \n  \n    \n      0\n      Africa\n      Algeria\n      43.077\n      45.685\n      48.303\n      51.407\n      54.518\n      58.014\n      61.368\n      65.799\n      67.744\n      69.152\n      70.994\n      72.301\n    \n    \n      1\n      Africa\n      Angola\n      30.015\n      31.999\n      34.000\n      35.985\n      37.928\n      39.483\n      39.942\n      39.906\n      40.647\n      40.963\n      41.003\n      42.731\n    \n    \n      2\n      Africa\n      Benin\n      38.223\n      40.358\n      42.618\n      44.885\n      47.014\n      49.190\n      50.904\n      52.337\n      53.919\n      54.777\n      54.406\n      56.728\n    \n    \n      3\n      Africa\n      Botswana\n      47.622\n      49.618\n      51.520\n      53.298\n      56.024\n      59.319\n      61.484\n      63.622\n      62.745\n      52.556\n      46.634\n      50.728\n    \n    \n      4\n      Africa\n      Burkina Faso\n      31.975\n      34.906\n      37.814\n      40.697\n      43.591\n      46.137\n      48.122\n      49.557\n      50.260\n      50.324\n      50.650\n      52.295\n    \n  \n\n\n\n\n\n#Melting the 'year' column\ndata_melted=pd.melt(data_wide,id_vars = ['continent','country'],var_name = 'Year',value_name = 'LifeExp')\ndata_melted.head()\n\n\n\n\n\n  \n    \n      \n      continent\n      country\n      Year\n      LifeExp\n    \n  \n  \n    \n      0\n      Africa\n      Algeria\n      1952\n      43.077\n    \n    \n      1\n      Africa\n      Angola\n      1952\n      30.015\n    \n    \n      2\n      Africa\n      Benin\n      1952\n      38.223\n    \n    \n      3\n      Africa\n      Botswana\n      1952\n      47.622\n    \n    \n      4\n      Africa\n      Burkina Faso\n      1952\n      31.975\n    \n  \n\n\n\n\n\n\n8.4.2.2 Melting columns with multiple types of values\nConsider the dataset created in Section 8.4.1.2. It has two types of values - lifeExp and gdpPercapita, which are the column labels at the outer level. The melt() function will melt all the years of data into a single column. However, it will create another column based on the outer level column labels - lifeExp and gdpPercapita to distinguish between these two types of values. Here, we see that the function melt() internally uses hierarchical indexing to handle the transformation of multiple types of columns.\n\ndata_melt = pd.melt(data_wide_lifeExp_gdpPercap.reset_index(),id_vars = ['continent','country'],var_name = ['Metric','year'])\ndata_melt.head()\n\n\n\n\n\n  \n    \n      \n      continent\n      country\n      Metric\n      year\n      value\n    \n  \n  \n    \n      0\n      Africa\n      Algeria\n      lifeExp\n      1952\n      43.077\n    \n    \n      1\n      Africa\n      Angola\n      lifeExp\n      1952\n      30.015\n    \n    \n      2\n      Africa\n      Benin\n      lifeExp\n      1952\n      38.223\n    \n    \n      3\n      Africa\n      Botswana\n      lifeExp\n      1952\n      47.622\n    \n    \n      4\n      Africa\n      Burkina Faso\n      lifeExp\n      1952\n      31.975\n    \n  \n\n\n\n\nAlthough the data above is in ‘long’-form, it is not quiet in its original format, as in data_all_continents. We need to pivot again by Metric to have two separate columns of gdpPercap and lifeExp.\n\ndata_restore = data_melt.pivot(index = ['continent','country','year'],columns = 'Metric')\ndata_restore.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      value\n    \n    \n      \n      \n      Metric\n      gdpPercap\n      lifeExp\n    \n    \n      continent\n      country\n      year\n      \n      \n    \n  \n  \n    \n      Africa\n      Algeria\n      1952\n      2449.008185\n      43.077\n    \n    \n      1957\n      3013.976023\n      45.685\n    \n    \n      1962\n      2550.816880\n      48.303\n    \n    \n      1967\n      3246.991771\n      51.407\n    \n    \n      1972\n      4182.663766\n      54.518\n    \n  \n\n\n\n\nNow, we can convert the row indices of continent and country to columns to restore the dataset to the same form as data_all_continents.\n\ndata_restore.reset_index(inplace = True)\ndata_restore.head()\n\n\n\n\n\n  \n    \n      \n      continent\n      country\n      year\n      value\n    \n    \n      Metric\n      \n      \n      \n      gdpPercap\n      lifeExp\n    \n  \n  \n    \n      0\n      Africa\n      Algeria\n      1952\n      2449.008185\n      43.077\n    \n    \n      1\n      Africa\n      Algeria\n      1957\n      3013.976023\n      45.685\n    \n    \n      2\n      Africa\n      Algeria\n      1962\n      2550.816880\n      48.303\n    \n    \n      3\n      Africa\n      Algeria\n      1967\n      3246.991771\n      51.407\n    \n    \n      4\n      Africa\n      Algeria\n      1972\n      4182.663766\n      54.518\n    \n  \n\n\n\n\n\n\n\n8.4.3 Practice exercise 5\n\n8.4.3.1 \nBoth unstack() and pivot() seem to transform the data from the ‘long’ form to the ‘wide’ form. Is there a difference between the two functions?\nSolution:\nYes, both the functions transform the data from the ‘long’ form to the ‘wide’ form. However, unstack() pivots the row indices, while pivot() pivots the columns of the DataFrame.\nEven though both functions are a bit different, it is possible to just use one of them to perform a reshaping operation. If we wish to pivot a column, we can either use pivot() directly on the column, or we can convert the column to row indices and then use unstack(). If we wish to pivot row indices, we can either use unstack() directly on the row indices, or we can convert row indices to a column and then use pivot().\nTo summarise, using one function may be more straightforward than using the other one, but either can be used for reshaping data from the ‘long’ form to the ‘wide’ form.\nBelow is an example where we perform the same reshaping operation with either function.\nConsider the data data_all_continent. Suppose we wish to transform it to data_wide as we did using pivot() in Section 8.4.1.1. Let us do it using unstack(), instead of pivot().\nThe first step will be to reindex data to set year as row indices, and also continent and country as row indices because these two column were set as indices with the pivot() function in Section 8.4.1.1.\n\n#Reindexing data to make 'continent', 'country', and 'year' as hierarchical row indices\ndata_reindexed=data_all_continents.set_index(['continent','country','year'])\ndata_reindexed\n\n\n\n\n\n  \n    \n      \n      \n      \n      lifeExp\n      pop\n      gdpPercap\n    \n    \n      continent\n      country\n      year\n      \n      \n      \n    \n  \n  \n    \n      Asia\n      Afghanistan\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      1972\n      36.088\n      13079460\n      739.981106\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Americas\n      Venezuela\n      1987\n      70.190\n      17910182\n      9883.584648\n    \n    \n      1992\n      71.150\n      20265563\n      10733.926310\n    \n    \n      1997\n      72.146\n      22374398\n      10165.495180\n    \n    \n      2002\n      72.766\n      24287670\n      8605.047831\n    \n    \n      2007\n      73.747\n      26084662\n      11415.805690\n    \n  \n\n1704 rows × 3 columns\n\n\n\nNow we can use unstack() to pivot the desired row index, i.e., year. Also, since we are only interested in pivoting the values of lifeExp (as in the example in Section 8.4.1.1), we will filter the pivoted data with the lifeExp column label.\n\ndata_wide_with_unstack=data_reindexed.unstack('year')['lifeExp']\ndata_wide_with_unstack\n\n\n\n\n\n  \n    \n      \n      year\n      1952\n      1957\n      1962\n      1967\n      1972\n      1977\n      1982\n      1987\n      1992\n      1997\n      2002\n      2007\n    \n    \n      continent\n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Africa\n      Algeria\n      43.077\n      45.685\n      48.303\n      51.407\n      54.518\n      58.014\n      61.368\n      65.799\n      67.744\n      69.152\n      70.994\n      72.301\n    \n    \n      Angola\n      30.015\n      31.999\n      34.000\n      35.985\n      37.928\n      39.483\n      39.942\n      39.906\n      40.647\n      40.963\n      41.003\n      42.731\n    \n    \n      Benin\n      38.223\n      40.358\n      42.618\n      44.885\n      47.014\n      49.190\n      50.904\n      52.337\n      53.919\n      54.777\n      54.406\n      56.728\n    \n    \n      Botswana\n      47.622\n      49.618\n      51.520\n      53.298\n      56.024\n      59.319\n      61.484\n      63.622\n      62.745\n      52.556\n      46.634\n      50.728\n    \n    \n      Burkina Faso\n      31.975\n      34.906\n      37.814\n      40.697\n      43.591\n      46.137\n      48.122\n      49.557\n      50.260\n      50.324\n      50.650\n      52.295\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Europe\n      Switzerland\n      69.620\n      70.560\n      71.320\n      72.770\n      73.780\n      75.390\n      76.210\n      77.410\n      78.030\n      79.370\n      80.620\n      81.701\n    \n    \n      Turkey\n      43.585\n      48.079\n      52.098\n      54.336\n      57.005\n      59.507\n      61.036\n      63.108\n      66.146\n      68.835\n      70.845\n      71.777\n    \n    \n      United Kingdom\n      69.180\n      70.420\n      70.760\n      71.360\n      72.010\n      72.760\n      74.040\n      75.007\n      76.420\n      77.218\n      78.471\n      79.425\n    \n    \n      Oceania\n      Australia\n      69.120\n      70.330\n      70.930\n      71.100\n      71.930\n      73.490\n      74.740\n      76.320\n      77.560\n      78.830\n      80.370\n      81.235\n    \n    \n      New Zealand\n      69.390\n      70.260\n      71.240\n      71.520\n      71.890\n      72.220\n      73.840\n      74.320\n      76.330\n      77.550\n      79.110\n      80.204\n    \n  \n\n142 rows × 12 columns\n\n\n\nThe above dataset is the same as that obtained using the pivot() function in Section 8.4.1.1.\n\n\n8.4.3.2 \nBoth stack() and melt() seem to transform the data from the ‘wide’ form to the ‘long’ form. Is there a difference between the two functions?\nSolution:\nFollowing the trend of the previous question, we can always use stack() instead of melt() and vice-versa. The main difference is that melt() lets us choose the indentifier columns with the argument id_vars. However, if we use stack(), we will need to set the relevant melted row indices as columns. On the other hand, if we wished to have the melted columns as row indices, we can either directly use stack() or use melt() and then set the desired columns as row indices.\nTo summarise, using one function may be more straightforward than using the other one, but either can be used for reshaping data from the ‘wide’ form to the ‘long’ form.\nLet us melt the data data_wide_with_unstack using the stack() function to obtain the same dataset as obtained with the melt() function in Section 8.4.1.2.\n\n#Stacking the data\ndata_stacked = data_wide_with_unstack.stack()\ndata_stacked\n\ncontinent  country      year\nAfrica     Algeria      1952    43.077\n                        1957    45.685\n                        1962    48.303\n                        1967    51.407\n                        1972    54.518\n                                 ...  \nOceania    New Zealand  1987    74.320\n                        1992    76.330\n                        1997    77.550\n                        2002    79.110\n                        2007    80.204\nLength: 1704, dtype: float64\n\n\nNow we need to convert the row indices continent and country to columns as in the melted data in Section 8.4.1.2.\n\n#Putting 'continent' and 'country' as columns\ndata_long_with_stack = data_stacked.reset_index()\ndata_long_with_stack\n\n\n\n\n\n  \n    \n      \n      continent\n      country\n      year\n      0\n    \n  \n  \n    \n      0\n      Africa\n      Algeria\n      1952\n      43.077\n    \n    \n      1\n      Africa\n      Algeria\n      1957\n      45.685\n    \n    \n      2\n      Africa\n      Algeria\n      1962\n      48.303\n    \n    \n      3\n      Africa\n      Algeria\n      1967\n      51.407\n    \n    \n      4\n      Africa\n      Algeria\n      1972\n      54.518\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1699\n      Oceania\n      New Zealand\n      1987\n      74.320\n    \n    \n      1700\n      Oceania\n      New Zealand\n      1992\n      76.330\n    \n    \n      1701\n      Oceania\n      New Zealand\n      1997\n      77.550\n    \n    \n      1702\n      Oceania\n      New Zealand\n      2002\n      79.110\n    \n    \n      1703\n      Oceania\n      New Zealand\n      2007\n      80.204\n    \n  \n\n1704 rows × 4 columns\n\n\n\nFinally, we need to rename the column named as 0 to LifeExp to obtain the same dataset as in Section 8.4.1.2.\n\n#Renaming column 0 to 'LifeExp'\ndata_long_with_stack.rename(columns = {0:'LifeExp'},inplace=True)\ndata_long_with_stack\n\n\n\n\n\n  \n    \n      \n      continent\n      country\n      year\n      LifeExp\n    \n  \n  \n    \n      0\n      Africa\n      Algeria\n      1952\n      43.077\n    \n    \n      1\n      Africa\n      Algeria\n      1957\n      45.685\n    \n    \n      2\n      Africa\n      Algeria\n      1962\n      48.303\n    \n    \n      3\n      Africa\n      Algeria\n      1967\n      51.407\n    \n    \n      4\n      Africa\n      Algeria\n      1972\n      54.518\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1699\n      Oceania\n      New Zealand\n      1987\n      74.320\n    \n    \n      1700\n      Oceania\n      New Zealand\n      1992\n      76.330\n    \n    \n      1701\n      Oceania\n      New Zealand\n      1997\n      77.550\n    \n    \n      1702\n      Oceania\n      New Zealand\n      2002\n      79.110\n    \n    \n      1703\n      Oceania\n      New Zealand\n      2007\n      80.204\n    \n  \n\n1704 rows × 4 columns"
  },
  {
    "objectID": "Data aggregation.html",
    "href": "Data aggregation.html",
    "title": "9  Data aggregation",
    "section": "",
    "text": "Data aggregation refers to summarizing data with statistics such as sum, count, average, maximum, minimum etc. to provide a high level view of the data. Often there are mutually exclusive groups in the data that are of interest. In such cases, we may be interested in finding these statisitcs separately for each group. The Pandas DataFrame method groupby() is used to split the data into groups, and then the desired function(s) are applied on each of these groups for groupwise data aggregation. However, the groupby() method is not limited for groupwise data aggregation, but can also be used for several other kinds of groupwise oprations.\nGroupby mechanics: (Source: https://pandas.pydata.org/docs/user_guide/groupby.html)\nGroup by: split-apply-combine By ‘group by’ we are referring to a process involving one or more of the following steps:\nOut of these, the split step is the most straightforward. In fact, in many situations we may wish to split the data set into groups and do something with those groups. In the apply step, we may wish to do one of the following:\n1. Aggregation: compute a summary statistic (or statistics) for each group. Some examples:\n2. Transformation: perform some group-specific computations and return a like-indexed object. Some examples:\n3. Filtration: discard some groups, according to a group-wise computation that evaluates True or False. Some examples:\nSome combination of the above: GroupBy will examine the results of the apply step and try to return a sensibly combined result if it doesn’t fit into either of the above two categories.\nWe’ll use Pandas to group the data and perform GroupBy operations."
  },
  {
    "objectID": "Data aggregation.html#the-groupby-object",
    "href": "Data aggregation.html#the-groupby-object",
    "title": "9  Data aggregation",
    "section": "9.1 The GroupBy object",
    "text": "9.1 The GroupBy object\n\n9.1.1 Creating a GroupBy object: groupby\nThis Pandas DataFrame method groupby() is used to create a GroupBy object.\nA string passed to groupby() may refer to either a column or an index level. If a string matches both a column name and an index level name, a ValueError will be raised.\nExample: Consider the life expectancy dataset, gdp_lifeExpectancy.csv. Suppose we want to group by the observations by continent.\n\ndata = pd.read_csv('./Datasets/gdp_lifeExpectancy.csv')\ndata.head()\n\n\n\n\n\n  \n    \n      \n      country\n      continent\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      Asia\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1\n      Afghanistan\n      Asia\n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      2\n      Afghanistan\n      Asia\n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      3\n      Afghanistan\n      Asia\n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      4\n      Afghanistan\n      Asia\n      1972\n      36.088\n      13079460\n      739.981106\n    \n  \n\n\n\n\nWe will pass the column continent as an argument to the groupby() method.\n\n#Creating a GroupBy object\ngrouped = data.groupby('continent')\n#This will split the data into groups that correspond to values of the column 'continent'\n\nThe groupby() method returns a GroupBy object.\n\n#A 'GroupBy' objects is created with the `groupby()` function\ntype(grouped)\n\npandas.core.groupby.generic.DataFrameGroupBy\n\n\nThe GroupBy object grouped contains the information of the groups in which the data is distributed. Each observation has been assigned to a specific group of the column(s) used to group the data. However, note that the dataset is not physically split into different DataFrames. For example, in the above case, each observation is assigned to a particular group depending on the value of the continent for that observation. However, all the observations are still in the same DataFrame data.\n\n\n9.1.2 Attributes and methods of the GroupBy object\n\n9.1.2.1 keys\nThe object(s) grouping the data are called key(s). Here continent is the group key. The keys of the GroupBy object can be seen using Its keys attribute.\n\n#Key(s) of the GroupBy object\ngrouped.keys\n\n'continent'\n\n\n\n\n9.1.2.2 ngroups\nThe number of groups in which the data is distributed based on the keys can be seen with the ngroups attribute.\n\n#The number of groups based on the key(s)\ngrouped.ngroups\n\n5\n\n\n\n\n9.1.2.3 groups\nThe groups attribute of the GroupBy object contains the group labels (or names) and the row labels of the observations in each group, as a dictionary.\n\n#The groups (in the dictionary format)\ngrouped.groups\n\n{'Africa': [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, ...], 'Americas': [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 432, 433, 434, 435, ...], 'Asia': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, ...], 'Europe': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 516, 517, 518, 519, ...], 'Oceania': [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103]}\n\n\nThe group names are the keys of the dictionary, while the row labels are the corresponding values\n\n#Group names\ngrouped.groups.keys()\n\ndict_keys(['Africa', 'Americas', 'Asia', 'Europe', 'Oceania'])\n\n\n\n#Group values are the row labels corresponding to a particular group\ngrouped.groups.values()\n\ndict_values([Int64Index([  24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n            ...\n            1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703],\n           dtype='int64', length=624), Int64Index([  48,   49,   50,   51,   52,   53,   54,   55,   56,   57,\n            ...\n            1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643],\n           dtype='int64', length=300), Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n            ...\n            1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679],\n           dtype='int64', length=396), Int64Index([  12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n            ...\n            1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607],\n           dtype='int64', length=360), Int64Index([  60,   61,   62,   63,   64,   65,   66,   67,   68,   69,   70,\n              71, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101,\n            1102, 1103],\n           dtype='int64')])\n\n\n\n\n9.1.2.4 size()\nThe size() method of the GroupBy object returns the number of observations in each group.\n\n#Number of observations in each group\ngrouped.size()\n\ncontinent\nAfrica      624\nAmericas    300\nAsia        396\nEurope      360\nOceania      24\ndtype: int64\n\n\n\n\n9.1.2.5 first()\nThe first non missing element of each group is returned with the first() method of the GroupBy object.\n\n#The first element of the group can be printed using the first() method\ngrouped.first()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n    \n      continent\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Africa\n      Algeria\n      1952\n      43.077\n      9279525\n      2449.008185\n    \n    \n      Americas\n      Argentina\n      1952\n      62.485\n      17876956\n      5911.315053\n    \n    \n      Asia\n      Afghanistan\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      Europe\n      Albania\n      1952\n      55.230\n      1282697\n      1601.056136\n    \n    \n      Oceania\n      Australia\n      1952\n      69.120\n      8691212\n      10039.595640\n    \n  \n\n\n\n\n\n\n9.1.2.6 get_group()\nThis method returns the observations for a particular group of the GroupBy object.\n\n#Observations for individual groups can be obtained using the get_group() function\ngrouped.get_group('Asia')\n\n\n\n\n\n  \n    \n      \n      country\n      continent\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      Asia\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1\n      Afghanistan\n      Asia\n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      2\n      Afghanistan\n      Asia\n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      3\n      Afghanistan\n      Asia\n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      4\n      Afghanistan\n      Asia\n      1972\n      36.088\n      13079460\n      739.981106\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1675\n      Yemen, Rep.\n      Asia\n      1987\n      52.922\n      11219340\n      1971.741538\n    \n    \n      1676\n      Yemen, Rep.\n      Asia\n      1992\n      55.599\n      13367997\n      1879.496673\n    \n    \n      1677\n      Yemen, Rep.\n      Asia\n      1997\n      58.020\n      15826497\n      2117.484526\n    \n    \n      1678\n      Yemen, Rep.\n      Asia\n      2002\n      60.308\n      18701257\n      2234.820827\n    \n    \n      1679\n      Yemen, Rep.\n      Asia\n      2007\n      62.698\n      22211743\n      2280.769906\n    \n  \n\n396 rows × 6 columns"
  },
  {
    "objectID": "Data aggregation.html#data-aggregation-with-groupby-methods",
    "href": "Data aggregation.html#data-aggregation-with-groupby-methods",
    "title": "9  Data aggregation",
    "section": "9.2 Data aggregation with groupby() methods",
    "text": "9.2 Data aggregation with groupby() methods\n\n9.2.1 mean()\nThis method returns the mean of each group of the GroupBy object.\n\n9.2.1.1 Grouping observations\nExample: Find the mean life expectancy, population and GDP per capita for each country since 1952.\nFirst, we’ll group the data such that all observations corresponding to a country make a unique group.\n\n#Grouping the observations by 'country'\ngrouped = data.groupby('country')\n\nNow, we’ll find the mean statistics for each group with the mean() method. The method will be applied on all columns of the DataFrame and all groups.\n\n#Finding the mean stastistic of all columns of the DataFrame and all groups\ngrouped.mean()\n\n\n\n\n\n  \n    \n      \n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n    \n      country\n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      1979.5\n      37.478833\n      1.582372e+07\n      802.674598\n    \n    \n      Albania\n      1979.5\n      68.432917\n      2.580249e+06\n      3255.366633\n    \n    \n      Algeria\n      1979.5\n      59.030167\n      1.987541e+07\n      4426.025973\n    \n    \n      Angola\n      1979.5\n      37.883500\n      7.309390e+06\n      3607.100529\n    \n    \n      Argentina\n      1979.5\n      69.060417\n      2.860224e+07\n      8955.553783\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Vietnam\n      1979.5\n      57.479500\n      5.456857e+07\n      1017.712615\n    \n    \n      West Bank and Gaza\n      1979.5\n      60.328667\n      1.848606e+06\n      3759.996781\n    \n    \n      Yemen, Rep.\n      1979.5\n      46.780417\n      1.084319e+07\n      1569.274672\n    \n    \n      Zambia\n      1979.5\n      45.996333\n      6.353805e+06\n      1358.199409\n    \n    \n      Zimbabwe\n      1979.5\n      52.663167\n      7.641966e+06\n      635.858042\n    \n  \n\n142 rows × 4 columns\n\n\n\nNote that if we wished to retain the continent in the above dataset, we can group the data by both continent and country. If the data is to be grouped by multiple columns, we need to put them within [] brackets:\n\n#Grouping the observations by 'continent' and 'country'\ngrouped = data.groupby(['continent','country'])\n\n#Finding the mean stastistic of all columns of the DataFrame and all groups\ngrouped.mean()\n\n\n\n\n\n  \n    \n      \n      \n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n    \n      continent\n      country\n      \n      \n      \n      \n    \n  \n  \n    \n      Africa\n      Algeria\n      1979.5\n      59.030167\n      1.987541e+07\n      4426.025973\n    \n    \n      Angola\n      1979.5\n      37.883500\n      7.309390e+06\n      3607.100529\n    \n    \n      Benin\n      1979.5\n      48.779917\n      4.017497e+06\n      1155.395107\n    \n    \n      Botswana\n      1979.5\n      54.597500\n      9.711862e+05\n      5031.503557\n    \n    \n      Burkina Faso\n      1979.5\n      44.694000\n      7.548677e+06\n      843.990665\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Europe\n      Switzerland\n      1979.5\n      75.565083\n      6.384293e+06\n      27074.334405\n    \n    \n      Turkey\n      1979.5\n      59.696417\n      4.590901e+07\n      4469.453380\n    \n    \n      United Kingdom\n      1979.5\n      73.922583\n      5.608780e+07\n      19380.472986\n    \n    \n      Oceania\n      Australia\n      1979.5\n      74.662917\n      1.464931e+07\n      19980.595634\n    \n    \n      New Zealand\n      1979.5\n      73.989500\n      3.100032e+06\n      17262.622813\n    \n  \n\n142 rows × 4 columns\n\n\n\nHere the data has been aggregated according to the group keys - continent and country, and a new DataFrame is created that is indexed by the unique values of continent-country.\nFor large datasets, it may be desirable to aggregate only a few columns. For example, if we wish to compute the means of only lifeExp and gdpPercap, then we can filter those columns in the GroupBy object (just like we filter columns in a DataFrame), and then apply the mean() method:\n\ngrouped[['lifeExp','gdpPercap']].mean()\n\n\n\n\n\n  \n    \n      \n      \n      lifeExp\n      gdpPercap\n    \n    \n      continent\n      country\n      \n      \n    \n  \n  \n    \n      Africa\n      Algeria\n      59.030167\n      4426.025973\n    \n    \n      Angola\n      37.883500\n      3607.100529\n    \n    \n      Benin\n      48.779917\n      1155.395107\n    \n    \n      Botswana\n      54.597500\n      5031.503557\n    \n    \n      Burkina Faso\n      44.694000\n      843.990665\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      Europe\n      Switzerland\n      75.565083\n      27074.334405\n    \n    \n      Turkey\n      59.696417\n      4469.453380\n    \n    \n      United Kingdom\n      73.922583\n      19380.472986\n    \n    \n      Oceania\n      Australia\n      74.662917\n      19980.595634\n    \n    \n      New Zealand\n      73.989500\n      17262.622813\n    \n  \n\n142 rows × 2 columns\n\n\n\n\n\n9.2.1.2 Grouping columns\nBy default, the grouping takes place by rows. However, as with several other Pandas methods, grouping can also be done by columns by using the axis = 1 argument.\nExample: Consider we have the above dataset in the wide-format as follows.\n\ndata_wide = data.pivot(index = ['continent','country'],columns = 'year')\ndata_wide\n\n\n\n\n\n  \n    \n      \n      \n      lifeExp\n      ...\n      gdpPercap\n    \n    \n      \n      year\n      1952\n      1957\n      1962\n      1967\n      1972\n      1977\n      1982\n      1987\n      1992\n      1997\n      ...\n      1962\n      1967\n      1972\n      1977\n      1982\n      1987\n      1992\n      1997\n      2002\n      2007\n    \n    \n      continent\n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Africa\n      Algeria\n      43.077\n      45.685\n      48.303\n      51.407\n      54.518\n      58.014\n      61.368\n      65.799\n      67.744\n      69.152\n      ...\n      2550.816880\n      3246.991771\n      4182.663766\n      4910.416756\n      5745.160213\n      5681.358539\n      5023.216647\n      4797.295051\n      5288.040382\n      6223.367465\n    \n    \n      Angola\n      30.015\n      31.999\n      34.000\n      35.985\n      37.928\n      39.483\n      39.942\n      39.906\n      40.647\n      40.963\n      ...\n      4269.276742\n      5522.776375\n      5473.288005\n      3008.647355\n      2756.953672\n      2430.208311\n      2627.845685\n      2277.140884\n      2773.287312\n      4797.231267\n    \n    \n      Benin\n      38.223\n      40.358\n      42.618\n      44.885\n      47.014\n      49.190\n      50.904\n      52.337\n      53.919\n      54.777\n      ...\n      949.499064\n      1035.831411\n      1085.796879\n      1029.161251\n      1277.897616\n      1225.856010\n      1191.207681\n      1232.975292\n      1372.877931\n      1441.284873\n    \n    \n      Botswana\n      47.622\n      49.618\n      51.520\n      53.298\n      56.024\n      59.319\n      61.484\n      63.622\n      62.745\n      52.556\n      ...\n      983.653976\n      1214.709294\n      2263.611114\n      3214.857818\n      4551.142150\n      6205.883850\n      7954.111645\n      8647.142313\n      11003.605080\n      12569.851770\n    \n    \n      Burkina Faso\n      31.975\n      34.906\n      37.814\n      40.697\n      43.591\n      46.137\n      48.122\n      49.557\n      50.260\n      50.324\n      ...\n      722.512021\n      794.826560\n      854.735976\n      743.387037\n      807.198586\n      912.063142\n      931.752773\n      946.294962\n      1037.645221\n      1217.032994\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Europe\n      Switzerland\n      69.620\n      70.560\n      71.320\n      72.770\n      73.780\n      75.390\n      76.210\n      77.410\n      78.030\n      79.370\n      ...\n      20431.092700\n      22966.144320\n      27195.113040\n      26982.290520\n      28397.715120\n      30281.704590\n      31871.530300\n      32135.323010\n      34480.957710\n      37506.419070\n    \n    \n      Turkey\n      43.585\n      48.079\n      52.098\n      54.336\n      57.005\n      59.507\n      61.036\n      63.108\n      66.146\n      68.835\n      ...\n      2322.869908\n      2826.356387\n      3450.696380\n      4269.122326\n      4241.356344\n      5089.043686\n      5678.348271\n      6601.429915\n      6508.085718\n      8458.276384\n    \n    \n      United Kingdom\n      69.180\n      70.420\n      70.760\n      71.360\n      72.010\n      72.760\n      74.040\n      75.007\n      76.420\n      77.218\n      ...\n      12477.177070\n      14142.850890\n      15895.116410\n      17428.748460\n      18232.424520\n      21664.787670\n      22705.092540\n      26074.531360\n      29478.999190\n      33203.261280\n    \n    \n      Oceania\n      Australia\n      69.120\n      70.330\n      70.930\n      71.100\n      71.930\n      73.490\n      74.740\n      76.320\n      77.560\n      78.830\n      ...\n      12217.226860\n      14526.124650\n      16788.629480\n      18334.197510\n      19477.009280\n      21888.889030\n      23424.766830\n      26997.936570\n      30687.754730\n      34435.367440\n    \n    \n      New Zealand\n      69.390\n      70.260\n      71.240\n      71.520\n      71.890\n      72.220\n      73.840\n      74.320\n      76.330\n      77.550\n      ...\n      13175.678000\n      14463.918930\n      16046.037280\n      16233.717700\n      17632.410400\n      19007.191290\n      18363.324940\n      21050.413770\n      23189.801350\n      25185.009110\n    \n  \n\n142 rows × 36 columns\n\n\n\nNow, find the mean GDP per capita, life expectancy and population for each country.\nHere, we can group by the outer level column labels to obtain the means. Also, we need to use the argument axis=1 to indicate that we intend to group columns, instead of rows.\n\ndata_wide.groupby(axis=1,level=0).mean()\n\n\n\n\n\n  \n    \n      \n      \n      gdpPercap\n      lifeExp\n      pop\n    \n    \n      continent\n      country\n      \n      \n      \n    \n  \n  \n    \n      Africa\n      Algeria\n      4426.025973\n      59.030167\n      1.987541e+07\n    \n    \n      Angola\n      3607.100529\n      37.883500\n      7.309390e+06\n    \n    \n      Benin\n      1155.395107\n      48.779917\n      4.017497e+06\n    \n    \n      Botswana\n      5031.503557\n      54.597500\n      9.711862e+05\n    \n    \n      Burkina Faso\n      843.990665\n      44.694000\n      7.548677e+06\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Europe\n      Switzerland\n      27074.334405\n      75.565083\n      6.384293e+06\n    \n    \n      Turkey\n      4469.453380\n      59.696417\n      4.590901e+07\n    \n    \n      United Kingdom\n      19380.472986\n      73.922583\n      5.608780e+07\n    \n    \n      Oceania\n      Australia\n      19980.595634\n      74.662917\n      1.464931e+07\n    \n    \n      New Zealand\n      17262.622813\n      73.989500\n      3.100032e+06\n    \n  \n\n142 rows × 3 columns\n\n\n\n\n\n\n9.2.2 Practice exercise 1\nRead the table consisting of GDP per capita of countries from the webpage: https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita .\nTo only read the relevant table, read the tables that contain the word ‘Country’.\nEstimate the GDP per capita of each country as the average of the estimates of the three agencies - IMF, United Nations and World Bank.\nWe need to do a bit of data cleaning before we could directly use the groupby() function. Follow the steps below to answer this question:\n\nSet the first 3 columns containing country, sub-region and region as hierarchical row labels.\nApply the following function on all the columns to convert them to numeric: f = lambda x:pd.to_numeric(x,errors = 'coerce')\nNow use groupby() to find estimate the GDP per capita for each country.\n\n\n\n9.2.3 agg()"
  },
  {
    "objectID": "student_solutions.html",
    "href": "student_solutions.html",
    "title": "10  Alternative solutions by students",
    "section": "",
    "text": "This chapter will consist of solutions suggested by students of the STAT303-1 Fall 2022 class, which are more efficient than those presented in the book. However, these solutions do not replace those in the book to preserve the simplicity of the book."
  },
  {
    "objectID": "student_solutions.html#missing-value-imputation-based-on-correlated-variables-in-the-data",
    "href": "student_solutions.html#missing-value-imputation-based-on-correlated-variables-in-the-data",
    "title": "10  Alternative solutions by students",
    "section": "10.1 Missing value imputation based on correlated variables in the data",
    "text": "10.1 Missing value imputation based on correlated variables in the data\nThe code below refers to Section 7.1.5.3 of the book. Students have proposed some ways to avoid a for loop in the code, which will lead to parallel computations, thereby saving execution time.\n\n10.1.1 Original code (in the book)\n\nstart_time = tm.time()\ngdp_imputed_data = gdp_missing_values_data.copy()\nfor cont in avg_gdpPerCapita.index:\n    gdp_imputed_data.loc[(gdp_imputed_data.continent==cont) & (gdp_imputed_data.gdpPerCapita.isnull()),\n                     'gdpPerCapita']=avg_gdpPerCapita[cont]\nplot_actual_vs_predicted()\nprint(\"Time taken to execute code: \",np.round(tm.time()-start_time,3), \"seconds\")\n\nRMSE= 25473.20645170116\nTime taken to execute code:  0.032 seconds\n\n\n\n\n\nBelow are some more efficient ways to impute the missing values, as they avoid using the for loop.\n\n\n10.1.2 Alternative code 1:\nBy Victoria Shi\n\nstart_time = tm.time()\ngdp_imputed_data = gdp_missing_values_data.copy()\ngdp_imputed_data[\"gdpPerCapita\"] =  gdp_missing_values_data[['continent','gdpPerCapita']].groupby('continent').transform(lambda x: x.fillna(x.mean()))\nplot_actual_vs_predicted()\nprint(\"Time taken to execute code: \",np.round(tm.time()-start_time,3), \"seconds\")\n\nRMSE= 25473.20645170116\nTime taken to execute code:  0.024 seconds\n\n\n\n\n\n\n\n10.1.3 Alternative code 2:\nBy Elijah Nacar\n\nstart_time = tm.time()\ngdp_imputed_data = gdp_missing_values_data.copy()\ngdp_imputed_data.gdpPerCapita = gdp_imputed_data.apply(lambda x: avg_gdpPerCapita[x['continent']] if pd.isnull(x['gdpPerCapita']) else x['gdpPerCapita'],axis=1)\nplot_actual_vs_predicted()\nprint(\"Time taken to execute code: \",np.round(tm.time()-start_time,3), \"seconds\")\n\nRMSE= 25473.20645170116\nTime taken to execute code:  0.024 seconds\n\n\n\n\n\n\n\n10.1.4 Alternative code 3:\nBy Erica Zhou\n\nstart_time = tm.time()\ngdp_imputed_data = gdp_missing_values_data.copy()\n\ncond_list = [gdp_imputed_data['continent'] == 'Africa',gdp_imputed_data['continent'] == 'Asia',\n             gdp_imputed_data['continent'] == 'Europe',gdp_imputed_data['continent'] == 'North America',\n             gdp_imputed_data['continent'] == 'Oceania',gdp_imputed_data['continent'] == 'South America']\n\nchoice_list = list(avg_gdpPerCapita)\n\ngdp_imputed_data.gdpPerCapita = np.select(cond_list, choice_list, gdp_imputed_data.gdpPerCapita)\nplot_actual_vs_predicted()\nprint(\"Time taken to execute code: \",np.round(tm.time()-start_time,3), \"seconds\")\n\nRMSE= 25473.20645170116\nTime taken to execute code:  0.024 seconds"
  },
  {
    "objectID": "student_solutions.html#binning-with-equal-sized-bins",
    "href": "student_solutions.html#binning-with-equal-sized-bins",
    "title": "10  Alternative solutions by students",
    "section": "10.2 Binning with equal sized bins",
    "text": "10.2 Binning with equal sized bins\nThe code below refers to Section 7.2.2 of the book. A student has proposed a way to avoid the for loop in the code, which will lead to parallel computations, thereby saving execution time.\n\n10.2.1 Original code (in the book)\n\n#Bootstrapping to find 95% confidence intervals of Graduation Rate of US universities based on average expenditure per student\nstart_time = tm.time()\nfor expend_bin in college.Expend_bin.unique():\n    data_sub = college.loc[college.Expend_bin==expend_bin,:]\n    samples = np.random.choice(data_sub['Grad.Rate'], size=(10000,data_sub.shape[0]))\n    print(\"95% Confidence interval of Grad.Rate for \"+expend_bin+\" univeristies = [\"+str(np.round(np.percentile(samples.mean(axis=1),2.5),2))+\",\"+str(np.round(np.percentile(samples.mean(axis=1),97.5),2))+\"]\")\nprint(\"Time taken to execute code: \",np.round(tm.time()-start_time,3), \"seconds\")\n\n95% Confidence interval of Grad.Rate for Low expend univeristies = [55.31,59.35]\n95% Confidence interval of Grad.Rate for High expend univeristies = [71.03,74.9]\n95% Confidence interval of Grad.Rate for Med expend univeristies = [64.17,67.95]\nTime taken to execute code:  0.151 seconds\n\n\n\n\n10.2.2 Alternative code:\nBy Victoria Shi\n\nstart_time = tm.time()\ndef confidence_interval(df):\n    samples = np.random.choice(df['Grad.Rate'], size=(10000, df.shape[0]))\n    print(\"95% Confidence interval of Grad.Rate for \"+df[\"Expend_bin\"].iloc[0]+\" univeristies = [\"+str(np.round(np.percentile(samples.mean(axis=1),2.5),2))+\",\"+str(np.round(np.percentile(samples.mean(axis=1),97.5),2))+\"]\")\ncollege.groupby('Expend_bin').apply(confidence_interval)\nprint(\"Time taken to execute code: \",np.round(tm.time()-start_time,3), \"seconds\")\n\n95% Confidence interval of Grad.Rate for Low expend univeristies = [55.35,59.35]\n95% Confidence interval of Grad.Rate for Med expend univeristies = [64.16,67.95]\n95% Confidence interval of Grad.Rate for High expend univeristies = [71.05,74.96]\nTime taken to execute code:  0.139 seconds"
  },
  {
    "objectID": "Assignment 1 (Reading data).html",
    "href": "Assignment 1 (Reading data).html",
    "title": "11  Assignment 1 (Reading data)",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 6th October 2022 at 11:59 pm."
  },
  {
    "objectID": "Assignment 1 (Reading data).html#i-1.",
    "href": "Assignment 1 (Reading data).html#i-1.",
    "title": "11  Assignment 1 (Reading data)",
    "section": "12.1 I-1.",
    "text": "12.1 I-1.\n\n12.1.1 I-1(a)\nUse list comprehension to produce a list of the gaps between consecutive entries in T, i.e, the increase in GDP per capita with respect to the previous year. The list with gaps should look like: [60, 177, …].\n(6 points)\n\n\n12.1.2 I-1(b)\nUse the list comprehension developed in (a) to find the maximum gap size, i.e, the maximum increase in GDP per capita.\n(2 points)\n\n\n12.1.3 I-1(c)\nUse the list comprehension developed in (a) to find the percentage of gaps higher than $1000.\n(5 points)"
  },
  {
    "objectID": "Assignment 1 (Reading data).html#i-2",
    "href": "Assignment 1 (Reading data).html#i-2",
    "title": "11  Assignment 1 (Reading data)",
    "section": "12.2 I-2",
    "text": "12.2 I-2\n\n12.2.1 I-2(a)\nCreate a dictionary D, where the key is the year, and value for the key is the increase in GDP per capita in that year with respect to the previous year, i.e., the gaps computed in part (1).\n(6 points)\n\n\n12.2.2 I-2(b)\nUse the dictionary D to find the year when the GDP per capita increase from the previous year was the maximum.\n(4 points)\n\n\n12.2.3 I-2(c)\nUse the dictionary D to find the years when the GDP per capita decreased with respect to the previous year.\n(4 points)"
  },
  {
    "objectID": "Assignment 1 (Reading data).html#ii-1",
    "href": "Assignment 1 (Reading data).html#ii-1",
    "title": "11  Assignment 1 (Reading data)",
    "section": "13.1 II-1",
    "text": "13.1 II-1\nRead the data on ted talks.\n(2 points)"
  },
  {
    "objectID": "Assignment 1 (Reading data).html#ii-2",
    "href": "Assignment 1 (Reading data).html#ii-2",
    "title": "11  Assignment 1 (Reading data)",
    "section": "13.2 II-2",
    "text": "13.2 II-2\nFind the number of talks in the dataset.\n(2 points)"
  },
  {
    "objectID": "Assignment 1 (Reading data).html#ii-3",
    "href": "Assignment 1 (Reading data).html#ii-3",
    "title": "11  Assignment 1 (Reading data)",
    "section": "13.3 II-3",
    "text": "13.3 II-3\nFind the headline, speaker and year_filmed of the talk with the highest number of views.\n(5 points)"
  },
  {
    "objectID": "Assignment 1 (Reading data).html#ii-4",
    "href": "Assignment 1 (Reading data).html#ii-4",
    "title": "11  Assignment 1 (Reading data)",
    "section": "13.4 II-4",
    "text": "13.4 II-4\nDo the majority of talks have less views than the average number of views for a talk? Justify your answer.\n(4 points)\nHint: Print summary statistics for questions (4) and (5)."
  },
  {
    "objectID": "Assignment 1 (Reading data).html#ii-5",
    "href": "Assignment 1 (Reading data).html#ii-5",
    "title": "11  Assignment 1 (Reading data)",
    "section": "13.5 II-5",
    "text": "13.5 II-5\nDo at least 25% of the talks have more views than the average number of views for a talk? Justify your answer.\n(4 points)"
  },
  {
    "objectID": "Assignment 1 (Reading data).html#ii-6",
    "href": "Assignment 1 (Reading data).html#ii-6",
    "title": "11  Assignment 1 (Reading data)",
    "section": "13.6 II-6",
    "text": "13.6 II-6\n\n13.6.1 II-6(a)\nThe last column of the dataset consists of votes obtained by the talk under different categories, such as Funny, Confusing, Fascinating, etc. For each category, create a new column in the dataset that contains the votes obtained by the tedtalk in that category. Print the first 5 rows of the updated dataset.\n(20 points)\n\n\n13.6.2 II-6(b)\nWith the data created in (a), find the headline of the talk that received the highest number of Confusing votes.\n(5 points)\n\n\n13.6.3 II-6(c)\nWith the data created in (a), find the headline and the year of the talk that received the highest percentage of votes in the Fascinating category.\n\\[\\text{Percentage of } \\textit{Fascinating} \\text{ votes for a ted talk} = \\frac{Number \\ of \\  votes \\ in \\ the \\ category - \\ Fascinating}{Total \\ votes \\ in \\ all  \\ categories}\\]\n(10 points)"
  },
  {
    "objectID": "Assignment 1 (Reading data).html#iii-1",
    "href": "Assignment 1 (Reading data).html#iii-1",
    "title": "11  Assignment 1 (Reading data)",
    "section": "14.1 III-1",
    "text": "14.1 III-1\nDownload the data set “univ.txt”. Read it with python.\n(2 points)"
  },
  {
    "objectID": "Assignment 1 (Reading data).html#iii-2",
    "href": "Assignment 1 (Reading data).html#iii-2",
    "title": "11  Assignment 1 (Reading data)",
    "section": "14.2 III-2",
    "text": "14.2 III-2\n\n14.2.1 III-2(a)\nFind summary statistics of the data. Based on the statistics, answer parts b-e.\n(1 point)\n\n\n14.2.2 III-2(b)\nHow many universities are there in the data set?\n(2 points)\n\n\n14.2.3 III-2(c)\nEstimate the maximum Tuition and fees among universities that are in the bottom 25% when ranked by total tuition and fees.\n(3 points)\n\n\n14.2.4 III-2(d)\nHow many universities share the ranking of 220? (If s universities share the same rank, say r, then the next lower rank is r+s, and all the ranks in between r and r+s are dropped)\n(5 points)\n\n\n14.2.5 III-2(e)\nCan you find the mean Tuition and fees for an undergrad student in the US from the summary statistics? Justify your answer.\n(3 points)"
  },
  {
    "objectID": "Assignment 1 (Reading data).html#iii-3",
    "href": "Assignment 1 (Reading data).html#iii-3",
    "title": "11  Assignment 1 (Reading data)",
    "section": "14.3 III-3",
    "text": "14.3 III-3\nFind the average Tuition and fees for an undergrad student in the US.\n(5 points)"
  },
  {
    "objectID": "Assignment 2 (NumPy).html",
    "href": "Assignment 2 (NumPy).html",
    "title": "12  Assignment 2 (NumPy)",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 13th October 2022 at 11:59 pm."
  },
  {
    "objectID": "Assignment 2 (NumPy).html#a",
    "href": "Assignment 2 (NumPy).html#a",
    "title": "12  Assignment 2 (NumPy)",
    "section": "13.1 1(a)",
    "text": "13.1 1(a)\nAir quality sensors are used to measure the amount of contaminants in air. This question will guide you in finding the location of installing 50 air quality sensors in the State of Colorado, such that they are as far away from each other as possible. The approach below is a greedy algorithm to find an approximate Maximin design.\nThe file colorado_coordinate_grid.txt contains the coordinate-pairs (latitude and longitude) of potential locations for installing an air quality sensor.\nRead the file with NumPy. How many coordinate-pairs are there in the file?\nNote that:\n\nA coordinate-pair means a latitude-longitude pair.\n‘Air quality sensor’ will be referred as ‘sensor’ in the questions below for brevity.\n\n(4 points)"
  },
  {
    "objectID": "Assignment 2 (NumPy).html#b",
    "href": "Assignment 2 (NumPy).html#b",
    "title": "12  Assignment 2 (NumPy)",
    "section": "13.2 1(b)",
    "text": "13.2 1(b)\nThe first sensor is to be installed closest to Denver (closest in terms of Euclidean distance). Find the coordinate-pair of the location where the first sensor will be installed. The coordinate-pair of Denver is: [39.7392\\(^{\\circ}\\) N, 104.9903\\(^{\\circ}\\) W]\nNote that the suffixes \\(^{\\circ}\\) N and \\(^{\\circ}\\) W are omitted in the file colorado_coordinate_grid.txt.\nHint: Broadcasting\n(4 points)"
  },
  {
    "objectID": "Assignment 2 (NumPy).html#c",
    "href": "Assignment 2 (NumPy).html#c",
    "title": "12  Assignment 2 (NumPy)",
    "section": "13.3 1(c)",
    "text": "13.3 1(c)\nFind the coordinate-pair of the installation of the next sensor, such that it is as far as possible from the first sensor installed near Denver.\nHint: Broadcasting\n(4 points)"
  },
  {
    "objectID": "Assignment 2 (NumPy).html#d",
    "href": "Assignment 2 (NumPy).html#d",
    "title": "12  Assignment 2 (NumPy)",
    "section": "13.4 1(d)",
    "text": "13.4 1(d)\nStack the coordinate-pairs of the first and second sensors vertically to obtain a 2 x 2 NumPy array. Name the array as air_sensor_coordinates.\nRun the code below to check if your results seem correct. The coordinate-pairs of the two air quality sensors will be marked as blue dots.\n(4 points)\n\n\nCode\nimport matplotlib.pyplot as plt\ndef sensor_viz():\n    img = plt.imread(\"colorado.jpg\")\n    fig, ax = plt.subplots(figsize=(10, 100),dpi=80)\n    fig.set_size_inches(10.5, 15)\n    ax.imshow(img,extent=[-109, -102, 37, 41])\n    plt.scatter(y = air_sensor_coordinates[:,0], x = -air_sensor_coordinates[:,1])\n    plt.xlim(-109.05,-101.95)\n    plt.ylim(36.95,41.05)\n    plt.xlabel(\"Longitude\")\n    plt.ylabel(\"Latitude\")\nsensor_viz()"
  },
  {
    "objectID": "Assignment 2 (NumPy).html#e",
    "href": "Assignment 2 (NumPy).html#e",
    "title": "12  Assignment 2 (NumPy)",
    "section": "13.5 1(e)",
    "text": "13.5 1(e)\nNow you need to find the coordinate-pair for installing the third sensor such that it is far away from the two already-installed sensors. Proceed as follows:\n\nFind the minimum distance of each coordinate-pair in colorado_coordinate_grid.txt from the two already installed sensors. For example, if a coordinate-pair is at a distance of 5 units from the first sensor, and 10 units from the second sensor, then its minimum distance from the sensors will be \\(\\min(5,10) = 5\\) units.\nSelect the coordinate-pair (from colorado_coordinate_grid.txt) whose minimum distance from the two already installed sensors is the maximum.\nStack the coordinate-pair of the third air quality sensor vertically on the array air_sensor_coordinates.\n\nCall the function sensor_viz() to check if your results seem correct. The coordinate-pairs of the three air quality sensors will be marked as blue dots.\nHint:\nFor step (1) above:\n\nDefine a function which computes the distances of a coordinate-pair from all the coordinates of air_sensor_coordinates, and returns the minimum distance.\nApply the function on all the coordinate-pairs in colorado_coordinate_grid.txt using the NumPy function apply_along_axis().\n\n(25 points)"
  },
  {
    "objectID": "Assignment 2 (NumPy).html#f",
    "href": "Assignment 2 (NumPy).html#f",
    "title": "12  Assignment 2 (NumPy)",
    "section": "13.6 1(f)",
    "text": "13.6 1(f)\nYou need to find 47 more coordinate-pairs to install air quality sensors well-spread across Colorado. We will generalize the steps in 1(e)-5 to proceed as follows:\n\nSuppose you have already found the coordinate-pairs for the installation of i sensors.\nFind the minimum distance of each coordinate in colorado_coordinate_grid.txt from the i already installed sensors. For example, if a coordinate-pair is at a distance of \\(d_1\\) from the first sensor, \\(d_2\\) from the second sensor,…, and \\(d_i\\) from the \\(i^{th}\\) sensor, then its minimum distance from the sensors will be \\(min(d_1, d_2, ..., d_i\\)).\nSelect the \\(i+1^{th}\\) coordinate-pair (from colorado_coordinate_grid.txt) as the one whose minimum distance from the \\(i\\) already installed sensors is the maximum.\n\nCall the function sensor_viz() to check if your results seem correct. You should see 50 blue dots well spread across Colorado.\n(10 points)"
  },
  {
    "objectID": "Assignment 2 (NumPy).html#a-1",
    "href": "Assignment 2 (NumPy).html#a-1",
    "title": "12  Assignment 2 (NumPy)",
    "section": "14.1 2(a)",
    "text": "14.1 2(a)\nCreate a numpy array where the first column is all \\(1\\)s, the second column is a range of numbers from 1 to the total number of months from June 1 2017 to March 1 2020 and the third column is \\(\\sin(2*\\pi*x/12)\\) values with \\(x\\) values as plugged-in in the second column.\n(10 points)"
  },
  {
    "objectID": "Assignment 2 (NumPy).html#b-1",
    "href": "Assignment 2 (NumPy).html#b-1",
    "title": "12  Assignment 2 (NumPy)",
    "section": "14.2 2(b)",
    "text": "14.2 2(b)\nCreate an array from the list model_params.\n(3 points)"
  },
  {
    "objectID": "Assignment 2 (NumPy).html#c-1",
    "href": "Assignment 2 (NumPy).html#c-1",
    "title": "12  Assignment 2 (NumPy)",
    "section": "14.3 2(c)",
    "text": "14.3 2(c)\nUse matrix multiplication to get the monthly sales estimates for each month in the range: June 1 2017 and March 1, 2020.\n(8 points)"
  },
  {
    "objectID": "Assignment 2 (NumPy).html#d-1",
    "href": "Assignment 2 (NumPy).html#d-1",
    "title": "12  Assignment 2 (NumPy)",
    "section": "14.4 2(d)",
    "text": "14.4 2(d)\nFind the total sales between June 1 2017 and March 1, 2020.\n(3 points)"
  },
  {
    "objectID": "Assignment 2 (NumPy).html#a-2",
    "href": "Assignment 2 (NumPy).html#a-2",
    "title": "12  Assignment 2 (NumPy)",
    "section": "15.1 3(a)",
    "text": "15.1 3(a)\nWithout using NumPy, compute the:\n\nConfidence interval of mean exercise_minutes, and\nTime taken to execute the code\n\nHints:\n\nYou may use the library random.\nYou may use the library time for computing the time taken to execute the code.\n\n(12 points)"
  },
  {
    "objectID": "Assignment 2 (NumPy).html#b-2",
    "href": "Assignment 2 (NumPy).html#b-2",
    "title": "12  Assignment 2 (NumPy)",
    "section": "15.2 3(b)",
    "text": "15.2 3(b)\nUsing NumPy, and without using loops, compute the:\n\nConfidence interval of mean exercise_minutes, and\nTime taken to execute the code\n\n(12 points)"
  },
  {
    "objectID": "Assignment 2 (NumPy).html#c-2",
    "href": "Assignment 2 (NumPy).html#c-2",
    "title": "12  Assignment 2 (NumPy)",
    "section": "15.3 3(c)",
    "text": "15.3 3(c)\nReport the ratio of time taken to execute the code wihout NumPy to the time taken to execute the code with NumPy.\n(1 point)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html",
    "href": "Assignment 3 (Pandas).html",
    "title": "13  Assignment 3 (Pandas)",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 21st October 2022 at 11:59 pm. There is an optional bonus question worth 20 points. You can score a maximum of 120 (out of 100) points.\nYou are not allowed to use a for loop or any other kind of loop in this assignment."
  },
  {
    "objectID": "Assignment 3 (Pandas).html#a",
    "href": "Assignment 3 (Pandas).html#a",
    "title": "13  Assignment 3 (Pandas)",
    "section": "14.1 1(a)",
    "text": "14.1 1(a)\nRead the file social_indicator.txt with python. Set the first column as the index when reading the file. How many observations and variables are there in the data?\n(4 points)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#b",
    "href": "Assignment 3 (Pandas).html#b",
    "title": "13  Assignment 3 (Pandas)",
    "section": "14.2 1(b)",
    "text": "14.2 1(b)\nWhich variables have the strongest and weakest correlations with GDP per capita? Note that lifeFemale and lifeMale are the female and male life expectancies respectively.\n(4 points)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#c",
    "href": "Assignment 3 (Pandas).html#c",
    "title": "13  Assignment 3 (Pandas)",
    "section": "14.3 1(c)",
    "text": "14.3 1(c)\nDoes the male economic activity have a positive or negative correlation with GDP per capita? Did you expect the positive/negative correlation? If not, why do you think you are observing that correlation?\n(4 points)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#d",
    "href": "Assignment 3 (Pandas).html#d",
    "title": "13  Assignment 3 (Pandas)",
    "section": "14.4 1(d)",
    "text": "14.4 1(d)\nWhat is the rank of the US amongst all countries in terms of GDP per capita? Which countries lie immediately above, and immediately below the US in the ranking in terms of GDP per capita?\nNote that:\n\nThe US is mentioned as United States in the data.\nThe country with the highest GDP per capita will have rank 1, the country with the second highest GDP per capita will have rank 2, and so on.\n\nHint: rank()\n(4 points)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#e",
    "href": "Assignment 3 (Pandas).html#e",
    "title": "13  Assignment 3 (Pandas)",
    "section": "14.5 1(e)",
    "text": "14.5 1(e)\nWhich country or countries rank among the top 20 in terms of each of these social indicators - economicActivityFemale, economicActivityMale, gdpPerCapita, lifeFemale, lifeMale? For each of these social indicators, the country having the largest value ranks 1 for that indicator.\n(6 points)\nHint:\n\nUse rank(). Note that this method is different from the method given in the hint of 1(d). This method is of the DataFrame class, while the one in 1(d) is of the Series class. This part of the hint is just for your understanding. You don’t need to write any code in this part.\nUsing rank(), get the DataFrame consisting of the ranks of countries on each of the relevant social indicators (one line of code).\nIn the DataFrame obtained in (2), filter the rows for which the maximum rank is less than or equal to 20 (one line of code)."
  },
  {
    "objectID": "Assignment 3 (Pandas).html#f",
    "href": "Assignment 3 (Pandas).html#f",
    "title": "13  Assignment 3 (Pandas)",
    "section": "14.6 1(f)",
    "text": "14.6 1(f)\nOn which social indicator does the US have its worst ranking, and what is the rank? Note that for illiteracyFemale, illiteracyMale, and infantMortality, the country having the lowest value will rank 1, in contrast to the other social indicators.\n(8 points)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#g",
    "href": "Assignment 3 (Pandas).html#g",
    "title": "13  Assignment 3 (Pandas)",
    "section": "14.7 1(g)",
    "text": "14.7 1(g)\nFind all the countries that have a lower GDP per capita than the US, despite having lower illiteracy rates (for both genders), higher economic activity (for both genders), higher life expectancy (for both genders), and lower infant mortality rate than the US?\n(6 points)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#a-1",
    "href": "Assignment 3 (Pandas).html#a-1",
    "title": "13  Assignment 3 (Pandas)",
    "section": "15.1 2(a)",
    "text": "15.1 2(a)\nUse the column geographic_location to create a new column called continent. Merge the values of the geographic_location column appropriately to obtain 6 distinct values for the continent column – Asia, Africa, North America, South America, Europe and Oceania. Drop the column geographic_location. Print the first 5 observations of the updated DataFrame.\n(8 points)\nHint:\n\nUse value_counts() to see the values of geographic_location. The code if 'Asia' in 'something' will return True if ‘something’ contains the string ‘Asia’, for example, if ‘something’ is ‘North Asia’, the code with return True. This part of the hint is just for your understanding. You don’t need to write any code in this part.\nApply a lambda function on the Series geographic_location to replace a string that contains ‘Asia’ with ‘Asia’, replace a string that contains ‘Europe’ with ‘Europe’, and replace a string that contains ‘Africa’ with ‘Africa’. This will be a single line of code.\nRename the column georgaphic_location to continent."
  },
  {
    "objectID": "Assignment 3 (Pandas).html#b-1",
    "href": "Assignment 3 (Pandas).html#b-1",
    "title": "13  Assignment 3 (Pandas)",
    "section": "15.2 2(b)",
    "text": "15.2 2(b)\nSort the column labels lexicographically. Drop the columns region and contraception. Print the first 5 observations of the updated DataFrame.\nHint: sort_index()\n(4 points)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#c-1",
    "href": "Assignment 3 (Pandas).html#c-1",
    "title": "13  Assignment 3 (Pandas)",
    "section": "15.3 2(c)",
    "text": "15.3 2(c)\nFind the percentage of the total countries in each continent.\nHint: One line of code with value_counts() and shape\n(4 points)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#d-1",
    "href": "Assignment 3 (Pandas).html#d-1",
    "title": "13  Assignment 3 (Pandas)",
    "section": "15.4 2(d)",
    "text": "15.4 2(d)\nWhich country has the highest GDP per capita? Let us call it country \\(G\\).\n(4 points)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#e-1",
    "href": "Assignment 3 (Pandas).html#e-1",
    "title": "13  Assignment 3 (Pandas)",
    "section": "15.5 2(e)",
    "text": "15.5 2(e)\nWe need to find the African country that is the closest to country \\(G\\) with regard to social indicators. Perform the following steps:\n\n15.5.1 2(e)(i)\nStandardize each of the social indicators to a standard normal distribution so that all of them are on the same scale (remember to exclude GDP per capita from social indicators).\nHint:\n\nFor scaling a random variable to standard normal, subtract the mean from each value of the variable, and divide by its standard deviation.\nUse the apply method with a lambda function to scale all the social indicators to standard normal.\nThe above (1) and (2) together is a single line of code.\n\n(6 points)\n\n\n15.5.2 2(e)(ii)\nCompute the Manhattan distance between country \\(G\\) and each of the African countries, based on the scaled social indicators.\nHint:\n\nBroadcast a Series to a DataFrame\nThe Manhattan distance between two points \\((x_1, x_2, ..., x_p)\\) and \\((y_1, y_2, ..., y_p)\\) is \\(|x_1 - y_1| + |x_2 - y_2| + ... + |x_p-y_p|\\), where \\(|.|\\) stands for absolute value (for example, \\(|-2| = 2; |3| = 3\\)).\n\n(8 points)\n\n\n15.5.3 2(e)(iii)\nIdentify the African country, say country \\(A\\), with the least Manhattan distance to country \\(G\\).\n(8 points)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#f-1",
    "href": "Assignment 3 (Pandas).html#f-1",
    "title": "13  Assignment 3 (Pandas)",
    "section": "15.6 2(f)",
    "text": "15.6 2(f)\nFind the correlation between the Manhattan distance from country \\(G\\) and GDP per capita for African countries.\n(6 points)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#g-1",
    "href": "Assignment 3 (Pandas).html#g-1",
    "title": "13  Assignment 3 (Pandas)",
    "section": "15.7 2(g)",
    "text": "15.7 2(g)\nBased on the correlation coefficient in \\(2(f)\\), do you think African countries should try to emulate the social characteristics of country \\(G\\)? Justify your answer.\n(4 points)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#a-2",
    "href": "Assignment 3 (Pandas).html#a-2",
    "title": "13  Assignment 3 (Pandas)",
    "section": "16.1 3(a)",
    "text": "16.1 3(a)\nPrint the patient IDs of all the patients with prediabetes condition.\n(4 points)"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#b-2",
    "href": "Assignment 3 (Pandas).html#b-2",
    "title": "13  Assignment 3 (Pandas)",
    "section": "16.2 3(b)",
    "text": "16.2 3(b)\nMake a subset of the data with only prediabetes patients. How many prediabetes patients are there?\n(4 points)\nHint: .isin()"
  },
  {
    "objectID": "Assignment 3 (Pandas).html#c-2",
    "href": "Assignment 3 (Pandas).html#c-2",
    "title": "13  Assignment 3 (Pandas)",
    "section": "16.3 3(c)",
    "text": "16.3 3(c)\nWhat proportion of the total HEALTHCARE_EXPENSES of all the patients correspond to the HEALTHCARE_EXPENSES of prediabetes patients.\n(4 points)"
  },
  {
    "objectID": "Assignment 4 (Data Visualization).html",
    "href": "Assignment 4 (Data Visualization).html",
    "title": "14  Assignment 4 (Data Visualization)",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 4th November 2022 at 11:59 pm.\nSome questions in this assignment do not have a single correct answer. As data visualization is subject to interpretation, any logically sound answer / explanation is acceptable.\nThere is a bonus question worth 30 points. However, there is no partial credit for the bonus question. You will get 30 or 0. If everything is correct, you can score 130 out of 100 in the assignment."
  },
  {
    "objectID": "Assignment 4 (Data Visualization).html#time-trend",
    "href": "Assignment 4 (Data Visualization).html#time-trend",
    "title": "14  Assignment 4 (Data Visualization)",
    "section": "14.1 Time trend",
    "text": "14.1 Time trend\nLet us analyze if the profitability of a movie is associated with the time of its release.\n\n14.1.1 Month of release\n\n14.1.1.1 \nMake an appropriate plot to visualize the mean profit of movies released each month.\nHint:\n\nUse the Pandas function to_datetime() to convert Release Date to a datetime datatype.\nUse the library datetime to extract the month from Release Date.\n\n(6 points)\n\n\n14.1.1.2 \nBased on the plot, which seasons have been the most and least profitable (on an average) for a movie release. Don’t worry about the exact start and end date of seasons. Don’t perform any computations. Just make comments based on the plot. You can use seasons such as early summer, late spring etc.\n(2 points)\n\n\n\n14.1.2 Month of release with number of movies in each genre\n\n14.1.2.1 \nNow that we know the most profitable season for releasing movies, let us visualize if some genres are more popular during certain seasons.\nUse the code below to create a new column called genre.\n\n\nCode\n#Combining Major Genre\nmovies_data['genre'] = movies_data['Major Genre'].apply(lambda x:'Comedy' if x!=None and 'Comedy' in x else 'Horror' if x!=None and 'Thriller' in x else 'Action/Adventure' if x!=None and ('Action' in x or 'Adventure' in x) else 'Musical/Western' if x!=None and ('Musical' in x or 'Western' in x or 'Concert' in x) else x)\n\n\nMake an appropriate plot to visualize the number of movies released for each genre in each calendar month.\n(8 points)\nHint:\n\nUse barplot() with estimator as len\nUse the hue argument\n\n\n\n14.1.2.2 \nBased on the above plot, which genre is the most popular during the most profitable season of release? And which genre is the most popular during the least profitable season of release?\n(2 points)\n\n\n\n14.1.3 Month of release with proportion of movies in each genre\n\n14.1.3.1 \nVisualize the proportion of movies in each genre for each month of release.\nUse the code below to re-arrange your data that will help with creating the visualization\n\n\nCode\ngenre_proportion_release_month = pd.crosstab(index=movies_data['release_month'],\n                             columns=movies_data['genre'],\n                             normalize=\"index\")\ngenre_proportion_release_month.head()\n\n\nHint:\n\nMake a 100% stacked barplot with the Pandas plot() function\nUse the argument bbox_to_anchor with the Matplotlib function legend() to place the legend outside the plot area.\n\n(8 points)\n\n\n14.1.3.2 \nWhich genre is the most popular during the month of May, and which one is the most popular during December?\n(2 points)\n\n\n\n14.1.4 Year of release with genre\n\n14.1.4.1 \nMake an appropriate figure to visualize the average profit of movies of each genre for each year. Consider only the movies released from 1991 to 2010. Also show the 95% confidence interval in the average profit.\nHint:\n\nUse the library datetime to extract year from Release Date.\nUse the Seaborn Facetgrid() object.\nA figure can have multiple subplots. Put the figure for each genre in a separate subplot.\n\n(6 points)\n\n\n14.1.4.2 \nBased on the figure above, which genre’s profitability seems to be increasing over the years, and which genre has the least uncertainty in profit for most of the years.\n(2 points)"
  },
  {
    "objectID": "Assignment 4 (Data Visualization).html#associations",
    "href": "Assignment 4 (Data Visualization).html#associations",
    "title": "14  Assignment 4 (Data Visualization)",
    "section": "14.2 Associations",
    "text": "14.2 Associations\n\n14.2.1 Pairplot / heatmap\n\n14.2.1.1 \nMake a pairplot and heatmap of all the continuous variables in the data.\n(8 points)\n\n\n14.2.1.2 \nAre there any trends that you can see in the pairplot, but not in the heatmap?\n(2 points)\n\n\n14.2.1.3 \nBased on the plots in 2(a)(i), which variables are associated with profit?\n(2 points)\n\n\n14.2.1.4 \nAmong the variables listed in 2(a)(iii), select a subset of variables such that none of them are highly associated with each other. The rest of the variables identified in 2(a)(iii) are redundant with regard to association with profit.\n(2 points)\n\n\n\n14.2.2 Nested associations\n\n14.2.2.1 \nUse the code below to create some new columns.\n\n\nCode\nmovies_data['screenplay'] = movies_data.Source.apply(lambda x:'Non-original' if x!='Original Screenplay' else x)\nmovies_data['rating'] = movies_data['MPAA Rating'].apply(lambda x:'R rated' if x=='R' else 'Not R rated')\nmovies_data['fiction'] = movies_data['Creative Type'].apply(lambda x:'Contemporary' if x=='Contemporary Fiction' else 'other')\n\n\nMake an appropriate figure to visualize the association of the number of IMDB votes with profit for each genre (use the variable genre). Which genre has the highest association between profit and IMDB votes?\n(8 points)\n\n\n14.2.2.2 \nMake an appropriate figure to visualize the association between the number of IMDB votes and profit, for each combination of the fiction type (use the variable fiction) and the movie rating (use the variable rating).\nFor which combination of fiction and rating categories do you observe the highest association between IMDB votes and profit?\n(8 points)\nHint: Use row and col attributes of the Seaborn Facetgrid() object.\n\n\n\n14.2.3 Profit based on movie director\n\n14.2.3.1 \nConsider the directors who have directed more than 10 movies (based on the dataset). Make a horizontal barplot that shows the mean profit of the movies of these directors along with the 95% confidence interval. Sort the bars of the barplot such that the director with the highest mean profit is at the top.\nIf the dataset director_with_more_than_10_movies has only those movies that correspond to directors with more than 10 movies, then the following code will give you the order in which the names of the directors must appear in the barplot:\n(8 points)\n\n\nCode\ndirector_with_more_than_10_movies[['Director','profit']].groupby('Director').mean().sort_values(by = 'profit',\n                                            ascending= False).index.to_list()\n\n\n\n\n14.2.3.2 \nBased on the above plot, which director has the highest mean profitability, and which one has the highest variation in profitability?\n(2 points)"
  },
  {
    "objectID": "Assignment 4 (Data Visualization).html#distributions",
    "href": "Assignment 4 (Data Visualization).html#distributions",
    "title": "14  Assignment 4 (Data Visualization)",
    "section": "14.3 Distributions",
    "text": "14.3 Distributions\n\n14.3.1 Distribution of profit based on genre (boxplots)\n\n14.3.1.1 \nMake boxplots to visualize the distribution of profit based on genre. Based on the plot, which genre has the most profitable movies?\n(6 points)\n\n\n14.3.1.2 \nWhich genre has the most variation in profit, and which one has the least?\n(2 points)\n\n\n\n14.3.2 Distribution of profit based on genre (density plots)\n\n14.3.2.1 \nMake density plots of profit based on genre. Adjust the limit on the horizonal axis, so that the plots are clearly visible.\n(6 points)\n\n\n14.3.2.2 \nWhat additional insight / trend can you seen in the above plot that you cannot see with the boxplots?\n(2 points)"
  },
  {
    "objectID": "Assignment 4 (Data Visualization).html#insights",
    "href": "Assignment 4 (Data Visualization).html#insights",
    "title": "14  Assignment 4 (Data Visualization)",
    "section": "14.4 Insights",
    "text": "14.4 Insights\nFrom all the visualizations above, describe the insights you get about the factors associated with the profitability of a movie.\nAlso, elaborate on the extent to which these trends can be generalized. For example, comment on whether these trends be generalized to the current time and all the Hollywood movies? If not, is there any time period or type of movie to which these trends can be applicable?\n(4+ 4 points)"
  },
  {
    "objectID": "Assignment 4 (Data Visualization).html#return-on-investment",
    "href": "Assignment 4 (Data Visualization).html#return-on-investment",
    "title": "14  Assignment 4 (Data Visualization)",
    "section": "14.5 Return on investment",
    "text": "14.5 Return on investment\n\n14.5.1 Impatient investor (daily)\nSuppose there is an investor who only holds the index for a single day (buy yesterday sell today).\nBased on the data,\n\n14.5.1.1 \nShow the histogram graph for all the possible returns.\n\n\n14.5.1.2 \nWhat is the expected return, risk and sharpe ratio?\n\n\n14.5.1.3 \nIs the return significantly greater than zero (a.k.a positive return) (use a threshold 0.01 for \\(p\\)-value) ?\nHINT: use scipy.stats.ttest_1samp to do one-sided mean test. (We ignore the fact that T-test requires the data are sampled from a population of normal distribution, which might not be true in this exercise)\n(6 points)\n\n\n\n14.5.2 Patient Investor (yearly)\nSuppose there is an investor who will hold the index for a year (suppose there are 250 trading days in a year). Do the same analysis as the above:\n\n14.5.2.1 \nShow the histogram graph for all the possible returns.\n\n\n14.5.2.2 \nWhat is the expected return, risk and sharpe ratio?\n\n\n14.5.2.3 \nIs the return significantly greater than zero (a.k.a positive return) (use a threshold 0.01 for \\(p\\)-value) ?\n(6 points)\n\n\n\n14.5.3 From daily to yearly\nExplore how the expected return/risk/shape ratio change as we increase our holding period from 1 day to 1 year(250 days).\nShow/answer:\n\n14.5.3.1 \nAt least how many days do you need to hold the index in order to make a significant positive return (threshold 0.01)?\n\n\n14.5.3.2 \nHow are the returns associated with the risks for different investment strategies?\n\n\n14.5.3.3 \nMake a graph as shown below.\n(18 points)"
  },
  {
    "objectID": "Assignment 5 (Data cleaning and preparation).html",
    "href": "Assignment 5 (Data cleaning and preparation).html",
    "title": "15  Assignment 5 (Data cleaning & preparation)",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 12th November 2022 at 11:59 pm.\nThere is a bonus question worth 20 points, and an ultra bonus question worth 30 points. However, there is no partial credit for these questions. You will get 20 or 0 for the bonus question, and 30 or 0 for the ultra-bonus question. If everything is correct, you can score 150 out of 100 in the assignment."
  },
  {
    "objectID": "Assignment 5 (Data cleaning and preparation).html#missing-value-imputation",
    "href": "Assignment 5 (Data cleaning and preparation).html#missing-value-imputation",
    "title": "15  Assignment 5 (Data cleaning & preparation)",
    "section": "15.1 Missing value imputation",
    "text": "15.1 Missing value imputation\nRead Housing_missing_price.csv as housing_missing_price and Housing_complete.csv as housing_complete. The datasets consist of housing features, like number of bedrooms, bathrooms, etc., and the price. Both datasets are exactly the same, except that Housing_missing_price.csv has some missing values of price. In this question, you will try different methods to impute the missing values of house price in the file Housing_missing_price.csv. You will use the prices in Housing_complete.csv to check the accuracy of your imputation.\nNote that you cannot use Housing_complete.csv to impute missing price in any of the questions. Housing_complete.csv is just to check the accuracy of your imputation, after you have done the imputation. Before imputing the missing price, assume you do not have Housing_complete.csv.\n\n15.1.1 Number of missing values\nHow many values of price are missing in Housing_missing_price.csv?\n(3 points)\n\n\n15.1.2 Indices of missing values\nFind the row labels, where the price is missing. Assign those row labels as an array or a list to index_null_price.\n(4 points)\n\n\n15.1.3 Correlation of continuous variables with price\nFind the continuous variable having the highest correlation with price. Let the variable be \\(A_{cont}\\).\n(2 points)\n\n\n15.1.4 Missing value imputation using correlated continuous variable\nMake a scatterplot of price against \\(A_{cont}\\), with a trendline. Using the trendline, impute the missing values of price.\nPlot the imputed price vs true price (from Housing_complete.csv) and print the RMSE (Root mean squared error). This is to be done only for the imputed values of price.\n(10 points)\nHint:\n\nMake the trendline using non-missing values of price and \\(A_{cont}\\).\nImpute the missing values of price only where they are missing, i.e., at row indices index_null_price.\nYou may use the function below to plot the imputed price vs true price (from Housing_complete.csv) and print the RMSE. The function assumes that housing_imputed_price is the Housing_missing_price.csv dataset, with imputed values of price.\n\n\n\nCode\n#Defining a function to plot the imputed values vs actual values \ndef plot_actual_vs_predicted():\n    fig, ax = plt.subplots(figsize=(9, 6))\n    plt.rc('xtick', labelsize=15) \n    plt.rc('ytick', labelsize=15) \n    x = housing_complete.loc[index_null_price,'price']/1e3\n    y = housing_imputed_price.loc[index_null_price,'price']/1e3\n    plt.scatter(x,y)\n    z=np.polyfit(x,y,1)\n    p=np.poly1d(z)\n    plt.plot(x,x,color='orange')\n    plt.xlabel('Actual price',fontsize=20)\n    plt.ylabel('Imputed price',fontsize=20)\n    ax.xaxis.set_major_formatter('${x:,.0f}k')\n    ax.yaxis.set_major_formatter('${x:,.0f}k')\n    rmse = np.sqrt(((x-y).pow(2)).mean())\n    print(\"RMSE= $\"+str(np.round(rmse,2))+\"k\")\n\n\n\n\n15.1.5 Correlation of categorical variables with price\nSplit the categorical columns of the Housing_missing_price.csv, such that they transform into dummy variables with each category corresponding to a column of 0s and 1s. The continuous variables remain as they were in the original dataset. Name this dataset as housing_dummy.\nWhich categorical variable is the most highly correlated with price? Let that variable be \\(V_{cat}\\).\n(3 + 2 points)\nHint: pd.get_dummies()\n\n\n15.1.6 Missing value imputation using correlated categorical variable\nImpute the missing value of the price of a house as the average price of all the houses that have the same value of \\(V_{cat}\\). For example, if \\(V_{cat}\\) is basement, the missing price of a house that has a basement must be imputed as the average price of all the houses that have a basement, and the missing price of a house that lacks a basement must be imputed as the average price of all the houses that lack a basement.\nPlot the imputed price vs true price (from Housing_complete.csv) and print the RMSE (Root mean squared error). This is to be done only for the imputed values of price.\n(10 points)\nHint: You may use the following code to get the mean house price for each level of the variable \\(V_{cat}\\):\n\n\nCode\n#Replace 'Vcat' by the variable that you found to be the most correlated with 'price'\nprice_Vcat = housing_missing_price['price'].groupby(housing_missing_price[Vcat]).mean()\nprice_Vcat\n\n\n\n\n15.1.7 Missing value imputation using correlated continuous variable within the categories of correlated categorical variable\nExecute the following code. Note that the trendlines of price against area are different based on airconditioning.\nFor each house, select the appropriate trendline to impute the missing price.\nPlot the imputed price vs true price (from Housing_complete.csv) and print the RMSE (Root mean squared error). This is to be done only for the imputed values of price.\n(15 points)\n\n\nCode\nsns.set(font_scale=1.25)\na = sns.FacetGrid(housing_missing_price, hue = 'airconditioning',height=6, aspect=1.5)\na.map(sns.regplot,'area','price')\na.add_legend()\n\n\n<seaborn.axisgrid.FacetGrid at 0x226a8a07820>\n\n\n\n\n\n\n\n15.1.8 Bonus question: Missing value imputation with KNN\n(20 points)\n\n15.1.8.1 Identifying optimal \\(K\\) by \\(k\\)-fold cross validation\nYou need to impute the missing price using the KNN (\\(K\\)-nearest neighbor) algorithm. However, before implementing the algorithm, find the optimal value of \\(K\\), using \\(k\\)-fold cross-validation. Use all the variables in housing_dummy.\nNote that you cannot use Housing_complete.csv to find the optimal \\(K\\).\nFollow the \\(k\\)-fold cross validation procedure below to find the optimal \\(K\\), i.e., the optimal number of nearest neighbors to consider when imputing missing values:\n\nRemove observations with missing price from housing_dummy. Let us call the resulting DataFrame as housing_missing_removed.\nSplit housing_missing_removed into \\(k\\)-folds. Take \\(k=10\\). Each fold will have one-tenth of the observations of housing_missing_removed.\nIterate over the \\(K^{th}\\) potential value of \\(K\\), where \\(K \\in \\{1,2,...,50\\}\\).\nA. Iterate over the \\(i^{th}\\) fold, where \\(i \\in \\{1,2,...,10\\}\\)\nI. Assume that the all the price values of the \\(i^{th}\\) fold are missing. Impute the value of price for an observation in the \\(i^{th}\\) fold as the mean price of the \\(K\\)-nearest neighbors to the observation, where the neighbors are from among the observations in the remaining 9 folds.\n\nCompute the RMSE (Root mean squared error) for the \\(i^{th}\\) fold. Let us denote the RMSE for \\(i^{th}\\) fold, and considering \\(K\\) nearest neighbors as \\(RMSE_{iK}\\).\n\nB. Find the average of the 10 RMSEs obtained in 3(A). Let us denote it as \\(RMSE_K\\), i.e., RMSE for a given value of \\(K\\). Then, \\[RMSE_K = \\frac{1}{10} \\sum_{i=1}^{i=10} RMSE_{iK}\\]\nThe optimal value of \\(K\\) is the one for which \\(RMSE_K\\) is the minimum, i.e., \\[K_{optimal}=\\underset{K \\in \\{1,...,50\\}}{\\operatorname{\\ argmin}} RMSE_K\\]\n\nAssumption to make it a bit simpler: Ideally you should split the dataset randomly into \\(k\\)-folds. However, to make it simpler, you may assume that the data is already shuffled, and you may take the first \\(1/10^{th}\\) observations to be in the \\(1^{st}\\) fold, the next \\(1/10^{th}\\) to be in the \\(2^{nd}\\) fold and so on.\nMore explanation about \\(k\\)-fold cross validation and the optimal \\(K\\):\nYou need to impute the missing price using the KNN (K-nearest neighbor) algorithm. However, before implementing the algorithm, find the optimal value of \\(K\\), using \\(k\\)-fold cross-validation. This is an optimization method used for more advanced Machine Learning methods, such as KNN. In KNN, the number of neighbors, \\(K\\), is called a hyperparameter, which cannot be optimized with a mathematical method. Therefore, it needs a more coding-based optimization method called cross-validation.\nThe idea of cross-validation is to split the dataset into subsets, called folds. After that a range of \\(K\\) values is picked. For each \\(K\\) value in the range, the KNN imputer is created and evaluated on each fold separately, returning an RMSE value for each fold. The average value of these RMSE values is the cross-validation RMSE value of that \\(K\\) value. Cross-validation is a robust method to find the best \\(K\\) in the KNN algorithm for the data at hand because it evaluates different parts of the data separately and takes the average of all results. It is called \\(k\\)-fold cross-validation, with \\(k\\) as the number of folds we want to use, usually 3, 5 or 10. In this problem, we will use 10-fold cross-validation. Note that you need nested for loops to iterate over both each \\(K\\) value and each fold for this algorithm.\n\n\n15.1.8.2 Implementing KNN with optimal \\(K\\)\nUsing the optimal value of \\(K = K_{optimal}\\) obtained in the previous question, impute the missing values of price in housing_missing_price. Use all the variables in housing_dummy for implementing the KNN algorithm. Plot the imputed price vs true price (from Housing_complete.csv) and print the RMSE (Root mean squared error). This is to be done only for the imputed values of price.\nAnswer check: The RMSE obtained with KNN should be lower than that obtained with any of the earlier methods. If not, then there may be some mistake in your KNN implementation.\n\n\n\n15.1.9 Ultra-bonus question\n(30 points)\nDevelop an algorithm to impute the missing price, such that it reduces the imputation RMSE to below $650k. Plot the imputed price vs true price (from Housing_complete.csv) and print the RMSE (Root mean squared error). This is to be done only for the imputed values of price.\nNote that we have not attempted to solve this question yet. We are not sure if there exists a solution. However, if you find a solution, you will get 30 points.\nHint: In the bonus question, all variables were given equal weights when imputing missing values with KNN. However, shouldn’t some variables be given higher weight than others?\nIf you think you are successful, email your solution to krish@northwestern.edu for grading."
  },
  {
    "objectID": "Assignment 5 (Data cleaning and preparation).html#binning",
    "href": "Assignment 5 (Data cleaning and preparation).html#binning",
    "title": "15  Assignment 5 (Data cleaning & preparation)",
    "section": "15.2 Binning",
    "text": "15.2 Binning\nRead house_features_and_price.csv. We will bin a couple of variables to better analyze the trend of house_price with those variables.\n\n15.2.1 Trend with house_age\nMake a scatterplot of house_price against house_age, along with the trendline. What is the trend indicated by the trendline?\n(4 points)\n\n\n15.2.2 Trend with bins of house_age\n\n15.2.2.1 \nBin house_age into 5 approximately equal-sized bins.\n(3 points)\n\n\n15.2.2.2 \nAfter binning, plot the mean house_price against the house age bins.\n(3 points)\n\n\n15.2.2.3 \nIs the trend seen with this plot different from that seen with the trendline in the previous question? If yes, then is one of the trends incorrect? Why or why not?\n(4 points)\n\n\n15.2.2.4 \nIs one of the trends more informative? If yes, then which one and how?\n(4 points)\n\n\n\n15.2.3 Trend with number_convenience_stores\nMake a barplot to visualize the mean house_price against number_convenience_stores.\n(3 points)\n\n\n15.2.4 Trend with bins of number_convenience_stores\nBin number_convenience_stores into an appropriate number of bins such that the non-linear trend of the variation of house_price with the bins of number_convenience_stores is retained, while minimizing the number of bins.\nAfter binning, plot the mean house_price against the number_convenience_stores bins.\n(8 points)\n\n\n15.2.5 Size of number_convenience_stores bins\nPrint the size of bins obtained in the previous question. Are the bins of approximately equal size? If not, is it reasonable to have bins of unequal sizes to visualize the trend of house_price with number_convenience_stores.?\n(2 + 4 points)"
  },
  {
    "objectID": "Assignment 5 (Data cleaning and preparation).html#outliers",
    "href": "Assignment 5 (Data cleaning and preparation).html#outliers",
    "title": "15  Assignment 5 (Data cleaning & preparation)",
    "section": "15.3 Outliers",
    "text": "15.3 Outliers\n\n15.3.1 Identifying outlying prices\nContinue using house_features_and_price.csv. How many houses have outlying values of house_price. Are these houses extremely expensive or extremely cheap?\n(6 + 2 points)\n\n\n15.3.2 Quick EDA\nHow are these houses (identified in the previous question as outliers) different from the houses in the rest of the dataset, which might be making them extremely expensive / extremely cheap? Explore the data and mention your hypothesis.\n(8 points)"
  },
  {
    "objectID": "Assignment 6 (Data wrangling).html",
    "href": "Assignment 6 (Data wrangling).html",
    "title": "16  Assignment 6 (Data wrangling)",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 22nd November 2022 at 11:59 pm.\nYou are not allowed to use a for loop in this assignment."
  },
  {
    "objectID": "Assignment 6 (Data wrangling).html#fifa-world-cup",
    "href": "Assignment 6 (Data wrangling).html#fifa-world-cup",
    "title": "16  Assignment 6 (Data wrangling)",
    "section": "16.1 Fifa world cup",
    "text": "16.1 Fifa world cup\nRead FIFA world cup attendance data from the page: https://en.wikipedia.org/wiki/FIFA_World_Cup . Use ‘attendance’ as the matching string to find the table.\n\n16.1.1 \nFind the number of levels of column labels and row labels in the data.\n(2 points)\n\n\n16.1.2 \nReduce the multiple levels of column labels to a single level as follows. If the column names at all the levels are different, then concatenate the names together. Otherwise, keep the name at the outer level. For example, if the column name is (‘Hosts’,‘Hosts’), it should change to ‘Host’. If the column name is (‘Highest attendances †’,‘Number’), it should change to ‘Highest attendances †Number’. Do not rename each column manually. Use a method that will work efficiently if there were a large number of columns, say \\(10,000\\) columns.\nReminder: Do not use a for loop.\n(10 points)"
  },
  {
    "objectID": "Assignment 6 (Data wrangling).html#gdp-per-capita-and-population",
    "href": "Assignment 6 (Data wrangling).html#gdp-per-capita-and-population",
    "title": "16  Assignment 6 (Data wrangling)",
    "section": "16.2 GDP per capita and population",
    "text": "16.2 GDP per capita and population\nRead the GDP per capita data from https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita\n\n16.2.1 Preparing GDP per capita data\n\n16.2.1.1 \nDrop all the Year columns. Use the drop() method with the columns, level and inplace arguments. Print the first 2 rows of the updated DataFrame.\n(4 points)\n\n\n16.2.1.2 \nDrop the inner level of column labels. Use the droplevel() method. Print the first 2 rows of the updated DataFrame.\n(4 points)\n\n\n16.2.1.3 \nConvert the columns consisting of GDP per capita by IMF, World Bank, and the United Nations to numeric. Apply a lambda function on these columns to convert them to numeric. Print the number of missing values in each column of the updated DataFrame.\nNote: Do not apply the function 3 times. Apply it once on a DataFrame consisting of these 3 columns.\n(4 points)\n\n\n16.2.1.4 \nApply the lambda function below on all the column names of the dataset obtained in the previous question to clean the column names.\nimport re\ncolumn_name_cleaner = lambda x:re.split(r'\\[|/', x)[0]\nNote: You will need to edit the parameter of the function, i.e., x in the above function to make sure it is applied on column names and not columns.\nPrint the first 2 rows of the updated DataFrame.\n(5 points)\n\n\n16.2.1.5 \nCreate a new column GDP_per_capita that copies the GDP per capita values of the United Nations. If the GDP per capita is missing in the United Nations column, then copy it from the World Bank column.\nPrint the number of missing values in the GDP_per_capita column.\n(6 points)\n\n\n\n16.2.2 \nDrop all the columns except Country and GDP_per_capita. Print the first 2 rows of the updated DataFrame.\n(2 points)\n\n16.2.2.1 \nThe country names contain some special characters (characters other than letters) and need to be cleaned. The following function can help clean country names:\nimport re\ncountry_names_clean_gdp_data = lambda x: re.sub(r'[^\\w\\s]', '', x).strip()\nApply the above lambda function on the country column to clean country names. Save the cleaned dataset as gdp_per_capita_data. Print the first 2 rows of the updated DataFrame.\n(3 points)\n\n\n\n16.2.3 Preparing population data\n\n16.2.3.1 \nRead the population data from https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations). Drop all columns except Country / Area and Population (1 July 2019).\n(2 points)\n\n\n16.2.3.2 \nApply the lambda function below on all the column names of the dataset obtained in the previous question to clean the column names.\nimport re\ncolumn_name_cleaner = (lambda x:re.split(r'\\[|/|\\(| ', x.name)[0]\nNote: You will need to edit the parameter of the function, i.e., x in the above function to make sure it is applied on column names and not columns.\nPrint the first 2 rows of the updated DataFrame.\n(5 points)\n\n\n16.2.3.3 \nThe country names contain some special characters (characters other than letters) and need to be cleaned. The following function can help clean country names:\nimport re\ncountry_names_clean_population_data = lambda x: re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip()\nApply the above lambda function on the country column to clean country names. Save the cleaned dataset as population_data.\n(3 points)\n\n\n\n16.2.4 Merging GDP per capita and population datasets\n\n16.2.4.1 \nMerge gdp_per_capita_data with population_data to get the population and GDP per capita of countries in a single dataset. Print the first two rows of the merged DataFrame.\nAssume that:\n\nWe want to keep the GDP per capita of all countries in the merged dataset, even if their population in unavailable in the population dataset. For countries whose population in unavailable, their Population column will show NA.\nWe want to discard an observation of a country if its GDP per capita is unavailable.\n\n(4 points)\n\n\n16.2.4.2 \nFor how many countries in gdp_per_capita_data does the population seem to be unavailable in population_data? Note that you don’t need to clean country names any further than cleaned by the functions provided.\nPrint the observations of gdp_per_capita_data with missing Population.\n(3 points)\n\n\n\n16.2.5 Merging datasets with similar values in the key column\nWe suspect that population of more countries may be available in population_data. However, due to unclean country names, the observations could not merge. For example, the country Saint Vincent and the Grenadines is mentioned as Saint Vincent and theGrenadines in gdp_per_capita_data and Saint Vincent and the Grenadines in population_data. To resolve this issue, we’ll use a different approach to merge datasts. We’ll merge the population of a country to an observation in the GDP per capita dataset, whose name in population_data is the most ‘similar’ to the name of the country in gdp_per_capita_data.\n\n16.2.5.1 \nProceed as follows:\n\nFor each country in gdp_per_capita_data, find the country with the most ‘similar’ name in population_data, based on the similarity score. Use the lambda function provided below to compute the similarity score between two strings (The higher the score, the more similar are the strings. The similarity score is \\(1.0\\) if two strings are exactly the same).\nMerge the population of the most ‘similar’ country to the country in gdp_per_capita_data. The merged dataset must include 5 columns - the country name as it appears in gdp_per_capita_data, the GDP per capita, the country name of the most ‘similar’ country as it appears in population_data, the population of that country, and the similarity score between the country names.\nAfter creating the merged dataset, print the rows of the dataset that have similarity scores less than 1.\n\nUse the function below to compute the similarity score between the Country names of the two datasets:\nfrom difflib import SequenceMatcher\nsimilar = lambda a,b: SequenceMatcher(None, a, b).ratio()\nReminder: Do not use a for loop\n(18 points)\nHint:\n\nDefine a function that computes the index of the observation having the most ‘similar’ country name in population_data for an observation in gdp_per_capita_data. The function returns a Series consisting of the most ‘similar’ country name, its population, and its similarity score (This function can be written with only one line in its body, excluding the return statement and the definition statement. However, you may use as many lines as you wish).\nApply the function on the Country column of gdp_per_capita_data. A DataFrame will be obtained.\nConcatenate the DataFrame obtained in (2) with gdp_per_capita_data with the pandas concat() function.\n\n\n\n16.2.5.2 \nIn the dataset obtained in the previous question, for all observations where similarity score is less than 0.6, replace the population with Nan.\nPrint the observations of the dataset having missing values of population.\n(2 points)"
  },
  {
    "objectID": "Assignment 6 (Data wrangling).html#gdp-surplus-and-compensation",
    "href": "Assignment 6 (Data wrangling).html#gdp-surplus-and-compensation",
    "title": "16  Assignment 6 (Data wrangling)",
    "section": "16.3 GDP, surplus, and compensation",
    "text": "16.3 GDP, surplus, and compensation\nThe dataset Real GDP.csv contains the GDP of each US State for all years starting from 1997 until 2020. The data is at State level, i.e., each observation corresponds to a unique State.\nThe dataset Surplus.csv contains the surplus of each US State for all years starting from 1997 until 2020. The data is at year level, i.e., each observation corresponds to a unique year.\nThe dataset Compensation.csv contains Compensation and Chain-type quantity indexes for real GDP for each US State and year starting from 1997 to 2020. The dataset is at Year-State-Description level, i.e., each observation corresponds to a unique Year-State-Description combination where Description refers to either Compensation or Chain-type quantity indexes for real GDP.\n\n16.3.1 Combining datasets\nCombine all these datasets to obtain a dataset at State-Year level, i.e., each observation corresponds to a unique State-Year combination. The combined dataset must contain the GDP, surplus, Compensation, and Chain-type quantity indexes for real GDP for each US State and all years starting from 1997 until 2020. Note that each observation must contain the name of the US State, year, and the four values (GDP, surplus, compensation, and Chain-type quantity indexes for real GDP).\nHint: Here is one way to do it:\n\nMelt the GDP dataset to year-State level\nMelt the Surplus dataset to year-State level\nPivot the compensation dataset to year-State level\nNow that all the datasets are at the year-State level, merge them!\n\n(4 + 4 + 4 + 2 = 14 points)\n\n\n16.3.2 Time trend: GDP, surplus, and compensation\nUse a single plot to answer all three questions below by visualizing:\n\nHow does the mean GDP (mean over all States) change with year? (1 point for visualization)\nHow does the mean compensation (mean over all States) change with year? (1 point for visualization)\nHow does the mean surplus (mean over all States) change with year? (1 point for visualization)\n\nAlso show the 95% confidence interval for the mean GDP, mean compensation, and mean surplus in the plot.\nHint: Use the seaborn function lineplot() . No calculations are needed. Just use lineplot() three times.\n(4 points)\n\n\n16.3.3 Time trend: GDP with region\nMerge the file State_region_mapping.csv with the dataset obtained in the previous question. Make a lineplot showing the mean GDP for each of the five regions with year. Do not display the confidence interval. Which two regions seems to have the least growth in GDP over the past 24 years?\n(5 points)"
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix A — Datasets",
    "section": "",
    "text": "Datasets used in the book can be found here"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Rubin, Donald B. 1976. “Inference and Missing Data.”\nBiometrika 63 (3): 581–92."
  }
]