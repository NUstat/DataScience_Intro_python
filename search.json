[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science with Python",
    "section": "",
    "text": "Preface\nThis book is developed for the course STAT303-1 (Data Science with Python-1). The first two chapters of the book are a review of python, and will be covered very quickly. Students are expected to know the contents of these chapters beforehand, or be willing to learn it quickly. Students may use the STAT201 book (https://nustat.github.io/Intro_to_programming_for_data_sci/) to review the python basics required for the STAT303 sequence. The core part of the course begins from the third chapter - Reading data.\nPlease feel free to let the instructors know in case of any typos/mistakes/general feedback in this book."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#installation",
    "href": "Introduction to Python and Jupyter Notebooks.html#installation",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "1.1 Installation",
    "text": "1.1 Installation\nAnaconda: If you are new to python, we recommend downloading the Anaconda installer and following the instructions for installation. Once installed, we’ll use the Jupyter Notebook interface to write code.\nQuarto: We’ll use Quarto to publish the .ipynb file containing text, python code, and the output. Download and install Quarto from here."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#jupyter-notebook",
    "href": "Introduction to Python and Jupyter Notebooks.html#jupyter-notebook",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "1.2 Jupyter notebook",
    "text": "1.2 Jupyter notebook\n\n1.2.1 Introduction\nJupyter notebook is an interactive platform, where you can write code and text, and make visualizations. Jupyter is a loose acronym meaning Julia, Python, and R. However, now it supports Ruby, Haskell, Scala, Go, etc, besides Julia, Python, and R. You can access Jupyter notebook from the Anaconda Navigator, or directly open the Jupyter Notebook application itself. It should automatically open up in your default browser. The figure below shows a Jupyter Notebook opened with Google Chrome. This page is called the landing page of the notebook.\n\n\n\n\n\nTo create a new notebook, click on the New button and select the Python 3 option. You should see a blank notebook as in the figure below.\n\n\n\n\n\n\n\n1.2.2 Writing and executing code\nCode cell: By default, a cell is of type Code, i.e., for typing code, as seen as the default choice in the dropdown menu below the Widgets tab. Try typing a line of python code (say, 2+3) in an empty code cell and execute it by pressing Shift+Enter. This should execute the code, and create an new code cell. Pressing Ctlr+Enter for Windows (or Cmd+Enter for Mac) will execute the code without creating a new cell.\nCommenting code in a code cell: Comments should be made while writing the code to explain the purpose of the code or a brief explanation of the tasks being performed by the code. A comment can be added in a code cell by preceding it with a # sign. For example, see the comment in the code below.\nWriting comments will help other users understand your code. It is also useful for the coder to keep track of the tasks being performed by their code.\n\n#This code adds 3 and 5\n3+5\n\n8\n\n\nMarkdown cell: Although a comment can be written in a code cell, a code cell cannot be used for writing headings/sub-headings, and is not appropriate for writing lengthy chunks of text. In such cases, change the cell type to Markdown from the dropdown menu below the Widgets tab. Use any markdown cheat sheet found online, for example, this one to format text in the markdown cells.\nGive a name to the notebook by clicking on the text, which says ‘Untitled’.\n\n\n1.2.3 Saving and loading notebooks\nSave the notebook by clicking on File, and selecting Save as, or clicking on the Save and Checkpoint icon (below the File tab). Your notebook will be saved as a file with an extension ipynb. This file will contain all the code as well as the outputs, and can be loaded and edited by a Jupyter user. To load an existing Jupyter notebook, navigate to the folder of the notebook on the landing page, and then click on the file to open it.\n\n\n1.2.4 Rendering notebook as HTML\nWe’ll use Quarto to print the **.ipynb* file as HTML. Check the procedure for rendering a notebook as HTML here. You have several options to format the file.\nYou will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#in-class-exercise",
    "href": "Introduction to Python and Jupyter Notebooks.html#in-class-exercise",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "In-class exercise",
    "text": "In-class exercise\n\nCreate a new notebook.\nSave the file as In_class_exercise1.\nGive a heading to the file - First HTML file.\nPrint Today is day 1 of class.\nCompute and print the number of hours of this course in the quarter (that will be 10 weeks x 2 classes per week x 1.33 hours per class).\n\nThe HTML file should look like the picture below."
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#python-language-basics",
    "href": "Introduction to Python and Jupyter Notebooks.html#python-language-basics",
    "title": "1  Introduction to Python and Jupyter Notebooks",
    "section": "1.3 Python language basics",
    "text": "1.3 Python language basics\n\n1.3.1 Object Oriented Programming\nPython is an object-oriented programming language. In layman terms, it means that every number, string, data structure, function, class, module, etc., exists in the python interpreter as a python object. An object may have attributes and methods associated with it. For example, let us define a variable that stores an integer:\n\nvar = 2\n\nThe variable var is an object that has attributes and methods associated with it. For example a couple of its attributes are real and imag, which store the real and imaginary parts respectively, of the object var:\n\nprint(\"Real part of 'var': \",var.real)\nprint(\"Real part of 'var': \",var.imag)\n\nReal part of 'var':  2\nReal part of 'var':  0\n\n\nAttribute: An attribute is a value associated with an object, defined within the class of the object.\nMethod: A method is a function associated with an object, defined within the class of the object, and has access to the attributes associated with the object.\nFor looking at attributes and methods associated with an object, say obj, press tab key after typing obj..\nConsider the example below of a class example_class:\n\nclass example_class:\n    class_name = 'My Class'\n    def my_method(self):\n        print('Hello World!')\n\ne = example_class()\n\nIn the above class, class_name is an attribute, while my_method is a method.\n\n\n1.3.2 Assigning variable name to object\n\n1.3.2.1 Call by reference\nPython utilizes a system, which is known as Call by Object Reference. When an object is assigned to a variable name, the variable name serves as a reference to the object. For example, consider the following assignment:\n\nx = [5,3]\n\nThe variable name x is a reference to the memory location where the object [5, 3] is stored. Now, suppose we assign x to a new variable y:\n\ny = x\n\nIn the above statement the variable name y now refers to the same object [5,3]. The object [5,3] does not get copied to a new memory location referred by y. To prove this, let us add an element to y:\n\ny.append(4)\nprint(y)\n\n[5, 3, 4]\n\n\n\nprint(x)\n\n[5, 3, 4]\n\n\nWhen we changed y, note that x also changed to the same object, showing that x and y refer to the same object, instead of referring to different copies of the same object.\n\n\n1.3.2.2 Assigning multiple variable names\nValues can be assigned to multiple variables in a single statement by separating the variable names and values with commas.\n\ncolor1, color2, color3 = \"red\", \"green\", \"blue\"\n\n\ncolor1\n\n'red'\n\n\n\ncolor3\n\n'blue'\n\n\nThe same value can be assigned to multiple variables by chaining multiple assignment operations within a single statement.\n\ncolor4 = color5 = color6 = \"magenta\"\n\n\n\n1.3.2.3 Rules for variable names\nVariable names can be short (a, x, y, etc.) or descriptive ( my_favorite_color, profit_margin, the_3_musketeers, etc.). However, we recommend that you use descriptive variable names as it makes it easier to understand the code.\nThe rules below must be followed while naming Python variables:\n\nA variable’s name must start with a letter or the underscore character _. It cannot begin with a number.\nA variable name can only contain lowercase (small) or uppercase (capital) letters, digits, or underscores (a-z, A-Z, 0-9, and _).\nVariable names are case-sensitive, i.e., a_variable, A_Variable, and A_VARIABLE are all different variables.\n\nHere are some valid variable names:\n\na_variable = 23\nis_today_Saturday = False\nmy_favorite_car = \"Delorean\"\nthe_3_musketeers = [\"Athos\", \"Porthos\", \"Aramis\"] \n\nLet’s try creating some variables with invalid names. Python prints a syntax error if the variable’s name is invalid.\n\nSyntax: The syntax of a programming language refers to the rules that govern the structure of a valid instruction or statement. If a statement does not follow these rules, Python stops execution and informs you that there is a syntax error. Syntax can be thought of as the rules of grammar for a programming language.\n\n\na variable = 23\n\n\nis_today_$aturday = False\n\n\nmy-favorite-car = \"Delorean\"\n\n\n3_musketeers = [\"Athos\", \"Porthos\", \"Aramis\"]\n\n\n\n\n1.3.3 Built-in objects\n\n1.3.3.1 Built-in data types\nVariable is created as soon as a value is assigned to it. We don’t have to define the type of variable explicitly as in other programming languages because Python can automatically guess the type of data entered (dynamically typed).\nAny data or information stored within a Python variable has a type. We can view the type of data stored within a variable using the type function.\n\na_variable\n\n23\n\n\n\ntype(a_variable)\n\nint\n\n\n\nis_today_Saturday\n\nFalse\n\n\n\ntype(is_today_Saturday)\n\nbool\n\n\n\nmy_favorite_car\n\n'Delorean'\n\n\n\ntype(my_favorite_car)\n\nstr\n\n\n\nthe_3_musketeers\n\n['Athos', 'Porthos', 'Aramis']\n\n\n\ntype(the_3_musketeers)\n\nlist\n\n\nPython has several built-in data types for storing different kinds of information in variables.\n\n\n\n\n\nPrimitive: Integer, float, boolean, None, and string are primitive data types because they represent a single value.\nContainers: Other data types like list, tuple, and dictionary are often called data structures or containers because they hold multiple pieces of data together. We’ll discuss these datatypes in chapter 2.\nThe data type of the object can be identified using the in-built python function type(). For example, see the following objects and their types:\n\ntype(4)\n\nint\n\n\n\ntype(4.4)\n\nfloat\n\n\n\ntype('4')\n\nstr\n\n\n\ntype(True)\n\nbool\n\n\n\n\n1.3.3.2 Built-in modules and functions\nBuilt-in functions in Python are a set of predefined functions that are available for use without the need to import any additional libraries or modules. The Python Standard Library is very extensive. Besides built-in functions, it also contains many Python scripts (with the . py extension) containing useful utilities and modules written in Python that provide standardized solutions for many problems that occur in everyday programming.\nBelow are a couple of examples:\nrange(): The range() function returns a sequence of evenly-spaced integer values. It is commonly used in for loops to define the sequence of elements over which the iterations are performed.\nBelow is an example where the range() function is used to create a sequence of whole numbers upto 10:\n\nprint(list(range(1,10)))\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nThe advantage of the range type over a regular list or tuple is that a range object will always take the same (small) amount of memory, no matter the size of the range it represents (as it only stores the start, stop and step values, calculating individual items and subranges as needed).\nDate time: Python as a built-in datetime module for handling date/time objects:\n\nimport datetime as dt\n\n\n#Defining a date-time object \ndt_object = dt.datetime(2022, 9, 20, 11,30,0)\n\nInformation about date and time can be accessed with the relevant attribute of the datetime object.\n\ndt_object.day\n\n20\n\n\n\ndt_object.year\n\n2022\n\n\nThe strftime method of the datetime module formats a datetime object as a string. There are several types of formats for representing date as a string:\n\ndt_object.strftime('%m/%d/%Y')\n\n'09/20/2022'\n\n\n\ndt_object.strftime('%m/%d/%y %H:%M')\n\n'09/20/22 11:30'\n\n\n\ndt_object.strftime('%h-%d-%Y')\n\n'Sep-20-2022'\n\n\n\n\n\n1.3.4 Importing libraries\nThere are several built-in functions in python like print(), abs(), max(), sum() etc., which do not require importing any library. However, these functions will typically be insufficient for a analyzing data. Some of the popular libraries and their primary purposes are as follows:\n\nNumPy: NumPy is a fundamental library for numerical computing in Python. It provides support for arrays, matrices, and mathematical functions, making it essential for scientific and data analysis tasks.. It is mostly used for performing numerical operations and efficiently storing numerical data.\nPandas: Pandas is a powerful data manipulation and analysis library. It offers data structures like DataFrames and Series, which facilitate data reading, cleaning, transformation, and analysis, making it indispensable in data science projects.\nMatplotlib, Seaborn: Matplotlib is a comprehensive library for creating static, animated, or interactive plots and visualizations. It is commonly used for data visualization and exploration in data science. Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\nSciPy: SciPy is used for performing scientific computing such as solving differential equations, optimization, statistical tests, etc.\nScikit-learn: Scikit-learn is a machine learning library that provides a wide range of tools for data pre-procesing, classification, regression, clustering, dimensionality reduction, and more. It simplifies the implementation of machine learning algorithms and model evaluation.\nStatsmodels: Statsmodels is used for developing statistical models with a focus on inference (in contrast to focus on prediction as in scikit-learn).\n\nTo use libraries like NumPy, pandas, Matplotlib, and scikit-learn in Python, you typically need to follow these steps:\n\nInstall the libraries (Anaconda already does this)\nImport the libraries in the python script or jupyter notebook\nUse the Library Functions and Classes: After importing the libraries, you can use their functions, classes, and methods in your code. For instance, you can create NumPy arrays, manipulate data with pandas, create plots with Matplotlib, or train machine learning models with scikit-learn.\n\nA library can be imported using the import keyword. For example, a NumPy library can be imported as:\n\nimport numpy as np\n\nUsing the as keyboard, the NumPy library has been given the name np. All the functions and attributes of the library can be called using the ‘np.’ prefix. For example, let us generate a sequence of whole numbers upto 10 using the NumPy function arange():\n\nnp.arange(8)\n\narray([0, 1, 2, 3, 4, 5, 6, 7])\n\n\nThere’s different ways to import:\n\nImport the whole module using its original name:  import math, os\nImport specific things from the module:  from random import randint  from math import pi\nImport the whole library and rename it, usually using a shorter variable name:  import pandas as pd\nImport a specific method from the module and rename it as it is imported:  from os.path import join as join_path\n\n\n\n1.3.5 User-defined functions\nA function is a reusable set of instructions that takes one or more inputs, performs some operations, and often returns an output. Indeed, while python’s standard library and ecosystem libraries offer a wealth of pre-defined functions for a wide range of tasks, there are situations where defining your own functions is not just beneficial but necessary. \n\n1.3.5.1 Creating and using functions\nYou can define a new function using the def keyword.\n\ndef say_hello():\n    print('Hello there!')\n    print('How are you?')\n\nNote the round brackets or parentheses () and colon : after the function’s name. Both are essential parts of the syntax. The function’s body contains an indented block of statements. The statements inside a function’s body are not executed when the function is defined. To execute the statements, we need to call or invoke the function.\n\nsay_hello()\n\nHello there!\nHow are you?\n\n\n\ndef say_hello_to(name):\n    print('Hello ', name)\n    print('How are you?')\n\n\nsay_hello_to('Lizhen')\n\nHello  Lizhen\nHow are you?\n\n\n\nname = input ('Please enter your name: ')\nsay_hello_to(name)\n\nPlease enter your name: George\nHello  George\nHow are you?\n\n\n\n\n1.3.5.2 Variable scope: Local and global Variables\nLocal variable: When we declare variables inside a function, these variables will have a local scope (within the function). We cannot access them outside the function. These types of variables are called local variables. For example,\n\ndef greet(): \n    message = 'Hello'  # local variable\n    print('Local', message)\ngreet()\n\nLocal Hello\n\n\n\nprint(message) # try to access message variable outside greet() function\n\nNameError: name 'message' is not defined\n\n\nAs message was defined within the function greet(), it is local to the function, and cannot be called outside the function.\nGlobal variable: Aa variable declared outside of the function or in global scope is known as a global variable. This means that a global variable can be accessed inside or outside of the function.\nLet’s see an example of how a global variable is created.\n\nmessage = 'Hello'  # declare global variable\n\ndef greet():\n    print('Local', message)  # declare local variable\n\ngreet()\nprint('Global', message)\n\nLocal Hello\nGlobal Hello\n\n\n\n\n1.3.5.3 Named arguments\nInvoking a function with many arguments can often get confusing and is prone to human errors. Python provides the option of invoking functions with named arguments for better clarity. You can also split function invocation into multiple lines.\n\ndef loan_emi(amount, duration, rate, down_payment=0):\n    loan_amount = amount - down_payment\n    emi = loan_amount * rate * ((1+rate)**duration) / (((1+rate)**duration)-1)\n    return emi\n\n\nemi1 = loan_emi(\n    amount=1260000, \n    duration=8*12, \n    rate=0.1/12, \n    down_payment=3e5\n)\n\n\nemi1\n\n14567.19753389219\n\n\n\n\n1.3.5.4 Optional Arguments\nFunctions with optional arguments offer more flexibility in how you can use them. You can call the function with or without the argument, and if there is no argument in the function call, then a default value is used.\n\nemi2 = loan_emi(\n    amount=1260000, \n    duration=8*12, \n    rate=0.1/12)\n\nemi2\n\n19119.4467632335\n\n\n\n\n1.3.5.5 *args and **kwargs\nWe can pass a variable number of arguments to a function using special symbols. There are two special symbols:\n\nSpecial Symbols Used for passing arguments: 1. *args (Non-Keyword Arguments) 2. *kwargs (Keyword Arguments) &gt; Note: “We use the “wildcard” or “” notation like this – *args OR **kwargs – as our function’s argument when we have doubts about the number of arguments we should pass in a function.”\n\ndef myFun(*args,**kwargs):\n    print(\"args: \", args)\n    print(\"kwargs: \", kwargs)\n \n \n# Now we can use both *args ,**kwargs\n# to pass arguments to this function :\nmyFun('John',22,'cs',name=\"John\",age=22,major=\"cs\")\n\nargs:  ('John', 22, 'cs')\nkwargs:  {'name': 'John', 'age': 22, 'major': 'cs'}\n\n\n\n\n\n1.3.6 Branching and looping (control flow)\n\nAs in other languages, python has built-in keywords that provide conditional flow of control in the code.\n\n1.3.6.1 Branching with if, else and elif\nOne of the most powerful features of programming languages is branching: the ability to make decisions and execute a different set of statements based on whether one or more conditions are true.\nThe if statement\nIn Python, branching is implemented using the if statement, which is written as follows:\nif condition:\n    statement1\n    statement2\nThe condition can be a value, variable or expression. If the condition evaluates to True, then the statements within the if block are executed. Notice the four spaces before statement1, statement2, etc. The spaces inform Python that these statements are associated with the if statement above. This technique of structuring code by adding spaces is called indentation.\n\nIndentation: Python relies heavily on indentation (white space before a statement) to define code structure. This makes Python code easy to read and understand. You can run into problems if you don’t use indentation properly. Indent your code by placing the cursor at the start of the line and pressing the Tab key once to add 4 spaces. Pressing Tab again will indent the code further by 4 more spaces, and press Shift+Tab will reduce the indentation by 4 spaces.\n\nFor example, let’s write some code to check and print a message if a given number is even.\n\na_number = 34\n\n\nif a_number % 2 == 0:\n    print(\"We're inside an if block\")\n    print('The given number {} is even.'.format(a_number))\n\nWe're inside an if block\nThe given number 34 is even.\n\n\nThe else statement\nWe may want to print a different message if the number is not even in the above example. This can be done by adding the else statement. It is written as follows:\nif condition:\n    statement1\n    statement2\nelse:\n    statement4\n    statement5\n\nIf condition evaluates to True, the statements in the if block are executed. If it evaluates to False, the statements in the else block are executed.\n\nif a_number % 2 == 0:\n    print('The given number {} is even.'.format(a_number))\nelse:\n    print('The given number {} is odd.'.format(a_number))\n\nThe given number 34 is even.\n\n\nThe elif statement\nPython also provides an elif statement (short for “else if”) to chain a series of conditional blocks. The conditions are evaluated one by one. For the first condition that evaluates to True, the block of statements below it is executed. The remaining conditions and statements are not evaluated. So, in an if, elif, elif… chain, at most one block of statements is executed, the one corresponding to the first condition that evaluates to True.\n\ntoday = 'Wednesday'\n\n\nif today == 'Sunday':\n    print(\"Today is the day of the sun.\")\nelif today == 'Monday':\n    print(\"Today is the day of the moon.\")\nelif today == 'Tuesday':\n    print(\"Today is the day of Tyr, the god of war.\")\nelif today == 'Wednesday':\n    print(\"Today is the day of Odin, the supreme diety.\")\nelif today == 'Thursday':\n    print(\"Today is the day of Thor, the god of thunder.\")\nelif today == 'Friday':\n    print(\"Today is the day of Frigga, the goddess of beauty.\")\nelif today == 'Saturday':\n    print(\"Today is the day of Saturn, the god of fun and feasting.\")\n\nToday is the day of Odin, the supreme diety.\n\n\nIn the above example, the first 3 conditions evaluate to False, so none of the first 3 messages are printed. The fourth condition evaluates to True, so the corresponding message is printed. The remaining conditions are skipped. Try changing the value of today above and re-executing the cells to print all the different messages.\nUsing if, elif, and else together\nYou can also include an else statement at the end of a chain of if, elif… statements. This code within the else block is evaluated when none of the conditions hold true.\n\na_number = 49\n\n\nif a_number % 2 == 0:\n    print('{} is divisible by 2'.format(a_number))\nelif a_number % 3 == 0:\n    print('{} is divisible by 3'.format(a_number))\nelif a_number % 5 == 0:\n    print('{} is divisible by 5'.format(a_number))\nelse:\n    print('All checks failed!')\n    print('{} is not divisible by 2, 3 or 5'.format(a_number))\n\nAll checks failed!\n49 is not divisible by 2, 3 or 5\n\n\nNon-Boolean Conditions\nNote that conditions do not necessarily have to be booleans. In fact, a condition can be any value. The value is converted into a boolean automatically using the bool operator. Any value in Python can be converted to a Boolean using the bool function.\nOnly the following values evaluate to False (they are often called falsy values):\n\nThe value False itself\nThe integer 0\nThe float 0.0\nThe empty value None\nThe empty text \"\"\nThe empty list []\nThe empty tuple ()\nThe empty dictionary {}\nThe empty set set()\nThe empty range range(0)\n\nEverything else evaluates to True (a value that evaluates to True is often called a truthy value).\n\nif '':\n    print('The condition evaluted to True')\nelse:\n    print('The condition evaluted to False')\n\nThe condition evaluted to False\n\n\n\nif 'Hello':\n    print('The condition evaluted to True')\nelse:\n    print('The condition evaluted to False')\n\nThe condition evaluted to True\n\n\n\nif { 'a': 34 }:\n    print('The condition evaluted to True')\nelse:\n    print('The condition evaluted to False')\n\nThe condition evaluted to True\n\n\n\nif None:\n    print('The condition evaluted to True')\nelse:\n    print('The condition evaluted to False')\n\nThe condition evaluted to False\n\n\nNested conditional statements\nThe code inside an if block can also include an if statement inside it. This pattern is called nesting and is used to check for another condition after a particular condition holds true.\n\na_number = 15\n\n\nif a_number % 2 == 0:\n    print(\"{} is even\".format(a_number))\n    if a_number % 3 == 0:\n        print(\"{} is also divisible by 3\".format(a_number))\n    else:\n        print(\"{} is not divisibule by 3\".format(a_number))\nelse:\n    print(\"{} is odd\".format(a_number))\n    if a_number % 5 == 0:\n        print(\"{} is also divisible by 5\".format(a_number))\n    else:\n        print(\"{} is not divisibule by 5\".format(a_number))\n\n15 is odd\n15 is also divisible by 5\n\n\nNotice how the print statements are indented by 8 spaces to indicate that they are part of the inner if/else blocks.\n\nNested if, else statements are often confusing to read and prone to human error. It’s good to avoid nesting whenever possible, or limit the nesting to 1 or 2 levels.\n\nShorthand if conditional expression\nA frequent use case of the if statement involves testing a condition and setting a variable’s value based on the condition.\nPython provides a shorter syntax, which allows writing such conditions in a single line of code. It is known as a conditional expression, sometimes also referred to as a ternary operator. It has the following syntax:\nx = true_value if condition else false_value\nIt has the same behavior as the following if-else block:\nif condition:\n    x = true_value\nelse:\n    x = false_value\nLet’s try it out for the example above.\n\nparity = 'even' if a_number % 2 == 0 else 'odd'\n\n\nprint('The number {} is {}.'.format(a_number, parity))\n\nThe number 15 is odd.\n\n\nThe pass statement\nif statements cannot be empty, there must be at least one statement in every if and elif block. We can use the pass statement to do nothing and avoid getting an error.\n\na_number = 9\n\n\nif a_number % 2 == 0:\n    \nelif a_number % 3 == 0:\n    print('{} is divisible by 3 but not divisible by 2')\n\nIndentationError: expected an indented block (1562158884.py, line 3)\n\n\nAs there must be at least one statement withihng the if block, the above code throws an error.\n\nif a_number % 2 == 0:\n    pass\nelif a_number % 3 == 0:\n    print('{} is divisible by 3 but not divisible by 2'.format(a_number))\n\n9 is divisible by 3 but not divisible by 2\n\n\n\n\n1.3.6.2 Iteration with while loops\nAnother powerful feature of programming languages, closely related to branching, is running one or more statements multiple times. This feature is often referred to as iteration on looping, and there are two ways to do this in Python: using while loops and for loops.\nwhile loops have the following syntax:\nwhile condition:\n    statement(s)\nStatements in the code block under while are executed repeatedly as long as the condition evaluates to True. Generally, one of the statements under while makes some change to a variable that causes the condition to evaluate to False after a certain number of iterations.\nLet’s try to calculate the factorial of 100 using a while loop. The factorial of a number n is the product (multiplication) of all the numbers from 1 to n, i.e., 1*2*3*...*(n-2)*(n-1)*n.\n\nresult = 1\ni = 1\n\nwhile i &lt;= 10:\n    result = result * i\n    i = i+1\n\nprint('The factorial of 100 is: {}'.format(result))\n\nThe factorial of 100 is: 3628800\n\n\n\n\n1.3.6.3 Infinite Loops\nSuppose the condition in a while loop always holds true. In that case, Python repeatedly executes the code within the loop forever, and the execution of the code never completes. This situation is called an infinite loop. It generally indicates that you’ve made a mistake in your code. For example, you may have provided the wrong condition or forgotten to update a variable within the loop, eventually falsifying the condition.\nIf your code is stuck in an infinite loop during execution, just press the “Stop” button on the toolbar (next to “Run”) or select “Kernel &gt; Interrupt” from the menu bar. This will interrupt the execution of the code. The following two cells both lead to infinite loops and need to be interrupted.\n\n# INFINITE LOOP - INTERRUPT THIS CELL\n\nresult = 1\ni = 1\n\nwhile i &lt;= 100:\n    result = result * i\n    # forgot to increment i\n\n\n# INFINITE LOOP - INTERRUPT THIS CELL\n\nresult = 1\ni = 1\n\nwhile i &gt; 0 : # wrong condition\n    result *= i\n    i += 1\n\n\n\n1.3.6.4 break and continue statements\nIn Python, break and continue statements can alter the flow of a normal loop. \nWe can use the break statement within the loop’s body to immediately stop the execution and break out of the loop. with the continue statement. If the condition evaluates to True, then the loop will move to the next iteration.\n\ni = 1\nresult = 1\n\nwhile i &lt;= 100:\n    result *= i\n    if i == 42:\n        print('Magic number 42 reached! Stopping execution..')\n        break\n    i += 1\n    \nprint('i:', i)\nprint('result:', result)\n\nMagic number 42 reached! Stopping execution..\ni: 42\nresult: 1405006117752879898543142606244511569936384000000000\n\n\n\ni = 1\nresult = 1\n\nwhile i &lt; 8:\n    i += 1\n    if i % 2 == 0:\n        print('Skipping {}'.format(i))\n        continue\n    print('Multiplying with {}'.format(i))\n    result = result * i\n    \nprint('i:', i)\nprint('result:', result)\n\nSkipping 2\nMultiplying with 3\nSkipping 4\nMultiplying with 5\nSkipping 6\nMultiplying with 7\nSkipping 8\ni: 8\nresult: 105\n\n\nIn the example above, the statement result = result * i inside the loop is skipped when i is even, as indicated by the messages printed during execution.\n\nLogging: The process of adding print statements at different points in the code (often within loops and conditional statements) for inspecting the values of variables at various stages of execution is called logging. As our programs get larger, they naturally become prone to human errors. Logging can help in verifying the program is working as expected. In many cases, print statements are added while writing & testing some code and are removed later.\n\nTask: Guess the output and explain it.\n\n# Use of break statement inside the loop\n\nfor val in \"string\":\n    if val == \"i\":\n        break\n    print(val)\n\nprint(\"The end\")\n\ns\nt\nr\nThe end\n\n\n\n# Program to show the use of continue statement inside loops\n\nfor val in \"string\":\n    if val == \"i\":\n        continue\n    print(val)\n\nprint(\"The end\")\n\ns\nt\nr\nn\ng\nThe end\n\n\n\n\n1.3.6.5 Iteration with for loops\nA for loop is used for iterating or looping over sequences, i.e., lists, tuples, dictionaries, strings, and ranges. For loops have the following syntax:\nfor value in sequence:\n    statement(s)\nThe statements within the loop are executed once for each element in sequence. Here’s an example that prints all the element of a list.\n\ndays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n\nfor day in days:\n    print(day)\n\nMonday\nTuesday\nWednesday\nThursday\nFriday\n\n\n\n# Looping over a string\nfor char in 'Monday':\n    print(char)\n\nM\no\nn\nd\na\ny\n\n\n\n# Looping over a dictionary\nperson = {\n    'name': 'John Doe',\n    'sex': 'Male',\n    'age': 32,\n    'married': True\n}\n\nfor key, value in person.items():\n    print(\"Key:\", key, \",\", \"Value:\", value)\n\nKey: name , Value: John Doe\nKey: sex , Value: Male\nKey: age , Value: 32\nKey: married , Value: True\n\n\n\n\n\n1.3.7 Iterating using range and enumerate\nThe range function is used to create a sequence of numbers that can be iterated over using a for loop. It can be used in 3 ways:\n\nrange(n) - Creates a sequence of numbers from 0 to n-1\nrange(a, b) - Creates a sequence of numbers from a to b-1\nrange(a, b, step) - Creates a sequence of numbers from a to b-1 with increments of step\n\nLet’s try it out.\n\nfor i in range(4):\n    print(i)\n\n0\n1\n2\n3\n\n\n\nfor i in range(3, 8):\n    print(i)\n\n3\n4\n5\n6\n7\n\n\n\nfor i in range(3, 14, 4):\n    print(i)\n\n3\n7\n11\n\n\nRanges are used for iterating over lists when you need to track the index of elements while iterating.\n\na_list = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n\nfor i in range(len(a_list)):\n    print('The value at position {} is {}.'.format(i, a_list[i]))\n\nThe value at position 0 is Monday.\nThe value at position 1 is Tuesday.\nThe value at position 2 is Wednesday.\nThe value at position 3 is Thursday.\nThe value at position 4 is Friday.\n\n\nAnother way to achieve the same result is by using the enumerate function with a_list as an input, which returns a tuple containing the index and the corresponding element.\n\nfor i, val in enumerate(a_list):\n    print('The value at position {} is {}.'.format(i, val))\n\nThe value at position 0 is Monday.\nThe value at position 1 is Tuesday.\nThe value at position 2 is Wednesday.\nThe value at position 3 is Thursday.\nThe value at position 4 is Friday."
  },
  {
    "objectID": "data_structures.html#tuple",
    "href": "data_structures.html#tuple",
    "title": "2  Data structures",
    "section": "2.1 Tuple",
    "text": "2.1 Tuple\nTuple is a sequence of python objects, with two key characteristics: (1) the number of objects are fixed, and (2) the objects are immutable, i.e., they cannot be changed.\nTuple can be defined as a sequence of python objects separated by commas, and enclosed in rounded brackets (). For example, below is a tuple containing three integers.\n\ntuple_example = (2,7,4)\n\nWe can check the data type of a python object using the in-built python function type(). Let us check the data type of the object tuple_example.\n\ntype(tuple_example)\n\ntuple\n\n\n\n2.1.1 Tuple Indexing\nTuple is ordered, meaning you can access specific elements in a list using their index. Indexing in lists includes both positive indexing (starting from 0 for the first element) and negative indexing (starting from -1 for the last element).\n\nElements of a tuple can be extracted using their index within square brackets. For example the second element of the tuple tuple_example can be extracted as follows:\n\ntuple_example[1]\n\n7\n\n\n\ntuple_example[-1]\n\n4\n\n\nNote that an element of a tuple cannot be modified. For example, consider the following attempt in changing the second element of the tuple tuple_example.\n\ntuple_example[1] = 8\n\nTypeError: 'tuple' object does not support item assignment\n\n\nThe above code results in an error as tuple elements cannot be modified.\n\n\n2.1.2 Concatenating tuples\nTuples can be concatenated using the + operator to produce a longer tuple:\n\n(2,7,4) + (\"another\", \"tuple\") + (\"mixed\",\"datatypes\",5)\n\n(2, 7, 4, 'another', 'tuple', 'mixed', 'datatypes', 5)\n\n\nMultiplying a tuple by an integer results in repetition of the tuple:\n\n(2,7,\"hi\") * 3\n\n(2, 7, 'hi', 2, 7, 'hi', 2, 7, 'hi')\n\n\n\n\n2.1.3 Unpacking tuples\nIf tuples are assigned to an expression containing multiple variables, the tuple will be unpacked and each variable will be assigned a value as per the order in which it appears. See the example below.\n\nx,y,z  = (4.5, \"this is a string\", ((\"Nested tuple\",5)))\n\n\nx\n\n4.5\n\n\n\ny\n\n'this is a string'\n\n\n\nz\n\n('Nested tuple', 5)\n\n\nIf we are interested in retrieving only some values of the tuple, the expression *_ can be used to discard the other values. Let’s say we are interested in retrieving only the first and the last two values of the tuple:\n\nx,*_,y,z  = (4.5, \"this is a string\", ((\"Nested tuple\",5)),\"99\",99)\n\n\nx\n\n4.5\n\n\n\ny\n\n'99'\n\n\n\nz\n\n99\n\n\n\n\n2.1.4 Tuple methods\nA couple of useful tuple methods are count, which counts the occurrences of an element in the tuple and index, which returns the position of the first occurrence of an element in the tuple:\n\ntuple_example = (2,5,64,7,2,2)\n\n\ntuple_example.count(2)\n\n3\n\n\n\ntuple_example.index(2)\n\n0\n\n\nNow that we have an idea about tuple, let us try to think where it can be used."
  },
  {
    "objectID": "data_structures.html#list",
    "href": "data_structures.html#list",
    "title": "2  Data structures",
    "section": "2.2 List",
    "text": "2.2 List\nList is a sequence of python objects, with two key characeterisics that differentiates it from tuple: (1) the number of objects are variable, i.e., objects can be added or removed from a list, and (2) the objects are mutable, i.e., they can be changed.\nList can be defined as a sequence of python objects separated by commas, and enclosed in square brackets []. For example, below is a list consisting of three integers.\n\nlist_example = [2,7,4]\n\nList indexing works the same way as tuple indexing.\n\n2.2.1 Slicing a list\nList slicing is a technique in Python that allows you to extract a portion of a list by specifying a range of indices. It creates a new list containing the elements from the original list within that specified range. List slicing uses the colon : operator to indicate the start, stop, and step values for the slice. The general syntax is:  new_list = original_list[start:stop:step]\nHere’s what each part of the slice means: * start: The index at which the slice begins (inclusive). If omitted, it starts from the beginning (index 0). * stop: The index at which the slice ends (exclusive). If omitted, it goes until the end of the list. * step: The interval between elements in the slice. If omitted, it defaults to 1.\n\nlist_example6 = [4,7,3,5,7,1,5,87,5]\n\nLet us extract a slice containing all the elements from the the 3rd position to the 7th position.\n\nlist_example6[2:7]\n\n[3, 5, 7, 1, 5]\n\n\nNote that while the element at the start index is included, the element with the stop index is excluded in the above slice.\nIf either the start or stop index is not mentioned, the slicing will be done from the beginning or until the end of the list, respectively.\n\nlist_example6[:7]\n\n[4, 7, 3, 5, 7, 1, 5]\n\n\n\nlist_example6[2:]\n\n[3, 5, 7, 1, 5, 87, 5]\n\n\nTo slice the list relative to the end, we can use negative indices:\n\nlist_example6[-4:]\n\n[1, 5, 87, 5]\n\n\n\nlist_example6[-4:-2:]\n\n[1, 5]\n\n\nAn extra colon (‘:’) can be used to slice every \\(n\\)th element of a list.\n\n#Selecting every 3rd element of a list\nlist_example6[::3]\n\n[4, 5, 5]\n\n\n\n#Selecting every 3rd element of a list from the end\nlist_example6[::-3]\n\n[5, 1, 3]\n\n\n\n#Selecting every element of a list from the end or reversing a list \nlist_example6[::-1]\n\n[5, 87, 5, 1, 7, 5, 3, 7, 4]\n\n\n\n\n2.2.2 Adding and removing elements in a list\nWe can add elements at the end of the list using the append method. For example, we append the string ‘red’ to the list list_example below.\n\nlist_example.append('red')\n\n\nlist_example\n\n[2, 7, 4, 'red']\n\n\nNote that the objects of a list or a tuple can be of different datatypes.\nAn element can be added at a specific location of the list using the insert method. For example, if we wish to insert the number 2.32 as the second element of the list list_example, we can do it as follows:\n\nlist_example.insert(1,2.32)\n\n\nlist_example\n\n[2, 2.32, 7, 4, 'red']\n\n\nFor removing an element from the list, the pop and remove methods may be used. The pop method removes an element at a particular index, while the remove method removes the element’s first occurence in the list by its value. See the examples below.\nLet us say, we need to remove the third element of the list.\n\nlist_example.pop(2)\n\n7\n\n\n\nlist_example\n\n[2, 2.32, 4, 'red']\n\n\nLet us say, we need to remove the element ‘red’.\n\nlist_example.remove('red')\n\n\nlist_example\n\n[2, 2.32, 4]\n\n\n\n#If there are multiple occurences of an element in the list, the first occurence will be removed\nlist_example2 = [2,3,2,4,4]\nlist_example2.remove(2)\nlist_example2\n\n[3, 2, 4, 4]\n\n\nFor removing multiple elements in a list, either pop or remove can be used in a for loop, or a for loop can be used with a condition. See the examples below.\nLet’s say we need to remove intergers less than 100 from the following list.\n\nlist_example3 = list(range(95,106))\nlist_example3\n\n[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]\n\n\n\n#Method 1: For loop with remove, iterating over the elements of the original list, \n#but updating a copy of the original list\nlist_example3_filtered = list(list_example3) #\nfor element in list_example3:\n    if element&lt;100:\n        list_example3_filtered.remove(element)\nprint(list_example3_filtered)\n\n[100, 101, 102, 103, 104, 105]\n\n\n\\(\\color{red}{\\text{Q1}}\\): What’s the need to define a new variable list_example3_filtered in the above code?\n\\(\\color{blue}{\\text{A1}}\\): Replace list_example3_filtered with list_example3 and identify the issue. After an element is removed from the list, all the elements that come afterward have their index/position reduced by one. After the elment 95 is removed, 96 is at index 0, but the for loop will now look at the element at index 1, which is now 97. So, iterating over the same list that is being updated in the loop will keep 96 and 98. Using a new list gets rid of the issue by keeping the original list unchanged, so the for-loop iterates over all elements of the original list.\nAnother method could have been to interate over a copy of the original list and update the original list as shown below.\n\n#Method 2: For loop with remove, iterating over the elements of a copy of the original list, \n#but updating the original list\nfor element in list_example3[:]: #Slicing a list creates a new list, thus the loop is iterating over elements of a copy of the original list as all the elements are selected in the slicing\n    if element&lt;100:\n        list_example3.remove(element)\nprint(list_example3)\n\n[100, 101, 102, 103, 104, 105]\n\n\nBelow is another method that uses a shorthand notation - list comprehension (explained in the next section).\n\n#Method 3: For loop with condition in list comprehension\nlist_example3 = list(range(95,106))\n[element for element in list_example3 if element&gt;=100]\n\n[100, 101, 102, 103, 104, 105]\n\n\n\n\n2.2.3 List comprehensions\nList comprehensions provide a concise and readable way to create new lists by applying an expression to each item in an iterable (e.g., a list, tuple, or range) and optionally filtering the items based on a condition. They are a powerful and efficient way to generate lists without the need for explicit loops. The basic syntax of a list comprehension is as follows:\nnew_list = [expression for item in iterable if condition]\n\nexpression: This is the expression that is applied to each item in the iterable. It defines what will be included in the new list.\nitem: This is a variable that represents each element in the iterable as the comprehension iterates through it.\niterable: This is the source from which the elements are taken. It can be any iterable, such as a list, tuple, range, or other iterable objects.\ncondition (optional): This is an optional filter that can be applied to control which items from the iterable are included in the new list. If omitted, all items from the iterable are included.\n\nExample: Create a list that has squares of natural numbers from 5 to 15.\n\nsqrt_natural_no_5_15 = [(x**2) for x in range(5,16)]\nprint(sqrt_natural_no_5_15)\n\n[25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225]\n\n\nExample: Create a list of tuples, where each tuple consists of a natural number and its square, for natural numbers ranging from 5 to 15.\n\nsqrt_natural_no_5_15 = [(x,x**2) for x in range(5,16)]\nprint(sqrt_natural_no_5_15)\n\n[(5, 25), (6, 36), (7, 49), (8, 64), (9, 81), (10, 100), (11, 121), (12, 144), (13, 169), (14, 196), (15, 225)]\n\n\nExample: Creating a list of words that start with the letter ‘a’ in a given list of words.\n\nwords = ['apple', 'banana', 'avocado', 'grape', 'apricot']\na_words = [word for word in words if word.startswith('a')]\nprint(a_words)\n\n['apple', 'avocado', 'apricot']\n\n\nExample: Create a list of even numbers from 1 to 20.\n\neven_numbers = [x for x in range(1, 21) if x % 2 == 0]\nprint(even_numbers)\n\n[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n\n\nList comprehensions are not only concise but also considered more Pythonic and often more efficient than using explicit loops for simple operations. They can make your code cleaner and easier to read, especially for operations that transform or filter data in a list.\n\n\n2.2.4 Practice exercise 1\nBelow is a list consisting of responses to the question: “At what age do you think you will marry?” from students of the STAT303-1 Fall 2022 class.\n\nexp_marriage_age=['24','30','28','29','30','27','26','28','30+','26','28','30','30','30','probably never','30','25','25','30','28','30+ ','30','25','28','28','25','25','27','28','30','30','35','26','28','27','27','30','25','30','26','32','27','26','27','26','28','37','28','28','28','35','28','27','28','26','28','26','30','27','30','28','25','26','28','35','29','27','27','30','24','25','29','27','33','30','30','25','26','30','32','26','30','30','I wont','25','27','27','25','27','27','32','26','25','never','28','33','28','35','25','30','29','30','31','28','28','30','40','30','28','30','27','by 30','28','27','28','30-35','35','30','30','never','30','35','28','31','30','27','33','32','27','27','26','N/A','25','26','29','28','34','26','24','28','30','120','25','33','27','28','32','30','26','30','30','28','27','27','27','27','27','27','28','30','30','30','28','30','28','30','30','28','28','30','27','30','28','25','never','69','28','28','33','30','28','28','26','30','26','27','30','25','Never','27','27','25']\n\nUse list comprehension to:\n\n2.2.4.1 \nRemove the elements that are not integers - such as ‘probably never’, ‘30+’, etc. What is the length of the new list?\nHint: The built-in python function of the str class - isdigit() may be useful to check if the string contains only digits.\nSolution:\n\nexp_marriage_age_num = [x for x in exp_marriage_age if x.isdigit()==True]\nprint(\"Length of the new list = \",len(exp_marriage_age_num))\n\nLength of the new list =  181\n\n\n\n\n2.2.4.2 \nCap the values greater than 80 to 80, in the clean list obtained in (1). What is the mean age when people expect to marry in the new list?\n\nexp_marriage_age_capped = [min(int(x),80) for x in exp_marriage_age_num]\nprint(\"Mean age when people expect to marry = \", sum(exp_marriage_age_capped)/len(exp_marriage_age_capped))\n\nMean age when people expect to marry =  28.955801104972377\n\n\n\n\n2.2.4.3 \nDetermine the percentage of people who expect to marry at an age of 30 or more.\n\nprint(\"Percentage of people who expect to marry at an age of 30 or more =\", str(100*sum([1 for x in exp_marriage_age_capped if x&gt;=30])/len(exp_marriage_age_capped)),\"%\")\n\nPercentage of people who expect to marry at an age of 30 or more = 37.01657458563536 %\n\n\n\n\n2.2.4.4 \nRedo Q2.2.4.2 using the if-else statement within list comprehension.\n\n\n\n2.2.5 Practice exercise 2\nBelow is a list consisting of responses to the question: “What do you expect your starting salary to be after graduation, to the nearest thousand dollars? (ex: 47000)” from students of the STAT303-1 Fall 2023. class.\n\nexpected_salary = ['90000', '110000', '100000', '90k', '80000', '47000', '100000', '70000', '95000', '150000', '50000', '110000', '100000', '60000', '50000', '100000', '80000', '100000', '70000', '60000', '100k', '70000', '0', '60000', '50000', '150000', '90000', '80000', '110000', '85000', '90000', '50000', '60000', '150000', '100000', '100000', '125000', '30000', '100000', '110000', '90000', '600000', '80000', '100000', '100000', '70000', '60000', '0', '70000', '90000', '100000', '60000', '80000', '70000', '100000', '57000', '70000', '60000', '65000', '70000', '100000', '200000', '60000', '90000', '80000', '200000', '90000', '80000', '60000', '70000', '90000', '80000', '90000', '120000', '60000', '40000', '80000', '100000', '75000', '80000', '70000', '90000', '80000', '80000', '70000', '0', '50000', '65000', 'n/a', '100000', '60000', '65000', '100000', '100000', '65000', '90000', '50000', '80000', '90000', '100000', '100000', '100000', '100000', '80000', '60000', '100000', '80000', '55000', '80000', '100000', '60000', '130000', '35000', '70000', '50000', '120000', '110000', '110000', '80000', '70000', '90000', '100000', '90000', '100000', '70000', '110000', '300000', '90000', '45000', '90000', '60000', '44000', '1000000', '65000', '40000', '60000', '100000', '80000', '90000', '45000', '86000', '100000', '100,000+', '50000', '0']\n\nClean expected_salary using list comprehensions only, and find the mean expected salary.\n\n\n2.2.6 Concatenating lists\nAs in tuples, lists can be concatenated using the + operator:\n\nimport time as tm\n\n\nlist_example4 = [5,'hi',4] \nlist_example4 = list_example4 + [None,'7',9]\nlist_example4\n\n[5, 'hi', 4, None, '7', 9]\n\n\nFor adding elements to a list, the extend method is preferred over the + operator. This is because the + operator creates a new list, while the extend method adds elements to an existing list. Thus, the extend operator is more memory efficient.\n\nlist_example4 = [5,'hi',4]\nlist_example4.extend([None, '7', 9])\nlist_example4\n\n[5, 'hi', 4, None, '7', 9]\n\n\n\n\n2.2.7 Sorting a list\nA list can be sorted using the sort method:\n\nlist_example5 = [6,78,9]\nlist_example5.sort(reverse=True) #the reverse argument is used to specify if the sorting is in ascending or descending order\nlist_example5\n\n[78, 9, 6]\n\n\n\n\n2.2.8 Practice exercise 3\nStart with the list [8,9,10]. Do the following:\n\n2.2.8.1 \nSet the second entry (index 1) to 17\n\nL = [8,9,10]\nL[1]=17\n\n\n\n2.2.8.2 \nAdd 4, 5, and 6 to the end of the list\n\nL = L+[4,5,6]\n\n\n\n2.2.8.3 \nRemove the first entry from the list\n\nL.pop(0)\n\n8\n\n\n\n\n2.2.8.4 \nSort the list\n\nL.sort()\n\n\n\n2.2.8.5 \nDouble the list (concatenate the list to itself)\n\nL=L+L\n\n\n\n2.2.8.6 \nInsert 25 at index 3\nThe final list should equal [4,5,6,25,10,17,4,5,6,10,17]\n\nL.insert(3,25)\nL\n\n[4, 5, 6, 25, 10, 17, 4, 5, 6, 10, 17]\n\n\nNow that we have an idea about lists, let us try to think where it can be used.\n\n\n\n\n\n \n        \n\n\n\n\n\n2.2.9 Other list operations\nYou can test whether a list contains a value using the in operator.\n\nlist_example6\n\n[4, 7, 3, 5, 7, 1, 5, 87, 5]\n\n\n\n6 in list_example6\n\nFalse\n\n\n\n7 in list_example6\n\nTrue\n\n\n\n\n2.2.10 Lists: methods\nJust like strings, there are several in-built methods to manipulate a list. However, unlike strings, most list methods modify the original list rather than returning a new one. Here are some common list operations: \n\n\n\n2.2.11 Lists vs tuples\nNow that we have learned about lists and tuples, let us compare them.\n\\(\\color{red}{\\text{Q2}}\\): A list seems to be much more flexible than tuple, and can replace a tuple almost everywhere. Then why use tuple at all?\n\\(\\color{blue}{\\text{A2}}\\): The additional flexibility of a list comes at the cost of efficiency. Some of the advantages of a tuple over a list are as follows:\n\nSince a list can be extended, space is over-allocated when creating a list. A tuple takes less storage space as compared to a list of the same length.\nTuples are not copied. If a tuple is assigned to another tuple, both tuples point to the same memory location. However, if a list is assigned to another list, a new list is created consuming the same memory space as the original list.\nTuples refer to their element directly, while in a list, there is an extra layer of pointers that refers to their elements. Thus it is faster to retrieve elements from a tuple.\n\nThe examples below illustrate the above advantages of a tuple.\n\n#Example showing tuples take less storage space than lists for the same elements\ntuple_ex = (1, 2, 'Obama')\nlist_ex = [1, 2, 'Obama']\nprint(\"Space taken by tuple =\",tuple_ex.__sizeof__(),\" bytes\")\nprint(\"Space taken by list =\",list_ex.__sizeof__(),\" bytes\")\n\nSpace taken by tuple = 48  bytes\nSpace taken by list = 64  bytes\n\n\n\n#Examples showing that a tuples are not copied, while lists can be copied\ntuple_copy = tuple(tuple_ex)\nprint(\"Is tuple_copy same as tuple_ex?\", tuple_ex is tuple_copy)\nlist_copy = list(list_ex)\nprint(\"Is list_copy same as list_ex?\",list_ex is list_copy)\n\nIs tuple_copy same as tuple_ex? True\nIs list_copy same as list_ex? False\n\n\n\n#Examples showing tuples takes lesser time to retrieve elements\nimport time as tm\ntt = tm.time()\nlist_ex = list(range(1000000)) #List containinig whole numbers upto 1 million\na=(list_ex[::-2])\nprint(\"Time take to retrieve every 2nd element from a list = \", tm.time()-tt)\n\ntt = tm.time()\ntuple_ex = tuple(range(1000000)) #tuple containinig whole numbers upto 1 million\na=(tuple_ex[::-2])\nprint(\"Time take to retrieve every 2nd element from a tuple = \", tm.time()-tt)\n\nTime take to retrieve every 2nd element from a list =  0.03579902648925781\nTime take to retrieve every 2nd element from a tuple =  0.02684164047241211"
  },
  {
    "objectID": "data_structures.html#dictionary",
    "href": "data_structures.html#dictionary",
    "title": "2  Data structures",
    "section": "2.3 Dictionary",
    "text": "2.3 Dictionary\nUnlike lists and tuples, a dictionary is an unordered collection of items. Each item stored in a dictionary has a key and value. You can use a key to retrieve the corresponding value from the dictionary. Dictionaries have the type dict.\nDictionaries are often used to store many pieces of information e.g. details about a person, in a single variable. Dictionaries are created by enclosing key-value pairs within braces or curly brackets { and }, colons to separate keys and values, and commas to separate elements of a dictionary.\nThe dictionary keys and values are python objects. While values can be any python object, keys need to be immutable python objects, like strings, intergers, tuples, etc. Thus, a list can be a value, but not a key, as elements of list can be changed.\n\ndict_example = {'USA':'Joe Biden', 'India':'Narendra Modi', 'China':'Xi Jinping'}\n\nElements of a dictionary can be retrieved by using the corresponding key.\n\ndict_example['India']\n\n'Narendra Modi'\n\n\n\n2.3.1 Viewing keys and values\n\ndict_example.keys()\n\ndict_keys(['USA', 'India', 'China'])\n\n\n\ndict_example.values()\n\ndict_values(['Joe Biden', 'Narendra Modi', 'Xi Jinping'])\n\n\n\ndict_example.items()\n\ndict_items([('USA', 'Joe Biden'), ('India', 'Narendra Modi'), ('China', 'Xi Jinping')])\n\n\nThe results of keys, values, and items look like lists. However, they don’t support the indexing operator [] for retrieving elements.\n\ndict_example.items()[1]\n\nTypeError: 'dict_items' object is not subscriptable\n\n\n\n\n2.3.2 Adding and removing elements in a dictionary\nNew elements can be added to a dictionary by defining a key in square brackets and assiging it to a value:\n\ndict_example['Japan'] = 'Fumio Kishida'\ndict_example['Countries'] = 4\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Japan': 'Fumio Kishida',\n 'Countries': 4}\n\n\nElements can be removed from the dictionary using the del method or the pop method:\n\n#Removing the element having key as 'Countries'\ndel dict_example['Countries']\n\n\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Japan': 'Fumio Kishida'}\n\n\n\n#Removing the element having key as 'USA'\ndict_example.pop('USA')\n\n'Joe Biden'\n\n\n\ndict_example\n\n{'India': 'Narendra Modi', 'China': 'Xi Jinping', 'Japan': 'Fumio Kishida'}\n\n\nNew elements can be added, and values of exisiting keys can be changed using the update method:\n\ndict_example = {'USA':'Joe Biden', 'India':'Narendra Modi', 'China':'Xi Jinping','Countries':3}\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Countries': 3}\n\n\n\ndict_example.update({'Countries':4, 'Japan':'Fumio Kishida'})\n\n\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Countries': 4,\n 'Japan': 'Fumio Kishida'}\n\n\n\n\n2.3.3 Iterating over elements of a dictionary\nThe items() attribute of a dictionary can be used to iterate over elements of a dictionary.\n\nfor key,value in dict_example.items():\n    print(\"The Head of State of\",key,\"is\",value)\n\nThe Head of State of USA is Joe Biden\nThe Head of State of India is Narendra Modi\nThe Head of State of China is Xi Jinping\nThe Head of State of Countries is 4\nThe Head of State of Japan is Fumio Kishida\n\n\n\n\n2.3.4 Practice exercise 4\nThe GDP per capita of USA for most years from 1960 to 2021 is given by the dictionary D given in the code cell below.\nFind:\n\nThe GDP per capita in 2015\nThe GDP per capita of 2014 is missing. Update the dictionary to include the GDP per capita of 2014 as the average of the GDP per capita of 2013 and 2015.\nImpute the GDP per capita of other missing years in the same manner as in (2), i.e., as the average GDP per capita of the previous year and the next year. Note that the GDP per capita is not missing for any two consecutive years.\nPrint the years and the imputed GDP per capita for the years having a missing value of GDP per capita in (3).\n\n\nD = {'1960':3007,'1961':3067,'1962':3244,'1963':3375,'1964':3574,'1965':3828,'1966':4146,'1967':4336,'1968':4696,'1970':5234,'1971':5609,'1972':6094,'1973':6726,'1974':7226,'1975':7801,'1976':8592,'1978':10565,'1979':11674, '1980':12575,'1981':13976,'1982':14434,'1983':15544,'1984':17121,'1985':18237,  '1986':19071,'1987':20039,'1988':21417,'1989':22857,'1990':23889,'1991':24342,  '1992':25419,'1993':26387,'1994':27695,'1995':28691,'1996':29968,'1997':31459,  '1998':32854,'2000':36330,'2001':37134,'2002':37998,'2003':39490,'2004':41725,  '2005':44123,'2006':46302,'2007':48050,'2008':48570,'2009':47195,'2010':48651,  '2011':50066,'2012':51784,'2013':53291,'2015':56763,'2016':57867,'2017':59915,'2018':62805, '2019':65095,'2020':63028,'2021':69288}\n\nSolution:\n\nprint(\"GDP per capita in 2015 =\", D['2015'])\nD['2014'] = (D['2013']+D['2015'])/2\nfor i in range(1960,2021):\n    if str(i) not in D.keys():    \n        D[str(i)] = (D[str(i-1)]+D[str(i+1)])/2\n        print(\"Imputed GDP per capita for the year\",i,\"is $\",D[str(i)])\n\nGDP per capita in 2015 = 56763\nImputed GDP per capita for the year 1969 is $ 4965.0\nImputed GDP per capita for the year 1977 is $ 9578.5\nImputed GDP per capita for the year 1999 is $ 34592.0"
  },
  {
    "objectID": "data_structures.html#functions",
    "href": "data_structures.html#functions",
    "title": "2  Data structures",
    "section": "2.4 Functions",
    "text": "2.4 Functions\nIf an algorithm or block of code is being used several times in a code, then it can be separately defined as a function. This makes the code more organized and readable. For example, let us define a function that prints prime numbers between a and b, and returns the number of prime numbers found.\n\n#Function definition\ndef prime_numbers (a,b=100):\n    num_prime_nos = 0\n    \n    #Iterating over all numbers between a and b\n    for i in range(a,b):\n        num_divisors=0\n        \n        #Checking if the ith number has any factors\n        for j in range(2, i):\n            if i%j == 0:\n                num_divisors=1;break;\n                \n        #If there are no factors, then printing and counting the number as prime        \n        if num_divisors==0:\n            print(i)\n            num_prime_nos = num_prime_nos+1\n            \n    #Return count of the number of prime numbers\n    return num_prime_nos\n\nIn the above function, the keyword def is used to define the function, prime_numbers is the name of the function, a and b are the arguments that the function uses to compute the output.\nLet us use the defined function to print and count the prime numbers between 40 and 60.\n\n#Printing prime numbers between 40 and 60\nnum_prime_nos_found = prime_numbers(40,60)\n\n41\n43\n47\n53\n59\n\n\n\nnum_prime_nos_found\n\n5\n\n\nIf the user calls the function without specifying the value of the argument b, then it will take the default value of 100, as mentioned in the function definition. However, for the argument a, the user will need to specify a value, as there is no value defined as a default value in the function definition.\n\n2.4.1 Global and local variables with respect to a function\nA variable defined within a function is local to that function, while a variable defined outside the function is global to that function. In case a variable with the same name is defined both outside and inside a function, it will refer to its global value outside the function and local value within the function.\nThe example below shows a variable with the name var referring to its local value when called within the function, and global value when called outside the function.\n\nvar = 5\ndef sample_function(var):    \n    print(\"Local value of 'var' within 'sample_function()'= \",var)\n\nsample_function(4)\nprint(\"Global value of 'var' outside 'sample_function()' = \",var)\n\nLocal value of 'var' within 'sample_function()'=  4\nGlobal value of 'var' outside 'sample_function()' =  5\n\n\n\n\n2.4.2 Practice exercise 5\nThe object deck defined below corresponds to a deck of cards. Estimate the probablity that a five card hand will be a flush, as follows:\n\nWrite a function that accepts a hand of 5 cards as argument, and returns whether the hand is a flush or not.\nRandomly pull a hand of 5 cards from the deck. Call the function developed in (1) to determine if the hand is a flush.\nRepeat (2) 10,000 times.\nEstimate the probability of the hand being a flush from the results of the 10,000 simulations.\n\nYou may use the function shuffle() from the random library to shuffle the deck everytime before pulling a hand of 5 cards.\n\ndeck = [{'value':i, 'suit':c}\nfor c in ['spades', 'clubs', 'hearts', 'diamonds']\nfor i in range(2,15)]\n\nSolution:\n\nimport random as rm\n\n#Function to check if a 5-card hand is a flush\ndef chck_flush(hands):  \n    \n    #Assuming that the hand is a flush, before checking the cards\n    yes_flush =1\n    \n    #Storing the suit of the first card in 'first_suit'\n    first_suit = hands[0]['suit']\n    \n    #Iterating over the remaining 4 cards of the hand\n    for j in range(1,len(hands)):\n        \n        #If the suit of any of the cards does not match the suit of the first card, the hand is not a flush\n        if first_suit!=hands[j]['suit']:\n            yes_flush = 0; \n            \n            #As soon as a card with a different suit is found, the hand is not a flush and there is no need to check other cards. So, we 'break' out of the loop\n            break;\n    return yes_flush\n\nflush=0\nfor i in range(10000):\n    \n    #Shuffling the deck\n    rm.shuffle(deck)\n    \n    #Picking out the first 5 cards of the deck as a hand and checking if they are a flush\n    #If the hand is a flush it is counted\n    flush=flush+chck_flush(deck[0:5])\n    \nprint(\"Probability of obtaining a flush=\", 100*(flush/10000),\"%\")\n\nProbability of obtaining a flush= 0.18 %"
  },
  {
    "objectID": "data_structures.html#practice-exercise-6",
    "href": "data_structures.html#practice-exercise-6",
    "title": "2  Data structures",
    "section": "2.5 Practice exercise 6",
    "text": "2.5 Practice exercise 6\nThe code cell below defines an object having the nutrition information of drinks in starbucks. Assume that the manner in which the information is structured is consistent throughout the object.\n\nstarbucks_drinks_nutrition={'Cool Lime Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 45}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 11}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Strawberry Acai Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 80}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 18}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Very Berry Hibiscus Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 14}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Evolution Fresh™ Organic Ginger Limeade': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Iced Coffee': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Iced Espresso Classics - Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 21}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 65}], 'Iced Espresso Classics - Caffe Mocha': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 23}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 90}], 'Iced Espresso Classics - Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 21}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 65}], 'Shaken Sweet Tea': [{'Nutrition_type': 'Calories', 'value': 80}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Berry Blossom White': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Black Mango': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 38}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Black with Lemon': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Brambleberry': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Giant Peach': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Iced Passion': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Lemon Ginger': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 31}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Black Lemonade': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Iced Black Tea': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Iced Green Tea': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 31}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Plum Pomegranate': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Tazoberry': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 38}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled White Cranberry': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Teavana® Shaken Iced Black Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Black Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Teavana® Shaken Iced Green Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Green Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Passion Tango™ Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Passion Tango™ Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 24}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Teavana® Shaken Iced Peach Green Tea': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks Refreshers™ Raspberry Pomegranate': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 27}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks Refreshers™ Strawberry Lemonade': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 27}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Doubleshot Protein Dark Chocolate': [{'Nutrition_type': 'Calories', 'value': 210}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 33}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Starbucks® Doubleshot Protein Vanilla': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 120}], 'Starbucks® Iced Coffee Caramel': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 13}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Iced Coffee Light Sweetened': [{'Nutrition_type': 'Calories', 'value': 50}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 11}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Iced Coffee Unsweetened': [{'Nutrition_type': 'Calories', 'value': 10}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 2}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Blonde Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Clover® Brewed Coffee': [{'Nutrition_type': 'Calories', 'value': 10}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Decaf Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Featured Dark Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Nariño 70 Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Nariño 70 Cold Brew with Milk': [{'Nutrition_type': 'Calories', 'value': 0}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Nitro Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Nitro Cold Brew with Sweet Cream': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 5.0}, {'Nutrition_type': 'Carb', 'value': 5}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 20}], 'Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Vanilla Sweet Cream Cold Brew': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 14}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 25}], 'Hot Chocolate': [{'Nutrition_type': 'Calories', 'value': 320}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 47}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 14}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Starbucks® Signature Hot Chocolate': [{'Nutrition_type': 'Calories', 'value': 430}, {'Nutrition_type': 'Fat', 'value': 26.0}, {'Nutrition_type': 'Carb', 'value': 45}, {'Nutrition_type': 'Fiber', 'value': 5}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 13}, {'Nutrition_type': 'Sodium', 'value': 170}], 'Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 290}, {'Nutrition_type': 'Fat', 'value': 8.0}, {'Nutrition_type': 'Carb', 'value': 42}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 13}, {'Nutrition_type': 'Sodium', 'value': 140}], 'Cappuccino': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 12}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 8}, {'Nutrition_type': 'Sodium', 'value': 100}], 'Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 260}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 40}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 11}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 32}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Flat White': [{'Nutrition_type': 'Calories', 'value': 180}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 18}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Iced Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 4.5}, {'Nutrition_type': 'Carb', 'value': 13}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 8}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Iced Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 230}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 36}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 9}, {'Nutrition_type': 'Sodium', 'value': 90}], 'Iced Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Iced Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 7}, {'Nutrition_type': 'Sodium', 'value': 95}], 'Iced Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories', 'value': 260}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 11}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Iced Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 30}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 7}, {'Nutrition_type': 'Sodium', 'value': 100}], 'Iced White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 300}, {'Nutrition_type': 'Fat', 'value': 8.0}, {'Nutrition_type': 'Carb', 'value': 47}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 190}], 'Latte Macchiato': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Starbucks Doubleshot® on Ice Beverage': [{'Nutrition_type': 'Calories', 'value': 45}, {'Nutrition_type': 'Fat', 'value': 1.0}, {'Nutrition_type': 'Carb', 'value': 5}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 3}, {'Nutrition_type': 'Sodium', 'value': 40}], 'Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 150}], 'White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 360}, {'Nutrition_type': 'Fat', 'value': 11.0}, {'Nutrition_type': 'Carb', 'value': 53}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 14}, {'Nutrition_type': 'Sodium', 'value': 240}], 'Cinnamon Dolce Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 350}, {'Nutrition_type': 'Fat', 'value': 4.5}, {'Nutrition_type': 'Carb', 'value': 64}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 15}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Coffee Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 24}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 3}, {'Nutrition_type': 'Sodium', 'value': 200}], 'Mocha Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 280}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 60}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 4}, {'Nutrition_type': 'Sodium', 'value': 220}], 'Mocha Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.5}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 4}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Cinnamon Dolce Crème': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 135}], 'Vanilla Crème': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 135}], 'Chocolate Smoothie': [{'Nutrition_type': 'Calories', 'value': 320}, {'Nutrition_type': 'Fat', 'value': 5.0}, {'Nutrition_type': 'Carb', 'value': 53}, {'Nutrition_type': 'Fiber', 'value': 8}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 170}], 'Strawberry Smoothie': [{'Nutrition_type': 'Calories', 'value': 300}, {'Nutrition_type': 'Fat', 'value': 2.0}, {'Nutrition_type': 'Carb', 'value': 60}, {'Nutrition_type': 'Fiber', 'value': 7}, {'Nutrition_type': 'Protein', 'value': 16}, {'Nutrition_type': 'Sodium', 'value': 130}]}\n\nUse the object above to answer the following questions:\n\n2.5.1 \nWhat is the datatype of the object?\n\nprint(\"Datatype=\",type(starbucks_drinks_nutrition)) \n\nDatatype= &lt;class 'dict'&gt;\n\n\n\n2.5.1.1 \nIf the object in (1) is a dictonary, what is the datatype of the values of the dictionary?\n\nprint(\"Datatype=\",type(starbucks_drinks_nutrition[list(starbucks_drinks_nutrition.keys())[0]]))\n\nDatatype= &lt;class 'list'&gt;\n\n\n\n\n2.5.1.2 \nIf the object in (1) is a dictonary, what is the datatype of the elements within the values of the dictionary?\n\nprint(\"Datatype=\",type(starbucks_drinks_nutrition[list(starbucks_drinks_nutrition.keys())[0]][0]))\n\nDatatype= &lt;class 'dict'&gt;\n\n\n\n\n2.5.1.3 \nHow many calories are there in Iced Coffee?\n\nprint(\"Calories = \",starbucks_drinks_nutrition['Iced Coffee'][0]['value'])\n\nCalories =  5\n\n\n\n\n2.5.1.4 \nWhich drink(s) have the highest amount of protein in them, and what is that protein amount?\n\n#Defining an empty dictionary that will be used to store the protein of each drink\nprotein={}\n\nfor key,value in starbucks_drinks_nutrition.items():\n    for nutrition in value:        \n        if nutrition['Nutrition_type']=='Protein':\n            protein[key]=(nutrition['value'])\n\n#Using dictionary comprehension to find the key-value pair having the maximum value in the dictionary\n{key:value for key, value in protein.items() if value == max(protein.values())}\n\n{'Starbucks® Doubleshot Protein Dark Chocolate': 20,\n 'Starbucks® Doubleshot Protein Vanilla': 20,\n 'Chocolate Smoothie': 20}\n\n\n\n\n2.5.1.5 \nWhich drink(s) have a fat content of more than 10g, and what is their fat content?\n\n#Defining an empty dictionary that will be used to store the fat of each drink\nfat={}\nfor key,value in starbucks_drinks_nutrition.items():\n    for nutrition in value:        \n        if nutrition['Nutrition_type']=='Fat':\n            fat[key]=(nutrition['value'])\n            \n#Using dictionary comprehension to find the key-value pair having the value more than 10\n{key:value for key, value in fat.items() if value&gt;=10}\n\n{'Starbucks® Signature Hot Chocolate': 26.0, 'White Chocolate Mocha': 11.0}\n\n\n\n\n2.5.1.6 \nAnswer Q2.5.1.5 using only dictionary comprehension."
  },
  {
    "objectID": "Reading data.html#types-of-data---structured-and-unstructured",
    "href": "Reading data.html#types-of-data---structured-and-unstructured",
    "title": "3  Reading data",
    "section": "3.1 Types of data - structured and unstructured",
    "text": "3.1 Types of data - structured and unstructured\nReading data is the first step to extract information from it. Data can exist broadly in two formats:\n\nStructured data, and\nUntructured data.\n\nStructured data is typically stored in a tabular form, where rows in the data correspond to “observations” and columns correspond to “variables”. For example, the following dataset contains 5 observations, where each observation (or row) consists of information about a movie. The variables (or columns) contain different pieces of information about a given movie. As all variables for a given row are related to the same movie, the data below is also called relational data.\n\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nProduction Budget\nRelease Date\nMajor Genre\nCreative Type\nRotten Tomatoes Rating\nIMDB Rating\n\n\n\n\n0\nThe Shawshank Redemption\n28241469\n25000000\nSep 23 1994\nDrama\nFiction\n88\n9.2\n\n\n1\nInception\n285630280\n160000000\nJul 16 2010\nHorror/Thriller\nFiction\n87\n9.1\n\n\n2\nOne Flew Over the Cuckoo's Nest\n108981275\n4400000\nNov 19 1975\nComedy\nFiction\n96\n8.9\n\n\n3\nThe Dark Knight\n533345358\n185000000\nJul 18 2008\nAction/Adventure\nFiction\n93\n8.9\n\n\n4\nSchindler's List\n96067179\n25000000\nDec 15 1993\nDrama\nNon-Fiction\n97\n8.9\n\n\n\n\n\n\n\nUnstructured data is data that is not organized in any pre-defined manner. Examples of unstructured data can be text files, audio/video files, images, Internet of Things (IoT) data, etc. Unstructured data is relatively harder to analyze as most of the analytical methods and tools are oriented towards structured data. However, an unstructured data can be used to obtain structured data, which in turn can be analyzed. For example, an image can be converted to an array of pixels - which will be structured data. Machine learning algorithms can then be used on the array to classify the image as that of a dog or a cat.\nIn this course, we will focus on analyzing structured data."
  },
  {
    "objectID": "Reading data.html#reading-a-csv-file-with-pandas",
    "href": "Reading data.html#reading-a-csv-file-with-pandas",
    "title": "3  Reading data",
    "section": "3.2 Reading a csv file with Pandas",
    "text": "3.2 Reading a csv file with Pandas\nStructured data can be stored in a variety of formats. The most popular format is data_file_name.csv, where the extension csv stands for comma separated values. The variable values of each observation are separated by a comma in a .csv file. In other words, the delimiter is a comma in a csv file. However, the comma is not visible when a .csv file is opened with Microsoft Excel.\n\n3.2.1 Using the read_csv function\nWe will use functions from the Pandas library of Python to read data. Let us import Pandas to use its functions.\n\nimport pandas as pd\n\nNote that pd is the acronym that we will use to call a Pandas function. This acronym can be anything as desired by the user.\nThe function to read a csv file is read_csv(). It reads the dataset into an object of type Pandas DataFrame. Let us read the dataset movie_ratings.csv in Python.\n\nmovie_ratings = pd.read_csv('movie_ratings.csv')\n\nThe built-in python function type can be used to check the dataype of an object:\n\ntype(movie_ratings)\n\npandas.core.frame.DataFrame\n\n\nNote that the file movie_ratings.csv is stored at the same location as the python script containing the above code. If that is not the case, we’ll need to specify the location of the file as in the following code.\n\nmovie_ratings = pd.read_csv('D:/Books/DataScience_Intro_python/movie_ratings.csv')\n\nNote that forward slash is used instead of backslash while specifying the path of the data file. Another option is to use two consecutive backslashes instead of a single forward slash.\n\n\n3.2.2 Specifying the working directory\nIn case we need to read several datasets from a given location, it may be inconvenient to specify the path every time. In such a case we can change the current working directory to the location where the datasets are located.\nWe’ll use the os library of Python to view and/or change the current working directory.\n\nimport os #Importing the 'os' library\nos.getcwd() #Getting the path to the current working directory\n\n\n\nC:\\Users\\username\\STAT303-1\\Quarto Book\\DataScience_Intro_python\n\n\nThe function getcwd() stands for get current working directory.\nSuppose the dataset to be read is located at 'D:\\Books\\DataScience_Intro_python\\Datasets'. Then, we’ll use the function chdir to change the current working directory to this location.\n\nos.chdir('D:/Books/DataScience_Intro_python/Datasets')\n\nNow we can read the dataset from this location without mentioning the entire path as shown below.\n\nmovie_ratings = pd.read_csv('movie_ratings.csv')\n\n\n\n3.2.3 Data overview\nOnce the data has been read, we may want to see what the data looks like. We’ll use another Pandas function head() to view the first few rows of the data.\n\nmovie_ratings.head()\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n0\nOpal Dreams\n14443\n14443\n9000000\nNov 22 2006\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n6.5\n468\n\n\n1\nMajor Dundee\n14873\n14873\n3800000\nApr 07 1965\nPG/PG-13\nAdapted screenplay\nWestern/Musical\nFiction\n6.7\n2588\n\n\n2\nThe Informers\n315000\n315000\n18000000\nApr 24 2009\nR\nAdapted screenplay\nHorror/Thriller\nFiction\n5.2\n7595\n\n\n3\nBuffalo Soldiers\n353743\n353743\n15000000\nJul 25 2003\nR\nAdapted screenplay\nComedy\nFiction\n6.9\n13510\n\n\n4\nThe Last Sin Eater\n388390\n388390\n2200000\nFeb 09 2007\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n5.7\n1012\n\n\n\n\n\n\n\n\n3.2.3.1 Row Indices and column names (axis labels)\nThe bold integers on the left are the indices of the DataFrame. Each index refers to a distinct row. For example, the index 2 correponds to the row of the movie The Informers. By default, the indices are integers starting from 0. However, they can be changed (to even non-integer values) if desired by the user.\nThe bold text on top of the DataFrame refers to column names. For example, the column US Gross consists of the gross revenue of a movie in the US.\nCollectively, the indices and column names are referred as axis labels.\n\n\n3.2.3.2 Shape of DataFrame\nFor finding the number of rows and columns in the data, you may use the shape() function.\n\n#Finding the shape of movie_ratings dataset\nmovie_ratings.shape\n\n(2228, 11)\n\n\nThe movie_ratings dataset contains 2,228 observations (or rows) and 11 variables (or columns).\n\n\n\n3.2.4 Summary statistics\n\n3.2.4.1 Numeric columns summary\nThe Pandas function of the DataFrame class, describe() can be used very conviniently to print the summary statistics of numeric columns of the data.\n\n#Finding summary statistics of movie_ratings dataset\nmovie_ratings.describe()\n\n\n\n\n\n\nTable 3.1: Summary statistics of numeric variables\n\n\n\nUS Gross\nWorldwide Gross\nProduction Budget\nIMDB Rating\nIMDB Votes\nRelease Year\n\n\n\n\ncount\n2.228000e+03\n2.228000e+03\n2.228000e+03\n2228.000000\n2228.000000\n2228.000000\n\n\nmean\n5.076370e+07\n1.019370e+08\n3.816055e+07\n6.239004\n33585.154847\n2002.005386\n\n\nstd\n6.643081e+07\n1.648589e+08\n3.782604e+07\n1.243285\n47325.651561\n5.524324\n\n\nmin\n0.000000e+00\n8.840000e+02\n2.180000e+02\n1.400000\n18.000000\n1953.000000\n\n\n25%\n9.646188e+06\n1.320737e+07\n1.200000e+07\n5.500000\n6659.250000\n1999.000000\n\n\n50%\n2.838649e+07\n4.266892e+07\n2.600000e+07\n6.400000\n18169.000000\n2002.000000\n\n\n75%\n6.453140e+07\n1.200000e+08\n5.300000e+07\n7.100000\n40092.750000\n2006.000000\n\n\nmax\n7.601676e+08\n2.767891e+09\n3.000000e+08\n9.200000\n519541.000000\n2039.000000\n\n\n\n\n\n\n\n\nAnswer the following questions based on the above table.\n\n\n\n\n\n \n        \n\n\n\n\n\n\n\n \n        \n\n\n\n\n3.2.4.2 Summary statistics across rows/columns\nThe Pandas DataFrame class has functions such as sum() and mean() to compute sum over rows or columns of a DataFrame.\nLet us compute the mean of all the numeric columns of the data:\n\nmovie_ratings.mean(axis = 0)\n\nUS Gross             5.076370e+07\nWorldwide Gross      1.019370e+08\nProduction Budget    3.816055e+07\nIMDB Rating          6.239004e+00\nIMDB Votes           3.358515e+04\ndtype: float64\n\n\nThe argument axis=0 denotes that the mean is taken over all the rows of the DataFrame. For computing a statistic across column the argument axis=1 will be used.\nIf mean over a subset of columns is desired, then those column names can be subset from the data. For example, let us compute the mean IMDB rating, and mean IMDB votes of all the movies:\n\nmovie_ratings[['IMDB Rating','IMDB Votes']].mean(axis = 0)\n\nIMDB Rating        6.239004\nIMDB Votes     33585.154847\ndtype: float64\n\n\n\n\n\n3.2.5 Practice exercise 1\nRead the file Top 10 Albums By Year.csv. This file contains the top 10 albums for each year from 1990 to 2021. Each row corresponds to a unique album.\n\n3.2.5.1 \nPrint the first 5 rows of the data.\n\nalbum_data = pd.read_csv('./Datasets/Top 10 Albums By Year.csv')\nalbum_data.head()\n\n\n\n\n\n\n\n\nYear\nRanking\nArtist\nAlbum\nWorldwide Sales\nCDs\nTracks\nAlbum Length\nHours\nMinutes\nSeconds\nGenre\n\n\n\n\n0\n1990\n8\nPhil Collins\nSerious Hits... Live!\n9956520\n1\n15\n1:16:53\n1.28\n76.88\n4613\nRock\n\n\n1\n1990\n1\nMadonna\nThe Immaculate Collection\n30000000\n1\n17\n1:13:32\n1.23\n73.53\n4412\nPop\n\n\n2\n1990\n10\nThe Three Tenors\nCarreras Domingo Pavarotti In Concert 1990\n8533000\n1\n17\n1:07:55\n1.13\n67.92\n4075\nClassical\n\n\n3\n1990\n4\nMC Hammer\nPlease Hammer Don't Hurt Em\n18000000\n1\n13\n0:59:04\n0.98\n59.07\n3544\nHip Hop\n\n\n4\n1990\n6\nMovie Soundtrack\nAashiqui\n15000000\n1\n12\n0:58:13\n0.97\n58.22\n3493\nWorld\n\n\n\n\n\n\n\n\n\n3.2.5.2 \nHow many rows and columns are there in the data?\n\nalbum_data.shape\n\n(320, 12)\n\n\nThere are 320 rows and 12 columns in the data\n\n\n3.2.5.3 \nPrint the summary statistics of the data, and answer the following questions:\n\nWhat proportion of albums have 15 or lesser tracks? Mention a range for the proportion.\nWhat is the mean length of a track (in minutes)?\n\n\nalbum_data.describe()\n\n\n\n\n\n\n\n\nYear\nRanking\nCDs\nTracks\nHours\nMinutes\nSeconds\n\n\n\n\ncount\n320.000000\n320.00000\n320.000000\n320.000000\n320.000000\n320.000000\n320.000000\n\n\nmean\n2005.500000\n5.50000\n1.043750\n14.306250\n0.941406\n56.478500\n3388.715625\n\n\nstd\n9.247553\n2.87678\n0.246528\n5.868995\n0.382895\n22.970109\n1378.209812\n\n\nmin\n1990.000000\n1.00000\n1.000000\n6.000000\n0.320000\n19.430000\n1166.000000\n\n\n25%\n1997.750000\n3.00000\n1.000000\n12.000000\n0.740000\n44.137500\n2648.250000\n\n\n50%\n2005.500000\n5.50000\n1.000000\n13.000000\n0.860000\n51.555000\n3093.500000\n\n\n75%\n2013.250000\n8.00000\n1.000000\n15.000000\n1.090000\n65.112500\n3906.750000\n\n\nmax\n2021.000000\n10.00000\n4.000000\n67.000000\n5.070000\n304.030000\n18242.000000\n\n\n\n\n\n\n\nAt least 75% of the albums have 15 tracks or lesser tracks since the 75th percentile value of the number of tracks is 15. However, albums between those having 75th percentile value for the number of tracks and those having the maximum number of tracks can also have 15 tracks. Thus, the proportion of albums having 15 or lesser tracks = [75%-99.99%].\n\nprint(\"Mean length of a track =\",56.478500/14.306250, \"minutes\")\n\nMean length of a track = 3.9478200087374398 minutes\n\n\n\n\n\n3.2.6 Creating new columns from existing columns\nNew variables (or columns) can be created based on existing variables, or with external data (we’ll see adding external data later). For example, let us create a new variable ratio_wgross_by_budget, which is the ratio of Worldwide Gross and Production Budget for each movie:\n\nmovie_ratings['ratio_wgross_by_budget'] = movie_ratings['Worldwide Gross']/movie_ratings['Production Budget']\n\nThe new variable can be seen at the right end of the updated DataFrame as shown below.\n\nmovie_ratings.head()\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\nratio_wgross_by_budget\n\n\n\n\n0\nOpal Dreams\n14443\n14443\n9000000\nNov 22 2006\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n6.5\n468\n0.001605\n\n\n1\nMajor Dundee\n14873\n14873\n3800000\nApr 07 1965\nPG/PG-13\nAdapted screenplay\nWestern/Musical\nFiction\n6.7\n2588\n0.003914\n\n\n2\nThe Informers\n315000\n315000\n18000000\nApr 24 2009\nR\nAdapted screenplay\nHorror/Thriller\nFiction\n5.2\n7595\n0.017500\n\n\n3\nBuffalo Soldiers\n353743\n353743\n15000000\nJul 25 2003\nR\nAdapted screenplay\nComedy\nFiction\n6.9\n13510\n0.023583\n\n\n4\nThe Last Sin Eater\n388390\n388390\n2200000\nFeb 09 2007\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n5.7\n1012\n0.176541\n\n\n\n\n\n\n\n\n\n3.2.7 Datatype of variables\nNote that in Table 3.1 (summary statistics), we don’t see Release Date. This is because the datatype of Release Data is not numeric.\nThe datatype of each variable can be seen using the dtypes() function of the DataFrame class.\n\n#Checking the datatypes of the variables \nmovie_ratings.dtypes\n\nTitle                 object\nUS Gross               int64\nWorldwide Gross        int64\nProduction Budget      int64\nRelease Date          object\nMPAA Rating           object\nSource                object\nMajor Genre           object\nCreative Type         object\nIMDB Rating          float64\nIMDB Votes             int64\ndtype: object\n\n\nOften, we wish to convert the datatypes of some of the variables to make them suitable for analysis. For example, the datatype of Release Date in the DataFrame movie_ratings is object. To perform numerical computations on this variable, we’ll need to convert it to a datatime format. We’ll use the Pandas function to_datatime() to covert it to a datatime format. Similar functions such as to_numeric(), to_string() etc., can be used for other conversions.\n\npd.to_datetime(movie_ratings['Release Date'])\n\n0      2006-11-22\n1      1965-04-07\n2      2009-04-24\n3      2003-07-25\n4      2007-02-09\n          ...    \n2223   2004-07-07\n2224   1998-06-19\n2225   2010-05-14\n2226   1991-06-14\n2227   1998-01-23\nName: Release Date, Length: 2228, dtype: datetime64[ns]\n\n\nWe can see above that the function to_datetime() converts Release Date to a datetime format.\nNow, we’ll update the variable Release Date in the DataFrame to be in the datetime format:\n\nmovie_ratings['Release Date'] = pd.to_datetime(movie_ratings['Release Date'])\n\n\nmovie_ratings.dtypes\n\nTitle                        object\nUS Gross                      int64\nWorldwide Gross               int64\nProduction Budget             int64\nRelease Date         datetime64[ns]\nMPAA Rating                  object\nSource                       object\nMajor Genre                  object\nCreative Type                object\nIMDB Rating                 float64\nIMDB Votes                    int64\ndtype: object\n\n\nWe can see that the datatype of Release Date has changed to datetime in the updated DataFrame, movie_ratings. Now we can perform computations on Release Date. Suppose we wish to create a new variable Release_year that consists of the year of release of the movie. We’ll use the attribute year of the datetime module to extract the year from Release Date:\n\n#Extracting year from Release Date\nmovie_ratings['Release Year'] = movie_ratings['Release Date'].dt.year\n\n\nmovie_ratings.head()\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\nRelease Year\n\n\n\n\n0\nOpal Dreams\n14443\n14443\n9000000\n2006-11-22\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n6.5\n468\n2006\n\n\n1\nMajor Dundee\n14873\n14873\n3800000\n1965-04-07\nPG/PG-13\nAdapted screenplay\nWestern/Musical\nFiction\n6.7\n2588\n1965\n\n\n2\nThe Informers\n315000\n315000\n18000000\n2009-04-24\nR\nAdapted screenplay\nHorror/Thriller\nFiction\n5.2\n7595\n2009\n\n\n3\nBuffalo Soldiers\n353743\n353743\n15000000\n2003-07-25\nR\nAdapted screenplay\nComedy\nFiction\n6.9\n13510\n2003\n\n\n4\nThe Last Sin Eater\n388390\n388390\n2200000\n2007-02-09\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n5.7\n1012\n2007\n\n\n\n\n\n\n\nAs year is a numeric variable, it will appear in the numeric summary statistics with the describe() function, as shown below.\n\nmovie_ratings.describe()\n\n\n\n\n\n\n\n\nUS Gross\nWorldwide Gross\nProduction Budget\nIMDB Rating\nIMDB Votes\nRelease Year\n\n\n\n\ncount\n2.228000e+03\n2.228000e+03\n2.228000e+03\n2228.000000\n2228.000000\n2228.000000\n\n\nmean\n5.076370e+07\n1.019370e+08\n3.816055e+07\n6.239004\n33585.154847\n2002.005386\n\n\nstd\n6.643081e+07\n1.648589e+08\n3.782604e+07\n1.243285\n47325.651561\n5.524324\n\n\nmin\n0.000000e+00\n8.840000e+02\n2.180000e+02\n1.400000\n18.000000\n1953.000000\n\n\n25%\n9.646188e+06\n1.320737e+07\n1.200000e+07\n5.500000\n6659.250000\n1999.000000\n\n\n50%\n2.838649e+07\n4.266892e+07\n2.600000e+07\n6.400000\n18169.000000\n2002.000000\n\n\n75%\n6.453140e+07\n1.200000e+08\n5.300000e+07\n7.100000\n40092.750000\n2006.000000\n\n\nmax\n7.601676e+08\n2.767891e+09\n3.000000e+08\n9.200000\n519541.000000\n2039.000000\n\n\n\n\n\n\n\n\n\n3.2.8 Practice exercise 2\n\n3.2.8.1 \nWhy is Worldwide Sales not included in the summary statistics table printed in Practice exercise 1?\n\nalbum_data.dtypes\n\nYear                 int64\nRanking              int64\nArtist              object\nAlbum               object\nWorldwide Sales     object\nCDs                  int64\nTracks               int64\nAlbum Length        object\nHours              float64\nMinutes            float64\nSeconds              int64\nGenre               object\ndtype: object\n\n\nWorldwide Sales is not included in the summary statistics table printed in Practice exercise 1 because its data type is object and not int or float\n\n\n3.2.8.2 \nUpdate the DataFrame so that Worldwide Sales is included in the summary statistics table. Print the summary statistics table.\nHint: Sometimes it may not be possible to convert an object to numeric(). For example, the object ‘hi’ cannot be converted to a numeric() by the python compiler. To avoid getting an error, use the errors argument of to_numeric() to force such conversions to NaN (missing value).\n\nalbum_data['Worldwide Sales'] = pd.to_numeric(album_data['Worldwide Sales'], errors = 'coerce')\nalbum_data.describe()\n\n\n\n\n\n\n\n\nYear\nRanking\nWorldwide Sales\nCDs\nTracks\nHours\nMinutes\nSeconds\n\n\n\n\ncount\n320.000000\n320.00000\n3.190000e+02\n320.000000\n320.000000\n320.000000\n320.000000\n320.000000\n\n\nmean\n2005.500000\n5.50000\n1.071093e+07\n1.043750\n14.306250\n0.941406\n56.478500\n3388.715625\n\n\nstd\n9.247553\n2.87678\n7.566796e+06\n0.246528\n5.868995\n0.382895\n22.970109\n1378.209812\n\n\nmin\n1990.000000\n1.00000\n1.909009e+06\n1.000000\n6.000000\n0.320000\n19.430000\n1166.000000\n\n\n25%\n1997.750000\n3.00000\n5.000000e+06\n1.000000\n12.000000\n0.740000\n44.137500\n2648.250000\n\n\n50%\n2005.500000\n5.50000\n8.255866e+06\n1.000000\n13.000000\n0.860000\n51.555000\n3093.500000\n\n\n75%\n2013.250000\n8.00000\n1.400000e+07\n1.000000\n15.000000\n1.090000\n65.112500\n3906.750000\n\n\nmax\n2021.000000\n10.00000\n4.500000e+07\n4.000000\n67.000000\n5.070000\n304.030000\n18242.000000\n\n\n\n\n\n\n\n\n\n3.2.8.3 \nCreate a new column that computes the average worldwide sales per year for each album, assuming that the worldwide sales are as of 2022. Print the first 5 rows of the updated DataFrame.\n\nalbum_data['mean_sales_per_year'] = album_data['Worldwide Sales']/(2022-album_data['Year'])\nalbum_data.head()\n\n\n\n\n\n\n\n\nYear\nRanking\nArtist\nAlbum\nWorldwide Sales\nCDs\nTracks\nAlbum Length\nHours\nMinutes\nSeconds\nGenre\nmean_sales_per_year\n\n\n\n\n0\n1990\n8\nPhil Collins\nSerious Hits... Live!\n9956520.0\n1\n15\n1:16:53\n1.28\n76.88\n4613\nRock\n311141.25\n\n\n1\n1990\n1\nMadonna\nThe Immaculate Collection\n30000000.0\n1\n17\n1:13:32\n1.23\n73.53\n4412\nPop\n937500.00\n\n\n2\n1990\n10\nThe Three Tenors\nCarreras Domingo Pavarotti In Concert 1990\n8533000.0\n1\n17\n1:07:55\n1.13\n67.92\n4075\nClassical\n266656.25\n\n\n3\n1990\n4\nMC Hammer\nPlease Hammer Don't Hurt Em\n18000000.0\n1\n13\n0:59:04\n0.98\n59.07\n3544\nHip Hop\n562500.00\n\n\n4\n1990\n6\nMovie Soundtrack\nAashiqui\n15000000.0\n1\n12\n0:58:13\n0.97\n58.22\n3493\nWorld\n468750.00\n\n\n\n\n\n\n\n\n\n\n3.2.9 Reading a sub-set of data: loc and iloc\nSometimes we may be interested in working with a subset of rows and columns of the data, instead of working with the entire dataset. The indexing operators loc and iloc provide a convenient way of selecting a subset of desired rows and columns. The operator loc uses axis labels (row indices and column names) to subset the data, while iloc uses the position of rows or columns, where position has values 0,1,2,3,…and so on, for rows from top to bottom and columns from left to right. In other words, the first row has position 0, the second row has position 1, the third row has position 2, and so on. Similarly, the first column from left has position 0, the second column from left has position 1, the third column from left has position 2, and so on.\nLet us read the file movie_IMDBratings_sorted.csv, which has movies sorted in the descending order of their IMDB ratings.\n\nmovies_sorted = pd.read_csv('./Datasets/movie_IMDBratings_sorted.csv',index_col = 0)\n\nThe argument index_col=0 assigns the first column of the file as the row indices of the DataFrame.\n\nmovies_sorted.head()\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\nRank\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nThe Shawshank Redemption\n28241469\n28241469\n25000000\nSep 23 1994\nR\nAdapted screenplay\nDrama\nFiction\n9.2\n519541\n\n\n2\nInception\n285630280\n753830280\n160000000\nJul 16 2010\nPG/PG-13\nOriginal Screenplay\nHorror/Thriller\nFiction\n9.1\n188247\n\n\n3\nThe Dark Knight\n533345358\n1022345358\n185000000\nJul 18 2008\nPG/PG-13\nAdapted screenplay\nAction/Adventure\nFiction\n8.9\n465000\n\n\n4\nSchindler's List\n96067179\n321200000\n25000000\nDec 15 1993\nR\nAdapted screenplay\nDrama\nNon-Fiction\n8.9\n276283\n\n\n5\nPulp Fiction\n107928762\n212928762\n8000000\nOct 14 1994\nR\nOriginal Screenplay\nDrama\nFiction\n8.9\n417703\n\n\n\n\n\n\n\nLet us say, we wish to subset the title, worldwide gross, production budget, and IMDB rating of top 3 movies.\n\n# Subsetting the DataFrame by loc - using axis labels\nmovies_subset = movies_sorted.loc[1:3,['Title','Worldwide Gross','Production Budget','IMDB Rating']]\nmovies_subset\n\n\n\n\n\n\n\n\nTitle\nWorldwide Gross\nProduction Budget\nIMDB Rating\n\n\nRank\n\n\n\n\n\n\n\n\n1\nThe Shawshank Redemption\n28241469\n25000000\n9.2\n\n\n2\nInception\n753830280\n160000000\n9.1\n\n\n3\nThe Dark Knight\n1022345358\n185000000\n8.9\n\n\n\n\n\n\n\n\n# Subsetting the DataFrame by iloc - using index of the position of rows and columns\nmovies_subset = movies_sorted.iloc[0:3,[0,2,3,9]]\nmovies_subset\n\n\n\n\n\n\n\n\nTitle\nWorldwide Gross\nProduction Budget\nIMDB Rating\n\n\nRank\n\n\n\n\n\n\n\n\n1\nThe Shawshank Redemption\n28241469\n25000000\n9.2\n\n\n2\nInception\n753830280\n160000000\n9.1\n\n\n3\nThe Dark Knight\n1022345358\n185000000\n8.9\n\n\n\n\n\n\n\nLet us find the movie with the maximum Worldwide Gross.\nWe will use the argmax() function of the Pandas Series class to find the position of the movie with the maximum worldwide gross, and then use the position to find the movie.\n\nposition_max_wgross = movies_sorted['Worldwide Gross'].argmax()\n\n\nmovies_sorted.iloc[position_max_wgross,:]\n\nTitle                             Avatar\nUS Gross                       760167650\nWorldwide Gross               2767891499\nProduction Budget              237000000\nRelease Date                 Dec 18 2009\nMPAA Rating                     PG/PG-13\nSource               Original Screenplay\nMajor Genre             Action/Adventure\nCreative Type                    Fiction\nIMDB Rating                          8.3\nIMDB Votes                        261439\nName: 59, dtype: object\n\n\nAvatar has the highest worldwide gross of all the movies. Note that the : indicates that all the columns of the DataFrame are selected.\nKey differences betweenloc and iloc in pandas\n\nIndexing Type:\n\nloc uses labels (names) for indexing.\niloc uses integer positions for indexing.\n\nInclusion of Endpoints:\n\nIn a loc slice, both endpoints are included.\nIn an iloc slice, the endpoint is excluded.\n\n\nExamples: Assuming you have a DataFrame like this:\n\ndata = {'A': [1, 2, 3, 4, 5],\n        'B': [10, 20, 30, 40, 50],\n        'C': [100, 200, 300, 400, 500]}\n\ndf = pd.DataFrame(data, index=['row1', 'row2', 'row3', 'row4', 'row5'])\ndf.style.set_properties({'text-align': 'right'})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\nrow1\n1\n10\n100\n\n\nrow2\n2\n20\n200\n\n\nrow3\n3\n30\n300\n\n\nrow4\n4\n40\n400\n\n\nrow5\n5\n50\n500\n\n\n\n\n\n\n\n\n# using 'loc'\ndf.loc['row2':'row4', 'B']\n\nrow2    20\nrow3    30\nrow4    40\nName: B, dtype: int64\n\n\n\n# using 'iloc'\ndf.iloc[1:4, 1]\n\nrow2    20\nrow3    30\nrow4    40\nName: B, dtype: int64\n\n\nNote that in the loc example, both ‘row2’ and ‘row4’ are included in the result, whereas in the iloc example, the rows at integer positions 1 and 4 are included, but the row at position 4 is excluded.\n\n\n3.2.10 Practice exercise 3\n\n3.2.10.1 \nFind the album having the highest worldwide sales per year, and its artist.\n\nalbum_data.iloc[album_data['mean_sales_per_year'].argmax(),:]\n\nYear                        2021\nRanking                        1\nArtist                     Adele\nAlbum                         30\nWorldwide Sales        4485025.0\nCDs                            1\nTracks                        12\nAlbum Length             0:58:14\nHours                       0.97\nMinutes                    58.23\nSeconds                     3494\nGenre                        Pop\nmean_sales_per_year    4485025.0\nName: 312, dtype: object\n\n\n‘30’ has the highest worldwide sales and its artist is Adele.\n\n\n3.2.10.2 \nSubset the data to include only Hip-Hop albums. How many Hip_Hop albums are there?\n\nhiphop_albums = album_data.loc[album_data['Genre']=='Hip Hop',:]\nprint(\"There are\",hiphop_albums.shape[0], \"hip-hop albums\")\n\nThere are 42 hip-hop albums\n\n\n\n\n3.2.10.3 \nWhich album amongst hip-hop has the higest mean sales per year per track, and who is its artist?\n\nhiphop_albums.loc[:,'mean_sales_per_year_track'] = hiphop_albums.loc[:,'Worldwide Sales']/((2022-hiphop_albums.loc[:,'Year'])*(hiphop_albums.loc[:,'Tracks']))\nhiphop_albums.iloc[hiphop_albums['mean_sales_per_year_track'].argmax(),:]\n\nYear                                  2021\nRanking                                  6\nArtist                           Cai Xukun\nAlbum                                    迷\nWorldwide Sales                  3402981.0\nCDs                                      1\nTracks                                  11\nAlbum Length                       0:24:16\nHours                                  0.4\nMinutes                              24.27\nSeconds                               1456\nGenre                              Hip Hop\nmean_sales_per_year              3402981.0\nmean_sales_per_year_track    309361.909091\nName: 318, dtype: object\n\n\n迷 has the higest mean sales per year per track amongst hip-hop albumns, and its artist is Cai Xukun."
  },
  {
    "objectID": "Reading data.html#reading-other-data-formats---txt-html-json",
    "href": "Reading data.html#reading-other-data-formats---txt-html-json",
    "title": "3  Reading data",
    "section": "3.3 Reading other data formats - txt, html, json",
    "text": "3.3 Reading other data formats - txt, html, json\nAlthough csv is a very popular format for structured data, data is found in several other formats as well. Some of the other data formats are txt, html and json.\n\n3.3.1 Reading txt files\nThe txt format offers some additional flexibility as compared to the csv format. In the csv format, the delimiter is a comma (or the column values are separated by a comma). However, in a txt file, the delimiter can be anything as desired by the user. Let us read the file movie_ratings.txt, where the variable values are separated by a tab character.\n\nmovie_ratings_txt = pd.read_csv('movie_ratings.txt',sep='\\t')\n\nWe use the function read_csv to read a txt file. However, we mention the tab character (r”) as a separator of variable values.\nNote that there is no need to remember the argument name - sep for specifying the delimiter. You can always refer to the read_csv() documentation to find the relevant argument.\n\n\n3.3.2 Practice exercise 4\nRead the file bestseller_books.txt. It contains top 50 best-selling books on amazon from 2009 to 2019. Identify the delimiter without opening the file with Notepad or a text-editing software. How many rows and columns are there in the dataset?\nSolution:\n\n#Reading some lines with 'error_bad_lines=False' to identify the delimiter\nbestseller_books = pd.read_csv('./Datasets/bestseller_books.txt',error_bad_lines=False)\nbestseller_books.head()\n\nb'Skipping line 6: expected 1 fields, saw 2\\nSkipping line 10: expected 1 fields, saw 3\\nSkipping line 16: expected 1 fields, saw 5\\nSkipping line 17: expected 1 fields, saw 4\\nSkipping line 20: expected 1 fields, saw 3\\nSkipping line 29: expected 1 fields, saw 2\\nSkipping line 33: expected 1 fields, saw 2\\nSkipping line 40: expected 1 fields, saw 2\\nSkipping line 41: expected 1 fields, saw 2\\nSkipping line 42: expected 1 fields, saw 3\\nSkipping line 43: expected 1 fields, saw 3\\nSkipping line 44: expected 1 fields, saw 2\\nSkipping line 60: expected 1 fields, saw 4\\nSkipping line 61: expected 1 fields, saw 3\\nSkipping line 63: expected 1 fields, saw 2\\nSkipping line 64: expected 1 fields, saw 2\\nSkipping line 70: expected 1 fields, saw 3\\nSkipping line 71: expected 1 fields, saw 2\\nSkipping line 72: expected 1 fields, saw 2\\nSkipping line 73: expected 1 fields, saw 2\\nSkipping line 80: expected 1 fields, saw 4\\nSkipping line 82: expected 1 fields, saw 2\\nSkipping line 94: expected 1 fields, saw 4\\nSkipping line 95: expected 1 fields, saw 2\\nSkipping line 96: expected 1 fields, saw 2\\nSkipping line 101: expected 1 fields, saw 3\\nSkipping line 119: expected 1 fields, saw 3\\nSkipping line 130: expected 1 fields, saw 2\\nSkipping line 131: expected 1 fields, saw 2\\nSkipping line 132: expected 1 fields, saw 2\\nSkipping line 133: expected 1 fields, saw 2\\nSkipping line 148: expected 1 fields, saw 3\\nSkipping line 149: expected 1 fields, saw 3\\nSkipping line 150: expected 1 fields, saw 3\\nSkipping line 154: expected 1 fields, saw 3\\nSkipping line 155: expected 1 fields, saw 2\\nSkipping line 156: expected 1 fields, saw 3\\nSkipping line 157: expected 1 fields, saw 2\\nSkipping line 158: expected 1 fields, saw 2\\nSkipping line 159: expected 1 fields, saw 2\\nSkipping line 177: expected 1 fields, saw 4\\nSkipping line 178: expected 1 fields, saw 2\\nSkipping line 179: expected 1 fields, saw 2\\nSkipping line 183: expected 1 fields, saw 3\\nSkipping line 209: expected 1 fields, saw 2\\nSkipping line 215: expected 1 fields, saw 3\\nSkipping line 224: expected 1 fields, saw 3\\nSkipping line 230: expected 1 fields, saw 2\\nSkipping line 241: expected 1 fields, saw 2\\nSkipping line 247: expected 1 fields, saw 2\\nSkipping line 248: expected 1 fields, saw 2\\nSkipping line 249: expected 1 fields, saw 2\\nSkipping line 250: expected 1 fields, saw 2\\nSkipping line 251: expected 1 fields, saw 2\\nSkipping line 252: expected 1 fields, saw 2\\nSkipping line 253: expected 1 fields, saw 2\\nSkipping line 254: expected 1 fields, saw 2\\nSkipping line 259: expected 1 fields, saw 3\\nSkipping line 273: expected 1 fields, saw 2\\nSkipping line 274: expected 1 fields, saw 2\\nSkipping line 275: expected 1 fields, saw 2\\nSkipping line 276: expected 1 fields, saw 2\\nSkipping line 277: expected 1 fields, saw 2\\nSkipping line 278: expected 1 fields, saw 2\\nSkipping line 279: expected 1 fields, saw 2\\nSkipping line 280: expected 1 fields, saw 2\\nSkipping line 281: expected 1 fields, saw 2\\nSkipping line 282: expected 1 fields, saw 2\\nSkipping line 292: expected 1 fields, saw 4\\nSkipping line 293: expected 1 fields, saw 4\\nSkipping line 295: expected 1 fields, saw 7\\nSkipping line 296: expected 1 fields, saw 7\\nSkipping line 297: expected 1 fields, saw 2\\nSkipping line 302: expected 1 fields, saw 3\\nSkipping line 315: expected 1 fields, saw 3\\nSkipping line 321: expected 1 fields, saw 2\\nSkipping line 346: expected 1 fields, saw 3\\nSkipping line 347: expected 1 fields, saw 3\\nSkipping line 365: expected 1 fields, saw 2\\nSkipping line 408: expected 1 fields, saw 2\\nSkipping line 420: expected 1 fields, saw 2\\nSkipping line 421: expected 1 fields, saw 2\\nSkipping line 430: expected 1 fields, saw 2\\nSkipping line 434: expected 1 fields, saw 2\\nSkipping line 446: expected 1 fields, saw 2\\nSkipping line 448: expected 1 fields, saw 2\\nSkipping line 449: expected 1 fields, saw 4\\nSkipping line 451: expected 1 fields, saw 3\\nSkipping line 458: expected 1 fields, saw 2\\nSkipping line 459: expected 1 fields, saw 2\\nSkipping line 460: expected 1 fields, saw 2\\nSkipping line 465: expected 1 fields, saw 2\\nSkipping line 470: expected 1 fields, saw 2\\nSkipping line 471: expected 1 fields, saw 2\\nSkipping line 476: expected 1 fields, saw 2\\nSkipping line 495: expected 1 fields, saw 2\\nSkipping line 496: expected 1 fields, saw 2\\nSkipping line 497: expected 1 fields, saw 2\\nSkipping line 512: expected 1 fields, saw 5\\nSkipping line 513: expected 1 fields, saw 2\\nSkipping line 515: expected 1 fields, saw 2\\nSkipping line 517: expected 1 fields, saw 3\\nSkipping line 518: expected 1 fields, saw 3\\nSkipping line 519: expected 1 fields, saw 3\\nSkipping line 520: expected 1 fields, saw 3\\nSkipping line 521: expected 1 fields, saw 3\\nSkipping line 525: expected 1 fields, saw 3\\nSkipping line 533: expected 1 fields, saw 3\\nSkipping line 534: expected 1 fields, saw 3\\n'\n\n\n\n\n\n\n\n\n\n;Unnamed: 0;Name;Author;User Rating;Reviews;Price;Year;Genre\n\n\n\n\n0\n0;0;10-Day Green Smoothie Cleanse;JJ Smith;4.7...\n\n\n1\n1;1;11/22/63: A Novel;Stephen King;4.6;2052;22...\n\n\n2\n2;2;12 Rules for Life: An Antidote to Chaos;Jo...\n\n\n3\n3;3;1984 (Signet Classics);George Orwell;4.7;2...\n\n\n4\n5;5;A Dance with Dragons (A Song of Ice and Fi...\n\n\n\n\n\n\n\n\n#The delimiter seems to be ';' based on the output of the above code\nbestseller_books = pd.read_csv('./Datasets/bestseller_books.txt',sep=';')\nbestseller_books.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nUnnamed: 0.1\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n0\n0\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\n1\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n\n\n2\n2\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\n3\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n\n\n4\n4\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n\n\n\n\n\n\n#The file read with ';' as the delimited is correct\nprint(\"The file has\",bestseller_books.shape[0],\"rows and\",bestseller_books.shape[1],\"columns\")\n\nThe file has 550 rows and 9 columns\n\n\nAlternatively, you can use the argument sep = None, and engine = 'python'. The default engine is C. However, the ‘python’ engine has a ‘sniffer’ tool which may identify the delimiter automatically.\n\nbestseller_books = pd.read_csv('./Datasets/bestseller_books.txt',sep=None, engine = 'python')\nbestseller_books.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nUnnamed: 0.1\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n0\n0\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\n1\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n\n\n2\n2\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\n3\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n\n\n4\n4\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n\n\n\n\n\n\n\n3.3.3 Reading HTML data\nThe Pandas function read_html searches for tabular data, i.e., data contained within the &lt;table&gt; tags of an html file. Let us read the tables in the GDP per capita page on Wikipedia.\n\n#Reading all the tables from the Wikipedia page on GDP per capita\ntables = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita')\n\nAll the tables will be read and stored in the variable named as tables. Let us find the datatype of the variable tables.\n\n#Finidng datatype of the variable - tables\ntype(tables)\n\nlist\n\n\nThe variable - tables is a list of all the tables read from the HTML data.\n\n#Number of tables read from the page\nlen(tables)\n\n6\n\n\nThe in-built function len can be used to find the length of the list - tables or the number of tables read from the Wikipedia page. Let us check out the first table.\n\n#Checking out the first table. Note that the index of the first table will be 0.\ntables[0]\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n.mw-parser-output .legend{page-break-inside:av...\n$20,000 - $30,000 $10,000 - $20,000 $5,000 - $...\n$1,000 - $2,500 $500 - $1,000 &lt;$500 No data\n\n\n\n\n\n\n\nThe above table doesn’t seem to be useful. Let us check out the second table.\n\n#Checking out the second table. Note that the index of the first table will be 1.\ntables[1]\n\n\n\n\n\n\n\n\nCountry/Territory\nUN Region\nIMF[4][5]\nUnited Nations[6]\nWorld Bank[7]\n\n\n\nCountry/Territory\nUN Region\nEstimate\nYear\nEstimate\nYear\nEstimate\nYear\n\n\n\n\n0\nLiechtenstein *\nEurope\n—\n—\n180227\n2020\n169049\n2019\n\n\n1\nMonaco *\nEurope\n—\n—\n173696\n2020\n173688\n2020\n\n\n2\nLuxembourg *\nEurope\n135046\n2022\n117182\n2020\n135683\n2021\n\n\n3\nBermuda *\nAmericas\n—\n—\n123945\n2020\n110870\n2021\n\n\n4\nIreland *\nEurope\n101509\n2022\n86251\n2020\n85268\n2020\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n212\nCentral AfricanRepublic *\nAfrica\n527\n2022\n481\n2020\n477\n2020\n\n\n213\nSierra Leone *\nAfrica\n513\n2022\n475\n2020\n485\n2020\n\n\n214\nMadagascar *\nAfrica\n504\n2022\n470\n2020\n496\n2020\n\n\n215\nSouth Sudan *\nAfrica\n393\n2022\n1421\n2020\n1120\n2015\n\n\n216\nBurundi *\nAfrica\n272\n2022\n286\n2020\n274\n2020\n\n\n\n\n217 rows × 8 columns\n\n\n\nThe above table contains the estimated GDP per capita of all countries. This is the table that is likely to be relevant to a user interested in analyzing GDP per capita of countries. Instead of reading all tables of an HTML file, we can focus the search to tables containing certain relevant keywords. Let us try searching all table containing the word ‘Country’.\n\n#Reading all the tables from the Wikipedia page on GDP per capita, containing the word 'Country'\ntables = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita', match = 'Country')\n\nThe match argument can be used to specify the keywords to be present in the table to be read.\n\nlen(tables)\n\n1\n\n\nOnly one table contains the keyword - ‘Country’. Let us check out the table obtained.\n\n#Table having the keyword - 'Country' from the HTML page\ntables[0]\n\n\n\n\n\n\n\n\nCountry/Territory\nUN Region\nIMF[4][5]\nUnited Nations[6]\nWorld Bank[7]\n\n\n\nCountry/Territory\nUN Region\nEstimate\nYear\nEstimate\nYear\nEstimate\nYear\n\n\n\n\n0\nLiechtenstein *\nEurope\n—\n—\n180227\n2020\n169049\n2019\n\n\n1\nMonaco *\nEurope\n—\n—\n173696\n2020\n173688\n2020\n\n\n2\nLuxembourg *\nEurope\n135046\n2022\n117182\n2020\n135683\n2021\n\n\n3\nBermuda *\nAmericas\n—\n—\n123945\n2020\n110870\n2021\n\n\n4\nIreland *\nEurope\n101509\n2022\n86251\n2020\n85268\n2020\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n212\nCentral AfricanRepublic *\nAfrica\n527\n2022\n481\n2020\n477\n2020\n\n\n213\nSierra Leone *\nAfrica\n513\n2022\n475\n2020\n485\n2020\n\n\n214\nMadagascar *\nAfrica\n504\n2022\n470\n2020\n496\n2020\n\n\n215\nSouth Sudan *\nAfrica\n393\n2022\n1421\n2020\n1120\n2015\n\n\n216\nBurundi *\nAfrica\n272\n2022\n286\n2020\n274\n2020\n\n\n\n\n217 rows × 8 columns\n\n\n\nThe argument match helps with a more focussed search, and helps us discard irrelevant tables.\n\n\n3.3.4 Practice exercise 5\nRead the table(s) consisting of attendance of spectators in FIFA worlds cup from this page. Read only those table(s) that have the word ‘attendance’ in them. How many rows and columns are there in the table(s)?\n\ndfs = pd.read_html('https://en.wikipedia.org/wiki/FIFA_World_Cup',\n                       match='attendance')\nprint(len(dfs))\ndata = dfs[0]\nprint(\"Number of rows =\",data.shape[0], \"and number of columns=\",data.shape[1])\n\n1\nNumber of rows = 22 and number of columns= 9\n\n\n\n\n3.3.5 Reading JSON data\nJSON stands for JavaScript Object Notation, in which the data is stored and transmitted as plain text. A couple of benefits of the JSON format are:\n\nSince the format is text only, JSON data can easily be exchanged between web applications, and used by any programming language.\nUnlike the csv format, JSON supports a hierarchical data structure, and is easier to integrate with APIs.\n\nThe JSON format can support a hierachical data structure, as it is built on the following two data structures (Source: technical documentation):\n\nA collection of name/value pairs. In various languages, this is realized as an object, record, struct, dictionary, hash table, keyed list, or associative array.\nAn ordered list of values. In most languages, this is realized as an array, vector, list, or sequence.\n\nThese are universal data structures. Virtually all modern programming languages support them in one form or another. It makes sense that a data format that is interchangeable with programming languages also be based on these structures.\nThe Pandas function read_json converts a JSON string to a Pandas DataFrame. The function dumps() of the json library converts a Python object to a JSON string.\nLets read the JSON data on Ted Talks.\n\ntedtalks_data = pd.read_json('https://raw.githubusercontent.com/cwkenwaysun/TEDmap/master/data/TED_Talks.json')\n\n\ntedtalks_data.head()\n\n\n\n\n\n\n\n\nid\nspeaker\nheadline\nURL\ndescription\ntranscript_URL\nmonth_filmed\nyear_filmed\nevent\nduration\ndate_published\ntags\nnewURL\ndate\nviews\nrates\n\n\n\n\n0\n7\nDavid Pogue\nSimplicity sells\nhttp://www.ted.com/talks/view/id/7\nNew York Times columnist David Pogue takes aim...\nhttp://www.ted.com/talks/view/id/7/transcript?...\n2\n2006\nTED2006\n0:21:26\n6/27/06\nsimplicity,computers,software,interface design...\nhttps://www.ted.com/talks/david_pogue_says_sim...\n2006-06-27\n1646773\n[{'id': 7, 'name': 'Funny', 'count': 968}, {'i...\n\n\n1\n6\nCraig Venter\nSampling the ocean's DNA\nhttp://www.ted.com/talks/view/id/6\nGenomics pioneer Craig Venter takes a break fr...\nhttp://www.ted.com/talks/view/id/6/transcript?...\n7\n2005\nTEDGlobal 2005\n0:16:51\n2004/05/07\nbiotech,invention,oceans,genetics,DNA,biology,...\nhttps://www.ted.com/talks/craig_venter_on_dna_...\n2004-05-07\n562625\n[{'id': 3, 'name': 'Courageous', 'count': 21},...\n\n\n2\n4\nBurt Rutan\nThe real future of space exploration\nhttp://www.ted.com/talks/view/id/4\nIn this passionate talk, legendary spacecraft ...\nhttp://www.ted.com/talks/view/id/4/transcript?...\n2\n2006\nTED2006\n0:19:37\n10/25/06\naircraft,flight,industrial design,NASA,rocket ...\nhttps://www.ted.com/talks/burt_rutan_sees_the_...\n2006-10-25\n2046869\n[{'id': 3, 'name': 'Courageous', 'count': 169}...\n\n\n3\n3\nAshraf Ghani\nHow to rebuild a broken state\nhttp://www.ted.com/talks/view/id/3\nAshraf Ghani's passionate and powerful 10-minu...\nhttp://www.ted.com/talks/view/id/3/transcript?...\n7\n2005\nTEDGlobal 2005\n0:18:45\n10/18/06\ncorruption,poverty,economics,investment,milita...\nhttps://www.ted.com/talks/ashraf_ghani_on_rebu...\n2006-10-18\n814554\n[{'id': 3, 'name': 'Courageous', 'count': 140}...\n\n\n4\n5\nChris Bangle\nGreat cars are great art\nhttp://www.ted.com/talks/view/id/5\nAmerican designer Chris Bangle explains his ph...\nhttp://www.ted.com/talks/view/id/5/transcript?...\n2\n2002\nTED2002\n0:20:04\n2004/05/07\ncars,industrial design,transportation,inventio...\nhttps://www.ted.com/talks/chris_bangle_says_gr...\n2004-05-07\n870950\n[{'id': 1, 'name': 'Beautiful', 'count': 89}, ...\n\n\n\n\n\n\n\n\n\n3.3.6 Practice exercise 6\nRead the movies dataset from here. How many rows and columns are there in the data?\n\nmovies_data = pd.read_json('https://raw.githubusercontent.com/vega/vega-datasets/master/data/movies.json')\nprint(\"Number of rows =\",movies_data.shape[0], \"and number of columns=\",movies_data.shape[1])\n\nNumber of rows = 3201 and number of columns= 16\n\n\n\n\n3.3.7 Reading data from web APIs\nAPI, an acronym for Application programming interface, is a way of transferring information between systems. Many websites have public APIs that provide data via JSON or other formats. For example, the IMDb-API is a web service for receiving movies, serial, and cast information. API results are in the JSON format and include items such as movie specifications, ratings, Wikipedia page content, etc. One of these APIs contains ratings of the top 250 movies on IMDB. Let us read this data using the IMDB API.\nWe’ll use the get function from the python library requests to request data from the API and obtain a response code. The response code will let us know if our request to pull data from this API was successful.\n\n#Importing the requests library\nimport requests as rq\n\n\n# Downloading imdb top 250 movie's data\nurl = 'https://imdb-api.com/en/API/Top250Movies/k_v6gf8ppf' #URL of the API containing top 250 movies based on IMDB ratings\nresponse = rq.get(url) #Requesting data from the API\nresponse\n\n&lt;Response [200]&gt;\n\n\nWe have a response code of 200, which indicates that the request was successful.\nThe response object’s JSON method will return a dictionary containing JSON parsed into native Python objects.\n\nmovie_data = response.json()\n\n\nmovie_data.keys()\n\ndict_keys(['items', 'errorMessage'])\n\n\nThe movie_data contains only two keys. The items key seems likely to contain information about the top 250 movies. Let us convert the values of the items key (which is list of dictionaries) to a dataframe, so that we can view it in a tabular form.\n\n#Converting a list of dictionaries to a dataframe\nmovie_data_df = pd.DataFrame(movie_data['items'])\n\n\n#Checking the movie data pulled using the API\nmovie_data_df.head()\n\n\n\n\n\n\n\n\nid\nrank\ntitle\nfullTitle\nyear\nimage\ncrew\nimDbRating\nimDbRatingCount\n\n\n\n\n0\ntt0111161\n1\nThe Shawshank Redemption\nThe Shawshank Redemption (1994)\n1994\nhttps://m.media-amazon.com/images/M/MV5BMDFkYT...\nFrank Darabont (dir.), Tim Robbins, Morgan Fre...\n9.2\n2624065\n\n\n1\ntt0068646\n2\nThe Godfather\nThe Godfather (1972)\n1972\nhttps://m.media-amazon.com/images/M/MV5BM2MyNj...\nFrancis Ford Coppola (dir.), Marlon Brando, Al...\n9.2\n1817542\n\n\n2\ntt0468569\n3\nThe Dark Knight\nThe Dark Knight (2008)\n2008\nhttps://m.media-amazon.com/images/M/MV5BMTMxNT...\nChristopher Nolan (dir.), Christian Bale, Heat...\n9.0\n2595637\n\n\n3\ntt0071562\n4\nThe Godfather Part II\nThe Godfather Part II (1974)\n1974\nhttps://m.media-amazon.com/images/M/MV5BMWMwMG...\nFrancis Ford Coppola (dir.), Al Pacino, Robert...\n9.0\n1248050\n\n\n4\ntt0050083\n5\n12 Angry Men\n12 Angry Men (1957)\n1957\nhttps://m.media-amazon.com/images/M/MV5BMWU4N2...\nSidney Lumet (dir.), Henry Fonda, Lee J. Cobb\n8.9\n775140\n\n\n\n\n\n\n\n\n#Rows and columns of the movie data\nmovie_data_df.shape\n\n(250, 9)\n\n\nThis API provides the names of the top 250 movies along with the year of release, IMDB ratings, and cast information."
  },
  {
    "objectID": "Reading data.html#writing-data",
    "href": "Reading data.html#writing-data",
    "title": "3  Reading data",
    "section": "3.4 Writing data",
    "text": "3.4 Writing data\nThe Pandas function to_csv can be used to write (or export) data to a csv or txt file. Below are some examples.\nExample 1: Let us export the movies data of the top 250 movies to a csv file.\n\n#Exporting the data of the top 250 movies to a csv file\nmovie_data_df.to_csv('movie_data_exported.csv')\n\nThe file movie_data_exported.csv will appear in the working directory.\nExample 2: Let us export the movies data of the top 250 movies to a txt file with a semi-colon as the delimiter.\n\nmovie_data_df.to_csv('movie_data_exported.txt',sep=';')\n\nExample 3: Let us export the movies data of the top 250 movies to a JSON file.\n\nwith open('movie_data.json', 'w') as f:\n    json.dump(movie_data, f)"
  },
  {
    "objectID": "NumPy.html#why-do-we-need-numpy-arrays",
    "href": "NumPy.html#why-do-we-need-numpy-arrays",
    "title": "4  NumPy",
    "section": "4.1 Why do we need NumPy arrays?",
    "text": "4.1 Why do we need NumPy arrays?\nNumPy arrays can store data just like other data structures such as such as lists, tuples, and Pandas DataFrame. Computations performed using NumPy arrays can also be performed with data stored in the other data structures. However, NumPy is preferred for its efficiency, especially when working with large arrays of data.\n\n4.1.1 Numpy arrays are memory efficient\nA NumPy array is a collection of homogeneous data-types that are stored in contiguous memory locations. On the other hand, data structures such as lists are a collection of heterogeneous data types stored in non-contiguous memory locations. Homogenous data elements let the NumPy array be densely packed resulting in lesser memory consumption. The following example illustrates the smaller size of NumPy arrays as compared to other data structures.\n\n#Example showing NumPy arrays take less storage space than lists, tuples and Pandas DataFrame for the same elements\ntuple_ex = tuple(range(1000))\nlist_ex = list(range(1000))\nnumpy_ex = np.array([range(1000)])\npandas_df = pd.DataFrame(range(1000))\nprint(\"Space taken by tuple =\",tuple_ex.__sizeof__(),\" bytes\")\nprint(\"Space taken by list =\",list_ex.__sizeof__(),\" bytes\")\nprint(\"Space taken by Pandas DataFrame =\",pandas_df.__sizeof__(),\" bytes\")\nprint(\"Space taken by NumPy array =\",numpy_ex.__sizeof__(),\" bytes\")\n\nSpace taken by tuple = 8024  bytes\nSpace taken by list = 8040  bytes\nSpace taken by Pandas DataFrame = 8128  bytes\nSpace taken by NumPy array = 4120  bytes\n\n\nNote that NumPy arrays are memory efficient as long as they are homogenous. They will lose the memory efficiency if they are used to store elements of multiple data types.\nThe example below compares the size of a homogenous NumPy array to that of a similar heterogenous NumPy array to illustrate the point.\n\nnumpy_homogenous = np.array([[1,2],[3,3]])\nprint(\"Size of a homogenous numpy array = \",numpy_homogenous.__sizeof__(), \"bytes\")\n\nSize of homogenous numpy array =  136 bytes\n\n\nNow let us convert an element of the above array to a string, and check the size of the array.\n\nnumpy_homogenous = np.array([[1,'2'],[3,3]])\nprint(\"Size of a heterogenous numpy array = \",numpy_homogenous.__sizeof__(), \"bytes\")\n\nSize of a heterogenous numpy array =  296 bytes\n\n\nThe size of the homogenous NumPy array is much lesser than that of the one with heterogenous data. Thus, NumPy arrays are primarily used for storing homogenous data.\nOn the other hand, the size of other data structures, such as a list, does not depend on whether the elements in them are homogenous or heterogenous, as shown by the example below.\n\nlist_homogenous = list([1,2,3,4])\nprint(\"Size of a homogenous list = \",list_homogenous.__sizeof__(), \"bytes\")\nlist_heterogenous = list([1,'2',3,4])\nprint(\"Size of a heterogenous list = \",list_heterogenous.__sizeof__(), \"bytes\")\n\nSize of a homogenous list =  72 bytes\nSize of a heterogenous list =  72 bytes\n\n\nNote that the memory efficiency of NumPy arrays does not come into play with a very small amount of data. Thus, a list with four elements - 1,2,3 and 4, has a lesser size than a NumPy array with the same elements. However, with larger datasets, such as the one shown earlier (sequence of integers from 0 to 999), the memory efficiency of NumPy arrays can be seen.\nUnlike data structures such as lists, tuples, and dictionary, all elements of a NumPy array should be of same type to leverage the memory efficiency of NumPy arrays.\n\n\n4.1.2 NumPy arrays are fast\nWith NumPy arrays, mathematical computations can be performed faster, as compared to other data structures, due to the following reasons:\n\nAs the NumPy array is densely packed with homogenous data, it helps retrieve the data faster as well, thereby making computations faster.\nWith NumPy, vectorized computations can replace the relatively more expensive python for loops. The NumPy package breaks down the vectorized computations into multiple fragments and then processes all the fragments parallelly. However, with a for loop, computations will be one at a time.\nThe NumPy package integrates C, and C++ codes in Python. These programming languages have very little execution time as compared to Python.\n\nWe’ll see the faster speed on NumPy computations in the example below.\nExample: This example shows that computations using NumPy arrays are typically much faster than computations with other data structures.\nQ: Multiply whole numbers upto 1 million by an integer, say 2. Compare the time taken for the computation if the numbers are stored in a NumPy array vs a list.\nUse the numpy function arange() to define a one-dimensional NumPy array.\n\n#Examples showing NumPy arrays are more efficient for numerical computation\nimport time as tm\nstart_time = tm.time()\nlist_ex = list(range(1000000)) #List containinig whole numbers upto 1 million\na=(list_ex*2)\nprint(\"Time take to multiply numbers in a list = \", tm.time()-start_time)\n\nstart_time = tm.time()\ntuple_ex = tuple(range(1000000)) #Tuple containinig whole numbers upto 1 million\na=(tuple_ex*2)\nprint(\"Time take to multiply numbers in a tuple = \", tm.time()-start_time)\n\nstart_time = tm.time()\ndf_ex = pd.DataFrame(range(1000000)) #Pandas DataFrame containinig whole numbers upto 1 million\na=(df_ex*2)\nprint(\"Time take to multiply numbers in a Pandas DataFrame = \", tm.time()-start_time)\n\nstart_time = tm.time()\nnumpy_ex = np.arange(1000000) #NumPy array containinig whole numbers upto 1 million\na=(numpy_ex*2)\nprint(\"Time take to multiply numbers in a NumPy array = \", tm.time()-start_time)\n\nTime take to multiply numbers in a list =  0.023949384689331055\nTime take to multiply numbers in a tuple =  0.03192734718322754\nTime take to multiply numbers in a Pandas DataFrame =  0.047330617904663086\nTime take to multiply numbers in a NumPy array =  0.0"
  },
  {
    "objectID": "NumPy.html#numpy-array-basic-attributes",
    "href": "NumPy.html#numpy-array-basic-attributes",
    "title": "4  NumPy",
    "section": "4.2 NumPy array: Basic attributes",
    "text": "4.2 NumPy array: Basic attributes\nLet us define a NumPy array:\n\nnumpy_ex = np.array([[1,2,3],[4,5,6]])\nnumpy_ex\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\nThe attributes of numpy_ex can be seen by typing numpy_ex followed by a ., and then pressing the tab key.\nSome of the basic attributes of a NumPy array are the following:\n\n4.2.1 ndim\nShows the number of dimensions (or axes) of the array.\n\nnumpy_ex.ndim\n\n2\n\n\n\n\n4.2.2 shape\nThis is a tuple of integers indicating the size of the array in each dimension. For a matrix with n rows and m columns, the shape will be (n,m). The length of the shape tuple is therefore the rank, or the number of dimensions, ndim.\n\nnumpy_ex.shape\n\n(2, 3)\n\n\n\n\n4.2.3 size\nThis is the total number of elements of the array, which is the product of the elements of shape.\n\nnumpy_ex.size\n\n6\n\n\n\n\n4.2.4 dtype\nThis is an object describing the type of the elements in the array. One can create or specify dtype’s using standard Python types. NumPy provides many, for example bool_, character, int_, int8, int16, int32, int64, float_, float8, float16, float32, float64, complex_, complex64, object_.\n\nnumpy_ex.dtype\n\ndtype('int32')\n\n\n\n\n4.2.5 T\nThis attribute is used to transpose the NumPy array. This is often used to make matrices (2-dimensional arrays) compatible for multiplication.\nFor matrix multiplication, the columns of the first matrix must be equal to the rows of the second matrix. For example, consider the matrix below:\n\nmatrix_to_multiply = np.array([[1,2,1],[0,1,0]])\n\nSuppose we wish to multiply this matrix with numpy_ex. Note the shape of both the matrices below.\n\nmatrix_to_multiply.shape\n\n(2, 3)\n\n\n\nnumpy_ex.shape\n\n(2, 3)\n\n\nTo multiply the above matrices the number of columns of the one of the matrices must be the same as the number of rows of the other matrix. With the current matrices, this is not true as the number of columns of the first matrix is 3, and the the number of rows of the second matrix is 2 (no matter which matrix is considered to be the first one).\nHowever, if we transpose one of the matrices, their shapes will be compatible for multiplication. Let’s transpose matrix_to_multiply:\n\nmatrix_to_multiply_transpose = matrix_to_multiply.T\nmatrix_to_multiply_transpose\n\narray([[1, 0],\n       [2, 1],\n       [1, 0]])\n\n\nThe shape of matrix_to_multiply_transpose is:\n\nmatrix_to_multiply_transpose.shape\n\n(3, 2)\n\n\nThe matrices matrix_to_multiply_transpose and numpy_ex are compatible for matrix multiplication. However, the result will depend on the order in which the matrices are multiplied:\n\n#Matrix multiplication with matrix_to_multiply_transpose before numpy_ex\nmatrix_to_multiply_transpose.dot(numpy_ex)\n\narray([[ 1,  2,  3],\n       [ 6,  9, 12],\n       [ 1,  2,  3]])\n\n\n\n#Matrix multiplication with numpy_ex before matrix_to_multiply_transpose\nnumpy_ex.dot(matrix_to_multiply_transpose)\n\narray([[ 8,  2],\n       [20,  5]])\n\n\nThe shape of the resulting matrix is equal to the rows of the first matrix and the columns of the second matrix. The order of matrices must be decided as per the requirements of the problem."
  },
  {
    "objectID": "NumPy.html#arithmetic-operations",
    "href": "NumPy.html#arithmetic-operations",
    "title": "4  NumPy",
    "section": "4.3 Arithmetic operations",
    "text": "4.3 Arithmetic operations\nNumpy arrays support arithmetic operators like +, -, *, etc. We can perform an arithmetic operation on an array either with a single number (also called scalar) or with another array of the same shape. However, we cannot perform an arithmetic operation on an array with an array of a different shape.\nBelow are some examples of arithmetic operations on arrays.\n\n#Defining two arrays of the same shape\narr1 = np.array([[1, 2, 3, 4], \n                 [5, 6, 7, 8], \n                 [9, 1, 2, 3]])\narr2 = np.array([[11, 12, 13, 14], \n                 [15, 16, 17, 18], \n                 [19, 11, 12, 13]])\n\n\n#Element-wise summation of arrays\narr1 + arr2\n\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 12, 14, 16]])\n\n\n\n# Element-wise subtraction\narr2 - arr1\n\narray([[10, 10, 10, 10],\n       [10, 10, 10, 10],\n       [10, 10, 10, 10]])\n\n\n\n# Adding a scalar to an array adds the scalar to each element of the array\narr1 + 3\n\narray([[ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12,  4,  5,  6]])\n\n\n\n# Dividing an array by a scalar divides all elements of the array by the scalar\narr1 / 2\n\narray([[0.5, 1. , 1.5, 2. ],\n       [2.5, 3. , 3.5, 4. ],\n       [4.5, 0.5, 1. , 1.5]])\n\n\n\n# Element-wise multiplication\narr1 * arr2\n\narray([[ 11,  24,  39,  56],\n       [ 75,  96, 119, 144],\n       [171,  11,  24,  39]])\n\n\n\n# Modulus operator with scalar\narr1 % 4\n\narray([[1, 2, 3, 0],\n       [1, 2, 3, 0],\n       [1, 1, 2, 3]], dtype=int32)"
  },
  {
    "objectID": "NumPy.html#broadcasting",
    "href": "NumPy.html#broadcasting",
    "title": "4  NumPy",
    "section": "4.4 Broadcasting",
    "text": "4.4 Broadcasting\nBroadcasting allows arithmetic operations between two arrays with different numbers of dimensions but compatible shapes.\nThe Broadcasting documentation succinctly explains it as the following:\n“The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is broadcast across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations.”\nThe example below shows the broadcasting of two arrays.\n\narr1 = np.array([[1, 2, 3, 4], \n                 [5, 6, 7, 8], \n                 [9, 1, 2, 3]])\narr2 = np.array([4, 5, 6, 7])\n\n\narr1 + arr2\n\narray([[ 5,  7,  9, 11],\n       [ 9, 11, 13, 15],\n       [13,  6,  8, 10]])\n\n\nWhen the expression arr1 + arr2 is evaluated, arr2 (which has the shape (4,)) is replicated three times to match the shape (3, 4) of arr1. Numpy performs the replication without actually creating three copies of the smaller dimension array, thus improving performance and using lower memory.\nIn the above addition of arrays, arr2 was stretched or broadcast to the shape of arr1. However, this broadcasting was possible only because the right dimension of both the arrays is 4, and the left dimension of one of the arrays is 1.\nSee the broadcasting documentation to understand the rules for broadcasting:\n“When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when:\n\nthey are equal, or\none of them is 1”\n\nIf the rightmost dimension of arr2 is 3, broadcasting will not occur, as it is not equal to the rightmost dimension of arr1:\n\n#Defining arr2 as an array of shape (3,)\narr2 = np.array([4, 5, 6])\n\n\n#Broadcasting will not happen when the broadcasting rules are violated\narr1 + arr2\n\nValueError: operands could not be broadcast together with shapes (3,4) (3,)"
  },
  {
    "objectID": "NumPy.html#comparison",
    "href": "NumPy.html#comparison",
    "title": "4  NumPy",
    "section": "4.5 Comparison",
    "text": "4.5 Comparison\nNumpy arrays support comparison operations like ==, !=, &gt; etc. The result is an array of booleans.\n\narr1 = np.array([[1, 2, 3], [3, 4, 5]])\narr2 = np.array([[2, 2, 3], [1, 2, 5]])\n\n\narr1 == arr2\n\narray([[False,  True,  True],\n       [False, False,  True]])\n\n\n\narr1 != arr2\n\narray([[ True, False, False],\n       [ True,  True, False]])\n\n\n\narr1 &gt;= arr2\n\narray([[False,  True,  True],\n       [ True,  True,  True]])\n\n\n\narr1 &lt; arr2\n\narray([[ True, False, False],\n       [False, False, False]])\n\n\nArray comparison is frequently used to count the number of equal elements in two arrays using the sum method. Remember that True evaluates to 1 and False evaluates to 0 when booleans are used in arithmetic operations.\n\n(arr1 == arr2).sum()\n\n3"
  },
  {
    "objectID": "NumPy.html#concatenating-arrays",
    "href": "NumPy.html#concatenating-arrays",
    "title": "4  NumPy",
    "section": "4.6 Concatenating arrays",
    "text": "4.6 Concatenating arrays\nArrays can be concatenated along an axis with NumPy’s concatenate function. The axis argument specifies the dimension for concatenation. The arrays should have the same number of dimensions, and the same length along each axis except the axis used for concatenation.\nThe examples below show concatenation of arrays.\n\narr1 = np.array([[1, 2, 3], [3, 4, 5]])\narr2 = np.array([[2, 2, 3], [1, 2, 5]])\nprint(\"Array 1:\\n\",arr1)\nprint(\"Array 2:\\n\",arr2)\n\nArray 1:\n [[1 2 3]\n [3 4 5]]\nArray 2:\n [[2 2 3]\n [1 2 5]]\n\n\n\n#Concatenating the arrays along the default axis: axis=0\nnp.concatenate((arr1,arr2))\n\narray([[1, 2, 3],\n       [3, 4, 5],\n       [2, 2, 3],\n       [1, 2, 5]])\n\n\n\n#Concatenating the arrays along axis = 1\nnp.concatenate((arr1,arr2),axis=1)\n\narray([[1, 2, 3, 2, 2, 3],\n       [3, 4, 5, 1, 2, 5]])\n\n\nSince the arrays need to have the same dimension only along the axis of concatenation, let us try concatenate the array below (arr3) with arr1, along axis = 0.\n\narr3 = np.array([2, 2, 3])\n\n\nnp.concatenate((arr1,arr3),axis=0)\n\nValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)\n\n\nNote the above error, which indicates that arr3 has only one dimension. Let us check the shape of arr3.\n\narr3.shape\n\n(3,)\n\n\nWe can reshape arr3 to a shape of (1,3) to make it compatible for concatenation with arr1 along axis = 0.\n\narr3_reshaped = arr3.reshape(1,3)\narr3_reshaped\n\narray([[2, 2, 3]])\n\n\nNow we can concatenate the reshaped arr3 with arr1 along axis = 0.\n\nnp.concatenate((arr1,arr3_reshaped),axis=0)\n\narray([[1, 2, 3],\n       [3, 4, 5],\n       [2, 2, 3]])"
  },
  {
    "objectID": "NumPy.html#practice-exercise-1",
    "href": "NumPy.html#practice-exercise-1",
    "title": "4  NumPy",
    "section": "4.7 Practice exercise 1",
    "text": "4.7 Practice exercise 1\n\n4.7.0.1 \nRead the coordinates of the capital cities of the world from http://techslides.com/list-of-countries-and-capitals . Use NumPy to print the name and coordinates of the capital city closest to the US capital - Washington DC.\nNote that:\n\nThe Country Name for US is given as United States in the data.\nThe ‘closeness’ of capital cities from the US capital is based on the Euclidean distance of their coordinates to those of the US capital.\n\nHints:\n\nUse read_html() from the Pandas library to read the table.\nUse the to_numpy() function of the Pandas DataFrame class to convert a DataFrame to a Numpy array\nUse broadcasting to compute the euclidean distance of capital cities from Washington DC.\n\nSolution:\n\nimport pandas as pd\ncapital_cities = pd.read_html('http://techslides.com/list-of-countries-and-capitals',header=0)[0]\ncoordinates_capital_cities = capital_cities.iloc[:,2:4].to_numpy()\nus_coordinates = capital_cities.loc[capital_cities['Country Name']=='United States',['Capital Latitude','Capital Longitude']].to_numpy()\n\n#Broadcasting\ndistance_from_DC = np.sqrt(np.sum((us_coordinates-coordinates_capital_cities)**2,axis=1))\n\n#Assigning a high value of distance to DC, otherwise it will itself be selected as being closest to DC\ndistance_from_DC[distance_from_DC==0]=9999\nclosest_capital_index = np.argmin(distance_from_DC)\nprint(\"Closest capital city is:\" ,capital_cities.loc[closest_capital_index,'Capital Name'])\nprint(\"Coordinates of the closest capital city are:\",coordinates_capital_cities[closest_capital_index,:])\n\nClosest capital city is: Ottawa\nCoordinates of the closest capital city are: [ 45.41666667 -75.7       ]\n\n\n\n\n4.7.0.2 \nUse NumPy to:\n\nPrint the names of the countries of the top 10 capital cities closest to the US capital - Washington DC.\nCreate and print a NumPy array containing the coordinates of the top 10 cities.\n\nHint: Use the concatenate() function from the NumPy library to stack the coordinates of the top 10 cities.\n\ntop10_cities_coordinates = coordinates_capital_cities[closest_capital_index,:].reshape(1,2)\nprint(\"Top 10 countries closest to Washington DC are:\\n Canada\")\nfor i in range(9):\n    distance_from_DC[closest_capital_index]=9999\n    closest_capital_index = np.argmin(distance_from_DC)\n    print(capital_cities.loc[closest_capital_index,'Country Name'])\n    top10_cities_coordinates=np.concatenate((top10_cities_coordinates,coordinates_capital_cities[closest_capital_index,:].reshape(1,2)))\nprint(\"Coordinates of the top 10 cities closest to US are: \\n\",top10_cities_coordinates)\n\nTop 10 countries closest to Washington DC are:\n Canada\nBahamas\nBermuda\nCuba\nTurks and Caicos Islands\nCayman Islands\nHaiti\nJamaica\nDominican Republic\nSaint Pierre and Miquelon\nCoordinates of the top 10 cities closest to US are: \n [[ 45.41666667 -75.7       ]\n [ 25.08333333 -77.35      ]\n [ 32.28333333 -64.783333  ]\n [ 23.11666667 -82.35      ]\n [ 21.46666667 -71.133333  ]\n [ 19.3        -81.383333  ]\n [ 18.53333333 -72.333333  ]\n [ 18.         -76.8       ]\n [ 18.46666667 -69.9       ]\n [ 46.76666667 -56.183333  ]]"
  },
  {
    "objectID": "NumPy.html#vectorized-computation-with-numpy",
    "href": "NumPy.html#vectorized-computation-with-numpy",
    "title": "4  NumPy",
    "section": "4.8 Vectorized computation with NumPy",
    "text": "4.8 Vectorized computation with NumPy\nSeveral matrix algebra operations such as multiplications, decompositions, determinants, etc. can be performed conveniently with NumPy. However, we’ll focus on matrix multiplication as it is very commonly used to avoid python for loops and make computations faster. The dot function is used to multiply matrices:\n\n#Defining a 2x2 matrix\na = np.array([[0,1],[3,4]])\na\n\narray([[0, 1],\n       [3, 4]])\n\n\n\n#Defining a 2x2 matrix\nb = np.array([[6,-1],[2,1]])\nb\n\narray([[ 6, -1],\n       [ 2,  1]])\n\n\n\n#Multiplying matrices 'a' and 'b' using the dot function\na.dot(b)\n\narray([[ 2,  1],\n       [26,  1]])\n\n\n\n#Note that * results in element-wise multiplication\na*b\n\narray([[ 0, -1],\n       [ 6,  4]])\n\n\nExample 2: This example will show vectorized computations with NumPy. Vectorized computations help perform computations more efficiently, and also make the code concise.\nQ: Read the (1) quantities of roll, bun, cake and bread required by 3 people - Ben, Barbara & Beth, from food_quantity.csv, (2) price of these food items in two shops - Target and Kroger, from price.csv. Find out which shop should each person go to minimize their expenses.\n\n#Reading the datasets on food quantity and price\nimport pandas as pd\nfood_qty = pd.read_csv('./Datasets/food_quantity.csv')\nprice = pd.read_csv('./Datasets/price.csv')\n\n\nfood_qty\n\n\n\n\n\n\n\n\nPerson\nroll\nbun\ncake\nbread\n\n\n\n\n0\nBen\n6\n5\n3\n1\n\n\n1\nBarbara\n3\n6\n2\n2\n\n\n2\nBeth\n3\n4\n3\n1\n\n\n\n\n\n\n\n\nprice\n\n\n\n\n\n\n\n\nItem\nTarget\nKroger\n\n\n\n\n0\nroll\n1.5\n1.0\n\n\n1\nbun\n2.0\n2.5\n\n\n2\ncake\n5.0\n4.5\n\n\n3\nbread\n16.0\n17.0\n\n\n\n\n\n\n\nFirst, let’s start from a simple problem. We’ll compute the expenses of Ben if he prefers to buy all food items from Target\n\n#Method 1: Using loop\nbens_target_expense = 0 #Initializing Ben's expenses to 0\nfor k in range(4):   #Iterating over all the four desired food items\n    bens_target_expense += food_qty.iloc[0,k+1]*price.iloc[k,1] #Total expenses on the kth item\nbens_target_expense    #Total expenses for Ben if he goes to Target\n\n50.0\n\n\n\n#Method 2: Using NumPy array\nfood_num = food_qty.iloc[0,1:].to_numpy()  #Converting food quantity (for Ben) dataframe to NumPy array\nprice_num = price.iloc[:,1].to_numpy()     #Converting price (for Target) dataframe to NumPy array\nfood_num.dot(price_num)   #Matrix multiplication of the quantity vector with the price vector directly yields the result\n\n50.0\n\n\nBen will spend $50 if he goes to Target\nNow, let’s add another layer of complication. We’ll compute Ben’s expenses for both stores - Target and Kroger\n\n#Method 1: Using loops\n\n#Initializing a Series of length two to store the expenses in Target and Kroger for Ben\nbens_store_expense = pd.Series(0.0,index=price.columns[1:3])\nfor j in range(2):      #Iterating over both the stores - Target and Kroger\n    for k in range(4):        #Iterating over all the four desired food items\n        bens_store_expense[j] += food_qty.iloc[0,k+1]*price.iloc[k,j+1]\nbens_store_expense\n\nTarget    50.0\nKroger    49.0\ndtype: float64\n\n\n\n#Method 2: Using NumPy array\nfood_num = food_qty.iloc[0,1:].to_numpy()  #Converting food quantity (for Ben) dataframe to NumPy array\nprice_num = price.iloc[:,1:].to_numpy()    #Converting price dataframe to NumPy array\nfood_num.dot(price_num)      #Matrix multiplication of the quantity vector with the price matrix directly yields the result\n\narray([50.0, 49.0], dtype=object)\n\n\nBen will spend \\$50 if he goes to Target, and $49 if he goes to Kroger. Thus, he should choose Kroger.\nNow, let’s add the final layer of complication, and solve the problem. We’ll compute everyone’s expenses for both stores - Target and Kroger\n\n#Method 1: Using loops\nstore_expense = pd.DataFrame(0.0,index=price.columns[1:3],columns = food_qty['Person'])\nfor i in range(3):    #Iterating over all the three people - Ben, Barbara, and Beth\n    for j in range(2):     #Iterating over both the stores - Target and Kroger\n        for k in range(4):        #Iterating over all the four desired food items\n            store_expense.iloc[j,i] += food_qty.iloc[i,k+1]*price.iloc[k,j+1]\nstore_expense\n\n\n\n\n\n\n\nPerson\nBen\nBarbara\nBeth\n\n\n\n\nTarget\n50.0\n58.5\n43.5\n\n\nKroger\n49.0\n61.0\n43.5\n\n\n\n\n\n\n\n\n#Method 2: Using NumPy array\nfood_num = food_qty.iloc[:,1:].to_numpy() #Converting food quantity dataframe to NumPy array\nprice_num = price.iloc[:,1:].to_numpy()  #Converting price dataframe to NumPy array\nfood_num.dot(price_num)  #Matrix multiplication of the quantity matrix with the price matrix directly yields the result\n\narray([[50. , 49. ],\n       [58.5, 61. ],\n       [43.5, 43.5]])\n\n\nBased on the above table, Ben should go to Kroger, Barbara to Target and Beth can go to either store.\nNote that, with each layer of complication, the number of for loops keep increasing, thereby increasing the complexity of Method 1, while the method with NumPy array does not change much. Vectorized computations with arrays are much more efficient.\n\n4.8.1 Practice exercise 2\nUse matrix multiplication to find the average IMDB rating and average Rotten tomatoes rating for each genre - comedy, action, drama and horror. Use the data: movies_cleaned.csv. Which is the most preferred genre for IMDB users, and which is the least preferred genre for Rotten Tomatoes users?\nHint: 1. Create two matrices - one containing the IMDB and Rotten Tomatoes ratings, and the other containing the genre flags (comedy/action/drama/horror).\n\nMultiply the two matrices created in 1.\nDivide each row/column of the resulting matrix by a vector having the number of ratings in each genre to get the average rating for the genre.\n\nSolution:\n\nimport pandas as pd\ndata = pd.read_csv('./Datasets/movies_cleaned.csv')\ndata.head()\n\n\n\n\n\n\n\n\nTitle\nIMDB Rating\nRotten Tomatoes Rating\nRunning Time min\nRelease Date\nUS Gross\nWorldwide Gross\nProduction Budget\ncomedy\nAction\ndrama\nhorror\n\n\n\n\n0\nBroken Arrow\n5.8\n55\n108\nFeb 09 1996\n70645997\n148345997\n65000000\n0\n1\n0\n0\n\n\n1\nBrazil\n8.0\n98\n136\nDec 18 1985\n9929135\n9929135\n15000000\n1\n0\n0\n0\n\n\n2\nThe Cable Guy\n5.8\n52\n95\nJun 14 1996\n60240295\n102825796\n47000000\n1\n0\n0\n0\n\n\n3\nChain Reaction\n5.2\n13\n106\nAug 02 1996\n21226204\n60209334\n55000000\n0\n1\n0\n0\n\n\n4\nClash of the Titans\n5.9\n65\n108\nJun 12 1981\n30000000\n30000000\n15000000\n0\n1\n0\n0\n\n\n\n\n\n\n\n\n# Getting ratings of all movies\ndrating = data[['IMDB Rating','Rotten Tomatoes Rating']]\ndrating_num = drating.to_numpy() #Converting the data to NumPy array\ndrating_num\n\narray([[ 5.8, 55. ],\n       [ 8. , 98. ],\n       [ 5.8, 52. ],\n       ...,\n       [ 7. , 65. ],\n       [ 5.7, 26. ],\n       [ 6.7, 82. ]])\n\n\n\n# Getting the matrix indicating the genre of all movies\ndgenre = data.iloc[:,8:12]\ndgenre_num = dgenre.to_numpy() #Converting the data to NumPy array\ndgenre_num\n\narray([[0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       ...,\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 1, 0, 0]], dtype=int64)\n\n\nWe’ll first find the total IMDB and Rotten tomatoes ratings for all movies of each genre, and then divide them by the number of movies of the corresponding genre to find the average rating for the genre.\nFor finding the total IMDB and Rotten tomatoes ratings, we’ll multiply drating_num with dgenre_num. However, before multiplying, we’ll check if their shapes are compatible for matrix multiplication.\n\n#Shape of drating_num\ndrating_num.shape\n\n(980, 2)\n\n\n\n#Shape of dgenre_num\ndgenre_num.shape\n\n(980, 4)\n\n\nNote that the above shapes are not compatible for matrix multiplication. We’ll transpose dgenre_num to make the shapes compatible.\n\n#Total IMDB and Rotten tomatoes ratings for each genre\nratings_sum_genre = drating_num.T.dot(dgenre_num)\nratings_sum_genre\n\narray([[ 1785.6,  1673.1,  1630.3,   946.2],\n       [14119. , 13725. , 14535. ,  6533. ]])\n\n\n\n#Number of movies in the data will be stored in 'rows', and number of columns stored in 'cols'\nrows, cols = data.shape\n\n\n#Getting number of movies in each genre\nmovies_count_genre = dgenre_num.T.dot(np.ones(rows))\nmovies_count_genre\n\narray([302., 264., 239., 154.])\n\n\n\n#Finding the average IMDB and average Rotten tomatoes ratings for each genre\nratings_sum_genre/movies_count_genre\n\narray([[ 5.91258278,  6.3375    ,  6.82133891,  6.14415584],\n       [46.75165563, 51.98863636, 60.81589958, 42.42207792]])\n\n\n\npd.DataFrame(ratings_sum_genre/movies_count_genre,columns = ['comedy','Action','drama','horror'],\n             index = ['IMDB Rating','Rotten Tomatoes Rating'])\n\n\n\n\n\n\n\n\ncomedy\nAction\ndrama\nhorror\n\n\n\n\nIMDB Rating\n5.912583\n6.337500\n6.821339\n6.144156\n\n\nRotten Tomatoes Rating\n46.751656\n51.988636\n60.815900\n42.422078\n\n\n\n\n\n\n\nIMDB users prefer drama, and are amused the least by comedy movies, on an average. However, Rotten tomatoes critics would rather watch comedy than horror movies, on an average."
  },
  {
    "objectID": "NumPy.html#pseudorandom-number-generation",
    "href": "NumPy.html#pseudorandom-number-generation",
    "title": "4  NumPy",
    "section": "4.9 Pseudorandom number generation",
    "text": "4.9 Pseudorandom number generation\nRandom numbers often need to be generated to analyze processes or systems, especially in cases when these processes or systems are governed by known probability distrbutions. For example, the number of personnel required to answer calls at a call center can be analyzed by simulating occurence and duration of calls.\nNumPy’s random module can be used to generate arrays of random numbers from several different probability distributions. For example, a 3x5 array of uniformly distributed random numbers can be generated using the uniform function of the random module.\n\nnp.random.uniform(size = (3,5))\n\narray([[0.69256322, 0.69259973, 0.03515058, 0.45186048, 0.43513769],\n       [0.07373366, 0.07465425, 0.92195975, 0.72915895, 0.8906299 ],\n       [0.15816734, 0.88144978, 0.05954028, 0.81403832, 0.97725557]])\n\n\nRandom numbers can also be generated by Python’s built-in random module. However, it generates one random number at a time, which makes it much slower than NumPy’s random module.\nExample: Suppose 500 people eat at Food cart 1, and another 500 eat at Food cart 2, everyday.\nThe waiting time at Food cart 2 has a normal distribution with mean 8 minutes and standard deviation 3 minutes, while the waiting time at Food cart 1 has a uniform distribution with minimum 5 minutes and maximum 25 minutes.\nSimulate a dataset containing waiting times for 500 ppl for 30 days in each of the food joints. Assume that the waiting times are measured simultaneously at a certain time in both places, i.e., the observations are paired.\nOn how many days is the average waiting time at Food cart 2 higher than that at Food cart 1?\nWhat percentage of times the waiting time at Food cart 2 was higher than the waiting time at Food cart 1?\nTry both approaches: (1) Using loops to generate data, (2) numpy array to generate data. Compare the time taken in both approaches.\n\nimport time as tm\n\n\n#Method 1: Using loops\nstart_time = tm.time() #Current system time\n\n#Initializing waiting times for 500 ppl over 30 days\nwaiting_times_FoodCart1 = pd.DataFrame(0,index=range(500),columns=range(30)) #FoodCart1\nwaiting_times_FoodCart2 = pd.DataFrame(0,index=range(500),columns=range(30)) #FoodCart2\nimport random as rm\nfor i in range(500):  #Iterating over 500 ppl\n    for j in range(30): #Iterating over 30 days\n        waiting_times_FoodCart2.iloc[i,j] = rm.gauss(8,3) #Simulating waiting time in FoodCart2 for the ith person on jth day\n        waiting_times_FoodCart1.iloc[i,j] = rm.uniform(5,25) #Simulating waiting time in FoodCart1 for the ith person on jth day\ntime_diff = waiting_times_FoodCart2-waiting_times_FoodCart1\n\nprint(\"On \",sum(time_diff.mean()&gt;0),\" days, the average waiting time at FoodCart2 higher than that at FoodCart1\")\nprint(\"Percentage of times waiting time at FoodCart2 was greater than that at FoodCart1 = \",100*(time_diff&gt;0).sum().sum()/(30*500),\"%\")\nend_time = tm.time() #Current system time\nprint(\"Time taken = \", end_time-start_time)\n\nOn  0  days, the average waiting time at FoodCart2 higher than that at FoodCart1\nPercentage of times waiting time at FoodCart2 was greater than that at FoodCart1 =  16.226666666666667 %\nTime taken =  4.521248817443848\n\n\n\n#Method 2: Using NumPy arrays\nstart_time = tm.time()\nwaiting_time_FoodCart2 = np.random.normal(8,3,size = (500,30)) #Simultaneously generating the waiting times of 500 ppl over 30 days in FoodCart2\nwaiting_time_FoodCart1 = np.random.uniform(5,25,size = (500,30)) #Simultaneously generating the waiting times of 500 ppl over 30 days in FoodCart1\ntime_diff = waiting_time_FoodCart2-waiting_time_FoodCart1\nprint(\"On \",(time_diff.mean()&gt;0).sum(),\" days, the average waiting time at FoodCart2 higher than that at FoodCart1\")\nprint(\"Percentage of times waiting time at FoodCart2 was greater than that at FoodCart1 = \",100*(time_diff&gt;0).sum()/15000,\"%\")\nend_time = tm.time()\nprint(\"Time taken = \", end_time-start_time)\n\nOn  0  days, the average waiting time at FoodCart2 higher than that at FoodCart1\nPercentage of times waiting time at FoodCart2 was greater than that at FoodCart1 =  16.52 %\nTime taken =  0.008000850677490234\n\n\nThe approach with NumPy is much faster than the one with loops.\n\n4.9.1 Practice exercise 3\nBootstrapping: Find the 95% confidence interval of mean profit for ‘Action’ movies, using Bootstrapping.\nBootstrapping is a non-parametric method for obtaining confidence interval. Use the algorithm below to find the confidence interval:\n\nFind the profit for each of the ‘Action’ movies. Suppose there are N such movies. We will have a Profit column with N values.\n\nRandomly sample N values with replacement from the Profit column\n\nFind the mean of the N values obtained in (b)\n\nRepeat steps (b) and (c) M=1000 times\n\nThe 95% Confidence interval is the range between the 2.5% and 97.5% percentile values of the 1000 means obtained in (c)\nUse the movies_cleaned.csv dataset.\n\nSolution:\n\n#Reading data\nmovies = pd.read_csv('./Datasets/movies_cleaned.csv')\n\n#Filtering action movies\nmovies_action = movies.loc[movies['Action']==1,:]\n\n#Computing profit of movies\nmovies_action.loc[:,'Profit'] = movies_action.loc[:,'Worldwide Gross'] - movies_action.loc[:,'Production Budget']\n\n#Subsetting the profit column\nprofit_vec = movies_action['Profit']\n\n#Creating a matrix of 1000 samples with replacement from the profit column\nbootstrap_samples=np.random.choice(profit_vec,size = (1000,len(profit_vec)))\n\n#Computing the mean of each of the 1000 samples\nbootstrap_sample_means = bootstrap_samples.mean(axis=1)\n\n#The confidence interval is the 2.5th and 97.5th percentile of the mean of the 1000 samples\nprint(\"Confidence interval = [$\"+str(np.round(np.percentile(bootstrap_sample_means,2.5)/1e6,2))+\" million, $\"+str(np.round(np.percentile(bootstrap_sample_means,97.5)/1e6,2))+\" million]\")\n\nConfidence interval = [$132.53 million, $182.69 million]"
  },
  {
    "objectID": "Pandas.html#introduction",
    "href": "Pandas.html#introduction",
    "title": "5  Pandas",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nThe Pandas library contains several methods and functions for cleaning, manipulating and analyzing data. While NumPy is suited for working with homogenous numerical array data, Pandas is designed for working with tabular or heterogenous data.\nPandas is built on top of the NumPy package. Thus, there are some similarities between the two libraries. Like NumPy, Pandas provides the basic mathematical functionalities like addition, subtraction, conditional operations and broadcasting. However, unlike NumPy library which provides objects for multi-dimensional arrays, Pandas provides the 2D table object called Dataframe.\nData in pandas is often used to feed statistical analysis in SciPy, plotting functions from Matplotlib, and machine learning algorithms in Scikit-learn.\nTypically, the Pandas library is used for:\n\nCleaning the data by tasks such as removing missing values, filtering rows / columns, aggregating data, mutating data, etc.\nComputing summary statistics such as the mean, median, max, min, standard deviation, etc.\nComputing correlation among columns in the data\nComputing the data distribution\nVisualizing the data with help from the Matplotlib library\nWriting the cleaned and transformed data into a CSV file or other database formats\n\nLet’s import the Pandas library to use its methods and functions.\n\nimport pandas as pd"
  },
  {
    "objectID": "Pandas.html#pandas-data-structures---series-and-dataframe",
    "href": "Pandas.html#pandas-data-structures---series-and-dataframe",
    "title": "5  Pandas",
    "section": "5.2 Pandas data structures - Series and DataFrame",
    "text": "5.2 Pandas data structures - Series and DataFrame\nThere are two core components of the Pandas library - Series and DataFrame.\nA DataFrame is a two-dimensional object - comprising of tabular data organized in rows and columns, where individual columns can be of different value types (numeric / string / boolean etc.). A DataFrame has row labels (also called row indices) which refer to individual rows, and column labels (also called column names) that refer to individual columns. By default, the row indices are integers starting from zero. However, both the row indices and column names can be customized by the user.\nLet us read the spotify data - spotify_data.csv, using the Pandas function read_csv().\n\nspotify_data = pd.read_csv('./Datasets/spotify_data.csv')\nspotify_data.head()\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n0\n16996777\nrap\nJuice WRLD\n96\nAll Girls Are The Same\n0\n165820\n1\n2021\n0.673\n...\n0\n-7.226\n1\n0.3060\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\n\n\n1\n16996777\nrap\nJuice WRLD\n96\nLucid Dreams\n0\n239836\n1\n2021\n0.511\n...\n6\n-7.230\n0\n0.2000\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\n\n\n2\n16996777\nrap\nJuice WRLD\n96\nHear Me Calling\n0\n189977\n1\n2021\n0.699\n...\n7\n-3.997\n0\n0.1060\n0.3080\n0.000036\n0.1210\n0.499\n88.933\n4\n\n\n3\n16996777\nrap\nJuice WRLD\n96\nRobbery\n0\n240527\n1\n2021\n0.708\n...\n2\n-5.181\n1\n0.0442\n0.3480\n0.000000\n0.2220\n0.543\n79.993\n4\n\n\n4\n5988689\nrap\nRoddy Ricch\n88\nBig Stepper\n0\n175170\n0\n2021\n0.753\n...\n8\n-8.469\n1\n0.2920\n0.0477\n0.000000\n0.1970\n0.616\n76.997\n4\n\n\n\n\n5 rows × 21 columns\n\n\n\nThe object spotify_data is a pandas DataFrame:\n\ntype(spotify_data)\n\npandas.core.frame.DataFrame\n\n\nA Series is a one-dimensional object, containing a sequence of values, where each value has an index. Each column of a DataFrame is Series as shown in the example below.\n\n#Extracting song titles from the spotify_songs DataFrame\nspotify_songs = spotify_data['track_name']\nspotify_songs\n\n0                           All Girls Are The Same\n1                                     Lucid Dreams\n2                                  Hear Me Calling\n3                                          Robbery\n4                                      Big Stepper\n                            ...                   \n243185                                    Stardust\n243186             Knockin' A Jug - 78 rpm Version\n243187            When It's Sleepy Time Down South\n243188    On The Sunny Side Of The Street - Part 2\n243189                                    My Sweet\nName: track_name, Length: 243190, dtype: object\n\n\n\n#The object spotify_songs is a Series\ntype(spotify_songs)\n\npandas.core.series.Series\n\n\nA Series is essentially a column, and a DataFrame is a two-dimensional table made up of a collection of Series"
  },
  {
    "objectID": "Pandas.html#creating-a-pandas-series-dataframe",
    "href": "Pandas.html#creating-a-pandas-series-dataframe",
    "title": "5  Pandas",
    "section": "5.3 Creating a Pandas Series / DataFrame",
    "text": "5.3 Creating a Pandas Series / DataFrame\n\n5.3.1 Specifying data within the Series() / DataFrame() functions\nA Pandas Series and DataFrame can be created by specifying the data within the Series() / DataFrame() function. Below are examples of defining a Pandas Series / DataFrame.\n\n#Defining a Pandas Series\nseries_example = pd.Series(['these','are','english','words'])\nseries_example\n\n0      these\n1        are\n2    english\n3      words\ndtype: object\n\n\nNote that the default row indices are integers starting from 0. However, the index can be specified with the index argument if desired by the user:\n\n#Defining a Pandas Series with custom row labels\nseries_example = pd.Series(['these','are','english','words'], index = range(101,105))\nseries_example\n\n101      these\n102        are\n103    english\n104      words\ndtype: object\n\n\n\n\n5.3.2 Transforming in-built data structures\nA Pandas DataFrame can be created by converting the in-built python data structures such as lists, dictionaries, and list of dictionaries to DataFrame. See the examples below.\n\n#List consisting of expected age to marry of students of the STAT303-1 Fall 2022 class\nexp_marriage_age_list=['24','30','28','29','30','27','26','28','30+','26','28','30','30','30','probably never','30','25','25','30','28','30+ ','30','25','28','28','25','25','27','28','30','30','35','26','28','27','27','30','25','30','26','32','27','26','27','26','28','37','28','28','28','35','28','27','28','26','28','26','30','27','30','28','25','26','28','35','29','27','27','30','24','25','29','27','33','30','30','25','26','30','32','26','30','30','I wont','25','27','27','25','27','27','32','26','25','never','28','33','28','35','25','30','29','30','31','28','28','30','40','30','28','30','27','by 30','28','27','28','30-35','35','30','30','never','30','35','28','31','30','27','33','32','27','27','26','N/A','25','26','29','28','34','26','24','28','30','120','25','33','27','28','32','30','26','30','30','28','27','27','27','27','27','27','28','30','30','30','28','30','28','30','30','28','28','30','27','30','28','25','never','69','28','28','33','30','28','28','26','30','26','27','30','25','Never','27','27','25']\n\n\n#Example 1: Creating a Pandas Series from a list\nexp_marriage_age_series=pd.Series(exp_marriage_age_list,name = 'expected_marriage_age')\nexp_marriage_age_series.head()\n\n0    24\n1    30\n2    28\n3    29\n4    30\nName: expected_marriage_age, dtype: object\n\n\n\n#Dictionary consisting of the GDP per capita of the US from 1960 to 2021 with some missing values\nGDP_per_capita_dict = {'1960':3007,'1961':3067,'1962':3244,'1963':3375,'1964':3574,'1965':3828,'1966':4146,'1967':4336,'1968':4696,'1970':5234,'1971':5609,'1972':6094,'1973':6726,'1974':7226,'1975':7801,'1976':8592,'1978':10565,'1979':11674, '1980':12575,'1981':13976,'1982':14434,'1983':15544,'1984':17121,'1985':18237,  '1986':19071,'1987':20039,'1988':21417,'1989':22857,'1990':23889,'1991':24342,  '1992':25419,'1993':26387,'1994':27695,'1995':28691,'1996':29968,'1997':31459,  '1998':32854,'2000':36330,'2001':37134,'2002':37998,'2003':39490,'2004':41725,  '2005':44123,'2006':46302,'2007':48050,'2008':48570,'2009':47195,'2010':48651,  '2011':50066,'2012':51784,'2013':53291,'2015':56763,'2016':57867,'2017':59915,'2018':62805, '2019':65095,'2020':63028,'2021':69288}\n\n\n#Example 2: Creating a Pandas Series from a Dictionary\nGDP_per_capita_series = pd.Series(GDP_per_capita_dict)\nGDP_per_capita_series.head()\n\n1960    3007\n1961    3067\n1962    3244\n1963    3375\n1964    3574\ndtype: int64\n\n\n\n#List of dictionary consisting of 52 playing cards of the deck\ndeck_list_of_dictionaries = [{'value':i, 'suit':c}\nfor c in ['spades', 'clubs', 'hearts', 'diamonds']\nfor i in range(2,15)]\n\n\n#Example 3: Creating a Pandas DataFrame from a List of dictionaries\ndeck_df = pd.DataFrame(deck_list_of_dictionaries)\ndeck_df.head()\n\n\n\n\n\n\n\n\nvalue\nsuit\n\n\n\n\n0\n2\nspades\n\n\n1\n3\nspades\n\n\n2\n4\nspades\n\n\n3\n5\nspades\n\n\n4\n6\nspades\n\n\n\n\n\n\n\n\n\n5.3.3 Importing data from files\nIn the real world, a Pandas DataFrame will typically be created by loading the datasets from existing storage such as SQL Database, CSV file, Excel file, text files, HTML files, etc., as we learned in the third chapter of the book on Reading data."
  },
  {
    "objectID": "Pandas.html#attributes-and-methods-of-a-pandas-dataframe",
    "href": "Pandas.html#attributes-and-methods-of-a-pandas-dataframe",
    "title": "5  Pandas",
    "section": "5.4 Attributes and Methods of a Pandas DataFrame",
    "text": "5.4 Attributes and Methods of a Pandas DataFrame\nAll attributes and methods of a Pandas DataFrame object can be viewed with the python’s built-in dir() function.\n\n#List of attributes and methods of a Pandas DataFrame\n#This code is not executed as the list is too long\ndir(spotify_data)\n\nAlthough we’ll see examples of attributes and methods of a Pandas DataFrame, please note that most of these attributes and methods are also applicable to the Pandas Series object.\n\n5.4.1 Attributes of a Pandas DataFrame\nSome of the attributes of the Pandas DataFrame class are the following.\n\n5.4.1.1 dtypes\nThis attribute is a Series consisting the datatypes of columns of a Pandas DataFrame.\n\nspotify_data.dtypes\n\nartist_followers       int64\ngenres                object\nartist_name           object\nartist_popularity      int64\ntrack_name            object\ntrack_popularity       int64\nduration_ms            int64\nexplicit               int64\nrelease_year           int64\ndanceability         float64\nenergy               float64\nkey                    int64\nloudness             float64\nmode                   int64\nspeechiness          float64\nacousticness         float64\ninstrumentalness     float64\nliveness             float64\nvalence              float64\ntempo                float64\ntime_signature         int64\ndtype: object\n\n\nThe table below describes the datatypes of columns in a Pandas DataFrame.\n\n\n\nPandas Type\nNative Python Type\nDescription\n\n\n\n\nobject\nstring\nThe most general dtype. This datatype is assigned to a column if the column has mixed types (numbers and strings)\n\n\nint64\nint\nThis datatype is for integers from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807 or for integers having a maximum size of 64 bits\n\n\nfloat64\nfloat\nThis datatype is for real numbers. If a column contains integers and NaNs, Pandas will default to float64. This is because the missing values may be a real number\n\n\ndatetime64, timedelta[ns]\nN/A (but see the datetime module in Python’s standard library)\nValues meant to hold time data. This datatype is useful for time series analysis\n\n\n\n\n\n5.4.1.2 columns\nThis attribute consists of the column labels (or column names) of a Pandas DataFrame.\n\nspotify_data.columns\n\nIndex(['artist_followers', 'genres', 'artist_name', 'artist_popularity',\n       'track_name', 'track_popularity', 'duration_ms', 'explicit',\n       'release_year', 'danceability', 'energy', 'key', 'loudness', 'mode',\n       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n       'valence', 'tempo', 'time_signature'],\n      dtype='object')\n\n\n\n\n5.4.1.3 index\nThis attribute consists of the row lables (or row indices) of a Pandas DataFrame.\n\nspotify_data.index\n\nRangeIndex(start=0, stop=243190, step=1)\n\n\n\n\n5.4.1.4 axes\nThis is a list of length two, where the first element is the row labels, and the second element is the columns labels. In other words, this attribute combines the information in the attributes - index and columns.\n\nspotify_data.axes\n\n[RangeIndex(start=0, stop=243190, step=1),\n Index(['artist_followers', 'genres', 'artist_name', 'artist_popularity',\n        'track_name', 'track_popularity', 'duration_ms', 'explicit',\n        'release_year', 'danceability', 'energy', 'key', 'loudness', 'mode',\n        'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n        'valence', 'tempo', 'time_signature'],\n       dtype='object')]\n\n\n\n\n5.4.1.5 ndim\nAs in NumPy, this attribute specifies the number of dimensions. However, unlike NumPy, a Pandas DataFrame has a fixed dimenstion of 2, and a Pandas Series has a fixed dimesion of 1.\n\nspotify_data.ndim\n\n2\n\n\n\n\n5.4.1.6 size\nThis attribute specifies the number of elements in a DataFrame. Its value is the product of the number of rows and columns.\n\nspotify_data.size\n\n5106990\n\n\n\n\n5.4.1.7 shape\nThis is a tuple consisting of the number of rows and columns in a Pandas DataFrame.\n\nspotify_data.shape\n\n(243190, 21)\n\n\n\n\n5.4.1.8 values\nThis provides a NumPy representation of a Pandas DataFrame.\n\nspotify_data.values\n\narray([[16996777, 'rap', 'Juice WRLD', ..., 0.203, 161.991, 4],\n       [16996777, 'rap', 'Juice WRLD', ..., 0.218, 83.903, 4],\n       [16996777, 'rap', 'Juice WRLD', ..., 0.499, 88.933, 4],\n       ...,\n       [2256652, 'jazz', 'Louis Armstrong', ..., 0.37, 105.093, 4],\n       [2256652, 'jazz', 'Louis Armstrong', ..., 0.576, 101.279, 4],\n       [2256652, 'jazz', 'Louis Armstrong', ..., 0.816, 105.84, 4]],\n      dtype=object)\n\n\n\n\n\n5.4.2 Methods of a Pandas DataFrame\nSome of the commonly used methods of the Pandas DataFrame class are the following.\n\n5.4.2.1 head()\nPrints the first n rows of a DataFrame.\n\nspotify_data.head(2)\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n0\n16996777\nrap\nJuice WRLD\n96\nAll Girls Are The Same\n0\n165820\n1\n2021\n0.673\n...\n0\n-7.226\n1\n0.306\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\n\n\n1\n16996777\nrap\nJuice WRLD\n96\nLucid Dreams\n0\n239836\n1\n2021\n0.511\n...\n6\n-7.230\n0\n0.200\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\n\n\n\n\n2 rows × 21 columns\n\n\n\n\n\n5.4.2.2 tail()\nPrints the last n rows of a DataFrame.\n\nspotify_data.tail(3)\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n243187\n2256652\njazz\nLouis Armstrong\n74\nWhen It's Sleepy Time Down South\n4\n200200\n0\n1923\n0.527\n...\n3\n-14.814\n1\n0.0793\n0.989\n0.00001\n0.1040\n0.370\n105.093\n4\n\n\n243188\n2256652\njazz\nLouis Armstrong\n74\nOn The Sunny Side Of The Street - Part 2\n4\n185973\n0\n1923\n0.559\n...\n0\n-9.804\n1\n0.0512\n0.989\n0.84700\n0.4480\n0.576\n101.279\n4\n\n\n243189\n2256652\njazz\nLouis Armstrong\n74\nMy Sweet\n4\n195960\n0\n1923\n0.741\n...\n3\n-10.406\n1\n0.0505\n0.927\n0.07880\n0.0633\n0.816\n105.840\n4\n\n\n\n\n3 rows × 21 columns\n\n\n\n\n\n5.4.2.3 describe()\nPrint summary statistics of a Pandas DataFrame, as seen in chapter 3 on Reading Data.\n\nspotify_data.describe()\n\n\n\n\n\n\n\n\nartist_followers\nartist_popularity\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\ncount\n2.431900e+05\n243190.000000\n243190.000000\n2.431900e+05\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n\n\nmean\n1.960931e+06\n65.342633\n36.080772\n2.263209e+05\n0.050039\n1992.475258\n0.568357\n0.580633\n5.240326\n-9.432548\n0.670928\n0.111984\n0.383938\n0.071169\n0.223756\n0.552302\n119.335060\n3.884177\n\n\nstd\n5.028746e+06\n10.289182\n16.476836\n9.973214e+04\n0.218026\n18.481463\n0.159444\n0.236631\n3.532546\n4.449731\n0.469877\n0.198068\n0.321142\n0.209555\n0.198076\n0.250017\n29.864219\n0.458082\n\n\nmin\n2.300000e+01\n51.000000\n0.000000\n3.344000e+03\n0.000000\n1923.000000\n0.000000\n0.000000\n0.000000\n-60.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n1.832620e+05\n57.000000\n25.000000\n1.776670e+05\n0.000000\n1980.000000\n0.462000\n0.405000\n2.000000\n-11.990000\n0.000000\n0.033200\n0.070000\n0.000000\n0.098100\n0.353000\n96.099250\n4.000000\n\n\n50%\n5.352520e+05\n64.000000\n36.000000\n2.188670e+05\n0.000000\n1994.000000\n0.579000\n0.591000\n5.000000\n-8.645000\n1.000000\n0.043100\n0.325000\n0.000011\n0.141000\n0.560000\n118.002000\n4.000000\n\n\n75%\n1.587332e+06\n72.000000\n48.000000\n2.645465e+05\n0.000000\n2008.000000\n0.685000\n0.776000\n8.000000\n-6.131000\n1.000000\n0.075300\n0.671000\n0.002220\n0.292000\n0.760000\n137.929000\n4.000000\n\n\nmax\n7.890023e+07\n100.000000\n99.000000\n4.995083e+06\n1.000000\n2021.000000\n0.988000\n1.000000\n11.000000\n3.744000\n1.000000\n0.969000\n0.996000\n1.000000\n1.000000\n1.000000\n243.507000\n5.000000\n\n\n\n\n\n\n\n\n\n5.4.2.4 max()/min()\nReturns the max/min values of numeric columns. If the function is applied on non-numeric columns, it will return the maximum/minimum value based on the order of the alphabet.\n\n#The max() method applied on a Series\nspotify_data['artist_popularity'].max()\n\n100\n\n\n\n#The max() method applied on a DataFrame\nspotify_data.max()\n\nartist_followers                    78900234\ngenres                                  rock\nartist_name                          高爾宣 OSN\nartist_popularity                        100\ntrack_name           행복했던 날들이었다 days gone by\ntrack_popularity                          99\nduration_ms                          4995083\nexplicit                                   1\nrelease_year                            2021\ndanceability                           0.988\nenergy                                   1.0\nkey                                       11\nloudness                               3.744\nmode                                       1\nspeechiness                            0.969\nacousticness                           0.996\ninstrumentalness                         1.0\nliveness                                 1.0\nvalence                                  1.0\ntempo                                243.507\ntime_signature                             5\ndtype: object\n\n\n\n\n5.4.2.5 mean()/median()\nReturns the mean/median values of numeric columns.\n\nspotify_data.median()\n\nartist_followers     535252.000000\nartist_popularity        64.000000\ntrack_popularity         36.000000\nduration_ms          218867.000000\nexplicit                  0.000000\nrelease_year           1994.000000\ndanceability              0.579000\nenergy                    0.591000\nkey                       5.000000\nloudness                 -8.645000\nmode                      1.000000\nspeechiness               0.043100\nacousticness              0.325000\ninstrumentalness          0.000011\nliveness                  0.141000\nvalence                   0.560000\ntempo                   118.002000\ntime_signature            4.000000\ndtype: float64\n\n\n\n\n5.4.2.6 std()\nReturns the standard deviation of numeric columns.\n\nspotify_data.std()\n\nartist_followers     5.028746e+06\nartist_popularity    1.028918e+01\ntrack_popularity     1.647684e+01\nduration_ms          9.973214e+04\nexplicit             2.180260e-01\nrelease_year         1.848146e+01\ndanceability         1.594436e-01\nenergy               2.366309e-01\nkey                  3.532546e+00\nloudness             4.449731e+00\nmode                 4.698771e-01\nspeechiness          1.980684e-01\nacousticness         3.211417e-01\ninstrumentalness     2.095551e-01\nliveness             1.980759e-01\nvalence              2.500172e-01\ntempo                2.986422e+01\ntime_signature       4.580822e-01\ndtype: float64\n\n\n\n\n5.4.2.7 sample(n)\nReturns n random observations from a Pandas DataFrame.\n\nspotify_data.sample(4)\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n42809\n385756\nrock\nSaxon\n56\nNever Surrender - 2009 Remastered Version\n2\n195933\n0\n2012\n0.506\n...\n6\n-7.847\n1\n0.0633\n0.0535\n0.00001\n0.3330\n0.536\n90.989\n4\n\n\n25730\n810526\nhip hop\nFroid\n68\nPseudosocial\n54\n135963\n0\n2016\n0.644\n...\n7\n-9.098\n0\n0.3280\n0.8270\n0.00001\n0.1630\n0.886\n117.170\n4\n\n\n147392\n479209\njazz\nSarah Vaughan\n59\nLove Dance\n14\n204400\n0\n1982\n0.386\n...\n0\n-23.819\n1\n0.0372\n0.8970\n0.00000\n0.0943\n0.102\n110.981\n3\n\n\n233189\n1201905\nrock\nGrateful Dead\n72\nCold Rain and Snow - 2013 Remaster\n29\n151702\n0\n1967\n0.412\n...\n6\n-10.476\n0\n0.0487\n0.4090\n0.58300\n0.1630\n0.875\n168.803\n4\n\n\n\n\n4 rows × 21 columns\n\n\n\n\n\n5.4.2.8 dropna()\nDrops all observations with at least one missing value.\n\n#This code is not executed to avoid prining a large table\nspotify_data.dropna()\n\n\n\n5.4.2.9 apply()\nThis method is used to apply a function over all columns or rows of a Pandas DataFrame. For example, let us find the range of values of artist_followers, artist_popularity and release_year.\n\n#Defining the function to compute range of values of a columns\ndef range_of_values(x):\n    return x.max()-x.min()\n\n#Applying the function to three coluumns for which we wish to find the range of values\nspotify_data[['artist_followers','artist_popularity','release_year']].apply(range_of_values, axis=0)\n\nartist_followers     78900211\nartist_popularity          49\nrelease_year               98\ndtype: int64\n\n\nThe apply() method is often used with the one line function known as lambda function in python. These functions do not require a name, and can be defined using the keyword lambda. The above block of code can be concisely written as:\n\nspotify_data[['artist_followers','artist_popularity','release_year']].apply(lambda x:x.max()-x.min(), axis=0)\n\nartist_followers     78900211\nartist_popularity          49\nrelease_year               98\ndtype: int64\n\n\nNote that the Series object also has an apply() method associated with it. The method can be used to apply a function to each value of a Series.\n\n\n5.4.2.10 map()\nThe function is used to map distinct values of a Pandas Series to another set of corresponding values.\nFor example, suppose we wish to create a new column in the spotify dataset which indicates the modality of the song - major (mode = 1) or minor (mode = 0). We’ll map the values of the mode column to the categories major and minor:\n\n#Creating a dictionary that maps the values 0 and 1 to minor and major respectively\nmap_mode = {0:'minor', 1:'major'}\n\n#The map() function requires a dictionary object, and maps the 'values' of the 'keys' in the dictionary\nspotify_data['modality'] = spotify_data['mode'].map(map_mode)\n\nWe can see the variable modality in the updated DataFrame.\n\nspotify_data.head()\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\nmodality\n\n\n\n\n0\n16996777\nrap\nJuice WRLD\n96\nAll Girls Are The Same\n0\n165820\n1\n2021\n0.673\n...\n-7.226\n1\n0.3060\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\nmajor\n\n\n1\n16996777\nrap\nJuice WRLD\n96\nLucid Dreams\n0\n239836\n1\n2021\n0.511\n...\n-7.230\n0\n0.2000\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\nminor\n\n\n2\n16996777\nrap\nJuice WRLD\n96\nHear Me Calling\n0\n189977\n1\n2021\n0.699\n...\n-3.997\n0\n0.1060\n0.3080\n0.000036\n0.1210\n0.499\n88.933\n4\nminor\n\n\n3\n16996777\nrap\nJuice WRLD\n96\nRobbery\n0\n240527\n1\n2021\n0.708\n...\n-5.181\n1\n0.0442\n0.3480\n0.000000\n0.2220\n0.543\n79.993\n4\nmajor\n\n\n4\n5988689\nrap\nRoddy Ricch\n88\nBig Stepper\n0\n175170\n0\n2021\n0.753\n...\n-8.469\n1\n0.2920\n0.0477\n0.000000\n0.1970\n0.616\n76.997\n4\nmajor\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\n5.4.2.11 drop()\nThis function is used to drop rows/columns from a DataFrame.\nFor example, let us drop the columns mode from the spotify dataset:\n\n#Dropping the column 'mode'\nspotify_data_new = spotify_data.drop('mode',axis=1)\nspotify_data_new.head()\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\nmodality\n\n\n\n\n0\n16996777\nrap\nJuice WRLD\n96\nAll Girls Are The Same\n0\n165820\n1\n2021\n0.673\n...\n0\n-7.226\n0.3060\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\nmajor\n\n\n1\n16996777\nrap\nJuice WRLD\n96\nLucid Dreams\n0\n239836\n1\n2021\n0.511\n...\n6\n-7.230\n0.2000\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\nminor\n\n\n2\n16996777\nrap\nJuice WRLD\n96\nHear Me Calling\n0\n189977\n1\n2021\n0.699\n...\n7\n-3.997\n0.1060\n0.3080\n0.000036\n0.1210\n0.499\n88.933\n4\nminor\n\n\n3\n16996777\nrap\nJuice WRLD\n96\nRobbery\n0\n240527\n1\n2021\n0.708\n...\n2\n-5.181\n0.0442\n0.3480\n0.000000\n0.2220\n0.543\n79.993\n4\nmajor\n\n\n4\n5988689\nrap\nRoddy Ricch\n88\nBig Stepper\n0\n175170\n0\n2021\n0.753\n...\n8\n-8.469\n0.2920\n0.0477\n0.000000\n0.1970\n0.616\n76.997\n4\nmajor\n\n\n\n\n5 rows × 21 columns\n\n\n\nNote that if multiple columns or rows are to be dropped, they must be enclosed in box brackets.\n\n\n5.4.2.12 unique()\nThis functions provides the unique values of a Series. For example, let us find the number of unique genres of songs in the spotify dataset:\n\nspotify_data.genres.unique()\n\narray(['rap', 'pop', 'miscellaneous', 'metal', 'hip hop', 'rock',\n       'pop & rock', 'hoerspiel', 'folk', 'electronic', 'jazz', 'country',\n       'latin'], dtype=object)\n\n\n\n\n5.4.2.13 value_counts()\nThis function provides the number of observations of each value of a Series. For example, let us find the number of songs of each genre in the spotify dataset:\n\nspotify_data.genres.value_counts()\n\npop              70441\nrock             49785\npop & rock       43437\nmiscellaneous    35848\njazz             13363\nhoerspiel        12514\nhip hop           7373\nfolk              2821\nlatin             2125\nrap               1798\nmetal             1659\ncountry           1236\nelectronic         790\nName: genres, dtype: int64\n\n\nMore than half the songs in the dataset are pop, rock or pop & rock.\n\n\n5.4.2.14 isin()\nThis function provides a boolean Series indicating the position of certain values in a Series. The function is helpful in sub-setting data. For example, let us subset the songs that are either latin, rap, or metal:\n\nlatin_rap_metal_songs = spotify_data.loc[spotify_data.genres.isin(['latin','rap','metal']),:]\nlatin_rap_metal_songs.head()\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n0\n16996777\nrap\nJuice WRLD\n96\nAll Girls Are The Same\n0\n165820\n1\n2021\n0.673\n...\n0\n-7.226\n1\n0.3060\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\n\n\n1\n16996777\nrap\nJuice WRLD\n96\nLucid Dreams\n0\n239836\n1\n2021\n0.511\n...\n6\n-7.230\n0\n0.2000\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\n\n\n2\n16996777\nrap\nJuice WRLD\n96\nHear Me Calling\n0\n189977\n1\n2021\n0.699\n...\n7\n-3.997\n0\n0.1060\n0.3080\n0.000036\n0.1210\n0.499\n88.933\n4\n\n\n3\n16996777\nrap\nJuice WRLD\n96\nRobbery\n0\n240527\n1\n2021\n0.708\n...\n2\n-5.181\n1\n0.0442\n0.3480\n0.000000\n0.2220\n0.543\n79.993\n4\n\n\n4\n5988689\nrap\nRoddy Ricch\n88\nBig Stepper\n0\n175170\n0\n2021\n0.753\n...\n8\n-8.469\n1\n0.2920\n0.0477\n0.000000\n0.1970\n0.616\n76.997\n4\n\n\n\n\n5 rows × 21 columns"
  },
  {
    "objectID": "Pandas.html#data-manipulations-with-pandas",
    "href": "Pandas.html#data-manipulations-with-pandas",
    "title": "5  Pandas",
    "section": "5.5 Data manipulations with Pandas",
    "text": "5.5 Data manipulations with Pandas\n\n5.5.1 Sub-setting data\n\n5.5.1.1 loc and iloc with the original row / column index\nSubsetting observations: In the chapter on reading data, we learned about operators loc and iloc that can be used to subset data based on axis labels and position of rows/columns respectively. However, usually we are not aware of the relevant row indices, and we may want to subset data based on some condition(s). For example, suppose we wish to analyze only those songs whose track popularity is higher than 50.\nQ: Do we need to subset rows or columns in this case?\nA: Rows, as songs correspond to rows, while features of songs correspond to columns.\nAs we need to subset rows, the filter must be applied at the starting index, i.e., the index before the ,. As we don’t need to subset any specific features of the songs, there is no subsetting to be done on the columns. A : at the ending index means that all columns need to selected.\n\n#Subsetting spotify songs that have track popularity score of more than 50\npopular_songs = spotify_data.loc[spotify_data.track_popularity&gt;=50,:]\npopular_songs.head()\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n181\n1277325\nhip hop\nDave\n77\nTitanium\n69\n127750\n1\n2021\n0.959\n...\n0\n-8.687\n0\n0.4370\n0.152000\n0.000001\n0.1050\n0.510\n121.008\n4\n\n\n191\n1123869\nrap\nJay Wheeler\n85\nViendo el Techo\n64\n188955\n0\n2021\n0.741\n...\n11\n-6.029\n0\n0.2290\n0.306000\n0.000327\n0.1000\n0.265\n179.972\n4\n\n\n208\n3657199\nrap\nPolo G\n91\nRAPSTAR\n89\n165926\n1\n2021\n0.789\n...\n6\n-6.862\n1\n0.2420\n0.410000\n0.000000\n0.1290\n0.437\n81.039\n4\n\n\n263\n1461700\npop & rock\nTeoman\n67\nGecenin Sonuna Yolculuk\n52\n280600\n0\n2021\n0.686\n...\n11\n-7.457\n0\n0.0268\n0.119000\n0.000386\n0.1080\n0.560\n100.932\n4\n\n\n293\n299746\npop & rock\nLars Winnerbäck\n62\nSjäl och hjärta\n55\n271675\n0\n2021\n0.492\n...\n2\n-6.005\n0\n0.0349\n0.000735\n0.000207\n0.0953\n0.603\n142.042\n4\n\n\n\n\n5 rows × 21 columns\n\n\n\nSubsetting columns: Suppose we wish to analyze only track_name, release year and track_popularity of songs. Then, we can subset the revelant columns:\n\nrelevant_columns = spotify_data.loc[:,['track_name','release_year','track_popularity']]\nrelevant_columns.head()\n\n\n\n\n\n\n\n\ntrack_name\nrelease_year\ntrack_popularity\n\n\n\n\n0\nAll Girls Are The Same\n2021\n0\n\n\n1\nLucid Dreams\n2021\n0\n\n\n2\nHear Me Calling\n2021\n0\n\n\n3\nRobbery\n2021\n0\n\n\n4\nBig Stepper\n2021\n0\n\n\n\n\n\n\n\nNote that when multiple columns are subset with loc they are enclosed in a box bracket, unlike the case with a single column. Similarly if multiple observations are selected using the row labels, the row labels must be enclosed in box brackets.\n\n\n5.5.1.2 Re-indexing rows followed by loc / iloc\nSuppose we wish to subset data based on the genres. If we want to subset hiphop songs, we may subset as:\n\n#Subsetting hiphop songs\nhiphop_songs = spotify_data.loc[spotify_data['genres']=='hip hop',:]\nhiphop_songs.head()\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n64\n6485079\nhip hop\nDaBaby\n93\nFIND MY WAY\n0\n139890\n1\n2021\n0.836\n...\n4\n-6.750\n0\n0.1840\n0.1870\n0.00000\n0.1380\n0.7000\n103.000\n4\n\n\n80\n22831280\nhip hop\nDaddy Yankee\n91\nHula Hoop\n0\n236493\n0\n2021\n0.799\n...\n6\n-4.628\n1\n0.0801\n0.1130\n0.00315\n0.0942\n0.9510\n175.998\n4\n\n\n81\n22831280\nhip hop\nDaddy Yankee\n91\nGasolina - Live\n0\n306720\n0\n2021\n0.669\n...\n1\n-4.251\n1\n0.2700\n0.1530\n0.00000\n0.1540\n0.0814\n96.007\n4\n\n\n87\n22831280\nhip hop\nDaddy Yankee\n91\nLa Nueva Y La Ex\n0\n197053\n0\n2021\n0.639\n...\n5\n-3.542\n1\n0.1360\n0.0462\n0.00000\n0.1410\n0.6390\n198.051\n4\n\n\n88\n22831280\nhip hop\nDaddy Yankee\n91\nQue Tire Pa Lante\n0\n210520\n0\n2021\n0.659\n...\n7\n-2.814\n1\n0.0358\n0.0478\n0.00000\n0.1480\n0.7040\n93.979\n4\n\n\n\n\n5 rows × 21 columns\n\n\n\nHowever, if we need to subset data by genres frequently in our analysis, and we don’t need the current row labels, we may replace the row labels as genres to shorten the code for filtering the observations based on genres.\nWe use the set_index() function to re-index the rows based on existing column(s) of the DataFrame.\n\n#Defining row labels as the values of the column `genres`\nspotify_data_reindexed = spotify_data.set_index(keys=spotify_data['genres'])\nspotify_data_reindexed.head()\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\ngenres\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrap\n16996777\nrap\nJuice WRLD\n96\nAll Girls Are The Same\n0\n165820\n1\n2021\n0.673\n...\n0\n-7.226\n1\n0.3060\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\n\n\nrap\n16996777\nrap\nJuice WRLD\n96\nLucid Dreams\n0\n239836\n1\n2021\n0.511\n...\n6\n-7.230\n0\n0.2000\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\n\n\nrap\n16996777\nrap\nJuice WRLD\n96\nHear Me Calling\n0\n189977\n1\n2021\n0.699\n...\n7\n-3.997\n0\n0.1060\n0.3080\n0.000036\n0.1210\n0.499\n88.933\n4\n\n\nrap\n16996777\nrap\nJuice WRLD\n96\nRobbery\n0\n240527\n1\n2021\n0.708\n...\n2\n-5.181\n1\n0.0442\n0.3480\n0.000000\n0.2220\n0.543\n79.993\n4\n\n\nrap\n5988689\nrap\nRoddy Ricch\n88\nBig Stepper\n0\n175170\n0\n2021\n0.753\n...\n8\n-8.469\n1\n0.2920\n0.0477\n0.000000\n0.1970\n0.616\n76.997\n4\n\n\n\n\n5 rows × 21 columns\n\n\n\nNow, we can subset hiphop songs using the row label of the data:\n\n#Subsetting hiphop songs using row labels\nhiphop_songs = spotify_data_reindexed.loc['hip hop',:]\nhiphop_songs.head()\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\ngenres\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhip hop\n6485079\nhip hop\nDaBaby\n93\nFIND MY WAY\n0\n139890\n1\n2021\n0.836\n...\n4\n-6.750\n0\n0.1840\n0.1870\n0.00000\n0.1380\n0.7000\n103.000\n4\n\n\nhip hop\n22831280\nhip hop\nDaddy Yankee\n91\nHula Hoop\n0\n236493\n0\n2021\n0.799\n...\n6\n-4.628\n1\n0.0801\n0.1130\n0.00315\n0.0942\n0.9510\n175.998\n4\n\n\nhip hop\n22831280\nhip hop\nDaddy Yankee\n91\nGasolina - Live\n0\n306720\n0\n2021\n0.669\n...\n1\n-4.251\n1\n0.2700\n0.1530\n0.00000\n0.1540\n0.0814\n96.007\n4\n\n\nhip hop\n22831280\nhip hop\nDaddy Yankee\n91\nLa Nueva Y La Ex\n0\n197053\n0\n2021\n0.639\n...\n5\n-3.542\n1\n0.1360\n0.0462\n0.00000\n0.1410\n0.6390\n198.051\n4\n\n\nhip hop\n22831280\nhip hop\nDaddy Yankee\n91\nQue Tire Pa Lante\n0\n210520\n0\n2021\n0.659\n...\n7\n-2.814\n1\n0.0358\n0.0478\n0.00000\n0.1480\n0.7040\n93.979\n4\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n\n\n5.5.2 Sorting data\nSorting dataset is a very common operation. The sort_values() function of Pandas can be used to sort a Pandas DataFrame or Series. Let us sort the spotify data in decreasing order of track_popularity:\n\nspotify_sorted = spotify_data.sort_values(by = 'track_popularity', ascending = False)\nspotify_sorted.head()\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n2398\n1444702\npop\nOlivia Rodrigo\n88\ndrivers license\n99\n242014\n1\n2021\n0.585\n...\n10\n-8.761\n1\n0.0601\n0.72100\n0.000013\n0.1050\n0.132\n143.874\n4\n\n\n2442\n177401\nhip hop\nMasked Wolf\n85\nAstronaut In The Ocean\n98\n132780\n0\n2021\n0.778\n...\n4\n-6.865\n0\n0.0913\n0.17500\n0.000000\n0.1500\n0.472\n149.996\n4\n\n\n3133\n1698014\npop\nKali Uchis\n88\ntelepatía\n97\n160191\n0\n2020\n0.653\n...\n11\n-9.016\n0\n0.0502\n0.11200\n0.000000\n0.2030\n0.553\n83.970\n4\n\n\n6702\n31308207\npop\nThe Weeknd\n96\nSave Your Tears\n97\n215627\n1\n2020\n0.680\n...\n0\n-5.487\n1\n0.0309\n0.02120\n0.000012\n0.5430\n0.644\n118.051\n4\n\n\n6703\n31308207\npop\nThe Weeknd\n96\nBlinding Lights\n96\n200040\n0\n2020\n0.514\n...\n1\n-5.934\n1\n0.0598\n0.00146\n0.000095\n0.0897\n0.334\n171.005\n4\n\n\n\n\n5 rows × 21 columns\n\n\n\nDrivers license is the most popular song!\n\n\n\n\n\n \n        \n\n\n\n\n5.5.3 Ranking data\nWith the rank() function, we can rank the observations.\nFor example, let us add a new column to the spotify data that provides the rank of the track_popularity column:\n\nspotify_ranked = spotify_data.copy()\nspotify_ranked['track_popularity_rank']=spotify_sorted['track_popularity'].rank()\nspotify_ranked.head()\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\ntrack_popularity_rank\n\n\n\n\n0\n16996777\nrap\nJuice WRLD\n96\nAll Girls Are The Same\n0\n165820\n1\n2021\n0.673\n...\n-7.226\n1\n0.3060\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\n963.5\n\n\n1\n16996777\nrap\nJuice WRLD\n96\nLucid Dreams\n0\n239836\n1\n2021\n0.511\n...\n-7.230\n0\n0.2000\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\n963.5\n\n\n2\n16996777\nrap\nJuice WRLD\n96\nHear Me Calling\n0\n189977\n1\n2021\n0.699\n...\n-3.997\n0\n0.1060\n0.3080\n0.000036\n0.1210\n0.499\n88.933\n4\n963.5\n\n\n3\n16996777\nrap\nJuice WRLD\n96\nRobbery\n0\n240527\n1\n2021\n0.708\n...\n-5.181\n1\n0.0442\n0.3480\n0.000000\n0.2220\n0.543\n79.993\n4\n963.5\n\n\n4\n5988689\nrap\nRoddy Ricch\n88\nBig Stepper\n0\n175170\n0\n2021\n0.753\n...\n-8.469\n1\n0.2920\n0.0477\n0.000000\n0.1970\n0.616\n76.997\n4\n963.5\n\n\n\n\n5 rows × 22 columns\n\n\n\nNote the column track_popularity_rank. Why does it contain floating point numbers? Check the rank() documentation to find out!\n\n\n5.5.4 Practice exercise 1\n\n5.5.4.1 \nRead the file STAT303-1 survey for data analysis.csv.\n\nsurvey_data = pd.read_csv('./Datasets/STAT303-1 survey for data analysis.csv')\n\n\n\n5.5.4.2 \nHow many observations and variables are there in the data?\n\nprint(\"The data has \",survey_data.shape[0],\"observations, and\", survey_data.shape[1], \"columns\")\n\nThe data has  192 observations, and 51 columns\n\n\n\n\n5.5.4.3 \nRename all the columns of the data, except the first two columns, with the shorter names in the list new_col_names given below. The order of column names in the list is the same as the order in which the columns are to be renamed starting with the third column from the left.\n\nnew_col_names = ['parties_per_month', 'do_you_smoke', 'weed', 'are_you_an_introvert_or_extrovert', 'love_first_sight', 'learning_style', 'left_right_brained', 'personality_type', 'social_media', 'num_insta_followers', 'streaming_platforms', 'expected_marriage_age', 'expected_starting_salary', 'fav_sport', 'minutes_ex_per_week', 'sleep_hours_per_day', 'how_happy', 'farthest_distance_travelled', 'fav_number', 'fav_letter', 'internet_hours_per_day', 'only_child', 'birthdate_odd_even', 'birth_month', 'fav_season', 'living_location_on_campus', 'major', 'num_majors_minors', 'high_school_GPA', 'NU_GPA', 'age', 'height', 'height_father', 'height_mother', 'school_year', 'procrastinator', 'num_clubs', 'student_athlete', 'AP_stats', 'used_python_before', 'dominant_hand', 'childhood_in_US', 'gender', 'region_of_residence', 'political_affliation', 'cant_change_math_ability', 'can_change_math_ability', 'math_is_genetic', 'much_effort_is_lack_of_talent']\n\n\nsurvey_data.columns = list(survey_data.columns[0:2])+new_col_names\n\n\n\n5.5.4.4 \nRename the following columns again:\n\nRename do_you_smoke to smoke.\nRename are_you_an_introvert_or_extrovert to introvert_extrovert.\n\nHint: Use the function rename()\n\nsurvey_data.rename(columns={'do_you_smoke':'smoke','are_you_an_introvert_or_extrovert':'introvert_extrovert'},inplace=True)\n\n\n\n5.5.4.5 \nFind the proportion of people going to more than 4 parties per month. Use the variable parties_per_month.\n\nsurvey_data['parties_per_month']=pd.to_numeric(survey_data.parties_per_month,errors='coerce')\nsurvey_data.loc[survey_data['parties_per_month']&gt;4,:].shape[0]/survey_data.shape[0]\n\n0.3385416666666667\n\n\n\n\n5.5.4.6 \nAmong the people who go to more than 4 parties a month, what proportion of them are introverts?\n\nsurvey_data.loc[((survey_data['parties_per_month']&gt;4) & (survey_data.introvert_extrovert=='Introvert')),:].shape[0]/survey_data.loc[survey_data['parties_per_month']&gt;4,:].shape[0]\n\n0.5076923076923077\n\n\n\n\n5.5.4.7 \nFind the proportion of people in each category of the variable how_happy.\n\nsurvey_data.how_happy.value_counts()/survey_data.shape[0]\n\nPretty happy     0.703125\nVery happy       0.151042\nNot too happy    0.088542\nDon't know       0.057292\nName: how_happy, dtype: float64\n\n\n\n\n5.5.4.8 \nAmong the people who go to more than 4 parties a month, what proportion of them are either Pretty happy or Very happy?\n\nsurvey_data.loc[((survey_data['parties_per_month']&gt;4) & (survey_data.how_happy.isin(['Pretty happy','Very happy'])))].shape[0]/survey_data.loc[survey_data['parties_per_month']&gt;4,:].shape[0]\n\n0.9076923076923077\n\n\n\n\n5.5.4.9 \nExamine the column num_insta_followers. Some numbers in the column contain a comma(,) or a tilde(~). Remove both these characters from the numbers in the column.\nHint: You may use the function str.replace() of the Pandas Series class.\n\nsurvey_data_insta = survey_data.copy()\nsurvey_data_insta['num_insta_followers']=survey_data_insta['num_insta_followers'].str.replace(',','')\nsurvey_data_insta['num_insta_followers']=survey_data_insta['num_insta_followers'].str.replace('~','')\n\n\n\n5.5.4.10 \nConvert the column num_insta_followers to numeric. Coerce the errors.\n\nsurvey_data_insta.num_insta_followers = pd.to_numeric(survey_data_insta.num_insta_followers,errors='coerce')\n\n\n\n5.5.4.11 \nDrop the observations consisting of missing values for num_insta_followers. Report the number of observations dropped.\n\nsurvey_data.num_insta_followers.isna().sum()\n\n3\n\n\nThere are 3 missing values of num_insta_followers.\n\n#Dropping observations with missing values of num_insta_followers\nsurvey_data=survey_data[~survey_data.num_insta_followers.isna()]\n\n\n\n5.5.4.12 \nWhat is the mean internet_hours_per_day for the top 46 people in terms of number of instagram followers?\n\nsurvey_data_insta.sort_values(by = 'num_insta_followers',ascending=False, inplace=True)\ntop_insta = survey_data_insta.iloc[:46,:]\ntop_insta.internet_hours_per_day = pd.to_numeric(top_insta.internet_hours_per_day,errors = 'coerce')\ntop_insta.internet_hours_per_day.mean()\n\n5.088888888888889\n\n\n\n\n5.5.4.13 \nWhat is the mean internet_hours_per_day for the remaining people?\n\nlow_insta = survey_data_insta.iloc[46:,:]\nlow_insta.internet_hours_per_day = pd.to_numeric(low_insta.internet_hours_per_day,errors = 'coerce')\nlow_insta.internet_hours_per_day.mean()\n\n13.118881118881118"
  },
  {
    "objectID": "Pandas.html#arithematic-operations",
    "href": "Pandas.html#arithematic-operations",
    "title": "5  Pandas",
    "section": "5.6 Arithematic operations",
    "text": "5.6 Arithematic operations\n\n5.6.1 Arithematic operations between DataFrames\nLet us create two toy DataFrames:\n\n#Creating two toy DataFrames\ntoy_df1 = pd.DataFrame([(1,2),(3,4),(5,6)], columns=['a','b'])\ntoy_df2 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'])\n\n\n#DataFrame 1\ntoy_df1\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n3\n4\n\n\n2\n5\n6\n\n\n\n\n\n\n\n\n#DataFrame 2\ntoy_df2\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n100\n200\n\n\n1\n300\n400\n\n\n2\n500\n600\n\n\n\n\n\n\n\nElement by element operations between two DataFrames can be performed with the operators +, -, *,/,**, and %. Below is an example of element-by-element addition of two DataFrames:\n\n# Element-by-element arithmetic addition of the two DataFrames\ntoy_df1 + toy_df2\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n101\n202\n\n\n1\n303\n404\n\n\n2\n505\n606\n\n\n\n\n\n\n\nNote that these operations create problems when the row indices and/or column names of the two DataFrames do not match. See the example below:\n\n#Creating another toy example of a DataFrame\ntoy_df3 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'], index=[1,2,3])\ntoy_df3\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n1\n100\n200\n\n\n2\n300\n400\n\n\n3\n500\n600\n\n\n\n\n\n\n\n\n#Adding DataFrames with some unmatching row indices\ntoy_df1 + toy_df3\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\nNaN\nNaN\n\n\n1\n103.0\n204.0\n\n\n2\n305.0\n406.0\n\n\n3\nNaN\nNaN\n\n\n\n\n\n\n\nNote that the rows whose indices match between the two DataFrames are added up. The rest of the values are missing (or NaN) because only one of the DataFrames has that index.\nAs in the case of row indices, missing values will also appear in the case of unmatching column names, as shown in the example below.\n\ntoy_df4 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['b','c'])\ntoy_df4\n\n\n\n\n\n\n\n\nb\nc\n\n\n\n\n0\n100\n200\n\n\n1\n300\n400\n\n\n2\n500\n600\n\n\n\n\n\n\n\n\n#Adding DataFrames with some unmatching column names\ntoy_df1 + toy_df4\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\nNaN\n102\nNaN\n\n\n1\nNaN\n304\nNaN\n\n\n2\nNaN\n506\nNaN\n\n\n\n\n\n\n\n\n\n5.6.2 Arithematic operations between a Series and a DataFrame\nBroadcasting: As in NumPy, we can broadcast a Series to match the shape of another DataFrame:\n\n# Broadcasting: The row [1,2] (a Series) is added on every row in df2 \ntoy_df1.iloc[0,:] + toy_df2\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n101\n202\n\n\n1\n301\n402\n\n\n2\n501\n602\n\n\n\n\n\n\n\nNote that the + operator is used to add values of a Series to a DataFrame based on column names. For adding a Series to a DataFrame based on row indices, we cannot use the + operator. Instead, we’ll need to use the add() function as explained below.\nBroadcasting based on row/column labels: We can use the add() function to broadcast a Series to a DataFrame. By default the Series adds based on column names, as in the case of the + operator.\n\n# Add the first row of df1 (a Series) to every row in df2 \ntoy_df2.add(toy_df1.iloc[0,:])\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n101\n202\n\n\n1\n301\n402\n\n\n2\n501\n602\n\n\n\n\n\n\n\nFor broadcasting based on row indices, we use the axis argument of the add() function.\n\n# The second column of df1 (a Series) is added to every col in df2\ntoy_df2.add(toy_df1.iloc[:,1],axis='index')\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n102\n202\n\n\n1\n304\n404\n\n\n2\n506\n606\n\n\n\n\n\n\n\n\n\n5.6.3 Case study\nTo see the application of arithematic operations on DataFrames, let us see the case study below.\nSong recommendation: Spotify recommends songs based on songs listened by the user. Suppose you have listened to the song drivers license. Spotify intends to recommend you 5 songs that are similar to drivers license. Which songs should it recommend?\nLet us see the available song information that can help us identify songs similar to drivers license. The columns attribute of DataFrame will display all the columns names. The description of some of the column names relating to audio features is here.\n\nspotify_data.columns\n\nIndex(['artist_followers', 'genres', 'artist_name', 'artist_popularity',\n       'track_name', 'track_popularity', 'duration_ms', 'explicit',\n       'release_year', 'danceability', 'energy', 'key', 'loudness', 'mode',\n       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n       'valence', 'tempo', 'time_signature'],\n      dtype='object')\n\n\nSolution approach: We have several features of a song. Let us find songs similar to drivers license in terms of danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, time_signature and tempo. Note that we are considering only audio features for simplicity.\nTo find the songs most similar to drivers license, we need to define a measure that quantifies the similarity. Let us define similarity of a song with drivers license as the Euclidean distance of the song from drivers license, where the coordinates of a song are: (danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, time_signature, tempo). Thus, similarity can be formulated as:\n\\[Similarity_{DL-S} = \\sqrt{(danceability_{DL}-danceability_{S})^2+(energy_{DL}-energy_{S})^2 +...+ (tempo_{DL}-tempo_{S})^2)},\\]\nwhere the subscript DL stands for drivers license and S stands for any song. The top 5 songs with the least value of \\(Similarity_{DL-S}\\) will be the most similar to drivers lincense and should be recommended.\nLet us subset the columns that we need to use to compute the Euclidean distance.\n\naudio_features = spotify_data[['danceability', 'energy', 'key', 'loudness','mode','speechiness',\n                               'acousticness', 'instrumentalness', 'liveness','valence', 'tempo', 'time_signature']]\n\n\naudio_features.head()\n\n\n\n\n\n\n\n\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n0\n0.673\n0.529\n0\n-7.226\n1\n0.3060\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\n\n\n1\n0.511\n0.566\n6\n-7.230\n0\n0.2000\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\n\n\n2\n0.699\n0.687\n7\n-3.997\n0\n0.1060\n0.3080\n0.000036\n0.1210\n0.499\n88.933\n4\n\n\n3\n0.708\n0.690\n2\n-5.181\n1\n0.0442\n0.3480\n0.000000\n0.2220\n0.543\n79.993\n4\n\n\n4\n0.753\n0.597\n8\n-8.469\n1\n0.2920\n0.0477\n0.000000\n0.1970\n0.616\n76.997\n4\n\n\n\n\n\n\n\n\n#Distribution of values of audio_features\naudio_features.describe()\n\n\n\n\n\n\n\n\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\ncount\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n\n\nmean\n0.568357\n0.580633\n5.240326\n-9.432548\n0.670928\n0.111984\n0.383938\n0.071169\n0.223756\n0.552302\n119.335060\n3.884177\n\n\nstd\n0.159444\n0.236631\n3.532546\n4.449731\n0.469877\n0.198068\n0.321142\n0.209555\n0.198076\n0.250017\n29.864219\n0.458082\n\n\nmin\n0.000000\n0.000000\n0.000000\n-60.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.462000\n0.405000\n2.000000\n-11.990000\n0.000000\n0.033200\n0.070000\n0.000000\n0.098100\n0.353000\n96.099250\n4.000000\n\n\n50%\n0.579000\n0.591000\n5.000000\n-8.645000\n1.000000\n0.043100\n0.325000\n0.000011\n0.141000\n0.560000\n118.002000\n4.000000\n\n\n75%\n0.685000\n0.776000\n8.000000\n-6.131000\n1.000000\n0.075300\n0.671000\n0.002220\n0.292000\n0.760000\n137.929000\n4.000000\n\n\nmax\n0.988000\n1.000000\n11.000000\n3.744000\n1.000000\n0.969000\n0.996000\n1.000000\n1.000000\n1.000000\n243.507000\n5.000000\n\n\n\n\n\n\n\nNote that the audio features differ in terms of scale. Some features like key have a wide range of [0,11], while others like danceability have a very narrow range of [0,0.988]. If we use them directly, features like danceability will have a much higher influence on \\(Similarity_{DL-S}\\) as compared to features like key. Assuming we wish all the features to have equal weight in quantifying a song’s similarity to drivers license, we should scale the features, so that their values are comparable.\nLet us scale the value of each column to a standard uniform distribution: \\(U[0,1]\\).\nFor scaling the values of a column to \\(U[0,1]\\), we need to subtract the minimum value of the column from each value, and divide by the range of values of the column. For example, danceability can be standardized as follows:\n\n#Scaling danceability to U[0,1]\ndanceability_value_range = audio_features.danceability.max()-audio_features.danceability.min()\ndanceability_std = (audio_features.danceability-audio_features.danceability.min())/danceability_value_range\ndanceability_std\n\n0         0.681174\n1         0.517206\n2         0.707490\n3         0.716599\n4         0.762146\n            ...   \n243185    0.621457\n243186    0.797571\n243187    0.533401\n243188    0.565789\n243189    0.750000\nName: danceability, Length: 243190, dtype: float64\n\n\nHowever, it will be cumbersome to repeat the above code for each audio feature. We can instead write a function that scales values of a column to \\(U[0,1]\\), and apply the function on all the audio features.\n\n#Function to scale a column to U[0,1]\ndef scale_uniform(x):\n    return (x-x.min())/(x.max()-x.min())\n\nWe will use the Pandas function apply() to apply the above function to the DataFrame audio_features.\n\n#Scaling all audio features to U[0,1]\naudio_features_scaled = audio_features.apply(scale_uniform)\n\nThe above two blocks of code can be concisely written with the lambda function as:\n\naudio_features_scaled = audio_features.apply(lambda x: (x-x.min())/(x.max()-x.min()))\n\n\n#All the audio features are scaled to U[0,1]\naudio_features_scaled.describe()\n\n\n\n\n\n\n\n\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\ncount\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n\n\nmean\n0.575260\n0.580633\n0.476393\n0.793290\n0.670928\n0.115566\n0.385480\n0.071169\n0.223756\n0.552302\n0.490068\n0.776835\n\n\nstd\n0.161380\n0.236631\n0.321141\n0.069806\n0.469877\n0.204405\n0.322431\n0.209555\n0.198076\n0.250017\n0.122642\n0.091616\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.467611\n0.405000\n0.181818\n0.753169\n0.000000\n0.034262\n0.070281\n0.000000\n0.098100\n0.353000\n0.394647\n0.800000\n\n\n50%\n0.586032\n0.591000\n0.454545\n0.805644\n1.000000\n0.044479\n0.326305\n0.000011\n0.141000\n0.560000\n0.484594\n0.800000\n\n\n75%\n0.693320\n0.776000\n0.727273\n0.845083\n1.000000\n0.077709\n0.673695\n0.002220\n0.292000\n0.760000\n0.566427\n0.800000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\nSince we need to find the Euclidean distance from the song drivers license, let us find the index of the row containing features of drivers license.\n\n#Index of the row consisting of drivers license can be found with the index attribute\ndrivers_license_index = spotify_data[spotify_data.track_name=='drivers license'].index[0]\n\nNote that the object returned by the index attribute is of type pandas.core.indexes.numeric.Int64Index. The elements of this object can be retrieved like the elements of a python list. That is why the object is sliced with [0] to return the first element of the object. As there is only one observation with the track_name as drivers license, we sliced the first element. If there were multiple observations with track_name as drivers license, we will obtain the indices of all those observations with the index attribute.\nNow, we’ll subtract the audio features of drivers license from all other songs:\n\n#Audio features of drivers license are being subtracted from audio features of all songs by broadcasting\nsongs_minus_DL = audio_features_scaled-audio_features_scaled.loc[drivers_license_index,:]\n\nNow, let us square the difference computed above. We’ll use the in-built python function pow() to square the difference:\n\nsongs_minus_DL_sq = songs_minus_DL.pow(2)\nsongs_minus_DL_sq.head()\n\n\n\n\n\n\n\n\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n0\n0.007933\n0.008649\n0.826446\n0.000580\n0.0\n0.064398\n0.418204\n1.055600e-07\n0.000376\n0.005041\n0.005535\n0.0\n\n\n1\n0.005610\n0.016900\n0.132231\n0.000577\n1.0\n0.020844\n0.139498\n1.716100e-10\n0.055225\n0.007396\n0.060654\n0.0\n\n\n2\n0.013314\n0.063001\n0.074380\n0.005586\n1.0\n0.002244\n0.171942\n5.382400e-10\n0.000256\n0.134689\n0.050906\n0.0\n\n\n3\n0.015499\n0.064516\n0.528926\n0.003154\n0.0\n0.000269\n0.140249\n1.716100e-10\n0.013689\n0.168921\n0.068821\n0.0\n\n\n4\n0.028914\n0.025921\n0.033058\n0.000021\n0.0\n0.057274\n0.456981\n1.716100e-10\n0.008464\n0.234256\n0.075428\n0.0\n\n\n\n\n\n\n\nNow, we’ll sum the squares of differences from all audio features to compute the similarity of all songs to drivers license.\n\ndistance_squared = songs_minus_DL_sq.sum(axis = 1)\ndistance_squared.head()\n\n0    1.337163\n1    1.438935\n2    1.516317\n3    1.004043\n4    0.920316\ndtype: float64\n\n\nNow, we’ll sort these distances to find the top 5 songs closest to drivers’s license.\n\ndistances_sorted = distance_squared.sort_values()\ndistances_sorted.head()\n\n2398      0.000000\n81844     0.008633\n4397      0.011160\n130789    0.015018\n143744    0.015058\ndtype: float64\n\n\nUsing the indices of the top 5 distances, we will identify the top 5 songs most similar to drivers license:\n\nspotify_data.loc[distances_sorted.index[0:6],:]\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n2398\n1444702\npop\nOlivia Rodrigo\n88\ndrivers license\n99\n242014\n1\n2021\n0.585\n...\n10\n-8.761\n1\n0.0601\n0.721\n0.000013\n0.105\n0.132\n143.874\n4\n\n\n81844\n2264501\npop\nJay Chou\n74\n安靜\n49\n334240\n0\n2001\n0.513\n...\n10\n-7.853\n1\n0.0281\n0.688\n0.000008\n0.116\n0.123\n143.924\n4\n\n\n4397\n25457\npop\nTerence Lam\n60\n拼命無恙 in Bb major\n52\n241062\n0\n2020\n0.532\n...\n10\n-9.690\n1\n0.0269\n0.674\n0.000000\n0.117\n0.190\n151.996\n4\n\n\n130789\n176266\npop\nAlan Tam\n54\n從後趕上\n8\n258427\n0\n1988\n0.584\n...\n10\n-11.889\n1\n0.0282\n0.707\n0.000002\n0.107\n0.124\n140.147\n4\n\n\n143744\n396326\npop & rock\nLaura Branigan\n64\nHow Am I Supposed to Live Without You\n40\n263320\n0\n1983\n0.559\n...\n10\n-8.260\n1\n0.0355\n0.813\n0.000083\n0.134\n0.185\n139.079\n4\n\n\n35627\n1600562\npop\nTiziano Ferro\n68\nNon Me Lo So Spiegare\n44\n240040\n0\n2014\n0.609\n...\n11\n-7.087\n1\n0.0352\n0.706\n0.000000\n0.130\n0.207\n146.078\n4\n\n\n\n\n6 rows × 21 columns\n\n\n\nWe can see the top 5 songs most similar to drivers license in the track_name column above. Interestingly, three of the five songs are Asian! These songs indeed sound similar to drivers license!"
  },
  {
    "objectID": "Pandas.html#correlation",
    "href": "Pandas.html#correlation",
    "title": "5  Pandas",
    "section": "5.7 Correlation",
    "text": "5.7 Correlation\nCorrelation may refer to any kind of association between two random variables. However, in this book, we will always consider correlation as the linear association between two random variables, or the Pearson’s correlation coefficient. Note that correlation does not imply causality and vice-versa.\nThe Pandas function corr() provides the pairwise correlation between all columns of a DataFrame, or between two Series. The function corrwith() provides the pairwise correlation of a DataFrame with another DataFrame or Series.\n\n#Pairwise correlation amongst all columns\nspotify_data.corr()\n\n\n\n\n\n\n\n\nartist_followers\nartist_popularity\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\nartist_followers\n1.000000\n0.577861\n0.197426\n0.040435\n0.082857\n0.098589\n-0.010120\n0.080085\n-0.000119\n0.123771\n0.004313\n-0.059933\n-0.107475\n-0.033986\n0.002425\n-0.053317\n0.016524\n0.030826\n\n\nartist_popularity\n0.577861\n1.000000\n0.285565\n-0.097996\n0.092147\n0.062007\n0.038784\n0.039583\n-0.011005\n0.045165\n0.018758\n0.236942\n-0.075715\n-0.066679\n0.099678\n-0.034501\n-0.032036\n-0.033423\n\n\ntrack_popularity\n0.197426\n0.285565\n1.000000\n0.060474\n0.193685\n0.568329\n0.158507\n0.217342\n0.013369\n0.296350\n-0.022486\n-0.056537\n-0.284433\n-0.124283\n-0.090479\n-0.038859\n0.058408\n0.071741\n\n\nduration_ms\n0.040435\n-0.097996\n0.060474\n1.000000\n-0.024226\n0.067665\n-0.145779\n0.075990\n0.007710\n0.078586\n-0.034818\n-0.332585\n-0.133960\n0.067055\n-0.034631\n-0.155354\n0.051046\n0.085015\n\n\nexplicit\n0.082857\n0.092147\n0.193685\n-0.024226\n1.000000\n0.215656\n0.138522\n0.104734\n0.011818\n0.124410\n-0.060350\n0.077268\n-0.129363\n-0.039472\n-0.024283\n-0.032549\n0.006585\n0.043538\n\n\nrelease_year\n0.098589\n0.062007\n0.568329\n0.067665\n0.215656\n1.000000\n0.204743\n0.338096\n0.021497\n0.430054\n-0.071338\n-0.032968\n-0.369038\n-0.149644\n-0.045160\n-0.070025\n0.079382\n0.089485\n\n\ndanceability\n-0.010120\n0.038784\n0.158507\n-0.145779\n0.138522\n0.204743\n1.000000\n0.137615\n0.020128\n0.142239\n-0.051130\n0.198509\n-0.143936\n-0.179213\n-0.114999\n0.505350\n-0.125061\n0.111015\n\n\nenergy\n0.080085\n0.039583\n0.217342\n0.075990\n0.104734\n0.338096\n0.137615\n1.000000\n0.030824\n0.747829\n-0.053374\n-0.043377\n-0.678745\n-0.131269\n0.126050\n0.348158\n0.205960\n0.170854\n\n\nkey\n-0.000119\n-0.011005\n0.013369\n0.007710\n0.011818\n0.021497\n0.020128\n0.030824\n1.000000\n0.024674\n-0.139688\n-0.003533\n-0.023179\n-0.006600\n-0.011566\n0.024206\n0.008336\n0.007738\n\n\nloudness\n0.123771\n0.045165\n0.296350\n0.078586\n0.124410\n0.430054\n0.142239\n0.747829\n0.024674\n1.000000\n-0.028151\n-0.173444\n-0.493020\n-0.269008\n0.002959\n0.209588\n0.171926\n0.146030\n\n\nmode\n0.004313\n0.018758\n-0.022486\n-0.034818\n-0.060350\n-0.071338\n-0.051130\n-0.053374\n-0.139688\n-0.028151\n1.000000\n-0.037237\n0.043773\n-0.024695\n0.005657\n0.010305\n0.015399\n-0.015225\n\n\nspeechiness\n-0.059933\n0.236942\n-0.056537\n-0.332585\n0.077268\n-0.032968\n0.198509\n-0.043377\n-0.003533\n-0.173444\n-0.037237\n1.000000\n0.112061\n-0.094796\n0.263630\n0.052171\n-0.127945\n-0.150350\n\n\nacousticness\n-0.107475\n-0.075715\n-0.284433\n-0.133960\n-0.129363\n-0.369038\n-0.143936\n-0.678745\n-0.023179\n-0.493020\n0.043773\n0.112061\n1.000000\n0.112107\n0.007415\n-0.175674\n-0.173152\n-0.163243\n\n\ninstrumentalness\n-0.033986\n-0.066679\n-0.124283\n0.067055\n-0.039472\n-0.149644\n-0.179213\n-0.131269\n-0.006600\n-0.269008\n-0.024695\n-0.094796\n0.112107\n1.000000\n-0.031301\n-0.150172\n-0.027369\n-0.022034\n\n\nliveness\n0.002425\n0.099678\n-0.090479\n-0.034631\n-0.024283\n-0.045160\n-0.114999\n0.126050\n-0.011566\n0.002959\n0.005657\n0.263630\n0.007415\n-0.031301\n1.000000\n-0.011137\n-0.027716\n-0.040789\n\n\nvalence\n-0.053317\n-0.034501\n-0.038859\n-0.155354\n-0.032549\n-0.070025\n0.505350\n0.348158\n0.024206\n0.209588\n0.010305\n0.052171\n-0.175674\n-0.150172\n-0.011137\n1.000000\n0.100947\n0.084783\n\n\ntempo\n0.016524\n-0.032036\n0.058408\n0.051046\n0.006585\n0.079382\n-0.125061\n0.205960\n0.008336\n0.171926\n0.015399\n-0.127945\n-0.173152\n-0.027369\n-0.027716\n0.100947\n1.000000\n0.017423\n\n\ntime_signature\n0.030826\n-0.033423\n0.071741\n0.085015\n0.043538\n0.089485\n0.111015\n0.170854\n0.007738\n0.146030\n-0.015225\n-0.150350\n-0.163243\n-0.022034\n-0.040789\n0.084783\n0.017423\n1.000000\n\n\n\n\n\n\n\nQ: Which audio feature is the most correlated with track_popularity?\n\nspotify_data.corrwith(spotify_data.track_popularity).sort_values(ascending = False)\n\ntrack_popularity     1.000000\nrelease_year         0.568329\nloudness             0.296350\nartist_popularity    0.285565\nenergy               0.217342\nartist_followers     0.197426\nexplicit             0.193685\ndanceability         0.158507\ntime_signature       0.071741\nduration_ms          0.060474\ntempo                0.058408\nkey                  0.013369\nmode                -0.022486\nvalence             -0.038859\nspeechiness         -0.056537\nliveness            -0.090479\ninstrumentalness    -0.124283\nacousticness        -0.284433\ndtype: float64\n\n\nLoudness is the audio feature having the highest correlation with track_popularity.\nQ: Which audio feature is the most weakly correlated with track_popularity?\n\n5.7.1 Practice exercise 2\n\n5.7.1.1 \nUse the updated dataset from Practice exercise 1.\nThe last four variables in the dataset are:\n\ncant_change_math_ability\ncan_change_math_ability\nmath_is_genetic\nmuch_effort_is_lack_of_talent\n\nEach of the above variables has values - Agree / Disagree. Replace Agree with 1 and Disagree with 0.\nHint : You can do it with any one of the following methods:\n\nUse the map() function\nUse the apply() function with the lambda function\nUse the replace() function\nUse the applymap() function\n\nTwo of the above methods avoid a for-loop. Which ones?\nSolution:\n\n#Making a copy of data to avoid changing the original data. \nsurvey_data_copy = survey_data.copy()\n\n#Using the map function\nfor i in range(47,51):\n    survey_data_copy.iloc[:,i] = survey_data_copy.iloc[:,i].map({'Agree':1,'Disagree':0})\nsurvey_data_copy.iloc[:,47:51].head()\n\n\n\n\n\n\n\n\ncant_change_math_ability\ncan_change_math_ability\nmath_is_genetic\nmuch_effort_is_lack_of_talent\n\n\n\n\n0\n0\n1\n0\n0\n\n\n1\n0\n1\n0\n0\n\n\n2\n0\n1\n0\n0\n\n\n3\n0\n1\n0\n0\n\n\n4\n1\n0\n0\n0\n\n\n\n\n\n\n\n\n#Making a copy of data to avoid changing the original data. \nsurvey_data_copy = survey_data.copy()\n\n#Using the lambda function with apply()\nfor i in range(47,51):\n    survey_data_copy.iloc[:,i] = survey_data_copy.iloc[:,i].apply(lambda x: 1 if x=='Agree' else 0)\nsurvey_data_copy.iloc[:,47:51].head()\n\n\n\n\n\n\n\n\ncant_change_math_ability\ncan_change_math_ability\nmath_is_genetic\nmuch_effort_is_lack_of_talent\n\n\n\n\n0\n0\n1\n0\n0\n\n\n1\n0\n1\n0\n0\n\n\n2\n0\n1\n0\n0\n\n\n3\n0\n1\n0\n0\n\n\n4\n1\n0\n0\n0\n\n\n\n\n\n\n\n\n#Making a copy of data to avoid changing the original data. \nsurvey_data_copy = survey_data.copy()\n\n#Using the replace() function\nsurvey_data_copy.iloc[:,47:51] = survey_data_copy.iloc[:,47:51].replace('Agree','1')\nsurvey_data_copy.iloc[:,47:51] = survey_data_copy.iloc[:,47:51].replace('Disagree','0')\nsurvey_data_copy.iloc[:,47:51].head()\n\n\n\n\n\n\n\n\ncant_change_math_ability\ncan_change_math_ability\nmath_is_genetic\nmuch_effort_is_lack_of_talent\n\n\n\n\n0\n0\n1\n0\n0\n\n\n1\n0\n1\n0\n0\n\n\n2\n0\n1\n0\n0\n\n\n3\n0\n1\n0\n0\n\n\n4\n1\n0\n0\n0\n\n\n\n\n\n\n\n\n#Making a copy of data to avoid changing the original data. \nsurvey_data_copy = survey_data.copy()\n\n#Using the lambda function with applymap()\nsurvey_data_copy.iloc[:,47:51] = survey_data_copy.iloc[:,47:51].applymap(lambda x: 1 if x=='Agree' else 0)\nsurvey_data_copy.iloc[:,47:51].head()\n\n\n\n\n\n\n\n\ncant_change_math_ability\ncan_change_math_ability\nmath_is_genetic\nmuch_effort_is_lack_of_talent\n\n\n\n\n0\n0\n1\n0\n0\n\n\n1\n0\n1\n0\n0\n\n\n2\n0\n1\n0\n0\n\n\n3\n0\n1\n0\n0\n\n\n4\n1\n0\n0\n0\n\n\n\n\n\n\n\n\n\n5.7.1.2 \nAmong the four variables, which one is the most negatively correlated with math_is_genetic?\n\n#Computing correlation\nsurvey_data_copy.iloc[:,47:51].corrwith(survey_data_copy.math_is_genetic)\n\ncant_change_math_ability         0.294544\ncan_change_math_ability         -0.361546\nmath_is_genetic                  1.000000\nmuch_effort_is_lack_of_talent    0.154083\ndtype: float64\n\n\nThe variable can_change_math_ability is the most negatively correlated wtih math_is_genetic."
  },
  {
    "objectID": "Data visualization.html#matplotlib",
    "href": "Data visualization.html#matplotlib",
    "title": "6  Data visualization",
    "section": "6.1 Matplotlib",
    "text": "6.1 Matplotlib\nMatplotlib is:\n\na low-level graph plotting library in python that strives to emulate MATLAB,\ncan be used in Python scripts, Python and IPython shells, Jupyter notebooks and web application servers.\nis mostly written in python, a few segments are written in C, Objective-C and Javascript for Platform compatibility.\n\nConceptual model: Plotting requires action on a range of levels, ranging from the size of the figure to the text object in the plot. Matplotlib provides object-oriented interface in the hierarchical fashion to provide complete control over the plot. The user generates and keeps track of the figure and axes objects. These axes objects are then used for most plotting actions.\n\n6.1.1 Matplotlib: Object hierarchy\nA hierarchy means that there is a tree-like structure of matplotlib objects underlying each plot.\nA Figure object is the outermost container for a matplotlib graphic, which can contain multiple Axes objects. Note that an Axes actually translates into what we think of as an individual plot or graph (rather than the plural of axis as we might expect).\nThe Figure object is a box-like container holding one or more Axes (actual plots), as shown in Figure 6.1. Below the Axes in the hierarchy are smaller objects such as tick marks, individual lines, legends, and text boxes. Almost every element of a chart is its own manipulable Python object, all the way down to the ticks and labels.\n\n\n\n\nFigure 6.1: Matplotlib Object hierarchy\n\n\n\nHowever, Matplotlib presents this as a figure anatomy, rather than an explicit hierarchy. Figure 6.2 shows the components of a figure that can be customized with Matplotlib. (Source: https://matplotlib.org/stable/gallery/showcase/anatomy.html ).\n\n\n\n\nFigure 6.2: Matplotlib anatomy of a figure\n\n\n\nLet’s visualize the life expectancy of different countries with GDP per capita. We’ll read the data file gdp_lifeExpectancy.csv, which contains the GDP per capita and life expectancy of countries from 1952 to 2007.\n\nimport pandas as pd\nimport numpy as np\n\n\ngdp_data = pd.read_csv('./Datasets/gdp_lifeExpectancy.csv')\ngdp_data.head()\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n\n\n\n\n\n\n\n6.1.2 Scatterplots and trendline with Matplotlib\nPurpose of scatterplots: Scatterplots (with or without a trendline) allow us to visualize the relationship between two numerical variables.\nWe’ll import the pyplot module of matplotlib to make plots. We’ll use the plot() function to make the scatter plot, and the functions xlabel() and ylabel() for labeling the plot axes.\n\nimport matplotlib.pyplot as plt\n\nQ: Make a scatterplot of Life expectancy vs GDP per capita.\nThere are two ways of plotting the figure:\n\nExplicitly creating figures and axes, and call methods on them (object-oriented style).\nLetting pyplot implicitly track the plot that it wants to reference. Simple functions are used to add plot elements (lines, images, text, etc.) to the current axes in the current figure (pyplot-style).\n\nWe’ll plot the figure in both ways.\n\n#Method 1: Object-oriented style\nfig, ax = plt.subplots() #Create a figure and an axes\nx = gdp_data.gdpPercap \ny = gdp_data.lifeExp\nax.plot(x,y,'o')   #Plot data on the axes\nax.set_xlabel('GDP per capita')    #Add an x-label to the axes\nax.set_ylabel('Life expectancy')   #Add a y-label to the axes\nax.set_title('Life expectancy vs GDP per capita from 1952 to 2007')\n\nText(0.5, 1.0, 'Life expectancy vs GDP per capita from 1952 to 2007')\n\n\n\n\n\n\n#Method 2: pyplot style\nx = gdp_data.gdpPercap\ny = gdp_data.lifeExp\nplt.plot(x,y,'o') #By default, the plot() function makes a lineplot. The 'o' arguments specifies a scatterplot\nplt.xlabel('GDP per capita')  #Labelling the horizontal X-axis\nplt.ylabel('Life expectancy') #Labelling the verical Y-axis\nplt.title('Life expectancy vs GDP per capita from 1952 to 2007')\n\nText(0.5, 1.0, 'Life expectancy vs GDP per capita from 1952 to 2007')\n\n\n\n\n\nBoth the plotting styles - object-oriented style and the pyplot style are perfectly valid and have their pros and cons.\n\nPyplot style is easier for simple plots\nObject-oriented style is slightly more complicated but more powerful as it allows for greater control over the axes in figure. This proves to be quite useful when we are dealing with a figure with multiple axes.\n\nFrom the above plot, we observe that life expectancy seems to be positively correlated with the GDP per capita of the country, as one may expect. However, there are a few outliers in the data - which are countries having extremely high GDP per capita, but not a correspondingly high life expectancy.\nSometimes it is difficult to get an idea of the overall trend (positive or negative correlation). In such cases, it may help to add a trendline to the scatter plot. In the plot below we add a trendline over the scatterplot showing that the life expectancy on an average increases with increasing GDP per capita. The trendline is actually a linear regression of life expectancy on GDP per capita. However, we’ll not discuss linear regression in this book.\nQ: Add a trendline over the scatterplot of life expectancy vs GDP per capita.\n\n#Making a scatterplot of Life expectancy vs GDP per capita\nx = gdp_data.gdpPercap\ny = gdp_data.lifeExp\nplt.plot(x,y,'o') #By default, the plot() function makes a lineplot. The 'o' arguments specifies a scatterplot\nplt.xlabel('GDP per capita')  #Labelling the horizontal X-axis\nplt.ylabel('Life expectancy') #Labelling the verical Y-axis\n\n#Plotting a trendline (linear regression) on the scatterplot\nslope_intercept_trendline = np.polyfit(x,y,1)   #Finding the slope and intercept for the trendline\ncompute_y_given_x = np.poly1d(slope_intercept_trendline) #Defining a function that computes the 'y' given 'x' for the trendline\nplt.plot(x,compute_y_given_x(x)) #Plotting the trendline\n\n\n\n\nThe above plot shows that our earlier intuition of a postive correlation between Life expectancy and GDP per capita was correct.\nWe used the NumPy function polyfit() to compute the slope and intercept of the trendline. Then, we defined an object compute_y_given_x of poly1d class and used it to compute the trendline.\n\n\n6.1.3 Subplots\nThere is often a need to make a few plots together to compare them. See the example below.\nQ: Make scatterplots of life expectancy vs GDP per capita separately for each of the 4 continents of Asia, Europe, Africa and America. Arrange the plots in a 2 x 2 grid.\n\n#Defining a 2x2 grid of subplots\nfig, axes = plt.subplots(2,2,figsize=(16,10))\nplt.subplots_adjust(wspace=0.2) #adjusting white space between individual plots\n\n#Making a scatterplot of Life expectancy vs GDP per capita for each continent\ncontinents = np.array([['Asia', 'Europe'], ['Africa', 'Americas']])\n\n#Looping over the 2x2 grid\nfor i in range(2):\n    for j in range(2):\n        \n        #Getting the GDP per capita and life expectancy of the countries of the (i,j)th continent\n        x = gdp_data.loc[gdp_data.continent==continents[i,j],:].gdpPercap\n        y = gdp_data.loc[gdp_data.continent==continents[i,j],:].lifeExp\n        \n        #Making the scatterplot\n        axes[i,j].plot(x,y,'o') \n        \n        #Setting limits on the 'x' and 'y' axes\n        axes[i,j].set_xlim([gdp_data.gdpPercap.min(), gdp_data.gdpPercap.max()])\n        axes[i,j].set_ylim([gdp_data.lifeExp.min(), gdp_data.lifeExp.max()])\n        \n        #Labelling the 'x' and 'y' axes\n        axes[i,j].set_xlabel('GDP per capita for '+ continents[i,j],fontsize = 14)\n        axes[i,j].set_ylabel('Life expectancy for '+ continents[i,j],fontsize = 14)\n        \n        #Putting a dollar sign, and thousand-comma separator on x-axis labels\n        axes[i,j].xaxis.set_major_formatter('${x:,.0f}')\n        \n        #Increasing font size of axis labels\n        axes[i,j].tick_params(axis = 'both',labelsize=14)\n\n\n\n\nWe observe that for each continent, except Africa, initially life expectancy increases rapidly with increasing GDP per capita. However, after a certain threshold of GDP per capita, life expectancy increases slowly. Several countries in Europe enjoy a relatively high GDP per capita as well as high life expectancy. Some countries in Asia have an extremely high GDP per capita, but a relatively low life expectancy. It will be interesting to see the proportion of GDP associated with healthcare for these outlying Asian countries, and European countries.\nWe used the subplot function of matplotlib to define the 2x2 grid of subplots. The function subplots_adjust() can be used to adjust white spaces around the plot. We used a for loop to iterate over each subplot. The axes object returned by the subplot() function was used to refer to individual subplots.\n\n\n6.1.4 Practice problem 1\nIs NU_GPA associated with parties_per_month? Analyze the association separately for Sophomores, Juniors, and Seniors (categories of the variable school_year).\nMake scatterplots of NU_GPA vs parties_per_month in a 1 x 3 grid, where each grid is for a distinct school_year. Plot the trendline as well for each scatterplot. Use the file survey_data_clean.csv.\nSolution:\n\nsurvey_data = pd.read_csv('./Datasets/survey_data_clean.csv')\n\ndef NU_GPA_vs_parties_per_month(data):\n    fig, axes = plt.subplots(1,3,figsize=(15,5))\n    plt.subplots_adjust(wspace=0.4) \n    \n    school_years = np.array(['Sophomore', 'Junior','Senior'])\n    for i in range(3):\n        x = data.loc[data.school_year==school_years[i],:].parties_per_month\n        y = data.loc[data.school_year==school_years[i],:].NU_GPA\n        \n        #The data has missing values. We can draw a trendline using only the non-missing value-pairs of NU_GPA and parties_per_month\n        #`idx_non_missing` will have the indices of the non-missing value-pairs of NU_GPA and parties_per_month\n        idx_non_missing = np.isfinite(x) & np.isfinite(y)\n        \n        axes[i].plot(x,y,'o',label = school_years[i]) \n        axes[i].set_xlim([data.parties_per_month.min(), data.parties_per_month.max()])\n        axes[i].set_ylim([data.NU_GPA.min(), data.NU_GPA.max()])\n        axes[i].set_xlabel('Parties per month',fontsize = 14)  \n        axes[i].set_ylabel('NU GPA',fontsize = 14) \n        axes[i].set_title(school_years[i],fontsize = 15)\n        slope_intercept_trendline = np.polyfit(x[idx_non_missing],y[idx_non_missing],1)   #Finding the slope and intercept for the trendline\n        compute_y_given_x = np.poly1d(slope_intercept_trendline) #Defining a function that computes the 'y' given 'x' for the trendline\n        axes[i].plot(x,compute_y_given_x(x)) #Plotting the trendline\n\nNU_GPA_vs_parties_per_month(survey_data)\n\n\n\n\nNote that the trendline in the above plots seems to be influenced by a few points having extreme values of parties_per_month. These points have a high leverage (a concept we’ll learn in a future course on linear regression) in influencing the trendline. So, we should visualize the trend by removing or capping these high-leverage points, to avoid the distortion of the trend by a few points.\nLet us cap the the values of parties_per_month to 30, and make the visualizations again.\n\nsurvey_data_parties_capped = survey_data.copy()\nsurvey_data_parties_capped.parties_per_month = survey_data.parties_per_month.apply(lambda x: min(30,x))\nNU_GPA_vs_parties_per_month(survey_data_parties_capped)\n\n\n\n\nWe see that the trend didn’t change much after removing the high leverage points. (Note that although the high leverage points have the leverage to influence the trendline, they need not necessarily influence it). From the visualization, NU_GPA doesn’t seem to be associated with parties_per_month for students of any of the school years.\n\n\n6.1.5 Overlapping plots with legend\nWe can also have the scatterplot of all the continents on the sample plot, with a distinct color for each continent. A legend will be required to identify the continent’s color.\n\ncontinents = np.array([['Asia', 'Europe'], ['Africa', 'Americas']])\nplt.rcParams[\"figure.figsize\"] = (9,6)\nfor i in range(2):\n    for j in range(2):\n        x = gdp_data.loc[gdp_data.continent==continents[i,j],:].gdpPercap\n        y = gdp_data.loc[gdp_data.continent==continents[i,j],:].lifeExp\n        plt.plot(x,y,'o',label = continents[i,j]) \n        plt.xlim([gdp_data.gdpPercap.min(), gdp_data.gdpPercap.max()])\n        plt.ylim([gdp_data.lifeExp.min(), gdp_data.lifeExp.max()])\n        plt.xlabel('GDP per capita')  \n        plt.ylabel('Life expectancy') \nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x2320d6d70a0&gt;\n\n\n\n\n\nNote that a disadvantage of the above plot is overplotting. The data points corresponding to the Americas are hiding the data points of other continents. However, if the data points corresponding to different categories are spread apart, then it may be convenient to visualize all the categories on the same plot."
  },
  {
    "objectID": "Data visualization.html#pandas",
    "href": "Data visualization.html#pandas",
    "title": "6  Data visualization",
    "section": "6.2 Pandas",
    "text": "6.2 Pandas\nMatplotlib is a low-level tool, in which different components of the plot, such as points, legend, axis titles, etc. need to be specified separately. The Pandas plot() function can be used directly with a DataFrame or Series to make plots.\n\n6.2.1 Scatterplots with Pandas\n\n#Plotting life expectancy vs GDP per capita using the Pandas plot() function\nax = gdp_data.plot(x = 'gdpPercap', y = 'lifeExp', kind = 'scatter',figsize=(10, 6),xlabel = 'GDP per capita', \n              ylabel = 'Life expectancy')\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n\n\n\n\n\nIn the above plot, note that:\n\nWith matplotlib, it will take 3 lines to make the same plot - one for the scatterplot, and two for the axis titles.\nThe object ax is of type matplotlib.axes._subplots.AxesSubplot (check the code below). This means we can use the attributes and methods associated with the axes object of Matplotlib. If you see the documentation of the Pandas plot() function, you will find that under the kwargs** argument, you have Options to pass to matplotlib plotting method. Thus, you get the convenience of using the Pandas plot() function, while also having the attributes and methods associated with Matplotlib.\n\n\ntype(ax)\n\nmatplotlib.axes._subplots.AxesSubplot\n\n\n\n\n6.2.2 Lineplots with Pandas\nPurpose of lineplots: Lineplots show the relationship between two numerical variables when the variable on the x-axis, also called the explanatory variable, is of a sequential nature; in other words there is an inherent ordering to the variable. The most common example of lineplots have some notion of time on the x-axis (or the horizontal axis): hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Lineplots that have some notion of time on the x-axis are also called time series plots. Lineplots should be avoided when there is not a clear sequential ordering to the variable on the x-axis.\nLet us re-arrange the data to show other benefits of the Pandas plot() function. Note that data resphaping is explained in Chapter 8 of the book, so you may ignore the code block below that uses the pivot_table() function.\n\n#You may ignore this code block until Chapter 8.\nmean_gdp_per_capita = gdp_data.pivot_table(index = 'year', columns = 'continent',values = 'gdpPercap')\nmean_gdp_per_capita.head()\n\n\n\n\n\n\n\ncontinent\nAfrica\nAmericas\nAsia\nEurope\nOceania\n\n\nyear\n\n\n\n\n\n\n\n\n\n1952\n1252.572466\n4079.062552\n5195.484004\n5661.057435\n10298.085650\n\n\n1957\n1385.236062\n4616.043733\n5787.732940\n6963.012816\n11598.522455\n\n\n1962\n1598.078825\n4901.541870\n5729.369625\n8365.486814\n12696.452430\n\n\n1967\n2050.363801\n5668.253496\n5971.173374\n10143.823757\n14495.021790\n\n\n1972\n2339.615674\n6491.334139\n8187.468699\n12479.575246\n16417.333380\n\n\n\n\n\n\n\nWe have reshaped the data to obtain the mean GDP per capita of each continent for each year.\nThe pandas plot() function can be directly used with this DataFrame to create line plots showing mean GDP per capita of each continent with year.\n\nax = mean_gdp_per_capita.plot(ylabel = 'GDP per capita',figsize = (10,6),marker='o')\nax.yaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nWe observe that the mean GDP per capita of of Europe and Oceania have increased rapidly, while that for Africa is increasing very slowly.\nThe above plot will take several lines of code if developed using only matplotlib. The pandas plot() function has a framework to conveniently make commonly used plots.\nNote that argument marker = ‘o’ puts a solid circle at each of the data points.\n\n\n6.2.3 Bar plots with Pandas\nPurpose of bar plots: Barplots are used to visualize any aggregate statistics of a continuous variable with respect to the categories or levels of a categorical variable. For example, we may visualize the average IMDB rating (aggregate statistics) of movies based on their genre (the categorical variable).\nBar plots can be made using the pandas bar function with the DataFrame or Series, just like the line plots and scatterplots.\nBelow, we are reading the dataset of noise complaints of type Loud music/Party received the police in New York City in 2016.\n\nnyc_party_complaints = pd.read_csv('./Datasets/party_nyc.csv')\nnyc_party_complaints.head()\n\n\n\n\n\n\n\n\nCreated Date\nClosed Date\nLocation Type\nIncident Zip\nCity\nBorough\nLatitude\nLongitude\nHour_of_the_day\nMonth_of_the_year\n\n\n\n\n0\n12/31/2015 0:01\n12/31/2015 3:48\nStore/Commercial\n10034.0\nNEW YORK\nMANHATTAN\n40.866183\n-73.918930\n0\n12\n\n\n1\n12/31/2015 0:02\n12/31/2015 4:36\nStore/Commercial\n10040.0\nNEW YORK\nMANHATTAN\n40.859324\n-73.931237\n0\n12\n\n\n2\n12/31/2015 0:03\n12/31/2015 0:40\nResidential Building/House\n10026.0\nNEW YORK\nMANHATTAN\n40.799415\n-73.953371\n0\n12\n\n\n3\n12/31/2015 0:03\n12/31/2015 1:53\nResidential Building/House\n11231.0\nBROOKLYN\nBROOKLYN\n40.678285\n-73.994668\n0\n12\n\n\n4\n12/31/2015 0:05\n12/31/2015 3:49\nResidential Building/House\n10033.0\nNEW YORK\nMANHATTAN\n40.850304\n-73.938516\n0\n12\n\n\n\n\n\n\n\nLet us visualise the locations from where the the complaints are coming.\n\n#Using the pandas function bar() to create bar plot\nax = nyc_party_complaints['Location Type'].value_counts().plot.bar(ylabel = 'Number of complaints')\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\nFrom the above plot, we observe that most of the complaints come from residential buildings and houses, as one may expect.\nLet is visualize the time of the year when most complaints occur.\n\n#Using the pandas function bar() to create bar plot\nax = nyc_party_complaints['Month_of_the_year'].value_counts().sort_index().plot.bar(ylabel = 'Number of complaints',\n                                                                              xlabel = \"Month\")\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\nTry executing the code without sort_index() to figure out the purpose of using the function.\nFrom the above plot, we observe that most of the complaints occur during summer and early Fall.\nLet us create a stacked bar chart that combines both the above plots into a single plot. You may ignore the code used for re-shaping the data until Chapter 8. The purpose here is to show the utility of the pandas bar() function.\n\n#Reshaping the data to make it suitable for a stacked barplot - ignore this code until chapter 8\ncomplaints_location=pd.crosstab(nyc_party_complaints.Month_of_the_year, nyc_party_complaints['Location Type'])\ncomplaints_location.head()\n\n\n\n\n\n\n\nLocation Type\nClub/Bar/Restaurant\nHouse of Worship\nPark/Playground\nResidential Building/House\nStore/Commercial\nStreet/Sidewalk\n\n\nMonth_of_the_year\n\n\n\n\n\n\n\n\n\n\n1\n748\n24\n17\n9393\n1157\n832\n\n\n2\n570\n29\n16\n8383\n1197\n782\n\n\n3\n747\n39\n90\n9689\n1480\n1835\n\n\n4\n848\n53\n129\n11984\n1761\n2943\n\n\n5\n2091\n72\n322\n15676\n1941\n5090\n\n\n\n\n\n\n\n\n#Stacked bar plot showing number of complaints at different months of the year, and from different locations\nax = complaints_location.plot.bar(stacked=True,ylabel = 'Number of complaints',figsize=(15, 10), xlabel = 'Month')\nax.tick_params(axis = 'both',labelsize=15)\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\nFigure 6.3: Stacked bar plot with Pandas\n\n\n\n\nThe above plots gives the insights about location and day of the year simultaneously that were previously separately obtained by the individual plots.\nAn alternative to stacked barplots are side-by-side barplots, as shown below.\n\n#Side-by-side bar plot showing number of complaints at different months of the year, and from different locations\nax = complaints_location.plot.bar(ylabel = 'Number of complaints',figsize=(15, 10), xlabel = 'Month')\nax.tick_params(axis = 'both',labelsize=15)\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\n\\(\\color{red}{\\text{Q1}}\\) In which scenarios should we use a stacked barplot instead of a side-by-side barplot and vice-versa?"
  },
  {
    "objectID": "Data visualization.html#seaborn",
    "href": "Data visualization.html#seaborn",
    "title": "6  Data visualization",
    "section": "6.3 Seaborn",
    "text": "6.3 Seaborn\nSeaborn offers the flexibility of simultaneously visualizing multiple variables in a single plot, and offers several themes to develop plots.\n\n#Importing the seaborn library\nimport seaborn as sns\n\n\n6.3.1 Bar plots with confidence intervals with Seaborn\nWe’ll group the data to obtain the total complaints for each Location Type, Borough, Month_of_the_year, and Hour_of_the_day. Note that you’ll learn grouping data in Chapter 9, so you may ignore the next code block. The grouping is done to shape the data in a suitable form for visualization.\n\n#Grouping the data to make it suitable for visualization using Seaborn. Ignore this code block until learn chapter 9.\nnyc_complaints_grouped = nyc_party_complaints[['Location Type','Borough','Month_of_the_year','Latitude','Hour_of_the_day']].groupby(['Location Type','Borough','Month_of_the_year','Hour_of_the_day'])['Latitude'].agg([('complaints','count')]).reset_index()\nnyc_complaints_grouped.head()\n\n\n\n\n\n\n\n\nLocation Type\nBorough\nMonth_of_the_year\nHour_of_the_day\ncomplaints\n\n\n\n\n0\nClub/Bar/Restaurant\nBRONX\n1\n0\n10\n\n\n1\nClub/Bar/Restaurant\nBRONX\n1\n1\n10\n\n\n2\nClub/Bar/Restaurant\nBRONX\n1\n2\n6\n\n\n3\nClub/Bar/Restaurant\nBRONX\n1\n3\n6\n\n\n4\nClub/Bar/Restaurant\nBRONX\n1\n4\n3\n\n\n\n\n\n\n\nLet us create a bar plot visualizing the average number of complaints with the time of the day.\n\nax = sns.barplot(x=\"Hour_of_the_day\", y = 'complaints',  data=nyc_complaints_grouped)\nax.figure.set_figwidth(15)\n\n\n\n\nFrom the above plot, we observe that most of the complaints are made around midnight. However, interestingly, there are some complaints at each hour of the day.\nNote that the above barplot shows the mean number of complaints in a month at each hour of the day. The black lines are the 95% confidence intervals of the mean number of complaints.\n\n\n6.3.2 Facetgrid: Multi-plot grid for plotting conditional relationships\nWith pandas, we simultaneously visualized the number of complaints with month of the year and location type in Figure 6.3. We’ll use Seaborn to add another variable - Borough to the visualization.\nQ: Visualize the mean number of complaints with Month_of_the_year, Location Type, and Borough.\nThe seaborn class FacetGrid is used to design the plot, i.e., specify the way the data will be divided in mutually exclusive subsets for visualization. Then the [map] function of the FacetGrid class is used to apply a plotting function to each subset of the data.\n\n#Visualizing the number of complaints with Month_of_the_year, Location Type, and Borough.\na = sns.FacetGrid(nyc_complaints_grouped, hue = 'Location Type', col = 'Borough',col_wrap=3,height=3.5,aspect = 1)\na.map(sns.lineplot,'Month_of_the_year','complaints')\na.set_axis_labels(\"Month of the year\", \"Complaints\")\na.add_legend()\n\n\n\n\nFrom the above plot, we get a couple of interesting insights: 1. For Queens and Staten Island, most of the complaints occur in summer, for Manhattan and Bronx it is mostly during late spring, while Brooklyn has a spike of complaints in early Fall. 2. In most of the Boroughs, the majority complaints always occur in residential areas. However, for Manhattan, the number of street/sidewalk complaints in the summer are comparable to those from residential areas.\nWe have visualized 4 variables simultaneously in the above plot.\nLet us consider another example, where we will visualize the weather in a few cities of Australia. The file Australia_weather.csv consists of weather details of Sydney, Canberra, and Melbourne from 2007 to 2017.\n\naussie_weather = pd.read_csv('./Datasets/Australia_weather.csv')\naussie_weather.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRISK_MM\nRainTomorrow\n\n\n\n\n0\n10/20/2010\nSydney\n12.9\n20.3\n0.2\n3.0\n10.9\nENE\n37\nW\n...\n57\n1028.8\n1025.6\n3\n1\n16.9\n19.8\nNo\n0.0\nNo\n\n\n1\n10/21/2010\nSydney\n13.3\n21.5\n0.0\n6.6\n11.0\nENE\n41\nW\n...\n58\n1025.9\n1022.4\n2\n5\n17.6\n21.3\nNo\n0.0\nNo\n\n\n2\n10/22/2010\nSydney\n15.3\n23.0\n0.0\n5.6\n11.0\nNNE\n41\nW\n...\n63\n1021.4\n1017.8\n1\n4\n19.0\n22.2\nNo\n0.0\nNo\n\n\n3\n10/26/2010\nSydney\n12.9\n26.7\n0.2\n3.8\n12.1\nNE\n33\nW\n...\n56\n1018.0\n1015.0\n1\n5\n17.8\n22.5\nNo\n0.0\nNo\n\n\n4\n10/27/2010\nSydney\n14.8\n23.8\n0.0\n6.8\n9.6\nSSE\n54\nSSE\n...\n69\n1016.0\n1014.7\n2\n7\n20.2\n20.6\nNo\n1.8\nYes\n\n\n\n\n5 rows × 24 columns\n\n\n\n\naussie_weather.shape\n\n(4666, 24)\n\n\nQ: Visualize if it rains the next day (RainTomorrow) given whether it has rained today (RainToday), the current day’s humidity (Humidity9am), maximum temperature (MaxTemp) and the city (Location).\n\na = sns.FacetGrid(aussie_weather,col='Location',row='RainToday',height = 4,aspect = 1,hue = 'RainTomorrow')\na.map(plt.scatter,'MaxTemp','Humidity9am')\na.set_axis_labels(\"Maximum temperature\", \"Humidity at 9 am\")\na.set_titles(col_template=\"{col_name}\", row_template=\"Rain today: {row_name}\")\na.add_legend()\n\n\n\n\nHumidity tends to be higher when it is going to rain the next day. However, the correlation is much more pronounced for Syndey. In case it is not raining on the current day, humidity seems to be slightly negatively correlated with temperature.\n\n\n6.3.3 Practice exercise 2\nHow does the expected marriage age of the people of STAT303-1 depend on their characteristics? We’ll use visualizations to answer this question. Use data from the file survey_data_clean.csv. Proceed as follows:\n\nMake a visualization that compares the mean expected_marriage_age of introverts and extroverts (use the variable introvert_extrovert). What insights do you obtain?\n\n\nplt.rcParams[\"figure.figsize\"] = (9,6)\nsns.barplot(x = 'introvert_extrovert' ,y = 'expected_marriage_age', data = survey_data)\nplt.xlabel('Personality', fontsize=16);\nplt.ylabel('Expected marriage age', fontsize=16);\nplt.xticks(fontsize=15);\nplt.yticks(fontsize=15);\n\n\n\n\nThe mean expected marriage age for introverts is about 2 years higher than that for extroverts. Also, there is a higher variation in the expected marriage age of introverts as compared to extroverts.\n\nDoes the mean expected_marriage_age of introverts and extroverts depend on whether they believe in love in first sight (variable name: love_first_sight)? Update the previous visualization to answer the question.\n\n\nsns.set(font_scale=1.25)\na = sns.FacetGrid(survey_data,col='love_first_sight',height = 5,aspect = 1)\na.map(sns.barplot, 'introvert_extrovert' , 'expected_marriage_age' ,order = ['Introvert', 'Extrovert'])\na.set_axis_labels(\"Personality\", \"Expected marriage age\")\na.add_legend()\n\n\n\n\nYes, only those introverts who do not believe in love in first sight have a higher mean value of expected marriage age.\n\nIn addition to love_first_sight, does the mean expected_marriage_age of introverts and extroverts depend on whether they are a procrastinator (variable name: procrastinator)? Update the previous visualization to answer the question.\n\n\nsns.set(font_scale=1.25)\na = sns.FacetGrid(survey_data,col='love_first_sight',height = 5,aspect = 1, row = 'procrastinator')\na.map(sns.barplot, 'introvert_extrovert' , 'expected_marriage_age' ,order = ['Introvert', 'Extrovert'])\na.set_axis_labels(\"Personality\", \"Expected marriage age\")\na.add_legend()\n\n\n\n\nProcrastination does not seem to make much of a difference in the expected marriage age. The mean expected marriage age of procrastinating introverts seems to be only a little higher than the non-procrastinating introverts.\n\nIs there any critical information missing in the above visualizations that, if revealed, may cast doubts on the patterns observed in them?\n\nYes, we don’t know the number of observations corresponding to each bar of the bar plots. If there are a very few observations in any of the categories, then the trend shown by that category may not be reliable. For example, in the data (survey_data), there are only 8 introverts who are not procrastinators and believe in love in first sight, while there are 52 introverts who are procrastinators and do not believe in love in first sight.\nIf there are more introverts in the class of STAT303-1 who are not procrastinators and believe in love at first sight (may be they didn’t fill the survey), then they are under-represented in the sample of people who filled the survey, and the trend observed for them may be less reliable than that for other people.\n\n#Code for finding the number of people in each category - you will understand this code later in chapter 9 on data aggregation\nsurvey_data[['introvert_extrovert','love_first_sight','procrastinator','Timestamp']].groupby(['introvert_extrovert',\n                                                                                  'love_first_sight','procrastinator']).count()\n\n\n\n\n\n\n\n\n\n\nTimestamp\n\n\nintrovert_extrovert\nlove_first_sight\nprocrastinator\n\n\n\n\n\nExtrovert\n0\n0\n19\n\n\n1\n35\n\n\n1\n0\n10\n\n\n1\n15\n\n\nIntrovert\n0\n0\n32\n\n\n1\n52\n\n\n1\n0\n8\n\n\n1\n21\n\n\n\n\n\n\n\n\n\n6.3.4 Histogram and density plots with Seaborn\nPurpose: Histogram and density plots visualize the distribution of a continuous variable.\nA histogram plots the number of observations occurring within discrete, evenly spaced bins of a random variable, to visualize the distribution of the variable. It may be considered a special case of a bar plot as bars are used to plot the observation counts.\nA density plot uses a kernel density estimate to approximate the distribution of random variable.\nWe can use the Seaborn displot() function to make both kinds of plots - histogram or density plot.\nExample: Make a histogram showing the distributions of maximum temperature in Sydney, Canberra and Melbourne.\n\nsns.set(font_scale = 1.4)\na = sns.displot(data = aussie_weather, x = 'MaxTemp',kind = 'hist',col='Location')\na.set_axis_labels(\"Maximum temperature\", \"Count\")\na.set_titles(\"{col_name}\")\n\n\n\n\nFrom the above plot, we observe that: 1. Melbourne has a right skewed distribution with the median temperature being smaller than the mean. 2. Canberra seems to have the highest variation in the temperature.\nExample: Make a density plot showing the distributions of maximum temperature in Sydney, Canberra and Melbourne.\n\nsns.displot(data = aussie_weather, x = 'MaxTemp',kind = 'kde', col = 'Location')\n\n\n\n\nExample: Show the distributions of the maximum and minimum temperatures in a single plot.\n\nsns.histplot(data=aussie_weather, x=\"MaxTemp\", color=\"skyblue\", label=\"MaxTemp\", kde=True)\nsns.histplot(data=aussie_weather, x=\"MinTemp\", color=\"red\", label=\"MinTemp\", kde=True)\nplt.legend()\nplt.xlabel('Temperature')\n\nText(0.5, 0, 'Temperature')\n\n\n\n\n\nThe Seaborn function histplot() can be used to make a density plot overlapping on a histogram.\n\n\n6.3.5 Boxplots with Seaborn\nPurpose: Boxplots is a standardized way of visualizing the distribution of a continuous variable. They show five key metrics that describe the data distribution - median, 25th percentile value, 75th percentile value, minimum and maximum, as shown in the figure below. Note that the minimum and maximum exclude the outliers.\n\n\n\n\n\nExample: Make a boxplot comparing the distributions of maximum temperatures of Sydney, Canberra and Melbourne, given whether or not it has rained on the day.\n\nsns.boxplot(data = aussie_weather,x = 'Location', y = 'MaxTemp',hue = 'RainToday')\n\n&lt;AxesSubplot:xlabel='Location', ylabel='MaxTemp'&gt;\n\n\n\n\n\nFrom the above plot, we observe that: 1. The maximum temperature of the day, on an average, is lower if it rained on the day. 2. Sydney and Melbourne have some extremely high outlying values of maximum temperature.\nWe have used the Seaborn boxplot() function for the above plot.\n\n\n6.3.6 Scatterplots with Seaborn\nWe made scatterplots with Matplotlib and Pandas earlier. With Seaborn, the regplot() function allows us to plot a trendline over the scatterplot, along with a 95% confidence interval for the trendline. Note that this is much easier than making a trendline with Matplotlib.\n\n#Scatterplot and trendline with seaborn\nplt.rcParams[\"figure.figsize\"] = (9,6)\nsns.set(font_scale=1.5)\nax=sns.regplot(x = 'gdpPercap', y = 'lifeExp', data = gdp_data,scatter_kws={\"color\": \"orange\"}, line_kws={\"color\": \"blue\"})\nax.xaxis.set_major_formatter('${x:,.0f}')\nax.set_xlabel('GDP per capita')\nax.set_ylabel('Life expectancy')\n\nText(0, 0.5, 'Life expectancy')\n\n\n\n\n\nNote that the confidence interval of the trendline broadens as we move farther away from most of the data points. In other words, there is more uncertainty about the trend as we move to a domain space farther away from the data.\n\n\n6.3.7 Heatmaps with Seaborn\nPurpose: Heatmaps help us visualize the correlation between all variable-pairs.\nBelow is a heatmap visualizing the pairwise correlation of all the numerical variables of survey_data_clean. With a heatmap it becomes easier to see strongly correlated variables.\n\nsns.set(rc={'figure.figsize':(12,10)})\nsns.heatmap(survey_data.corr())\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nFrom the above map, we can see that:\n\nstudent athlete is strongly postively correlated with minutes_ex_per_week\nprocrastinator is strongly negatively correlated with NU_GPA\n\n\n\n6.3.8 Pairplots with Seaborn\nPurpose: Pairplots are used to visualize the association between all variable-pairs in the data. In other words, pairplots simultaneously visualize the scatterplots between all variable-pairs.\nLet us visualize the pair-wise association of nutrition variables in the starbucks drinks data.\n\nstarbucks_drinks = pd.read_csv('./Datasets/starbucks-menu-nutrition-drinks.csv')\nsns.pairplot(starbucks_drinks)\n\n\n\n\nIn the above pairplot, note that:\n\nThe histograms on the diagonal of the grid show the distribution of each of the variables.\nInstead of a histogram, we can visualize the density plot with the argument kde = True.\nThe scatterplots in the rest of the grid are the pair-wise plots of all the variables.\n\nFrom the above plot, we observe that:\n\nAlmost all the variable pairs have a positive correlation, i.e., if one of the nutrients increase in a drink, others also are likely to increase.\nThe number of calories seem to be strongly positively correlated with the amount of carbs in the drink.\nFrom the density plots we can see that there is a lot of choice for consumers to buy a drink that has a zero value for any of the nutrients - fat, protein, fiber, or sodium."
  },
  {
    "objectID": "Data cleaning and preparation.html#handling-missing-data",
    "href": "Data cleaning and preparation.html#handling-missing-data",
    "title": "7  Data cleaning and preparation",
    "section": "7.1 Handling missing data",
    "text": "7.1 Handling missing data\nMissing values in a dataset can occur due to several reasons such as breakdown of measuring equipment, accidental removal of observations, lack of response by respondents, error on the part of the researcher, etc.\nLet us read the dataset GDP_missing_data.csv, in which we have randomly removed some values, or put missing values in some of the columns.\nWe’ll also read GDP_complete_data.csv, in which we have not removed any values. We’ll use this data later to assess the accuracy of our guess or estimate of missing values in GDP_missing_data.csv.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn as sk\nimport seaborn as sns\ngdp_missing_values_data = pd.read_csv('./Datasets/GDP_missing_data.csv')\ngdp_complete_data = pd.read_csv('./Datasets/GDP_complete_data.csv')\n\n\ngdp_missing_values_data.head()\n\n\n\n\n\n\n\n\neconomicActivityFemale\ncountry\nlifeMale\ninfantMortality\ngdpPerCapita\neconomicActivityMale\nilliteracyMale\nilliteracyFemale\nlifeFemale\ngeographic_location\ncontraception\ncontinent\n\n\n\n\n0\n7.2\nAfghanistan\n45.0\n154.0\n2474.0\n87.5\nNaN\n85.0\n46.0\nSouthern Asia\nNaN\nAsia\n\n\n1\n7.8\nAlgeria\n67.5\n44.0\n11433.0\n76.4\n26.1\n51.0\n70.3\nNorthern Africa\nNaN\nAfrica\n\n\n2\n41.3\nArgentina\n69.6\n22.0\nNaN\n76.2\n3.8\n3.8\n76.8\nSouth America\nNaN\nSouth America\n\n\n3\n52.0\nArmenia\n67.2\n25.0\n13638.0\n65.0\nNaN\n0.5\n74.0\nWestern Asia\nNaN\nAsia\n\n\n4\n53.8\nAustralia\nNaN\n6.0\n54891.0\nNaN\n1.0\n1.0\n81.2\nOceania\nNaN\nOceania\n\n\n\n\n\n\n\nObserve that the gdp_missing_values_data dataset consists of some missing values shown as NaN (Not a Number).\n\n7.1.1 Identifying missing values (isnull())\nMissing values in a Pandas DataFrame can be identified with the isnull() method. The Pandas Series object also consists of the isnull() method. For finding the number of missing values in each column of gdp_missing_values_data, we will sum up the missing values in each column of the dataset:\n\ngdp_missing_values_data.isnull().sum()\n\neconomicActivityFemale    10\ncountry                    0\nlifeMale                  10\ninfantMortality           10\ngdpPerCapita              10\neconomicActivityMale      10\nilliteracyMale            10\nilliteracyFemale          10\nlifeFemale                10\ngeographic_location        0\ncontraception             71\ncontinent                  0\ndtype: int64\n\n\nNote that the descriptive statistics methods associated with Pandas objects ignore missing values by default. Consider the summary statistics of gdp_missing_values_data:\n\ngdp_missing_values_data.describe()\n\n\n\n\n\n\n\n\neconomicActivityFemale\nlifeMale\ninfantMortality\ngdpPerCapita\neconomicActivityMale\nilliteracyMale\nilliteracyFemale\nlifeFemale\ncontraception\n\n\n\n\ncount\n145.000000\n145.000000\n145.000000\n145.000000\n145.000000\n145.000000\n145.000000\n145.000000\n84.000000\n\n\nmean\n45.935172\n65.491724\n37.158621\n24193.482759\n76.563448\n13.570028\n21.448897\n70.615862\n51.773810\n\n\nstd\n16.875922\n9.099256\n34.465699\n22748.764444\n7.854730\n16.497954\n25.497045\n9.923791\n31.930026\n\n\nmin\n1.900000\n36.000000\n3.000000\n772.000000\n51.200000\n0.000000\n0.000000\n39.100000\n0.000000\n\n\n25%\n35.500000\n62.900000\n10.000000\n6837.000000\n72.000000\n1.000000\n2.300000\n67.500000\n17.000000\n\n\n50%\n47.600000\n67.800000\n24.000000\n15184.000000\n77.300000\n6.600000\n9.720000\n73.900000\n65.000000\n\n\n75%\n55.900000\n72.400000\n54.000000\n35957.000000\n81.600000\n19.500000\n30.200000\n78.100000\n77.000000\n\n\nmax\n90.600000\n77.400000\n169.000000\n122740.000000\n93.000000\n70.500000\n90.800000\n82.900000\n79.000000\n\n\n\n\n\n\n\nObserve that the count statistics report the number of non-missing values of each column in the data, as the number of rows in the data (see code below) is more than the number of non-missing values of all the variables in the above table. Similarly, for the rest of the statistics, such as mean, std, etc., the missing values are ignored.\n\n#The dataset gdp_missing_values_data has 155 rows\ngdp_missing_values_data.shape[0]\n\n155\n\n\n\n\n7.1.2 Types of missing values\nNow that we know how to identify missing values in the dataset, let us learn about the types of missing values that can be there. Rubin (1976) classified missing values in three categories.\n\n7.1.2.1 Missing Completely at Random (MCAR)\nIf the probability of being missing is the same for all cases, then the data are said to be missing completely at random. An example of MCAR is a weighing scale that ran out of batteries. Some of the data will be missing simply because of bad luck.\n\n\n7.1.2.2 Missing at Random (MAR)\nIf the probability of being missing is the same only within groups defined by the observed data, then the data are missing at random (MAR). MAR is a much broader class than MCAR. For example, when placed on a soft surface, a weighing scale may produce more missing values than when placed on a hard surface. Such data are thus not MCAR. If, however, we know surface type and if we can assume MCAR within the type of surface, then the data are MAR\n\n\n7.1.2.3 Missing Not at Random (MNAR)\nMNAR means that the probability of being missing varies for reasons that are unknown to us. For example, the weighing scale mechanism may wear out over time, producing more missing data as time progresses, but we may fail to note this. If the heavier objects are measured later in time, then we obtain a distribution of the measurements that will be distorted. MNAR includes the possibility that the scale produces more missing values for the heavier objects (as above), a situation that might be difficult to recognize and handle.\nSource: https://stefvanbuuren.name/fimd/sec-MCAR.html\n\n\n\n7.1.3 Practice exercise 1\n\n7.1.3.1 \nIn which of the above scenarios can we ignore the observations corresponding to missing values without the risk of skewing the analysis/trends in the data?\n\n\n7.1.3.2 \nIn which of the above scenarios will it be the more risky to impute or estimate missing values?\n\n\n7.1.3.3 \nFor the datset consisting of GDP per capita, think of hypothetical scenarios in which the missing values of GDP per capita can correspond to MCAR / MAR / MNAR.\n\n\n\n7.1.4 Dropping observations with missing values (dropna())\nSometimes our analysis requires that there should be no missing values in the dataset. For example, while building statistical models, we may require the values of all the predictor variables. The quickest way is to use the dropna() method, which drops the observations that even have a single missing value, and leaves only complete observations in the data.\nLet us drop the rows containing even a single value from gdp_missing_values_data.\n\ngdp_no_missing_data = gdp_missing_values_data.dropna()\n\n\n#Shape of gdp_no_missing_data\ngdp_no_missing_data.shape\n\n(42, 12)\n\n\nDropping rows with even a single missing value has reduced the number of rows from 155 to 42! However, earlier we saw that all the columns except contraception had at most 10 missing values. Removing all rows / columns with even a single missing value results in loss of data that is non-missing in the respective rows/columns. Thus, it is typically a bad idea to drop observations with even a single missing value, except in cases where we have a very small number of missing-value observations.\nIf a few values of a column are missing, we can possibly estimate them using the rest of the data, so that we can (hopefully) maximize the information that can be extracted from the data. However, if most of the values of a column are missing, it may be harder to estimate its values.\nIn this case, we see that around 50% values of the contraception column is missing. Thus, we’ll drop the column as it may be hard to impute its values based on a relatively small number of non-missing values.\n\n#Deleting column with missing values in almost half of the observations\ngdp_missing_values_data.drop(['contraception'],axis=1,inplace=True)\ngdp_missing_values_data.shape\n\n(155, 11)\n\n\n\n\n7.1.5 Some methods to impute missing values\nThere are an unlimited number of ways to impute missing values. Some imputation methods are provided in the Pandas documentation.\nThe best way to impute them will depend on the problem, and the assumptions taken. Below are just a few examples.\n\n7.1.5.1 Method 1: Naive Method\nFilling the missing value of a column by copying the value of the previous non-missing observation.\n\n#Filling missing values: Method 1- Naive way\ngdp_imputed_data = gdp_missing_values_data.fillna(method = 'ffill')\n\n\n#Checking if any missing values are remaining\ngdp_imputed_data.isnull().sum()\n\neconomicActivityFemale    0\ncountry                   0\nlifeMale                  0\ninfantMortality           0\ngdpPerCapita              0\neconomicActivityMale      0\nilliteracyMale            1\nilliteracyFemale          0\nlifeFemale                0\ngeographic_location       0\ncontinent                 0\ndtype: int64\n\n\nAfter imputing missing values, note there is still one missing value for illiteracyMale. Can you guess why one missing value remained?\nLet us check how good is this method in imputing missing values. We’ll compare the imputed values of gdpPerCapita with the actual values. Recall that we had randomly put some missing values in gdp_missing_values_data, and we have the actual values in gdp_complete_data.\n\n#Index of rows with missing values for GDP per capita\nnull_ind_gdpPC = gdp_missing_values_data.index[gdp_missing_values_data.gdpPerCapita.isnull()]\n\n\n#Defining a function to plot the imputed values vs actual values \ndef plot_actual_vs_predicted():\n    fig, ax = plt.subplots(figsize=(8, 6))\n    plt.rc('xtick', labelsize=15) \n    plt.rc('ytick', labelsize=15) \n    x = gdp_complete_data.loc[null_ind_gdpPC,'gdpPerCapita']\n    y = gdp_imputed_data.loc[null_ind_gdpPC,'gdpPerCapita']\n    plt.scatter(x,y)\n    z=np.polyfit(x,y,1)\n    p=np.poly1d(z)\n    plt.plot(x,x,color='orange')\n    plt.xlabel('Actual GDP per capita',fontsize=20)\n    plt.ylabel('Imputed GDP per capita',fontsize=20)\n    ax.xaxis.set_major_formatter('${x:,.0f}')\n    ax.yaxis.set_major_formatter('${x:,.0f}')\n    rmse = np.sqrt(((x-y).pow(2)).mean())\n    print(\"RMSE=\",rmse)\n\n\n#Plot comparing imputed values with actual values, and computing the Root mean square error (RMSE) of the imputed values\nplot_actual_vs_predicted()\n\nRMSE= 34843.91091137732\n\n\n\n\n\nWe observe that the accuracy of imputation is poor as GDP per capita can vary a lot across countries, and the data is not sorted by GDP per capita. There is no reason why the GDP per capita of a country should be close to the GDP per capita of the country in the observation above it.\n\n\n7.1.5.2 Method 2: Imputing missing values as the mean of non-missing values\nLet us impute missing values in the column as the average of the non-missing values of the column. The sum of squared differences between actual values and the imputed values is likely to be smaller if we impute using the mean. However, this may not be true in cases other than MCAR (Missing completely at random).\n\n#Filling missing values: Method 2\ngdp_imputed_data = gdp_missing_values_data.fillna(gdp_missing_values_data.mean())\n\n\nplot_actual_vs_predicted()\n\nRMSE= 30793.549983587087\n\n\n\n\n\nAlthough this method of imputation doesn’t seem impressive, the RMSE of the estimates is lower than that of the naive method. Since we had introduced missing values randomly in gdp_missing_values_data, the mean GDP per capita will be the closest constant to the GDP per capita values, in terms of squared error.\n\n\n7.1.5.3 Method 3: Imputing missing values based on correlated variables in the data\nIf a variable is highly correlated with another variable in the dataset, we can approximate its missing values using the trendline with the highly correlated variable.\nLet us visualize the distribution of GDP per capita for different continents.\n\nplt.rcParams[\"figure.figsize\"] = (12,6)\nsns.boxplot(x = 'continent',y='gdpPerCapita',data = gdp_missing_values_data)\n\n&lt;AxesSubplot:xlabel='continent', ylabel='gdpPerCapita'&gt;\n\n\n\n\n\nWe observe that there is a distinct difference between the GDPs per capita of some of the contents. Let us impute the missing GDP per capita of a country as the mean GDP per capita of the corresponding continent. This imputation should be better than imputing the missing GDP per capita as the mean of all the non-missing values, as the GDP per capita of a country is likely to be closer to the mean GDP per capita of the continent, rather the mean GDP per capita of the whole world.\n\n#Finding the mean GDP per capita of the continent - please defer the understanding of this code to chapter 9.\navg_gdpPerCapita = gdp_missing_values_data['gdpPerCapita'].groupby(gdp_missing_values_data['continent']).mean()\navg_gdpPerCapita\n\ncontinent\nAfrica            7638.178571\nAsia             25922.750000\nEurope           45455.303030\nNorth America    19625.210526\nOceania          15385.857143\nSouth America    15360.909091\nName: gdpPerCapita, dtype: float64\n\n\n\n#Creating a copy of missing data to impute missing values\ngdp_imputed_data = gdp_missing_values_data.copy()\n\n\n#Replacing missing GDP per capita with the mean GDP per capita for the corresponding continent\nfor cont in avg_gdpPerCapita.index:\n    gdp_imputed_data.loc[(gdp_imputed_data.continent==cont) & (gdp_imputed_data.gdpPerCapita.isnull()),\n                     'gdpPerCapita']=avg_gdpPerCapita[cont]\nplot_actual_vs_predicted()\n\nRMSE= 25473.20645170116\n\n\n\n\n\nNote that the imputed values are closer to the actual values, and the RMSE has further reduced as expected.\n\n\n7.1.5.4 Practice exercise 2\nFind the numeric variable most strongly correlated with GDP per capita, and use it to impute its missing values. Find the RMSE of the imputed values.\nSolution:\n\n#Let us identify the variable highly correlated with GDP per capita.\ngdp_missing_values_data.corrwith(gdp_missing_values_data.gdpPerCapita)\n\neconomicActivityFemale    0.078332\nlifeMale                  0.579850\ninfantMortality          -0.572201\ngdpPerCapita              1.000000\neconomicActivityMale     -0.134108\nilliteracyMale           -0.479143\nilliteracyFemale         -0.448273\nlifeFemale                0.615954\ncontraception             0.057923\ndtype: float64\n\n\n\n#The variable *lifeFemale* has the strongest correlation with GDP per capita. Let us use it to impute missing values of GDP per capita.\nx = gdp_missing_values_data.lifeFemale\ny = gdp_missing_values_data.gdpPerCapita\nidx_non_missing = np.isfinite(x) & np.isfinite(y)\nslope_intercept_trendline = np.polyfit(x[idx_non_missing],y[idx_non_missing],1)   #Finding the slope and intercept for the trendline\ncompute_y_given_x = np.poly1d(slope_intercept_trendline)\n#Creating a copy of missing data to impute missing values\ngdp_imputed_data = gdp_missing_values_data.copy()\ngdp_imputed_data.loc[null_ind_gdpPC,'gdpPerCapita']=compute_y_given_x(gdp_missing_values_data.loc[null_ind_gdpPC,'lifeFemale'])\nplot_actual_vs_predicted()\n\nRMSE= 25570.361516956993\n\n\n\n\n\n\n\n7.1.5.5 Method 4: KNN: K-nearest neighbor\nIn this method, we’ll impute the missing value of the variable as the mean value of the \\(K\\)-nearest neighbors having non-missing values for that variable. The neighbors to a data-point are identified based on their Euclidean distance to the point in terms of the standardized values of rest of the variables in the data.\nLet’s consider a toy example to understand missing value imputation by KNN. Suppose we have to impute missing values in a toy dataset, named as toy_data having 4 observations and 3 variables.\n\n#Toy example - A 4x3 array with missing values\nnan = np.nan\ntoy_data = np.array([[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]])\ntoy_data\n\narray([[ 1.,  2., nan],\n       [ 3.,  4.,  3.],\n       [nan,  6.,  5.],\n       [ 8.,  8.,  7.]])\n\n\nWe’ll use some functions from the sklearn library to perform the KNN imputation. It is much easier to directly use the algorithm from sklearn, instead of coding it from scratch.\n\n#Library to compute pair-wise Euclidean distance between all observations in the data\nfrom sklearn import metrics\n\n#Library to impute missing values with the KNN algorithm\nfrom sklearn import impute\n\nWe’ll use the sklearn function nan_euclidean_distances() to compute the Euclidean distance between all pairs of observations in the data.\n\n#This is the distance matrix containing the distance of the ith observation from the jth observation at the (i,j) position in the matrix\nmetrics.pairwise.nan_euclidean_distances(toy_data,toy_data)\n\narray([[ 0.        ,  3.46410162,  6.92820323, 11.29158979],\n       [ 3.46410162,  0.        ,  3.46410162,  7.54983444],\n       [ 6.92820323,  3.46410162,  0.        ,  3.46410162],\n       [11.29158979,  7.54983444,  3.46410162,  0.        ]])\n\n\nNote that the size of the above matrix is 4x4. This is because the \\((i,j)^{th}\\) element of the matrix is the distance of the \\(i^{th}\\) observation from the \\(j^{th}\\) observation. The matrix is symmetric because the distance of \\(i^{th}\\) observation to the \\(j^{th}\\) observation is the same as the distance of the \\(j^{th}\\) observation to the \\(i^{th}\\) observation.\nWe’ll use the sklearn function KNNImputer() to impute the missing value of a column in toy_data as the mean of the values of the \\(K\\) nearest neighbors to the observation that have non-missing values for that column.\nLet us impute the missing values in toy_data using the values of \\(K=2\\) nearest neighbors from the corresponding observation.\n\n#imputing missing values with 2 nearest neighbors, where the neighbors have equal weights\n\n#Define an object of type KNNImputer\nimputer = impute.KNNImputer(n_neighbors=2)\n\n#Use the object method 'fit_transform' to impute missing values\nimputer.fit_transform(toy_data)\n\narray([[1. , 2. , 4. ],\n       [3. , 4. , 3. ],\n       [5.5, 6. , 5. ],\n       [8. , 8. , 7. ]])\n\n\nThe third observation was the closest to the \\(2nd\\) and \\(4th\\) observations based on the Euclidean distance matrix. Thus, the missing value in the \\(3rd\\) row of the toy_data has been imputed as the mean of the values in the \\(2nd\\) and \\(4th\\) observations for the corresponding column. Similarly, the \\(1st\\) observation is the closest to the \\(2nd\\) and \\(3rd\\) observations. Thus the missing value in the \\(1st\\) row of toy_data has been imputed as the mean of the values in the \\(1st\\) and \\(2nd\\) observations for the corresponding column.\nLet us use KNN to impute the missing values of gdpPerCapita in gdp_missing_values_data. We’ll use only the numeric columns of the data in imputing the missing values. Also, we’ll ignore contraception as it has a lot of missing values, and thus may not be useful.\n\n#Considering numeric columns in the data to use KNN\nnum_cols = list(range(0,1))+list(range(2,9))\nnum_cols\n\n[0, 2, 3, 4, 5, 6, 7, 8]\n\n\nBefore computing the pair-wise Euclidean distance of observations, we must standardize the data so that all columns are at the same scale. This will avoid columns with a higher magnitude of values having a higher weight in determining the Euclidean distance. Unless there is a reason to give a higher weight to a column, we assume all columns to have the same weight in the Euclidean distance computation.\nWe can use the code below to scale the data. However, after imputing the missing values, the data is to be scaled back to the original scale, so that each variable is in the same units as in the original dataset. However, if the code below is used, we’ll lose the orginal scale of each of the columns.\n\n#Scaling data to compute equally weighted distances from the 'k' nearest neighbors\nscaled_data = gdp_missing_values_data.iloc[:,num_cols].apply(lambda x:(x-x.min())/(x.max()-x.min()))\n\nTo alleviate the problem of losing the orignial scale of the data, we’ll use the MinMaxScaler object of the sklearn library. The object will store the original scale of the data, which will help transform the data back to the original scale once the missing values have been imputed in the standardized data.\n\n# Scaling data - using sklearn\n\n#Create an object of type MinMaxScaler\nscaler = sk.preprocessing.MinMaxScaler()\n\n#Use the object method 'fit_transform' to scale the values to a standard uniform distribution\nscaled_data = pd.DataFrame(scaler.fit_transform(gdp_missing_values_data.iloc[:,num_cols]))\n\n\n#Imputing missing values with KNNImputer\n\n#Define an object of type KNNImputer\nimputer = impute.KNNImputer(n_neighbors=3, weights=\"uniform\")\n\n#Use the object method 'fit_transform' to impute missing values\nimputed_arr = imputer.fit_transform(scaled_data)\n\n\n#Scaling back the scaled array to obtain the data at the original scale\n\n#Use the object method 'inverse_transform' to scale back the values to the original scale of the data\nunscaled_data = scaler.inverse_transform(imputed_arr)\n\n\n#Note the method imputes the missing value of all the columns\n#However, we are interested in imputing the missing values of only the 'gdpPerCapita' column\ngdp_imputed_data = gdp_missing_values_data.copy()\ngdp_imputed_data.loc[:,'gdpPerCapita'] = unscaled_data[:,3]\n\n\n#Visualizing the accuracy of missing value imputation with KNN\nplot_actual_vs_predicted()\n\nRMSE= 16804.195967740387\n\n\n\n\n\nNote that the RMSE is the lowest in this method. It is because this method imputes missing values as the average of the values of “similar” observations, which is smarter and more robust than the previous methods.\nWe chose \\(K=3\\) in the missing value imputation for GDP per capita. However, the value of \\(K\\) is typically chosen using a method known as cross validation. We’ll learn about cross-validation in the next course of the sequence."
  },
  {
    "objectID": "Data cleaning and preparation.html#data-binning",
    "href": "Data cleaning and preparation.html#data-binning",
    "title": "7  Data cleaning and preparation",
    "section": "7.2 Data binning",
    "text": "7.2 Data binning\nData binning is a method to group values of a continuous / categorical variable into bins (or categories). Binning may help with\n\nBetter intepretation of data\n\nMaking better recommendations\n\nSmooth data, reduce noise\n\nExamples:\nBinning to better interpret data\n\nThe number of flu cases everyday may be binned to seasons such as fall, spring, winter and summer, to understand the effect of season on flu.\n\nBinning to make recommendations:\n\nA doctor may like to group patient age into bins. Grouping patient ages into categories such as Age &lt;=12, 12&lt;Age&lt;=18, 18&lt;Age&lt;=65, Age&gt;65 may help recommend the kind/doses of covid vaccine a patient needs.\nA credit card company may want to bin customers based on their spend, as “High spenders”, “Medium spenders” and “Low spenders”. Binning will help them design customized marketing campaigns for each bin, thereby increasing customer response (or revenue). On the other hand, they use the same campaign for customers withing the same bin, thus minimizng marketing costs.\n\nBinning to smooth data, and reduce noise\n\nA sales company may want to bin their total sales to a weekly / monthly / yearly level to reduce the noise in day-to-day sales.\n\nExample: The dataset College.csv contains information about US universities. The description of variables of the dataset can be found on page 54 of this book. Let’s see if we can apply binning to better interpret the association of instructional expenditure per student (Expend) with graduation rate (Grad.Rate) for US universities, and make recommendations.\n\ncollege = pd.read_csv('./Datasets/College.csv')\ncollege.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nPrivate\nApps\nAccept\nEnroll\nTop10perc\nTop25perc\nF.Undergrad\nP.Undergrad\nOutstate\nRoom.Board\nBooks\nPersonal\nPhD\nTerminal\nS.F.Ratio\nperc.alumni\nExpend\nGrad.Rate\n\n\n\n\n0\nAbilene Christian University\nYes\n1660\n1232\n721\n23\n52\n2885\n537\n7440\n3300\n450\n2200\n70\n78\n18.1\n12\n7041\n60\n\n\n1\nAdelphi University\nYes\n2186\n1924\n512\n16\n29\n2683\n1227\n12280\n6450\n750\n1500\n29\n30\n12.2\n16\n10527\n56\n\n\n2\nAdrian College\nYes\n1428\n1097\n336\n22\n50\n1036\n99\n11250\n3750\n400\n1165\n53\n66\n12.9\n30\n8735\n54\n\n\n3\nAgnes Scott College\nYes\n417\n349\n137\n60\n89\n510\n63\n12960\n5450\n450\n875\n92\n97\n7.7\n37\n19016\n59\n\n\n4\nAlaska Pacific University\nYes\n193\n146\n55\n16\n44\n249\n869\n7560\n4120\n800\n1500\n76\n72\n11.9\n2\n10922\n15\n\n\n\n\n\n\n\nTo visualize the association between two numeric variables, we typically make a scatterplot. Let us make a scatterplot of graduation rate with expenditure per student, with a trendline.\n\n#Let's make a scatterplot of 'Grad.Rate' vs 'Expend' with a trendline, to visualize any trend(s).\nsns.set(font_scale=1.5)\nax=sns.regplot(data = college, x = \"Expend\", y = \"Grad.Rate\",scatter_kws={\"color\": \"orange\"}, line_kws={\"color\": \"blue\"})\nax.xaxis.set_major_formatter('${x:,.0f}')\nax.set_xlabel('Expenditure per student')\nax.set_ylabel('Graduation rate')\n\nText(0, 0.5, 'Graduation rate')\n\n\n\n\n\nThe trendline indicates a positive correlation between Expend and Grad.Rate. However, there seems to be a lot of noise and presence of outliers in the data, which makes it hard to interpret the overall trend.\nWe’ll bin Expend to see if we can better analyze its association with Grad.Rate. However, let us first visualize the distribution of Expend.\n\n#Visualizing the distribution of expend\nax=sns.histplot(data = college, x= 'Expend')\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nThe distribution of Extend is right skewed with potentially some extremely high outlying values.\n\n7.2.1 Binning with equal width bins\nWe’ll use the Pandas function cut() to bin Expend. This function creates bins such that all bins have the same width.\n\n#Using the cut() function in Pandas to bin \"Expend\"\nBinned_expend = pd.cut(college['Expend'],3,retbins = True)\nBinned_expend\n\n(0      (3132.953, 20868.333]\n 1      (3132.953, 20868.333]\n 2      (3132.953, 20868.333]\n 3      (3132.953, 20868.333]\n 4      (3132.953, 20868.333]\n                ...          \n 772    (3132.953, 20868.333]\n 773    (3132.953, 20868.333]\n 774    (3132.953, 20868.333]\n 775     (38550.667, 56233.0]\n 776    (3132.953, 20868.333]\n Name: Expend, Length: 777, dtype: category\n Categories (3, interval[float64]): [(3132.953, 20868.333] &lt; (20868.333, 38550.667] &lt; (38550.667, 56233.0]],\n array([ 3132.953     , 20868.33333333, 38550.66666667, 56233.        ]))\n\n\nThe cut() function returns a tuple of length 2. The first element of the tuple are the bins, while the second element is an array containing the cut-off values for the bins.\n\ntype(Binned_expend)\n\ntuple\n\n\n\nlen(Binned_expend)\n\n2\n\n\nOnce the bins are obtained, we’ll add a column in the dataset that indicates the bin for Expend.\n\n#Creating a categorical variable to store the level of expenditure on a student\ncollege['Expend_bin'] = Binned_expend[0]\ncollege.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nPrivate\nApps\nAccept\nEnroll\nTop10perc\nTop25perc\nF.Undergrad\nP.Undergrad\nOutstate\nRoom.Board\nBooks\nPersonal\nPhD\nTerminal\nS.F.Ratio\nperc.alumni\nExpend\nGrad.Rate\nExpend_bin\n\n\n\n\n0\nAbilene Christian University\nYes\n1660\n1232\n721\n23\n52\n2885\n537\n7440\n3300\n450\n2200\n70\n78\n18.1\n12\n7041\n60\n(3132.953, 20868.333]\n\n\n1\nAdelphi University\nYes\n2186\n1924\n512\n16\n29\n2683\n1227\n12280\n6450\n750\n1500\n29\n30\n12.2\n16\n10527\n56\n(3132.953, 20868.333]\n\n\n2\nAdrian College\nYes\n1428\n1097\n336\n22\n50\n1036\n99\n11250\n3750\n400\n1165\n53\n66\n12.9\n30\n8735\n54\n(3132.953, 20868.333]\n\n\n3\nAgnes Scott College\nYes\n417\n349\n137\n60\n89\n510\n63\n12960\n5450\n450\n875\n92\n97\n7.7\n37\n19016\n59\n(3132.953, 20868.333]\n\n\n4\nAlaska Pacific University\nYes\n193\n146\n55\n16\n44\n249\n869\n7560\n4120\n800\n1500\n76\n72\n11.9\n2\n10922\n15\n(3132.953, 20868.333]\n\n\n\n\n\n\n\nSee the variable Expend_bin in the above dataset.\nLet us visualize the Expend bins over the distribution of the Expend variable.\n\n#Visualizing the bins for instructional expediture on a student\nsns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nax=sns.histplot(data = college, x= 'Expend')\nplt.vlines(Binned_expend[1], 0,100,color='red')\nplt.xlabel('Expenditure per student');\nplt.ylabel('Graduation rate');\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nBy default, the bins created have equal width. They are created by dividing the range between the maximum and minimum value of Expend into the desired number of equal-width intervals. We can label the bins as well as follows.\n\ncollege['Expend_bin'] = pd.cut(college['Expend'],3,labels = ['Low expend','Med expend','High expend'])\ncollege['Expend_bin']\n\n0       Low expend\n1       Low expend\n2       Low expend\n3       Low expend\n4       Low expend\n          ...     \n772     Low expend\n773     Low expend\n774     Low expend\n775    High expend\n776     Low expend\nName: Expend_bin, Length: 777, dtype: category\nCategories (3, object): ['Low expend' &lt; 'Med expend' &lt; 'High expend']\n\n\nNow that we have binned the variable Expend, let us see if we can better visualize the association of graduation rate with expenditure per student using Expened_bin.\n\n#Visualizing average graduation rate vs categories of instructional expenditure per student\nsns.barplot(x = 'Expend_bin', y = 'Grad.Rate', data = college)\n\n&lt;AxesSubplot:xlabel='Expend_bin', ylabel='Grad.Rate'&gt;\n\n\n\n\n\nIt seems that the graduation rate is the highest for universities with medium level of expenditure per student. This is different from the trend we saw earlier in the scatter plot. Let us investigate.\nLet us find the number of universities in each bin.\n\npd.value_counts(college['Expend_bin'])\n\nLow expend     751\nMed expend      21\nHigh expend      5\nName: Expend_bin, dtype: int64\n\n\nThe bin High expend consists of only 5 universities, or 0.6% of all the universities in the dataset. These universities may be outliers that are skewing the trend (as also evident in the histogram above).\nIn such cases, we should bin observations such that all bins are of equal size, i.e., they have the same number of observations.\n\n\n7.2.2 Binning with equal sized bins\nLet us bin the variable Expend such that each bin consists of the same number of observations.\nWe’ll use the Pandas function qcut() to make equal-sized bins (in contrast to equal-width bins in the previous section).\n\n#Using the Pandas function qcut() to create bins with the same number of observations\nBinned_expend = pd.qcut(college['Expend'],3,retbins = True)\ncollege['Expend_bin'] = Binned_expend[0]\n\nEach bin has the same number of observations with qcut():\n\npd.value_counts(college['Expend_bin'])\n\n(3185.999, 7334.333]    259\n(7334.333, 9682.333]    259\n(9682.333, 56233.0]     259\nName: Expend_bin, dtype: int64\n\n\nLet us visualize the Expend bins over the distribution of the Expend variable.\n\n#Visualizing the bins for instructional expediture on a student\nsns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nax=sns.histplot(data = college, x= 'Expend')\nplt.vlines(Binned_expend[1], 0,100,color='red')\nplt.xlabel('Expenditure per student');\nplt.ylabel('Graduation rate');\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nNote that the bin-widths have been adjusted to have the same number of observations in each bin. The bins are narrower in domains of high density, and wider in domains of sparse density.\nLet us again make the barplot visualizing the average graduate rate with level of instructional expenditure per student.\n\ncollege['Expend_bin'] = pd.qcut(college['Expend'],3,labels = ['Low expend','Med expend','High expend'])\na=sns.barplot(x = 'Expend_bin', y = 'Grad.Rate', data = college)\n\n\n\n\nNow we see the same trend that we saw in the scatterplot, but without the noise. We have smoothed the data. Note that making equal-sized bins helps reduce the effect of outliers in the overall trend.\nSuppose this analysis was done to provide recommendations to universities for increasing their graduation rate. With binning, we can can provide one recommendation to ‘Low expend’ universities, and another one to ‘Med expend’ universities. For example, the recommendations can be:\n\n‘Low expend’ universities can expect an increase of 9 percentage points in Grad.Rate, if they migrate to the ‘Med expend’ category.\n‘Med expend’ universities can expect an increase of 7 percentage points in Grad.Rate, if they migrate to the ‘High expend’ category.\n\nThe numbers in the above recommendations are based on the table below.\n\ncollege['Grad.Rate'].groupby(college.Expend_bin).mean()\n\nExpend_bin\nLow expend     57.343629\nMed expend     66.057915\nHigh expend    72.988417\nName: Grad.Rate, dtype: float64\n\n\nWe can also make recommendations based on the confidence intervals of mean Grad.Rate. Confidence intervals are computed below. We are finding confidence intervals based on a method known as bootstrapping. Refer https://en.wikipedia.org/wiki/Bootstrapping_(statistics) for a detailed description of Bootstrapping.\n\n#Bootstrapping to find 95% confidence intervals of Graduation Rate of US universities based on average expenditure per student\nfor expend_bin in college.Expend_bin.unique():\n    data_sub = college.loc[college.Expend_bin==expend_bin,:]\n    samples = np.random.choice(data_sub['Grad.Rate'], size=(10000,data_sub.shape[0]))\n    print(\"95% Confidence interval of Grad.Rate for \"+expend_bin+\" univeristies = [\"+str(np.round(np.percentile(samples.mean(axis=1),2.5),2))+\",\"+str(np.round(np.percentile(samples.mean(axis=1),97.5),2))+\"]\")\n\n95% Confidence interval of Grad.Rate for Low expend univeristies = [55.34,59.34]\n95% Confidence interval of Grad.Rate for High expend univeristies = [71.01,74.92]\n95% Confidence interval of Grad.Rate for Med expend univeristies = [64.22,67.93]\n\n\nApart from equal-width and equal-sized bins, custom bins can be created using the bins argument. Suppose, bins are to be created for Expend with cutoffs \\(\\$10,000, \\$20,000, \\$30,000... \\$60,000\\). Then, we can use the bins argument as in the code below:\n\n\n7.2.3 Binning with custom bins\n\nBinned_expend = pd.cut(college.Expend,bins = list(range(0,70000,10000)),retbins=True)\n\n\n#Visualizing the bins for instructional expediture on a student\nax=sns.histplot(data = college, x= 'Expend')\nplt.vlines(Binned_expend[1], 0,100,color='red')\nplt.xlabel('Expenditure per student');\nplt.ylabel('Graduation rate');\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nAs custom bin-cutoffs can be specified with the cut() function, custom bin quantiles can be specified with the qcut() function."
  },
  {
    "objectID": "Data cleaning and preparation.html#dummy-indicator-variables",
    "href": "Data cleaning and preparation.html#dummy-indicator-variables",
    "title": "7  Data cleaning and preparation",
    "section": "7.3 Dummy / Indicator variables",
    "text": "7.3 Dummy / Indicator variables\nDummy variables (or indicator variables) take only the values of 0 and 1 to indicate the presence or absence of a catagorical effect. They are particularly useful in regression modeling to help explain the dependent variable.\nIf a column in a DataFrame has \\(k\\) distinct values, we will get a DataFrame with \\(k\\) columns containing 0s and 1s with the Pandas get_dummies() function.\nLet us make dummy variables with the equal-sized bins we created for the average instruction expenditure per student.\n\n#Using the Pandas function qcut() to create bins with the same number of observations\nBinned_expend = pd.qcut(college['Expend'],3,retbins = True,labels = ['Low_expend','Med_expend','High_expend'])\ncollege['Expend_bin'] = Binned_expend[0]\n\n\n#Making dummy variables based on the levels (categories) of the 'Expend_bin' variable\ndummy_Expend = pd.get_dummies(college['Expend_bin'])\n\nThe dummy data dummy_Expend has a value of \\(1\\) if the observation corresponds to the category referenced by the column name.\n\ndummy_Expend.head()\n\n\n\n\n\n\n\n\nLow_expend\nMed_expend\nHigh_expend\n\n\n\n\n0\n1\n0\n0\n\n\n1\n0\n0\n1\n\n\n2\n0\n1\n0\n\n\n3\n0\n0\n1\n\n\n4\n0\n0\n1\n\n\n\n\n\n\n\nWe can find the correlation between the dummy variables and graduation rate to identify if any of the dummy variables will be useful to estimate graduation rate (Grad.Rate).\n\n#Finding if dummy variables will be useful to estimate 'Grad.Rate'\ndummy_Expend.corrwith(college['Grad.Rate'])\n\nLow_expend    -0.334456\nMed_expend     0.024492\nHigh_expend    0.309964\ndtype: float64\n\n\nThe dummy variables Low expend and High expend may contribute in explaining Grad.Rate in a regression model.\n\n7.3.1 Practice exercise 3\nRead survey_data_clean.csv. Split the columns of the dataset, such that all columns with categorical values transform into dummy variables with each category corresponding to a column of 0s and 1s. Leave the Timestamp column.\nAs all categorical columns are transformed to dummy variables, all columns have numeric values.\nWhat is the total number of columns in the transformed data? What is the total number of columns of the original data?\nFind the:\n\nTop 5 variables having the highest positive correlation with NU_GPA.\nTop 5 variables having the highest negative correlation with NU_GPA.\n\nSolution:\n\nsurvey_data = pd.read_csv('./Datasets/survey_data_clean.csv')\nsurvey_dummies=pd.get_dummies(survey_data.iloc[:,1:])\nprint(\"The total number of columns in the transformed data are\", survey_dummies.shape[1])\nprint(\"The total number of columns in the original data are\", survey_dummies.shape[0])\n\nThe total number of columns in the transformed data are 308\nThe total number of columns in the original data are 192\n\n\nBelow are the top 5 variables having the highest positive correlation with NU_GPA:\n\nsurvey_dummies.corrwith(survey_dummies.NU_GPA).drop(index = 'NU_GPA').sort_values(ascending=False)[0:5]\n\nfav_letter_o                                                                              0.367140\nfav_sport_Dance!                                                                          0.367140\nmajor_Humanities / Communications;Physical Sciences / Natural Sciences / Engineering      0.271019\nfav_alcohol_I don't drink                                                                 0.213118\nlearning_style_Reading/Writing (learn best through words often note-taking or reading)    0.207451\ndtype: float64\n\n\nBelow are the top 5 variables having the highest negative correlation with NU_GPA:\n\nsurvey_dummies.corrwith(survey_dummies.NU_GPA).drop(index = 'NU_GPA').sort_values(ascending=True)[0:5]\n\nfav_number                                         -0.307656\nprocrastinator                                     -0.269552\nfav_sport_Underwater Basketweaving                 -0.224237\nbirth_month_February                               -0.222141\nstreaming_platforms_Netflix;Amazon Prime;HBO Max   -0.221099\ndtype: float64"
  },
  {
    "objectID": "Data cleaning and preparation.html#outlier-detection",
    "href": "Data cleaning and preparation.html#outlier-detection",
    "title": "7  Data cleaning and preparation",
    "section": "7.4 Outlier detection",
    "text": "7.4 Outlier detection\nAn outlier is an observation that is significantly different from the rest of the data. Detection of outliers is important as they may distort the general trends in data.\nLet us visualize outliers in average instructional expenditure per student given by the variable Expend.\n\nax=college.boxplot(column = 'Expend')\nax.yaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nThere are several outliers (shown as circles in the above boxplot), which correspond to high values of average instructional expenditure per student. Boxplot identifies outliers based on the Tukey’s fences criterion:\nTukey’s fences: John Tukey proposed that observations outside the range \\([Q1 - 1.5(Q3-Q1), Q3+1.5(Q3-Q1)]\\) are outliers, where \\(Q1\\) and \\(Q3\\) are the lower \\((25\\%)\\) and upper \\((75\\%)\\) quartiles respectively. Let us detect outliers based on Tukey’s fences.\n\n#Finding upper and lower quartiles and interquartile range\nq1 = np.percentile(college['Expend'],25)\nq3 = np.percentile(college['Expend'],75)\nintQ_range = q3-q1\n\n\n#Tukey's fences\nLower_fence = q1 - 1.5*intQ_range\nUpper_fence = q3 + 1.5*intQ_range\n\n\n#These are the outlying observations - those outside of Tukey's fences\nOutlying_obs = college[(college.Expend&lt;Lower_fence) | (college.Expend&gt;Upper_fence)]\n\n\n#Data without outliers\ncollege_data_without_outliers = college[((college.Expend&gt;=Lower_fence) & (college.Expend&lt;=Upper_fence))]\n\nEarlier, the trend was distorted by outliers when we created bins of equal width. Let us see if we get the correct trend with the outliers removed from the data.\n\nBinned_data = pd.cut(college_data_without_outliers['Expend'],3,labels = ['Low expend','Med expend','High expend'],retbins = True)\ncollege_data_without_outliers.loc[:,'Expend_bin'] = Binned_data[0]\n\n\nsns.barplot(x = 'Expend_bin', y = 'Grad.Rate', data = college_data_without_outliers)\n\n&lt;AxesSubplot:xlabel='Expend_bin', ylabel='Grad.Rate'&gt;\n\n\n\n\n\nWith the outliers removed, we obtain the correct overall trend, even in the case of equal-width bins. Note that these bins have unequal number of observations as shown below.\n\nsns.set(font_scale=1.35)\nax=sns.histplot(data = college_data_without_outliers, x= 'Expend')\nfor i in range(4):\n    plt.axvline(Binned_data[1][i], 0,100,color='red')\nplt.xlabel('Expenditure per student');\nplt.ylabel('Graduation rate');\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nNote that the right tail of the histogram has disappered since we removed outliers.\n\ncollege_data_without_outliers['Expend_bin'].value_counts()\n\nMed expend     327\nLow expend     314\nHigh expend     88\nName: Expend_bin, dtype: int64\n\n\n\n7.4.1 Practice exercise 4\nConsider the dataset created for survey_data_clean.csv in Practice exercise 3, which includes dummy variables for all the categorical variables. Find the number of outliers in each column of the dataset based on the Tukey’s fences criterion. Do not use a for loop.\nWhich column(s) have the maximum number of outliers?\nDo you think the outlying observations identified with the Tukey’s fences criterion for those columns(s) should be considered as outliers? If not, then which type of columns should be considered when finding outliers?\nSolution:\n\n#Function to identify outliers based on Tukey's fences\ndef rem_outliers(x):\n    q1 =x.quantile(0.25)\n    q3 = x.quantile(0.75)\n    intQ_range = q3-q1\n\n    #Tukey's fences\n    Lower_fence = q1 - 1.5*intQ_range\n    Upper_fence = q3 + 1.5*intQ_range\n    \n    #The object returned will be a data frame with bool values - True or False. 'True' will indicate that the value is an outlier\n    return ((x&lt;Lower_fence) | (x&gt;Upper_fence))\n\nsurvey_dummies.apply(rem_outliers).sum().sort_values(ascending=False)\n\nlearning_style_Reading/Writing (learn best through words often note-taking or reading)    48\nmuch_effort_is_lack_of_talent                                                             48\nleft_right_brained_Left-brained (logic, science, critical thinking, numbers)              47\nleft_right_brained_Right-brained (creative, art, imaginative, intuitive)                  47\nfav_season_Spring                                                                         47\n                                                                                          ..\nlove_first_sight                                                                           0\nbirthdate_odd_even_Odd                                                                     0\nprocrastinator                                                                             0\nbirthdate_odd_even_Even                                                                    0\nhow_happy_Pretty happy                                                                     0\nLength: 308, dtype: int64\n\n\nUsing Tukey’s criterion, the variables learning_style_Reading/Writing (learn best through words often note-taking or reading) and much_effort_is_lack_of_talent have the most number of outliers.\nHowever, these two variables only have 0s and 1s. For instance, let us consider learning_style_Reading/Writing (learn best through words often note-taking or reading).\n\nsurvey_dummies['learning_style_Reading/Writing (learn best through words often note-taking or reading)'].value_counts()\n\n0    144\n1     48\nName: learning_style_Reading/Writing (learn best through words often note-taking or reading), dtype: int64\n\n\nAs the percentage of 1s are \\(\\frac{48}{48+144}=25\\%\\), the \\(75^{th}\\) percentile value is 0.25, and the upper Tukey’s fence is \\(0.25+0.25*1.25 = 0.625\\), which makes the value \\(1\\) an outlier. However, we should not consider this value as an outlier, as a considerable fraction of the data (25%) has the value \\(1\\) for this variable.\nFurthermore, Tukey’s fences are developed for continuous variables. However, the variable learning_style_Reading/Writing (learn best through words often note-taking or reading) is discrete with only two levels. Thus, while finding outliers we must consider only continuous variables.\n\n#Finding continuous variables: Assuming numeric variables that have more than 2 distinct values are continuous\ncontinuous_variables = [x for x in list(survey_data.apply(lambda x: x.name if ((len(x.value_counts())&gt;2) & (x.dtype!='O')) else '')) if x!='']\n\n\n#Finding number of outliers for only continuous variables\nsurvey_data.loc[:,continuous_variables].apply(rem_outliers).sum().sort_values(ascending=False)\n\nnum_clubs                      22\nfav_number                     19\nparties_per_month              12\ninternet_hours_per_day         11\nsleep_hours_per_day            10\nexpected_marriage_age          10\nexpected_starting_salary        8\nhigh_school_GPA                 8\nheight_mother                   7\nnum_insta_followers             6\nheight_father                   6\nNU_GPA                          5\nminutes_ex_per_week             4\nheight                          3\nfarthest_distance_travelled     2\nage                             1\nnum_majors_minors               0\ndtype: int64\n\n\nThe variable num_clubs has the maximum number of outliers.\n\n#Finding how many clubs makes a person an outlier\nq3_num_clubs = survey_data.num_clubs.quantile(0.75)\nq1_num_clubs = survey_data.num_clubs.quantile(0.25)\nprint(\"Tukeys fences = [\",q1_num_clubs-1.5*(q3_num_clubs-q1_num_clubs),q3_num_clubs+1.5*(q3_num_clubs-q1_num_clubs),\"]\")\n\nTukeys fences = [ 0.5 4.5 ]\n\n\nPeople joining no club, or more than 4 clubs are outliers.\n\n\n\n\nRubin, Donald B. 1976. “Inference and Missing Data.” Biometrika 63 (3): 581–92."
  },
  {
    "objectID": "Data wrangling.html#hierarchical-indexing",
    "href": "Data wrangling.html#hierarchical-indexing",
    "title": "8  Data wrangling",
    "section": "8.1 Hierarchical indexing",
    "text": "8.1 Hierarchical indexing\nUntil now we have seen only a single level of indexing in the rows and columns of a Pandas DataFrame. Hierarchical indexing refers to having multiple index levels on an axis (row / column) of a Pandas DataFrame. It helps us to work with a higher dimensional data in a lower dimensional form.\n\n8.1.1 Hierarchical indexing in Pandas Series\nLet us define Pandas Series as we defined in Chapter 5:\n\n#Defining a Pandas Series\nseries_example = pd.Series(['these','are','english','words','estas','son','palabras','en','español',\n                            'ce','sont','des','françai','mots'])\nseries_example\n\n0        these\n1          are\n2      english\n3        words\n4        estas\n5          son\n6     palabras\n7           en\n8      español\n9           ce\n10        sont\n11         des\n12     françai\n13        mots\ndtype: object\n\n\nLet us use the attribute nlevels to find the number of levels of the row indices of this Series:\n\nseries_example.index.nlevels\n\n1\n\n\nThe Series series_example has only one level of row indices.\nLet us introduce another level of row indices while defining the Series:\n\n#Defining a Pandas Series with multiple levels of row indices\nseries_example = pd.Series(['these','are','english','words','estas','son','palabras','en','español',\n                           'ce','sont','des','françai','mots'], \n                          index=[['English']*4+['Spanish']*5+['French']*5,list(range(1,5))+list(range(1,6))*2])\nseries_example\n\nEnglish  1       these\n         2         are\n         3     english\n         4       words\nSpanish  1       estas\n         2         son\n         3    palabras\n         4          en\n         5     español\nFrench   1          ce\n         2        sont\n         3         des\n         4     françai\n         5        mots\ndtype: object\n\n\nIn the above Series, there are two levels of row indices:\n\nseries_example.index.nlevels\n\n2\n\n\n\n\n8.1.2 Hierarchical indexing in Pandas DataFrame\nIn a Pandas DataFrame, both the rows and the columns can have hierarchical indexing. For example, consider the DataFrame below:\n\ndata=np.array([[771517,2697000,815201,3849000],[4.2,5.6,2.8,4.6],\n             [7.8,234.5,46.9,502],[6749, 597, 52, 305]])\ndf_example = pd.DataFrame(data,index = [['Demographics']*2+['Geography']*2,\n                                      ['Population','Unemployment (%)','Area (mile-sq)','Elevation (feet)']],\n                    columns = [['Illinois']*2+['California']*2,['Evanston','Chicago','San Francisco','Los Angeles']])\ndf_example\n\n\n\n\n\n\n\n\n\nIllinois\nCalifornia\n\n\n\n\nEvanston\nChicago\nSan Francisco\nLos Angeles\n\n\n\n\nDemographics\nPopulation\n771517.0\n2697000.0\n815201.0\n3849000.0\n\n\nUnemployment (%)\n4.2\n5.6\n2.8\n4.6\n\n\nGeography\nArea (mile-sq)\n7.8\n234.5\n46.9\n502.0\n\n\nElevation (feet)\n6749.0\n597.0\n52.0\n305.0\n\n\n\n\n\n\n\nIn the above DataFrame, both the rows and columns have 2 levels of indexing. The number of levels of column indices can be found using the attribute nlevels:\n\ndf_example.columns.nlevels\n\n2\n\n\nThe columns attribute will now have a MultiIndex datatype in contrast to the Index datatype with single level of indexing. The same holds for row indices.\n\ntype(df_example.columns)\n\npandas.core.indexes.multi.MultiIndex\n\n\n\ndf_example.columns\n\nMultiIndex([(  'Illinois',      'Evanston'),\n            (  'Illinois',       'Chicago'),\n            ('California', 'San Francisco'),\n            ('California',   'Los Angeles')],\n           )\n\n\nThe hierarchical levels can have names. Let us assign names to the each level of the row and column labels:\n\n#Naming the row indices levels\ndf_example.index.names=['Information type', 'Statistic']\n\n#Naming the column indices levels\ndf_example.columns.names=['State', 'City']\n\n#Viewing the DataFrame\ndf_example\n\n\n\n\n\n\n\n\nState\nIllinois\nCalifornia\n\n\n\nCity\nEvanston\nChicago\nSan Francisco\nLos Angeles\n\n\nInformation type\nStatistic\n\n\n\n\n\n\n\n\nDemographics\nPopulation\n771517.0\n2697000.0\n815201.0\n3849000.0\n\n\nUnemployment (%)\n4.2\n5.6\n2.8\n4.6\n\n\nGeography\nArea (mile-sq)\n7.8\n234.5\n46.9\n502.0\n\n\nElevation (feet)\n6749.0\n597.0\n52.0\n305.0\n\n\n\n\n\n\n\nObserve that the names of the row and column labels appear when we view the DataFrame.\n\n8.1.2.1 get_level_values()\nThe names of the column levels can be obtained using the function get_level_values(). The outer-most level corresponds to the level = 0, and it increases as we go to the inner levels.\n\n#Column levels at level 0 (the outer level)\ndf_example.columns.get_level_values(0)\n\nIndex(['Illinois', 'Illinois', 'California', 'California'], dtype='object', name='State')\n\n\n\n#Column levels at level 1 (the inner level)\ndf_example.columns.get_level_values(1)\n\nIndex(['Evanston', 'Chicago', 'San Francisco', 'Los Angeles'], dtype='object', name='City')\n\n\n\n\n\n8.1.3 Subsetting data\nWe can use the indices at the outer levels to concisely subset a Series / DataFrame.\nThe first four observations of the Series series_example correspond to the outer row index English, while the last 5 rows correspond to the outer row index Spanish. Let us subset all the observations corresponding to the outer row index English:\n\n#Subsetting data by row-index\nseries_example['English']\n\n1      these\n2        are\n3    english\n4      words\ndtype: object\n\n\nJust like in the case of single level indices, if we wish to subset corresponding to multiple outer-level indices, we put the indices within an additional box bracket []. For example, let us subset all the observations corresponding to the row-indices English and French:\n\n#Subsetting data by multiple row-indices\nseries_example[['English','French']]\n\nEnglish  1      these\n         2        are\n         3    english\n         4      words\nFrench   1         ce\n         2       sont\n         3        des\n         4    françai\n         5       mots\ndtype: object\n\n\nWe can also subset data using the inner row index. However, we will need to put a : sign to indicate that the row label at the inner level is being used.\n\n#Subsetting data by row-index\nseries_example[:,2]\n\nEnglish     are\nSpanish     son\nFrench     sont\ndtype: object\n\n\n\n#Subsetting data by multiple row-indices\nseries_example.loc[:,[1,2]]\n\nEnglish  1    these\nSpanish  1    estas\nFrench   1       ce\nEnglish  2      are\nSpanish  2      son\nFrench   2     sont\ndtype: object\n\n\nAs in Series, we can concisely subset rows / columns in a DataFrame based on the index at the outer levels.\n\ndf_example['Illinois']\n\n\n\n\n\n\n\n\nCity\nEvanston\nChicago\n\n\nInformation type\nStatistic\n\n\n\n\n\n\nDemographics\nPopulation\n771517.0\n2697000.0\n\n\nUnemployment (%)\n4.2\n5.6\n\n\nGeography\nArea (mile-sq)\n7.8\n234.5\n\n\nElevation (feet)\n6749.0\n597.0\n\n\n\n\n\n\n\nNote that the dataype of each column name is a tuple. For example, let us find the datatype of the \\(1^{st}\\) column name:\n\n#First column name\ndf_example.columns[0]\n\n('Illinois', 'Evanston')\n\n\n\n#Datatype of first column name\ntype(df_example.columns[0])\n\ntuple\n\n\nThus columns at the inner levels can be accessed by specifying the name as a tuple. For example, let us subset the column Evanston:\n\n#Subsetting the column 'Evanston'\ndf_example[('Illinois','Evanston')]\n\nInformation type  Statistic       \nDemographics      Population          771517.0\n                  Unemployment (%)         4.2\nGeography         Area (mile-sq)           7.8\n                  Elevation (feet)      6749.0\nName: (Illinois, Evanston), dtype: float64\n\n\n\n#Subsetting the columns 'Evanston' and 'Chicago' of the outer column level 'Illinois'\ndf_example.loc[:,('Illinois',['Evanston','Chicago'])]\n\n\n\n\n\n\n\n\nState\nIllinois\n\n\n\nCity\nEvanston\nChicago\n\n\nInformation type\nStatistic\n\n\n\n\n\n\nDemographics\nPopulation\n771517.0\n2697000.0\n\n\nUnemployment (%)\n4.2\n5.6\n\n\nGeography\nArea (mile-sq)\n7.8\n234.5\n\n\nElevation (feet)\n6749.0\n597.0\n\n\n\n\n\n\n\n\n\n8.1.4 Practice exercise 1\nRead the table consisting of GDP per capita of countries from the webpage: https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita .\nTo only read the relevant table, read the tables that contain the word ‘Country’.\n\n8.1.4.1 \nHow many levels of indexing are there in the rows and columns?\n\ndfs = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita', match = 'Country')\ngdp_per_capita = dfs[0]\ngdp_per_capita.head()\n\n\n\n\n\n\n\n\nCountry/Territory\nUN Region\nIMF[4]\nWorld Bank[5]\nUnited Nations[6]\n\n\n\nCountry/Territory\nUN Region\nEstimate\nYear\nEstimate\nYear\nEstimate\nYear\n\n\n\n\n0\nLiechtenstein *\nEurope\n—\n—\n169049\n2019\n180227\n2020\n\n\n1\nMonaco *\nEurope\n—\n—\n173688\n2020\n173696\n2020\n\n\n2\nLuxembourg *\nEurope\n127673\n2022\n135683\n2021\n117182\n2020\n\n\n3\nBermuda *\nAmericas\n—\n—\n110870\n2021\n123945\n2020\n\n\n4\nIreland *\nEurope\n102217\n2022\n99152\n2021\n86251\n2020\n\n\n\n\n\n\n\nJust by looking at the DataFrame, it seems as if there are two levels of indexing for columns and one level of indexing for rows. However, let us confirm it with the nlevels attribute.\n\ngdp_per_capita.columns.nlevels\n\n2\n\n\nYes, there are 2 levels of indexing for columns.\n\ngdp_per_capita.index.nlevels\n\n1\n\n\nThere is one level of indexing for rows.\n\n\n8.1.4.2 \nSubset a DataFrame that selects the country, and the United Nations’ estimates of GDP per capita with the corresponding year.\n\ngdp_per_capita.loc[:,['Country/Territory','United Nations[6]']]\n\n\n\n\n\n\n\n\nCountry/Territory\nUnited Nations[6]\n\n\n\nCountry/Territory\nEstimate\nYear\n\n\n\n\n0\nLiechtenstein *\n180227\n2020\n\n\n1\nMonaco *\n173696\n2020\n\n\n2\nLuxembourg *\n117182\n2020\n\n\n3\nBermuda *\n123945\n2020\n\n\n4\nIreland *\n86251\n2020\n\n\n...\n...\n...\n...\n\n\n217\nMadagascar *\n470\n2020\n\n\n218\nCentral African Republic *\n481\n2020\n\n\n219\nSierra Leone *\n475\n2020\n\n\n220\nSouth Sudan *\n1421\n2020\n\n\n221\nBurundi *\n286\n2020\n\n\n\n\n222 rows × 3 columns\n\n\n\n\n\n8.1.4.3 \nSubset a DataFrame that selects only the World Bank and United Nations’ estimates of GDP per capita without the corresponding year or country.\n\ngdp_per_capita.loc[:,(['World Bank[5]','United Nations[6]'],'Estimate')]\n\n\n\n\n\n\n\n\nWorld Bank[5]\nUnited Nations[6]\n\n\n\nEstimate\nEstimate\n\n\n\n\n0\n169049\n180227\n\n\n1\n173688\n173696\n\n\n2\n135683\n117182\n\n\n3\n110870\n123945\n\n\n4\n99152\n86251\n\n\n...\n...\n...\n\n\n217\n515\n470\n\n\n218\n512\n481\n\n\n219\n516\n475\n\n\n220\n1120\n1421\n\n\n221\n237\n286\n\n\n\n\n222 rows × 2 columns\n\n\n\n\n\n8.1.4.4 \nSubset a DataFrame that selects the country and only the World Bank and United Nations’ estimates of GDP per capita without the corresponding year or country.\n\ngdp_per_capita.loc[:,[('Country/Territory','Country/Territory'),('United Nations[6]','Estimate'),('World Bank[5]','Estimate')]]\n\n\n\n\n\n\n\n\nCountry/Territory\nUnited Nations[6]\nWorld Bank[5]\n\n\n\nCountry/Territory\nEstimate\nEstimate\n\n\n\n\n0\nLiechtenstein *\n180227\n169049\n\n\n1\nMonaco *\n173696\n173688\n\n\n2\nLuxembourg *\n117182\n135683\n\n\n3\nBermuda *\n123945\n110870\n\n\n4\nIreland *\n86251\n99152\n\n\n...\n...\n...\n...\n\n\n217\nMadagascar *\n470\n515\n\n\n218\nCentral African Republic *\n481\n512\n\n\n219\nSierra Leone *\n475\n516\n\n\n220\nSouth Sudan *\n1421\n1120\n\n\n221\nBurundi *\n286\n237\n\n\n\n\n222 rows × 3 columns\n\n\n\n\n\n8.1.4.5 \nDrop all columns consisting of years. Use the level argument of the drop() method.\n\ngdp_per_capita = gdp_per_capita.drop(columns='Year',level=1)\ngdp_per_capita\n\n\n\n\n\n\n\n\nCountry/Territory\nUN Region\nIMF[4]\nWorld Bank[5]\nUnited Nations[6]\n\n\n\nCountry/Territory\nUN Region\nEstimate\nEstimate\nEstimate\n\n\n\n\n0\nLiechtenstein *\nEurope\n—\n169049\n180227\n\n\n1\nMonaco *\nEurope\n—\n173688\n173696\n\n\n2\nLuxembourg *\nEurope\n127673\n135683\n117182\n\n\n3\nBermuda *\nAmericas\n—\n110870\n123945\n\n\n4\nIreland *\nEurope\n102217\n99152\n86251\n\n\n...\n...\n...\n...\n...\n...\n\n\n217\nMadagascar *\nAfrica\n522\n515\n470\n\n\n218\nCentral African Republic *\nAfrica\n496\n512\n481\n\n\n219\nSierra Leone *\nAfrica\n494\n516\n475\n\n\n220\nSouth Sudan *\nAfrica\n328\n1120\n1421\n\n\n221\nBurundi *\nAfrica\n293\n237\n286\n\n\n\n\n222 rows × 5 columns\n\n\n\n\n\n8.1.4.6 \nIn the dataset obtained above, drop the inner level of the column labels. Use the droplevel() method.\n\ngdp_per_capita = gdp_per_capita.droplevel(1,axis=1)\ngdp_per_capita\n\n\n\n\n\n\n\n\nCountry/Territory\nUN Region\nIMF[4]\nWorld Bank[5]\nUnited Nations[6]\n\n\n\n\n0\nLiechtenstein *\nEurope\n—\n169049\n180227\n\n\n1\nMonaco *\nEurope\n—\n173688\n173696\n\n\n2\nLuxembourg *\nEurope\n127673\n135683\n117182\n\n\n3\nBermuda *\nAmericas\n—\n110870\n123945\n\n\n4\nIreland *\nEurope\n102217\n99152\n86251\n\n\n...\n...\n...\n...\n...\n...\n\n\n217\nMadagascar *\nAfrica\n522\n515\n470\n\n\n218\nCentral African Republic *\nAfrica\n496\n512\n481\n\n\n219\nSierra Leone *\nAfrica\n494\n516\n475\n\n\n220\nSouth Sudan *\nAfrica\n328\n1120\n1421\n\n\n221\nBurundi *\nAfrica\n293\n237\n286\n\n\n\n\n222 rows × 5 columns\n\n\n\n\n\n\n8.1.5 Practice exercise 2\nRecall problem 2(e) from assignment 3 on Pandas, where we needed to find the African country that is the closest to country \\(G\\) (Luxembourg) with regard to social indicators.\nWe will solve the question with the regular way in which we use single level of indexing (as you probably did during this assignment), and see if it is easier to do with hierarchical indexing.\nExecute the code below that we used to pre-process data to make it suitable for answering this question.\n\n#Pre-processing data - execute this code\nsocial_indicator = pd.read_csv(\"./Datasets/social_indicator.txt\",sep=\"\\t\",index_col = 0)\nsocial_indicator.geographic_location = social_indicator.geographic_location.apply(lambda x: 'Asia' if 'Asia' in x else 'Europe' if 'Europe' in x else 'Africa' if 'Africa' in x else x)\nsocial_indicator.rename(columns={'geographic_location':'continent'},inplace=True)\nsocial_indicator = social_indicator.sort_index(axis=1)\nsocial_indicator.drop(columns=['region','contraception'],inplace=True)\n\nBelow is the code to find the African country that is the closest to country \\(G\\) (Luxembourg) using single level of indexing. Your code in the assignment is probably similar to the one below:\n\n#Finding the index of the country G (Luxembourg) that has the maximum GDP per capita\ncountry_max_gdp_position = social_indicator.gdpPerCapita.argmax()\n\n#Scaling the social indicator dataset\nsocial_indicator_scaled = social_indicator.iloc[:,2:].apply(lambda x: (x-x.mean())/(x.std()))\n\n#Computing the Manhattan distances of all countries from country G (Luxembourg)\nmanhattan_distances = (social_indicator_scaled-social_indicator_scaled.iloc[country_max_gdp_position,:]).abs().sum(axis=1)\n\n#Finding the indices of African countries\nafrican_countries_indices = social_indicator.loc[social_indicator.continent=='Africa',:].index\n\n#Filtering the Manhattan distances of African countries from country G (Luxembourg)\nmanhattan_distances_African = manhattan_distances[african_countries_indices]\n\n#Finding the country among African countries that has the least Manhattan distance to country G (Luxembourg)\nsocial_indicator.loc[manhattan_distances_African.idxmin(),'country']\n\n'Reunion'\n\n\n\n8.1.5.1 \nUse the method set_index() to set continent and country as hierarchical indices of rows. Find the African country that is the closest to country \\(G\\) (Luxembourg) using this hierarchically indexed data. How many lines will be eliminated from the code above? Which lines will be eliminated?\nHint: Since continent and country are row indices, you don’t need to explicitly find:\n\nThe row index of country \\(G\\) (Luxembourg),\nThe row indices of African countries.\nThe Manhattan distances for African countries.\n\n\nsocial_indicator.set_index(['continent','country'],inplace = True)\nsocial_indicator_scaled = social_indicator.apply(lambda x: (x-x.mean())/(x.std()))\nmanhattan_distances = (social_indicator_scaled-social_indicator_scaled.loc[('Europe','Luxembourg'),:]).abs().sum(axis=1)\nmanhattan_distances['Africa'].idxmin()\n\n'Reunion'\n\n\nAs we have converted the columns continent and country to row indices, all the lines of code where we were keeping track of the index of country \\(G\\), African countries, and Manhattan distances of African countries are eliminated. Three lines of code are eliminated.\nHierarchical indexing relieves us from keeping track of indices, if we set indices that are relatable to our analysis.\n\n\n8.1.5.2 \nUse the Pandas DataFrame method mean() with the level argument to find the mean value of all social indicators for each continent.\n\nsocial_indicator.mean(level=0)\n\n\n\n\n\n\n\n\neconomicActivityFemale\neconomicActivityMale\ngdpPerCapita\nilliteracyFemale\nilliteracyMale\ninfantMortality\nlifeFemale\nlifeMale\ntotalfertilityrate\n\n\ncontinent\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsia\n41.592683\n79.282927\n27796.390244\n23.635951\n13.780878\n39.853659\n70.724390\n66.575610\n3.482927\n\n\nAfrica\n46.732258\n79.445161\n7127.483871\n52.907226\n33.673548\n77.967742\n56.841935\n53.367742\n4.889677\n\n\nOceania\n51.280000\n77.953333\n14525.666667\n9.666667\n6.585200\n23.666667\n72.406667\n67.813333\n3.509333\n\n\nNorth America\n45.238095\n77.166667\n18609.047619\n17.390286\n14.609905\n22.904762\n75.457143\n70.161905\n2.804286\n\n\nSouth America\n42.008333\n75.575000\n15925.916667\n9.991667\n6.750000\n34.750000\n72.691667\n66.975000\n2.872500\n\n\nEurope\n52.060000\n70.291429\n45438.200000\n2.308343\n1.413543\n10.571429\n77.757143\n70.374286\n1.581714\n\n\n\n\n\n\n\n\n\n\n8.1.6 Practice exercise 3\nLet us try to find the areas where NU students lack in diversity. Read survey_data_clean.csv. Use hierarchical indexing to classify the columns as follows:\nClassify the following variables as lifestyle:\n\nlifestyle = ['fav_alcohol', 'parties_per_month', 'smoke', 'weed','streaming_platforms', 'minutes_ex_per_week',\n       'sleep_hours_per_day', 'internet_hours_per_day', 'procrastinator', 'num_clubs','student_athlete','social_media']\n\nClassify the following variables as personality:\n\npersonality = ['introvert_extrovert', 'left_right_brained', 'personality_type', \n       'num_insta_followers', 'fav_sport','learning_style','dominant_hand']\n\nClassify the following variables as opinion:\n\nopinion = ['love_first_sight', 'expected_marriage_age',  'expected_starting_salary', 'how_happy', \n       'fav_number', 'fav_letter', 'fav_season',   'political_affliation', 'cant_change_math_ability',\n       'can_change_math_ability', 'math_is_genetic', 'much_effort_is_lack_of_talent']\n\nClassify the following variables as academic information:\n\nacademic_info = ['major', 'num_majors_minors',\n       'high_school_GPA', 'NU_GPA', 'school_year','AP_stats', 'used_python_before']\n\nClassify the following variables as demographics:\n\ndemographics = [ 'only_child','birth_month', \n       'living_location_on_campus', 'age', 'height', 'height_father',\n       'height_mother',  'childhood_in_US', 'gender', 'region_of_residence']\n\nWrite a function that finds the number of variables having outliers in a dataset. Apply the function to each of the 5 categories of variables in the dataset. Our hypothesis is that the category that has the maximum number of variables with outliers has the least amount of diversity. For continuous variables, use Tukey’s fences criterion to identify outliers. For categorical variables, consider levels having less than 1% observations as outliers. Assume that numeric variables that have more than 2 distinct values are continuous.\nSolution:\n\n#Using hierarchical indexing to classify columns\n\n#Reading data\nsurvey_data = pd.read_csv('./Datasets/survey_data_clean.csv')\n\n#Arranging columns in the order of categories\nsurvey_data_HI = survey_data[lifestyle+personality+opinion+academic_info+demographics]\n\n#Creating hierarchical indexing to classify columns\nsurvey_data_HI.columns=[['lifestyle']*len(lifestyle)+['personality']*len(personality)+['opinion']*len(opinion)+\\\n                       ['academic_info']*len(academic_info)+['demographics']*len(demographics),lifestyle+\\\n                        personality+opinion+academic_info+demographics]\n\n\n#Function to identify outliers based on Tukey's fences for continous variables and 1% criterion for categorical variables\ndef rem_outliers(x):\n    if ((len(x.value_counts())&gt;2) & (x.dtype!='O')):#continuous variable\n        q1 =x.quantile(0.25)\n        q3 = x.quantile(0.75)\n        intQ_range = q3-q1\n\n        #Tukey's fences\n        Lower_fence = q1 - 1.5*intQ_range\n        Upper_fence = q3 + 1.5*intQ_range\n\n        num_outliers = ((x&lt;Lower_fence) | (x&gt;Upper_fence)).sum()\n        if num_outliers&gt;0:\n            return True\n        return False\n    else:                                       #categorical variable\n        if np.min(x.value_counts()/len(x))&lt;0.01:\n            return True\n        return False\n\n\n#Number of variables containing outlier(s) in each category\nfor category in survey_data_HI.columns.get_level_values(0).unique():\n    print(\"Number of missing values for category \",category,\" = \",survey_data_HI[category].apply(rem_outliers).sum())\n\nNumber of missing values for category  lifestyle  =  7\nNumber of missing values for category  personality  =  2\nNumber of missing values for category  opinion  =  4\nNumber of missing values for category  academic_info  =  3\nNumber of missing values for category  demographics  =  4\n\n\nThe lifestyle category has the highest number of variables containing outlier(s). If the hypothesis is true, then NU students have the least diversity in their lifestyle, among all the categories.\nAlthough one may say that the lifestyle category has the the highest number of columns (as shown below), the proportion of columns having outlier(s) is also the highest for this category.\n\nfor category in survey_data_HI.columns.get_level_values(0).unique():\n    print(\"Number of columns in category \",category,\" = \",survey_data_HI[category].shape[1])\n\nNumber of columns in category  lifestyle  =  12\nNumber of columns in category  personality  =  7\nNumber of columns in category  opinion  =  12\nNumber of columns in category  academic_info  =  7\nNumber of columns in category  demographics  =  10\n\n\n\n\n8.1.7 Reshaping data\nApart from ease in subsetting data, hierarchical indexing also plays a role in reshaping data.\n\n8.1.7.1 unstack() (Pandas Series method)\nThe Pandas Series method unstack() pivots the desired level of row indices to columns, thereby creating a DataFrame. By default, the inner-most level of the row labels is pivoted.\n\n#Pivoting the inner-most Series row index to column labels\nseries_example_unstack = series_example.unstack()\nseries_example_unstack\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\nEnglish\nthese\nare\nenglish\nwords\nNaN\n\n\nFrench\nce\nsont\ndes\nfrançai\nmots\n\n\nSpanish\nestas\nson\npalabras\nen\nespañol\n\n\n\n\n\n\n\nWe can pivot the outer level of the row labels by specifying it in the level argument:\n\n#Pivoting the outer row indices to column labels\nseries_example_unstack = series_example.unstack(level=0)\nseries_example_unstack\n\n\n\n\n\n\n\n\nEnglish\nFrench\nSpanish\n\n\n\n\n1\nthese\nce\nestas\n\n\n2\nare\nsont\nson\n\n\n3\nenglish\ndes\npalabras\n\n\n4\nwords\nfrançai\nen\n\n\n5\nNaN\nmots\nespañol\n\n\n\n\n\n\n\n\n\n8.1.7.2 unstack() (Pandas DataFrame method)\nThe Pandas DataFrame method unstack() pivots the specified level of row indices to the new inner-most level of column labels. By default, the inner-most level of the row labels is pivoted.\n\n#Pivoting the inner level of row labels to the inner-most level of column labels\ndf_example.unstack()\n\n\n\n\n\n\n\nState\nIllinois\nCalifornia\n\n\nCity\nEvanston\nChicago\nSan Francisco\nLos Angeles\n\n\nStatistic\nArea (mile-sq)\nElevation (feet)\nPopulation\nUnemployement (%)\nArea (mile-sq)\nElevation (feet)\nPopulation\nUnemployement (%)\nArea (mile-sq)\nElevation (feet)\nPopulation\nUnemployement (%)\nArea (mile-sq)\nElevation (feet)\nPopulation\nUnemployement (%)\n\n\nInformation type\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDemographics\nNaN\nNaN\n771517.0\n4.2\nNaN\nNaN\n2697000.0\n5.6\nNaN\nNaN\n815201.0\n2.8\nNaN\nNaN\n3849000.0\n4.6\n\n\nGeography\n7.8\n6749.0\nNaN\nNaN\n234.5\n597.0\nNaN\nNaN\n46.9\n52.0\nNaN\nNaN\n502.0\n305.0\nNaN\nNaN\n\n\n\n\n\n\n\nAs with Series, we can pivot the outer level of the row labels by specifying it in the level argument:\n\n#Pivoting the outer level (level = 0) of row labels to the inner-most level of column labels\ndf_example.unstack(level=0)\n\n\n\n\n\n\n\nState\nIllinois\nCalifornia\n\n\nCity\nEvanston\nChicago\nSan Francisco\nLos Angeles\n\n\nInformation type\nDemographics\nGeography\nDemographics\nGeography\nDemographics\nGeography\nDemographics\nGeography\n\n\nStatistic\n\n\n\n\n\n\n\n\n\n\n\n\nArea (mile-sq)\nNaN\n7.8\nNaN\n234.5\nNaN\n46.9\nNaN\n502.0\n\n\nElevation (feet)\nNaN\n6749.0\nNaN\n597.0\nNaN\n52.0\nNaN\n305.0\n\n\nPopulation\n771517.0\nNaN\n2697000.0\nNaN\n815201.0\nNaN\n3849000.0\nNaN\n\n\nUnemployement (%)\n4.2\nNaN\n5.6\nNaN\n2.8\nNaN\n4.6\nNaN\n\n\n\n\n\n\n\n\n\n8.1.7.3 stack()\nThe inverse of unstack() is the stack() method, which creates the inner-most level of row indices by pivoting the column labels of the prescribed level.\nNote that if the column labels have only one level, we don’t need to specify a level.\n\n#Stacking the columns of a DataFrame\nseries_example_unstack.stack()\n\nEnglish  1       these\n         2         are\n         3     english\n         4       words\nFrench   1          ce\n         2        sont\n         3         des\n         4     françai\n         5        mots\nSpanish  1       estas\n         2         son\n         3    palabras\n         4          en\n         5     español\ndtype: object\n\n\nHowever, if the columns have multiple levels, we can specify the level to stack as the inner-most row level. By default, the inner-most column level is stacked.\n\n#Stacking the inner-most column labels inner-most row indices\ndf_example.stack()\n\n\n\n\n\n\n\n\n\nState\nCalifornia\nIllinois\n\n\nInformation type\nStatistic\nCity\n\n\n\n\n\n\nDemographics\nPopulation\nChicago\nNaN\n2697000.0\n\n\nEvanston\nNaN\n771517.0\n\n\nLos Angeles\n3849000.0\nNaN\n\n\nSan Francisco\n815201.0\nNaN\n\n\nUnemployement (%)\nChicago\nNaN\n5.6\n\n\nEvanston\nNaN\n4.2\n\n\nLos Angeles\n4.6\nNaN\n\n\nSan Francisco\n2.8\nNaN\n\n\nGeography\nArea (mile-sq)\nChicago\nNaN\n234.5\n\n\nEvanston\nNaN\n7.8\n\n\nLos Angeles\n502.0\nNaN\n\n\nSan Francisco\n46.9\nNaN\n\n\nElevation (feet)\nChicago\nNaN\n597.0\n\n\nEvanston\nNaN\n6749.0\n\n\nLos Angeles\n305.0\nNaN\n\n\nSan Francisco\n52.0\nNaN\n\n\n\n\n\n\n\n\n#Stacking the outer column labels inner-most row indices\ndf_example.stack(level=0)\n\n\n\n\n\n\n\n\n\nCity\nChicago\nEvanston\nLos Angeles\nSan Francisco\n\n\nInformation type\nStatistic\nState\n\n\n\n\n\n\n\n\nDemographics\nPopulation\nCalifornia\nNaN\nNaN\n3849000.0\n815201.0\n\n\nIllinois\n2697000.0\n771517.0\nNaN\nNaN\n\n\nUnemployement (%)\nCalifornia\nNaN\nNaN\n4.6\n2.8\n\n\nIllinois\n5.6\n4.2\nNaN\nNaN\n\n\nGeography\nArea (mile-sq)\nCalifornia\nNaN\nNaN\n502.0\n46.9\n\n\nIllinois\n234.5\n7.8\nNaN\nNaN\n\n\nElevation (feet)\nCalifornia\nNaN\nNaN\n305.0\n52.0\n\n\nIllinois\n597.0\n6749.0\nNaN\nNaN"
  },
  {
    "objectID": "Data wrangling.html#merging-data",
    "href": "Data wrangling.html#merging-data",
    "title": "8  Data wrangling",
    "section": "8.2 Merging data",
    "text": "8.2 Merging data\nThe Pandas DataFrame method merge() uses columns defined as key column(s) to merge two datasets. In case the key column(s) are not defined, the overlapping column(s) are considered as the key columns.\n\n8.2.1 Join types\nWhen a dataset is merged with another based on key column(s), one of the following four types of join will occur depending on the repetition of the values of the key(s) in the datasets.\n\nOne-to-one, (ii) Many-to-one, (iii) One-to-Many, and (iv) Many-to-many\n\nThe type of join may sometimes determine the number of rows to be obtained in the merged dataset. If we don’t get the expected number of rows in the merged dataset, an investigation of the datsets may be neccessary to identify and resolve the issue. There may be several possible issues, for example, the dataset may not be arranged in a way that we have assumed it to be arranged.\nWe’ll use toy datasets to understand the above types of joins. The .csv files with the prefix student consist of the names of a few students along with their majors, and the files with the prefix skills consist of the names of majors along with the skills imparted by the respective majors.\n\ndata_student = pd.read_csv('./Datasets/student_one.csv')\ndata_skill = pd.read_csv('./Datasets/skills_one.csv')\n\n\n8.2.1.1 One-to-one join\nEach row in one dataset is linked (or related) to a single row in another dataset based on the key column(s).\n\ndata_student\n\n\n\n\n\n\n\n\nStudent\nMajor\n\n\n\n\n0\nKitana\nStatistics\n\n\n1\nJax\nComputer Science\n\n\n2\nSonya\nMaterial Science\n\n\n3\nJohnny\nMusic\n\n\n\n\n\n\n\n\ndata_skill\n\n\n\n\n\n\n\n\nMajor\nSkills\n\n\n\n\n0\nStatistics\nInference\n\n\n1\nComputer Science\nMachine learning\n\n\n2\nMaterial Science\nStructure prediction\n\n\n3\nMusic\nOpera\n\n\n\n\n\n\n\n\npd.merge(data_student,data_skill)\n\n\n\n\n\n\n\n\nStudent\nMajor\nSkills\n\n\n\n\n0\nKitana\nStatistics\nInference\n\n\n1\nJax\nComputer Science\nMachine learning\n\n\n2\nSonya\nMaterial Science\nStructure prediction\n\n\n3\nJohnny\nMusic\nOpera\n\n\n\n\n\n\n\n\n\n8.2.1.2 Many-to-one join\nOne or more rows in one dataset is linked (or related) to a single row in another dataset based on the key column(s).\n\ndata_student = pd.read_csv('./Datasets/student_many.csv')\ndata_skill = pd.read_csv('./Datasets/skills_one.csv')\n\n\ndata_student\n\n\n\n\n\n\n\n\nStudent\nMajor\n\n\n\n\n0\nKitana\nStatistics\n\n\n1\nKitana\nComputer Science\n\n\n2\nJax\nComputer Science\n\n\n3\nSonya\nMaterial Science\n\n\n4\nJohnny\nMusic\n\n\n5\nJohnny\nStatistics\n\n\n\n\n\n\n\n\ndata_skill\n\n\n\n\n\n\n\n\nMajor\nSkills\n\n\n\n\n0\nStatistics\nInference\n\n\n1\nComputer Science\nMachine learning\n\n\n2\nMaterial Science\nStructure prediction\n\n\n3\nMusic\nOpera\n\n\n\n\n\n\n\n\npd.merge(data_student,data_skill)\n\n\n\n\n\n\n\n\nStudent\nMajor\nSkills\n\n\n\n\n0\nKitana\nStatistics\nInference\n\n\n1\nJohnny\nStatistics\nInference\n\n\n2\nKitana\nComputer Science\nMachine learning\n\n\n3\nJax\nComputer Science\nMachine learning\n\n\n4\nSonya\nMaterial Science\nStructure prediction\n\n\n5\nJohnny\nMusic\nOpera\n\n\n\n\n\n\n\n\n\n8.2.1.3 One-to-many join\nEach row in one dataset is linked (or related) to one, or more rows in another dataset based on the key column(s).\n\ndata_student = pd.read_csv('./Datasets/student_one.csv')\ndata_skill = pd.read_csv('./Datasets/skills_many.csv')\n\n\ndata_student\n\n\n\n\n\n\n\n\nStudent\nMajor\n\n\n\n\n0\nKitana\nStatistics\n\n\n1\nJax\nComputer Science\n\n\n2\nSonya\nMaterial Science\n\n\n3\nJohnny\nMusic\n\n\n\n\n\n\n\n\ndata_skill\n\n\n\n\n\n\n\n\nMajor\nSkills\n\n\n\n\n0\nStatistics\nInference\n\n\n1\nStatistics\nModeling\n\n\n2\nComputer Science\nMachine learning\n\n\n3\nComputer Science\nComputing\n\n\n4\nMaterial Science\nStructure prediction\n\n\n5\nMusic\nOpera\n\n\n6\nMusic\nPop\n\n\n7\nMusic\nClassical\n\n\n\n\n\n\n\n\npd.merge(data_student,data_skill)\n\n\n\n\n\n\n\n\nStudent\nMajor\nSkills\n\n\n\n\n0\nKitana\nStatistics\nInference\n\n\n1\nKitana\nStatistics\nModeling\n\n\n2\nJax\nComputer Science\nMachine learning\n\n\n3\nJax\nComputer Science\nComputing\n\n\n4\nSonya\nMaterial Science\nStructure prediction\n\n\n5\nJohnny\nMusic\nOpera\n\n\n6\nJohnny\nMusic\nPop\n\n\n7\nJohnny\nMusic\nClassical\n\n\n\n\n\n\n\n\n\n8.2.1.4 Many-to-many join\nOne, or more, rows in one dataset is linked (or related) to one, or more, rows in another dataset using the key column(s).\n\ndata_student = pd.read_csv('./Datasets/student_many.csv')\ndata_skill = pd.read_csv('./Datasets/skills_many.csv')\n\n\ndata_student\n\n\n\n\n\n\n\n\nStudent\nMajor\n\n\n\n\n0\nKitana\nStatistics\n\n\n1\nKitana\nComputer Science\n\n\n2\nJax\nComputer Science\n\n\n3\nSonya\nMaterial Science\n\n\n4\nJohnny\nMusic\n\n\n5\nJohnny\nStatistics\n\n\n\n\n\n\n\n\ndata_skill\n\n\n\n\n\n\n\n\nMajor\nSkills\n\n\n\n\n0\nStatistics\nInference\n\n\n1\nStatistics\nModeling\n\n\n2\nComputer Science\nMachine learning\n\n\n3\nComputer Science\nComputing\n\n\n4\nMaterial Science\nStructure prediction\n\n\n5\nMusic\nOpera\n\n\n6\nMusic\nPop\n\n\n7\nMusic\nClassical\n\n\n\n\n\n\n\n\npd.merge(data_student,data_skill)\n\n\n\n\n\n\n\n\nStudent\nMajor\nSkills\n\n\n\n\n0\nKitana\nStatistics\nInference\n\n\n1\nKitana\nStatistics\nModeling\n\n\n2\nJohnny\nStatistics\nInference\n\n\n3\nJohnny\nStatistics\nModeling\n\n\n4\nKitana\nComputer Science\nMachine learning\n\n\n5\nKitana\nComputer Science\nComputing\n\n\n6\nJax\nComputer Science\nMachine learning\n\n\n7\nJax\nComputer Science\nComputing\n\n\n8\nSonya\nMaterial Science\nStructure prediction\n\n\n9\nJohnny\nMusic\nOpera\n\n\n10\nJohnny\nMusic\nPop\n\n\n11\nJohnny\nMusic\nClassical\n\n\n\n\n\n\n\nNote that there are two ‘Statistics’ rows in data_student, and two ‘Statistics’ rows in data_skill, resulting in 2x2 = 4 ‘Statistics’ rows in the merged data. The same is true for the ‘Computer Science’ Major.\n\n\n\n8.2.2 Join types with how argument\nThe above mentioned types of join (one-to-one, many-to-one, etc.) occur depening on the structure of the datasets being merged. We don’t have control over the type of join. However, we can control how the joins are occurring. We can merge (or join) two datasets in one of the following four ways:\n\ninner join, (ii) left join, (iii) right join, (iv) outer join\n\n\n8.2.2.1 inner join\nThis is the join that occurs by default, i.e., without specifying the how argument in the merge() function. In inner join, only those observations are merged that have the same value(s) in the key column(s) of both the datasets.\n\ndata_student = pd.read_csv('./Datasets/student_how.csv')\ndata_skill = pd.read_csv('./Datasets/skills_how.csv')\n\n\ndata_student\n\n\n\n\n\n\n\n\nStudent\nMajor\n\n\n\n\n0\nKitana\nStatistics\n\n\n1\nJax\nComputer Science\n\n\n2\nSonya\nMaterial Science\n\n\n\n\n\n\n\n\ndata_skill\n\n\n\n\n\n\n\n\nMajor\nSkills\n\n\n\n\n0\nStatistics\nInference\n\n\n1\nComputer Science\nMachine learning\n\n\n2\nMusic\nOpera\n\n\n\n\n\n\n\n\npd.merge(data_student,data_skill)\n\n\n\n\n\n\n\n\nStudent\nMajor\nSkills\n\n\n\n\n0\nKitana\nStatistics\nInference\n\n\n1\nJax\nComputer Science\nMachine learning\n\n\n\n\n\n\n\nWhen you may use inner join? You should use inner join when you cannot carry out the analysis unless the observation corresponding to the key column(s) is present in both the tables.\nExample: Suppose you wish to analyze the association between vaccinations and covid infection rate based on country-level data. In one of the datasets, you have the infection rate for each country, while in the other one you have the number of vaccinations in each country. The countries which have either the vaccination or the infection rate missing, cannot help analyze the association. In such as case you may be interested only in countries that have values for both the variables. Thus, you will use inner join to discard the countries with either value missing.\n\n\n8.2.2.2 left join\nIn left join, the merged dataset will have all the rows of the dataset that is specified first in the merge() function. Only those observations of the other dataset will be merged whose value(s) in the key column(s) exist in the dataset specified first in the merge() function.\n\npd.merge(data_student,data_skill,how='left')\n\n\n\n\n\n\n\n\nStudent\nMajor\nSkills\n\n\n\n\n0\nKitana\nStatistics\nInference\n\n\n1\nJax\nComputer Science\nMachine learning\n\n\n2\nSonya\nMaterial Science\nNaN\n\n\n\n\n\n\n\nWhen you may use left join? You should use left join when the primary variable(s) of interest are present in the one of the datasets, and whose missing values cannot be imputed. The variable(s) in the other dataset may not be as important or it may be possible to reasonably impute their values, if missing corresponding to the observation in the primary dataset.\nExamples:\n\nSuppose you wish to analyze the association between the covid infection rate and the government effectiveness score (a metric used to determine the effectiveness of the government in implementing policies, upholding law and order etc.) based on the data of all countries. Let us say that one of the datasets contains the covid infection rate, while the other one contains the government effectiveness score for each country. If the infection rate for a country is missing, it might be hard to impute. However, the government effectiveness score may be easier to impute based on GDP per capita, crime rate etc. - information that is easily available online. In such a case, you may wish to use a left join where you keep all the countries for which the infection rate is known.\nSuppose you wish to analyze the association between demographics such as age, income etc. and the amount of credit card spend. Let us say one of the datasets contains the demographic information of each customer, while the other one contains the credit card spend for the customers who made at least one purchase. In such as case, you may want to do a left join as customers not making any purchase might be absent in the card spend data. Their spend can be imputed as zero after merging the datasets.\n\n\n\n8.2.2.3 right join\nIn right join, the merged dataset will have all the rows of the dataset that is specified second in the merge() function. Only those observations of the other dataset will be merged whose value(s) in the key column(s) exist in the dataset specified second in the merge() function.\n\npd.merge(data_student,data_skill,how='right')\n\n\n\n\n\n\n\n\nStudent\nMajor\nSkills\n\n\n\n\n0\nKitana\nStatistics\nInference\n\n\n1\nJax\nComputer Science\nMachine learning\n\n\n2\nNaN\nMusic\nOpera\n\n\n\n\n\n\n\nWhen you may use right join? You can always use a left join instead of a right join. Their purpose is the same.\n\n\n8.2.2.4 outer join\nIn outer join, the merged dataset will have all the rows of both the datasets being merged.\n\npd.merge(data_student,data_skill,how='outer')\n\n\n\n\n\n\n\n\nStudent\nMajor\nSkills\n\n\n\n\n0\nKitana\nStatistics\nInference\n\n\n1\nJax\nComputer Science\nMachine learning\n\n\n2\nSonya\nMaterial Science\nNaN\n\n\n3\nNaN\nMusic\nOpera\n\n\n\n\n\n\n\nWhen you may use outer join? You should use an outer join when you cannot afford to lose data present in either of the tables. All the other joins may result in loss of data.\nExample: Suppose I took two course surveys for this course. If I need to analyze student sentiment during the course, I will take an outer join of both the surveys. Assume that each survey is a dataset, where each row corresponds to a unique student. Even if a student has answered one of the two surverys, it will be indicative of the sentiment, and will be useful to keep in the merged dataset."
  },
  {
    "objectID": "Data wrangling.html#concatenating-datasets",
    "href": "Data wrangling.html#concatenating-datasets",
    "title": "8  Data wrangling",
    "section": "8.3 Concatenating datasets",
    "text": "8.3 Concatenating datasets\nThe Pandas DataFrame method concat() is used to stack datasets along an axis. The method is similar to NumPy’s concatenate() method.\nExample: You are given the life expectancy data of each continent as a separate *.csv file. Visualize the change of life expectancy over time for different continents.\n\ndata_asia = pd.read_csv('./Datasets/gdp_lifeExpec_Asia.csv')\ndata_europe = pd.read_csv('./Datasets/gdp_lifeExpec_Europe.csv')\ndata_africa = pd.read_csv('./Datasets/gdp_lifeExpec_Africa.csv')\ndata_oceania = pd.read_csv('./Datasets/gdp_lifeExpec_Oceania.csv')\ndata_americas = pd.read_csv('./Datasets/gdp_lifeExpec_Americas.csv')\n\n\n#Appending all the data files, i.e., stacking them on top of each other\ndata_all_continents = pd.concat([data_asia,data_europe,data_africa,data_oceania,data_americas],keys = ['Asia','Europe','Africa','Oceania','Americas'])\ndata_all_continents\n\n\n\n\n\n\n\n\n\ncountry\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAsia\n0\nAfghanistan\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\n1972\n36.088\n13079460\n739.981106\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nAmericas\n295\nVenezuela\n1987\n70.190\n17910182\n9883.584648\n\n\n296\nVenezuela\n1992\n71.150\n20265563\n10733.926310\n\n\n297\nVenezuela\n1997\n72.146\n22374398\n10165.495180\n\n\n298\nVenezuela\n2002\n72.766\n24287670\n8605.047831\n\n\n299\nVenezuela\n2007\n73.747\n26084662\n11415.805690\n\n\n\n\n1704 rows × 5 columns\n\n\n\nLet’s have the continent as a column as we need to use that in the visualization.\n\ndata_all_continents.reset_index(inplace = True)\n\n\ndata_all_continents.head()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\ncountry\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAsia\n0\nAfghanistan\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAsia\n1\nAfghanistan\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAsia\n2\nAfghanistan\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAsia\n3\nAfghanistan\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAsia\n4\nAfghanistan\n1972\n36.088\n13079460\n739.981106\n\n\n\n\n\n\n\n\ndata_all_continents.drop(columns = 'level_1',inplace = True)\ndata_all_continents.rename(columns = {'level_0':'continent'},inplace = True)\ndata_all_continents.head()\n\n\n\n\n\n\n\n\ncontinent\ncountry\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAsia\nAfghanistan\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAsia\nAfghanistan\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAsia\nAfghanistan\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAsia\nAfghanistan\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAsia\nAfghanistan\n1972\n36.088\n13079460\n739.981106\n\n\n\n\n\n\n\n\n#change of life expectancy over time for different continents\na = sns.FacetGrid(data_all_continents,col = 'continent',col_wrap = 3,height = 4.5,aspect = 1)#height = 3,aspect = 0.8)\na.map(sns.lineplot,'year','lifeExp')\na.add_legend()\n\n\n\n\nIn the above example, datasets were appended (or stacked on top of each other).\nDatasets can also be concatenated side-by-side (by providing the argument axis = 1 with the concat() function) as we saw with the merge function.\n\n8.3.1 Practice exercise 4\nRead the documentations of the Pandas DataFrame methods merge() and concat(), and identify the differences. Mention examples when you can use (i) either, (ii) only concat(), (iii) only merge()\nSolution:\n\nIf we need to merge datasets using row indices, we can use either function.\nIf we need to stack datasets one on top of the other, we can only use concat()\nIf we need to merge datasets using overlapping columns we can only use merge()"
  },
  {
    "objectID": "Data wrangling.html#reshaping-data-1",
    "href": "Data wrangling.html#reshaping-data-1",
    "title": "8  Data wrangling",
    "section": "8.4 Reshaping data",
    "text": "8.4 Reshaping data\nData often needs to be re-arranged to ease analysis.\n\n8.4.1 Pivoting “long” to “wide” format\n\npivot()\nThis function helps re-arrange data from the ‘long’ form to a ‘wide’ form.\nExample: Let us consider the dataset data_all_continents obtained in the previous section after concatenating the data of all the continents.\n\ndata_all_continents.head()\n\n\n\n\n\n\n\n\ncontinent\ncountry\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAsia\nAfghanistan\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAsia\nAfghanistan\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAsia\nAfghanistan\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAsia\nAfghanistan\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAsia\nAfghanistan\n1972\n36.088\n13079460\n739.981106\n\n\n\n\n\n\n\n\n\n8.4.1.1 Pivoting a single column\nFor visualizing life expectancy in 2007 against life expectancy in 1957, we will need to filter the data, and then make the plot. Everytime that we need to compare a metric for a year against another year, we will need to filter the data.\nIf we need to often compare metrics of a year against another year, it will be easier to have each year as a separate column, instead of having all years in a single column.\nAs we are increasing the number of columns and decreasing the number of rows, we are re-arranging the data from long-form to wide-form.\n\ndata_wide = data_all_continents.pivot(index = ['continent','country'],columns = 'year',values = 'lifeExp')\n\n\ndata_wide.head()\n\n\n\n\n\n\n\n\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n43.077\n45.685\n48.303\n51.407\n54.518\n58.014\n61.368\n65.799\n67.744\n69.152\n70.994\n72.301\n\n\nAngola\n30.015\n31.999\n34.000\n35.985\n37.928\n39.483\n39.942\n39.906\n40.647\n40.963\n41.003\n42.731\n\n\nBenin\n38.223\n40.358\n42.618\n44.885\n47.014\n49.190\n50.904\n52.337\n53.919\n54.777\n54.406\n56.728\n\n\nBotswana\n47.622\n49.618\n51.520\n53.298\n56.024\n59.319\n61.484\n63.622\n62.745\n52.556\n46.634\n50.728\n\n\nBurkina Faso\n31.975\n34.906\n37.814\n40.697\n43.591\n46.137\n48.122\n49.557\n50.260\n50.324\n50.650\n52.295\n\n\n\n\n\n\n\nWith values of year as columns, it is easy to compare any metric for different years.\n\n#visualizing the change in life expectancy of all countries in 2007 as compared to that in 1957, i.e., the overall change in life expectancy in 50 years. \nsns.scatterplot(data = data_wide, x = 1957,y=2007,hue = 'continent')\nsns.lineplot(data = data_wide, x = 1957,y = 1957)\n\n&lt;AxesSubplot:xlabel='1957', ylabel='2007'&gt;\n\n\n\n\n\nObserve that for some African countries, the life expectancy has decreased after 50 years. It is worth investigating these countries to identify factors associated with the decrease.\n\n\n8.4.1.2 Pivoting multiple columns\nIn the above transformation, we retained only lifeExp in the ‘wide’ dataset. Suppose, we are also interested in visualizing GDP per capita of countries in one year against another year. In that case, we must have gdpPercap in the ’wide’-form data as well.\nLet us create a dataset named as data_wide_lifeExp_gdpPercap that will contain both lifeExp and gdpPercap for each year in a separate column. We will specify the columns to pivot in the values argument of the pivot() function.\n\ndata_wide_lifeExp_gdpPercap = data_all_continents.pivot(index = ['continent','country'],columns = 'year',values = ['lifeExp','gdpPercap'])\ndata_wide_lifeExp_gdpPercap.head()\n\n\n\n\n\n\n\n\n\nlifeExp\n...\ngdpPercap\n\n\n\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n...\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n43.077\n45.685\n48.303\n51.407\n54.518\n58.014\n61.368\n65.799\n67.744\n69.152\n...\n2550.816880\n3246.991771\n4182.663766\n4910.416756\n5745.160213\n5681.358539\n5023.216647\n4797.295051\n5288.040382\n6223.367465\n\n\nAngola\n30.015\n31.999\n34.000\n35.985\n37.928\n39.483\n39.942\n39.906\n40.647\n40.963\n...\n4269.276742\n5522.776375\n5473.288005\n3008.647355\n2756.953672\n2430.208311\n2627.845685\n2277.140884\n2773.287312\n4797.231267\n\n\nBenin\n38.223\n40.358\n42.618\n44.885\n47.014\n49.190\n50.904\n52.337\n53.919\n54.777\n...\n949.499064\n1035.831411\n1085.796879\n1029.161251\n1277.897616\n1225.856010\n1191.207681\n1232.975292\n1372.877931\n1441.284873\n\n\nBotswana\n47.622\n49.618\n51.520\n53.298\n56.024\n59.319\n61.484\n63.622\n62.745\n52.556\n...\n983.653976\n1214.709294\n2263.611114\n3214.857818\n4551.142150\n6205.883850\n7954.111645\n8647.142313\n11003.605080\n12569.851770\n\n\nBurkina Faso\n31.975\n34.906\n37.814\n40.697\n43.591\n46.137\n48.122\n49.557\n50.260\n50.324\n...\n722.512021\n794.826560\n854.735976\n743.387037\n807.198586\n912.063142\n931.752773\n946.294962\n1037.645221\n1217.032994\n\n\n\n\n5 rows × 24 columns\n\n\n\nThe metric for each year is now in a separate column, and can be visualized directly. Note that re-arranging the dataset from the ‘long’-form to ‘wide-form’ leads to hierarchical indexing of columns when multiple ‘values’ need to be re-arranged. In this case, the multiple ‘values’ that need to be re-arranged are lifeExp and gdpPercap.\n\n\n\n8.4.2 Melting “wide” to “long” format\n\nmelt()\nThis function is used to re-arrange the dataset from the ‘wide’ form to the ‘long’ form.\n\n\n8.4.2.1 Melting columns with a single type of value\nLet us consider data_wide created in the previous section.\n\ndata_wide.head()\n\n\n\n\n\n\n\n\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n43.077\n45.685\n48.303\n51.407\n54.518\n58.014\n61.368\n65.799\n67.744\n69.152\n70.994\n72.301\n\n\nAngola\n30.015\n31.999\n34.000\n35.985\n37.928\n39.483\n39.942\n39.906\n40.647\n40.963\n41.003\n42.731\n\n\nBenin\n38.223\n40.358\n42.618\n44.885\n47.014\n49.190\n50.904\n52.337\n53.919\n54.777\n54.406\n56.728\n\n\nBotswana\n47.622\n49.618\n51.520\n53.298\n56.024\n59.319\n61.484\n63.622\n62.745\n52.556\n46.634\n50.728\n\n\nBurkina Faso\n31.975\n34.906\n37.814\n40.697\n43.591\n46.137\n48.122\n49.557\n50.260\n50.324\n50.650\n52.295\n\n\n\n\n\n\n\nSuppose, we wish to visualize the change of life expectancy over time for different continents, as we did in section 8.3. For plotting lifeExp against year, all the years must be in a single column. Thus, we need to melt the columns of data_wide to a single column and call it year.\nBut before melting the columns in the above dataset, we will convert continent to a column, as we need to make subplots based on continent.\nThe Pandas DataFrame method reset_index() can be used to remove one or more levels of indexing from the DataFrame.\n\n#Making 'continent' a column instead of row-index at level 0\ndata_wide.reset_index(inplace=True,level=0)\ndata_wide.head()\n\n\n\n\n\n\n\nyear\ncontinent\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgeria\nAfrica\n43.077\n45.685\n48.303\n51.407\n54.518\n58.014\n61.368\n65.799\n67.744\n69.152\n70.994\n72.301\n\n\nAngola\nAfrica\n30.015\n31.999\n34.000\n35.985\n37.928\n39.483\n39.942\n39.906\n40.647\n40.963\n41.003\n42.731\n\n\nBenin\nAfrica\n38.223\n40.358\n42.618\n44.885\n47.014\n49.190\n50.904\n52.337\n53.919\n54.777\n54.406\n56.728\n\n\nBotswana\nAfrica\n47.622\n49.618\n51.520\n53.298\n56.024\n59.319\n61.484\n63.622\n62.745\n52.556\n46.634\n50.728\n\n\nBurkina Faso\nAfrica\n31.975\n34.906\n37.814\n40.697\n43.591\n46.137\n48.122\n49.557\n50.260\n50.324\n50.650\n52.295\n\n\n\n\n\n\n\n\ndata_melted=pd.melt(data_wide,id_vars = ['continent'],var_name = 'Year',value_name = 'LifeExp')\ndata_melted.head()\n\n\n\n\n\n\n\n\ncontinent\nYear\nLifeExp\n\n\n\n\n0\nAfrica\n1952\n43.077\n\n\n1\nAfrica\n1952\n30.015\n\n\n2\nAfrica\n1952\n38.223\n\n\n3\nAfrica\n1952\n47.622\n\n\n4\nAfrica\n1952\n31.975\n\n\n\n\n\n\n\nWith the above DataFrame, we can visualize the mean life expectancy against year separately for each continent.\nIf we wish to have country also in the above data, we can keep it while resetting the index:\n\n#Creating 'data_wide' again\ndata_wide = data_all_continents.pivot(index = ['continent','country'],columns = 'year',values = 'lifeExp')\n\n#Resetting the row-indices to default values\ndata_wide.reset_index(inplace=True)\ndata_wide.head()\n\n\n\n\n\n\n\nyear\ncontinent\ncountry\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\n\n\n0\nAfrica\nAlgeria\n43.077\n45.685\n48.303\n51.407\n54.518\n58.014\n61.368\n65.799\n67.744\n69.152\n70.994\n72.301\n\n\n1\nAfrica\nAngola\n30.015\n31.999\n34.000\n35.985\n37.928\n39.483\n39.942\n39.906\n40.647\n40.963\n41.003\n42.731\n\n\n2\nAfrica\nBenin\n38.223\n40.358\n42.618\n44.885\n47.014\n49.190\n50.904\n52.337\n53.919\n54.777\n54.406\n56.728\n\n\n3\nAfrica\nBotswana\n47.622\n49.618\n51.520\n53.298\n56.024\n59.319\n61.484\n63.622\n62.745\n52.556\n46.634\n50.728\n\n\n4\nAfrica\nBurkina Faso\n31.975\n34.906\n37.814\n40.697\n43.591\n46.137\n48.122\n49.557\n50.260\n50.324\n50.650\n52.295\n\n\n\n\n\n\n\n\n#Melting the 'year' column\ndata_melted=pd.melt(data_wide,id_vars = ['continent','country'],var_name = 'Year',value_name = 'LifeExp')\ndata_melted.head()\n\n\n\n\n\n\n\n\ncontinent\ncountry\nYear\nLifeExp\n\n\n\n\n0\nAfrica\nAlgeria\n1952\n43.077\n\n\n1\nAfrica\nAngola\n1952\n30.015\n\n\n2\nAfrica\nBenin\n1952\n38.223\n\n\n3\nAfrica\nBotswana\n1952\n47.622\n\n\n4\nAfrica\nBurkina Faso\n1952\n31.975\n\n\n\n\n\n\n\n\n\n8.4.2.2 Melting columns with multiple types of values\nConsider the dataset created in Section 8.4.1.2. It has two types of values - lifeExp and gdpPercapita, which are the column labels at the outer level. The melt() function will melt all the years of data into a single column. However, it will create another column based on the outer level column labels - lifeExp and gdpPercapita to distinguish between these two types of values. Here, we see that the function melt() internally uses hierarchical indexing to handle the transformation of multiple types of columns.\n\ndata_melt = pd.melt(data_wide_lifeExp_gdpPercap.reset_index(),id_vars = ['continent','country'],var_name = ['Metric','year'])\ndata_melt.head()\n\n\n\n\n\n\n\n\ncontinent\ncountry\nMetric\nyear\nvalue\n\n\n\n\n0\nAfrica\nAlgeria\nlifeExp\n1952\n43.077\n\n\n1\nAfrica\nAngola\nlifeExp\n1952\n30.015\n\n\n2\nAfrica\nBenin\nlifeExp\n1952\n38.223\n\n\n3\nAfrica\nBotswana\nlifeExp\n1952\n47.622\n\n\n4\nAfrica\nBurkina Faso\nlifeExp\n1952\n31.975\n\n\n\n\n\n\n\nAlthough the data above is in ‘long’-form, it is not quiet in its original format, as in data_all_continents. We need to pivot again by Metric to have two separate columns of gdpPercap and lifeExp.\n\ndata_restore = data_melt.pivot(index = ['continent','country','year'],columns = 'Metric')\ndata_restore.head()\n\n\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\nMetric\ngdpPercap\nlifeExp\n\n\ncontinent\ncountry\nyear\n\n\n\n\n\n\nAfrica\nAlgeria\n1952\n2449.008185\n43.077\n\n\n1957\n3013.976023\n45.685\n\n\n1962\n2550.816880\n48.303\n\n\n1967\n3246.991771\n51.407\n\n\n1972\n4182.663766\n54.518\n\n\n\n\n\n\n\nNow, we can convert the row indices of continent and country to columns to restore the dataset to the same form as data_all_continents.\n\ndata_restore.reset_index(inplace = True)\ndata_restore.head()\n\n\n\n\n\n\n\n\ncontinent\ncountry\nyear\nvalue\n\n\nMetric\n\n\n\ngdpPercap\nlifeExp\n\n\n\n\n0\nAfrica\nAlgeria\n1952\n2449.008185\n43.077\n\n\n1\nAfrica\nAlgeria\n1957\n3013.976023\n45.685\n\n\n2\nAfrica\nAlgeria\n1962\n2550.816880\n48.303\n\n\n3\nAfrica\nAlgeria\n1967\n3246.991771\n51.407\n\n\n4\nAfrica\nAlgeria\n1972\n4182.663766\n54.518\n\n\n\n\n\n\n\n\n\n\n8.4.3 Practice exercise 5\n\n8.4.3.1 \nBoth unstack() and pivot() seem to transform the data from the ‘long’ form to the ‘wide’ form. Is there a difference between the two functions?\nSolution:\nYes, both the functions transform the data from the ‘long’ form to the ‘wide’ form. However, unstack() pivots the row indices, while pivot() pivots the columns of the DataFrame.\nEven though both functions are a bit different, it is possible to just use one of them to perform a reshaping operation. If we wish to pivot a column, we can either use pivot() directly on the column, or we can convert the column to row indices and then use unstack(). If we wish to pivot row indices, we can either use unstack() directly on the row indices, or we can convert row indices to a column and then use pivot().\nTo summarise, using one function may be more straightforward than using the other one, but either can be used for reshaping data from the ‘long’ form to the ‘wide’ form.\nBelow is an example where we perform the same reshaping operation with either function.\nConsider the data data_all_continent. Suppose we wish to transform it to data_wide as we did using pivot() in Section 8.4.1.1. Let us do it using unstack(), instead of pivot().\nThe first step will be to reindex data to set year as row indices, and also continent and country as row indices because these two column were set as indices with the pivot() function in Section 8.4.1.1.\n\n#Reindexing data to make 'continent', 'country', and 'year' as hierarchical row indices\ndata_reindexed=data_all_continents.set_index(['continent','country','year'])\ndata_reindexed\n\n\n\n\n\n\n\n\n\n\nlifeExp\npop\ngdpPercap\n\n\ncontinent\ncountry\nyear\n\n\n\n\n\n\n\nAsia\nAfghanistan\n1952\n28.801\n8425333\n779.445314\n\n\n1957\n30.332\n9240934\n820.853030\n\n\n1962\n31.997\n10267083\n853.100710\n\n\n1967\n34.020\n11537966\n836.197138\n\n\n1972\n36.088\n13079460\n739.981106\n\n\n...\n...\n...\n...\n...\n...\n\n\nAmericas\nVenezuela\n1987\n70.190\n17910182\n9883.584648\n\n\n1992\n71.150\n20265563\n10733.926310\n\n\n1997\n72.146\n22374398\n10165.495180\n\n\n2002\n72.766\n24287670\n8605.047831\n\n\n2007\n73.747\n26084662\n11415.805690\n\n\n\n\n1704 rows × 3 columns\n\n\n\nNow we can use unstack() to pivot the desired row index, i.e., year. Also, since we are only interested in pivoting the values of lifeExp (as in the example in Section 8.4.1.1), we will filter the pivoted data with the lifeExp column label.\n\ndata_wide_with_unstack=data_reindexed.unstack('year')['lifeExp']\ndata_wide_with_unstack\n\n\n\n\n\n\n\n\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n43.077\n45.685\n48.303\n51.407\n54.518\n58.014\n61.368\n65.799\n67.744\n69.152\n70.994\n72.301\n\n\nAngola\n30.015\n31.999\n34.000\n35.985\n37.928\n39.483\n39.942\n39.906\n40.647\n40.963\n41.003\n42.731\n\n\nBenin\n38.223\n40.358\n42.618\n44.885\n47.014\n49.190\n50.904\n52.337\n53.919\n54.777\n54.406\n56.728\n\n\nBotswana\n47.622\n49.618\n51.520\n53.298\n56.024\n59.319\n61.484\n63.622\n62.745\n52.556\n46.634\n50.728\n\n\nBurkina Faso\n31.975\n34.906\n37.814\n40.697\n43.591\n46.137\n48.122\n49.557\n50.260\n50.324\n50.650\n52.295\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nEurope\nSwitzerland\n69.620\n70.560\n71.320\n72.770\n73.780\n75.390\n76.210\n77.410\n78.030\n79.370\n80.620\n81.701\n\n\nTurkey\n43.585\n48.079\n52.098\n54.336\n57.005\n59.507\n61.036\n63.108\n66.146\n68.835\n70.845\n71.777\n\n\nUnited Kingdom\n69.180\n70.420\n70.760\n71.360\n72.010\n72.760\n74.040\n75.007\n76.420\n77.218\n78.471\n79.425\n\n\nOceania\nAustralia\n69.120\n70.330\n70.930\n71.100\n71.930\n73.490\n74.740\n76.320\n77.560\n78.830\n80.370\n81.235\n\n\nNew Zealand\n69.390\n70.260\n71.240\n71.520\n71.890\n72.220\n73.840\n74.320\n76.330\n77.550\n79.110\n80.204\n\n\n\n\n142 rows × 12 columns\n\n\n\nThe above dataset is the same as that obtained using the pivot() function in Section 8.4.1.1.\n\n\n8.4.3.2 \nBoth stack() and melt() seem to transform the data from the ‘wide’ form to the ‘long’ form. Is there a difference between the two functions?\nSolution:\nFollowing the trend of the previous question, we can always use stack() instead of melt() and vice-versa. The main difference is that melt() lets us choose the indentifier columns with the argument id_vars. However, if we use stack(), we will need to set the relevant melted row indices as columns. On the other hand, if we wished to have the melted columns as row indices, we can either directly use stack() or use melt() and then set the desired columns as row indices.\nTo summarise, using one function may be more straightforward than using the other one, but either can be used for reshaping data from the ‘wide’ form to the ‘long’ form.\nLet us melt the data data_wide_with_unstack using the stack() function to obtain the same dataset as obtained with the melt() function in Section 8.4.1.2.\n\n#Stacking the data\ndata_stacked = data_wide_with_unstack.stack()\ndata_stacked\n\ncontinent  country      year\nAfrica     Algeria      1952    43.077\n                        1957    45.685\n                        1962    48.303\n                        1967    51.407\n                        1972    54.518\n                                 ...  \nOceania    New Zealand  1987    74.320\n                        1992    76.330\n                        1997    77.550\n                        2002    79.110\n                        2007    80.204\nLength: 1704, dtype: float64\n\n\nNow we need to convert the row indices continent and country to columns as in the melted data in Section 8.4.1.2.\n\n#Putting 'continent' and 'country' as columns\ndata_long_with_stack = data_stacked.reset_index()\ndata_long_with_stack\n\n\n\n\n\n\n\n\ncontinent\ncountry\nyear\n0\n\n\n\n\n0\nAfrica\nAlgeria\n1952\n43.077\n\n\n1\nAfrica\nAlgeria\n1957\n45.685\n\n\n2\nAfrica\nAlgeria\n1962\n48.303\n\n\n3\nAfrica\nAlgeria\n1967\n51.407\n\n\n4\nAfrica\nAlgeria\n1972\n54.518\n\n\n...\n...\n...\n...\n...\n\n\n1699\nOceania\nNew Zealand\n1987\n74.320\n\n\n1700\nOceania\nNew Zealand\n1992\n76.330\n\n\n1701\nOceania\nNew Zealand\n1997\n77.550\n\n\n1702\nOceania\nNew Zealand\n2002\n79.110\n\n\n1703\nOceania\nNew Zealand\n2007\n80.204\n\n\n\n\n1704 rows × 4 columns\n\n\n\nFinally, we need to rename the column named as 0 to LifeExp to obtain the same dataset as in Section 8.4.1.2.\n\n#Renaming column 0 to 'LifeExp'\ndata_long_with_stack.rename(columns = {0:'LifeExp'},inplace=True)\ndata_long_with_stack\n\n\n\n\n\n\n\n\ncontinent\ncountry\nyear\nLifeExp\n\n\n\n\n0\nAfrica\nAlgeria\n1952\n43.077\n\n\n1\nAfrica\nAlgeria\n1957\n45.685\n\n\n2\nAfrica\nAlgeria\n1962\n48.303\n\n\n3\nAfrica\nAlgeria\n1967\n51.407\n\n\n4\nAfrica\nAlgeria\n1972\n54.518\n\n\n...\n...\n...\n...\n...\n\n\n1699\nOceania\nNew Zealand\n1987\n74.320\n\n\n1700\nOceania\nNew Zealand\n1992\n76.330\n\n\n1701\nOceania\nNew Zealand\n1997\n77.550\n\n\n1702\nOceania\nNew Zealand\n2002\n79.110\n\n\n1703\nOceania\nNew Zealand\n2007\n80.204\n\n\n\n\n1704 rows × 4 columns"
  },
  {
    "objectID": "Data aggregation.html#the-groupby-object",
    "href": "Data aggregation.html#the-groupby-object",
    "title": "9  Data aggregation",
    "section": "9.1 The GroupBy object",
    "text": "9.1 The GroupBy object\n\n9.1.1 Creating a GroupBy object: groupby\nThis Pandas DataFrame method groupby() is used to create a GroupBy object.\nA string passed to groupby() may refer to either a column or an index level. If a string matches both a column name and an index level name, a ValueError will be raised.\nExample: Consider the life expectancy dataset, gdp_lifeExpectancy.csv. Suppose we want to group by the observations by continent.\n\ngdp_lifeExp_data = pd.read_csv('./Datasets/gdp_lifeExpectancy.csv')\ngdp_lifeExp_data.head()\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n\n\n\n\n\nWe will pass the column continent as an argument to the groupby() method.\n\n#Creating a GroupBy object\ngrouped = gdp_lifeExp_data.groupby('continent')\n#This will split the data into groups that correspond to values of the column 'continent'\n\nThe groupby() method returns a GroupBy object.\n\n#A 'GroupBy' objects is created with the `groupby()` function\ntype(grouped)\n\npandas.core.groupby.generic.DataFrameGroupBy\n\n\nThe GroupBy object grouped contains the information of the groups in which the data is distributed. Each observation has been assigned to a specific group of the column(s) used to group the data. However, note that the dataset is not physically split into different DataFrames. For example, in the above case, each observation is assigned to a particular group depending on the value of the continent for that observation. However, all the observations are still in the same DataFrame data.\n\n\n9.1.2 Attributes and methods of the GroupBy object\n\n9.1.2.1 keys\nThe object(s) grouping the data are called key(s). Here continent is the group key. The keys of the GroupBy object can be seen using Its keys attribute.\n\n#Key(s) of the GroupBy object\ngrouped.keys\n\n'continent'\n\n\n\n\n9.1.2.2 ngroups\nThe number of groups in which the data is distributed based on the keys can be seen with the ngroups attribute.\n\n#The number of groups based on the key(s)\ngrouped.ngroups\n\n5\n\n\n\n\n9.1.2.3 groups\nThe groups attribute of the GroupBy object contains the group labels (or names) and the row labels of the observations in each group, as a dictionary.\n\n#The groups (in the dictionary format)\ngrouped.groups\n\n{'Africa': [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, ...], 'Americas': [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 432, 433, 434, 435, ...], 'Asia': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, ...], 'Europe': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 516, 517, 518, 519, ...], 'Oceania': [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103]}\n\n\nThe group names are the keys of the dictionary, while the row labels are the corresponding values\n\n#Group names\ngrouped.groups.keys()\n\ndict_keys(['Africa', 'Americas', 'Asia', 'Europe', 'Oceania'])\n\n\n\n#Group values are the row labels corresponding to a particular group\ngrouped.groups.values()\n\ndict_values([Int64Index([  24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n            ...\n            1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703],\n           dtype='int64', length=624), Int64Index([  48,   49,   50,   51,   52,   53,   54,   55,   56,   57,\n            ...\n            1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643],\n           dtype='int64', length=300), Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n            ...\n            1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679],\n           dtype='int64', length=396), Int64Index([  12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n            ...\n            1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607],\n           dtype='int64', length=360), Int64Index([  60,   61,   62,   63,   64,   65,   66,   67,   68,   69,   70,\n              71, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101,\n            1102, 1103],\n           dtype='int64')])\n\n\n\n\n9.1.2.4 size()\nThe size() method of the GroupBy object returns the number of observations in each group.\n\n#Number of observations in each group\ngrouped.size()\n\ncontinent\nAfrica      624\nAmericas    300\nAsia        396\nEurope      360\nOceania      24\ndtype: int64\n\n\n\n\n9.1.2.5 first()\nThe first non missing element of each group is returned with the first() method of the GroupBy object.\n\n#The first element of the group can be printed using the first() method\ngrouped.first()\n\n\n\n\n\n\n\n\ncountry\nyear\nlifeExp\npop\ngdpPercap\n\n\ncontinent\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n1952\n43.077\n9279525\n2449.008185\n\n\nAmericas\nArgentina\n1952\n62.485\n17876956\n5911.315053\n\n\nAsia\nAfghanistan\n1952\n28.801\n8425333\n779.445314\n\n\nEurope\nAlbania\n1952\n55.230\n1282697\n1601.056136\n\n\nOceania\nAustralia\n1952\n69.120\n8691212\n10039.595640\n\n\n\n\n\n\n\n\n\n9.1.2.6 get_group()\nThis method returns the observations for a particular group of the GroupBy object.\n\n#Observations for individual groups can be obtained using the get_group() function\ngrouped.get_group('Asia')\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1675\nYemen, Rep.\nAsia\n1987\n52.922\n11219340\n1971.741538\n\n\n1676\nYemen, Rep.\nAsia\n1992\n55.599\n13367997\n1879.496673\n\n\n1677\nYemen, Rep.\nAsia\n1997\n58.020\n15826497\n2117.484526\n\n\n1678\nYemen, Rep.\nAsia\n2002\n60.308\n18701257\n2234.820827\n\n\n1679\nYemen, Rep.\nAsia\n2007\n62.698\n22211743\n2280.769906\n\n\n\n\n396 rows × 6 columns"
  },
  {
    "objectID": "Data aggregation.html#data-aggregation-with-groupby-methods",
    "href": "Data aggregation.html#data-aggregation-with-groupby-methods",
    "title": "9  Data aggregation",
    "section": "9.2 Data aggregation with groupby() methods",
    "text": "9.2 Data aggregation with groupby() methods\n\n9.2.1 mean()\nThis method returns the mean of each group of the GroupBy object.\n\n9.2.1.1 Grouping observations\nExample: Find the mean life expectancy, population and GDP per capita for each country since 1952.\nFirst, we’ll group the data such that all observations corresponding to a country make a unique group.\n\n#Grouping the observations by 'country'\ngrouped = gdp_lifeExp_data.groupby('country')\n\nNow, we’ll find the mean statistics for each group with the mean() method. The method will be applied on all columns of the DataFrame and all groups.\n\n#Finding the mean stastistic of all columns of the DataFrame and all groups\ngrouped.mean()\n\n\n\n\n\n\n\n\nyear\nlifeExp\npop\ngdpPercap\n\n\ncountry\n\n\n\n\n\n\n\n\nAfghanistan\n1979.5\n37.478833\n1.582372e+07\n802.674598\n\n\nAlbania\n1979.5\n68.432917\n2.580249e+06\n3255.366633\n\n\nAlgeria\n1979.5\n59.030167\n1.987541e+07\n4426.025973\n\n\nAngola\n1979.5\n37.883500\n7.309390e+06\n3607.100529\n\n\nArgentina\n1979.5\n69.060417\n2.860224e+07\n8955.553783\n\n\n...\n...\n...\n...\n...\n\n\nVietnam\n1979.5\n57.479500\n5.456857e+07\n1017.712615\n\n\nWest Bank and Gaza\n1979.5\n60.328667\n1.848606e+06\n3759.996781\n\n\nYemen, Rep.\n1979.5\n46.780417\n1.084319e+07\n1569.274672\n\n\nZambia\n1979.5\n45.996333\n6.353805e+06\n1358.199409\n\n\nZimbabwe\n1979.5\n52.663167\n7.641966e+06\n635.858042\n\n\n\n\n142 rows × 4 columns\n\n\n\nNote that if we wished to retain the continent in the above dataset, we can group the data by both continent and country. If the data is to be grouped by multiple columns, we need to put them within [] brackets:\n\n#Grouping the observations by 'continent' and 'country'\ngrouped = gdp_lifeExp_data.groupby(['continent','country'])\n\n#Finding the mean stastistic of all columns of the DataFrame and all groups\ngrouped.mean()\n\n\n\n\n\n\n\n\n\nyear\nlifeExp\npop\ngdpPercap\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n1979.5\n59.030167\n1.987541e+07\n4426.025973\n\n\nAngola\n1979.5\n37.883500\n7.309390e+06\n3607.100529\n\n\nBenin\n1979.5\n48.779917\n4.017497e+06\n1155.395107\n\n\nBotswana\n1979.5\n54.597500\n9.711862e+05\n5031.503557\n\n\nBurkina Faso\n1979.5\n44.694000\n7.548677e+06\n843.990665\n\n\n...\n...\n...\n...\n...\n...\n\n\nEurope\nSwitzerland\n1979.5\n75.565083\n6.384293e+06\n27074.334405\n\n\nTurkey\n1979.5\n59.696417\n4.590901e+07\n4469.453380\n\n\nUnited Kingdom\n1979.5\n73.922583\n5.608780e+07\n19380.472986\n\n\nOceania\nAustralia\n1979.5\n74.662917\n1.464931e+07\n19980.595634\n\n\nNew Zealand\n1979.5\n73.989500\n3.100032e+06\n17262.622813\n\n\n\n\n142 rows × 4 columns\n\n\n\nHere the data has been aggregated according to the group keys - continent and country, and a new DataFrame is created that is indexed by the unique values of continent-country.\nFor large datasets, it may be desirable to aggregate only a few columns. For example, if we wish to compute the means of only lifeExp and gdpPercap, then we can filter those columns in the GroupBy object (just like we filter columns in a DataFrame), and then apply the mean() method:\n\ngrouped[['lifeExp','gdpPercap']].mean()\n\n\n\n\n\n\n\n\n\nlifeExp\ngdpPercap\n\n\ncontinent\ncountry\n\n\n\n\n\n\nAfrica\nAlgeria\n59.030167\n4426.025973\n\n\nAngola\n37.883500\n3607.100529\n\n\nBenin\n48.779917\n1155.395107\n\n\nBotswana\n54.597500\n5031.503557\n\n\nBurkina Faso\n44.694000\n843.990665\n\n\n...\n...\n...\n...\n\n\nEurope\nSwitzerland\n75.565083\n27074.334405\n\n\nTurkey\n59.696417\n4469.453380\n\n\nUnited Kingdom\n73.922583\n19380.472986\n\n\nOceania\nAustralia\n74.662917\n19980.595634\n\n\nNew Zealand\n73.989500\n17262.622813\n\n\n\n\n142 rows × 2 columns\n\n\n\n\n\n9.2.1.2 Grouping columns\nBy default, the grouping takes place by rows. However, as with several other Pandas methods, grouping can also be done by columns by using the axis = 1 argument.\nExample: Consider we have the above dataset in the wide-format as follows.\n\ngdp_lifeExp_data_wide = gdp_lifeExp_data.pivot(index = ['continent','country'],columns = 'year')\ngdp_lifeExp_data_wide\n\n\n\n\n\n\n\n\n\nlifeExp\n...\ngdpPercap\n\n\n\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n...\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n43.077\n45.685\n48.303\n51.407\n54.518\n58.014\n61.368\n65.799\n67.744\n69.152\n...\n2550.816880\n3246.991771\n4182.663766\n4910.416756\n5745.160213\n5681.358539\n5023.216647\n4797.295051\n5288.040382\n6223.367465\n\n\nAngola\n30.015\n31.999\n34.000\n35.985\n37.928\n39.483\n39.942\n39.906\n40.647\n40.963\n...\n4269.276742\n5522.776375\n5473.288005\n3008.647355\n2756.953672\n2430.208311\n2627.845685\n2277.140884\n2773.287312\n4797.231267\n\n\nBenin\n38.223\n40.358\n42.618\n44.885\n47.014\n49.190\n50.904\n52.337\n53.919\n54.777\n...\n949.499064\n1035.831411\n1085.796879\n1029.161251\n1277.897616\n1225.856010\n1191.207681\n1232.975292\n1372.877931\n1441.284873\n\n\nBotswana\n47.622\n49.618\n51.520\n53.298\n56.024\n59.319\n61.484\n63.622\n62.745\n52.556\n...\n983.653976\n1214.709294\n2263.611114\n3214.857818\n4551.142150\n6205.883850\n7954.111645\n8647.142313\n11003.605080\n12569.851770\n\n\nBurkina Faso\n31.975\n34.906\n37.814\n40.697\n43.591\n46.137\n48.122\n49.557\n50.260\n50.324\n...\n722.512021\n794.826560\n854.735976\n743.387037\n807.198586\n912.063142\n931.752773\n946.294962\n1037.645221\n1217.032994\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nEurope\nSwitzerland\n69.620\n70.560\n71.320\n72.770\n73.780\n75.390\n76.210\n77.410\n78.030\n79.370\n...\n20431.092700\n22966.144320\n27195.113040\n26982.290520\n28397.715120\n30281.704590\n31871.530300\n32135.323010\n34480.957710\n37506.419070\n\n\nTurkey\n43.585\n48.079\n52.098\n54.336\n57.005\n59.507\n61.036\n63.108\n66.146\n68.835\n...\n2322.869908\n2826.356387\n3450.696380\n4269.122326\n4241.356344\n5089.043686\n5678.348271\n6601.429915\n6508.085718\n8458.276384\n\n\nUnited Kingdom\n69.180\n70.420\n70.760\n71.360\n72.010\n72.760\n74.040\n75.007\n76.420\n77.218\n...\n12477.177070\n14142.850890\n15895.116410\n17428.748460\n18232.424520\n21664.787670\n22705.092540\n26074.531360\n29478.999190\n33203.261280\n\n\nOceania\nAustralia\n69.120\n70.330\n70.930\n71.100\n71.930\n73.490\n74.740\n76.320\n77.560\n78.830\n...\n12217.226860\n14526.124650\n16788.629480\n18334.197510\n19477.009280\n21888.889030\n23424.766830\n26997.936570\n30687.754730\n34435.367440\n\n\nNew Zealand\n69.390\n70.260\n71.240\n71.520\n71.890\n72.220\n73.840\n74.320\n76.330\n77.550\n...\n13175.678000\n14463.918930\n16046.037280\n16233.717700\n17632.410400\n19007.191290\n18363.324940\n21050.413770\n23189.801350\n25185.009110\n\n\n\n\n142 rows × 36 columns\n\n\n\nNow, find the mean GDP per capita, life expectancy and population for each country.\nHere, we can group by the outer level column labels to obtain the means. Also, we need to use the argument axis=1 to indicate that we intend to group columns, instead of rows.\n\ngdp_lifeExp_data_wide.groupby(axis=1,level=0).mean()\n\n\n\n\n\n\n\n\n\ngdpPercap\nlifeExp\npop\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\nAfrica\nAlgeria\n4426.025973\n59.030167\n1.987541e+07\n\n\nAngola\n3607.100529\n37.883500\n7.309390e+06\n\n\nBenin\n1155.395107\n48.779917\n4.017497e+06\n\n\nBotswana\n5031.503557\n54.597500\n9.711862e+05\n\n\nBurkina Faso\n843.990665\n44.694000\n7.548677e+06\n\n\n...\n...\n...\n...\n...\n\n\nEurope\nSwitzerland\n27074.334405\n75.565083\n6.384293e+06\n\n\nTurkey\n4469.453380\n59.696417\n4.590901e+07\n\n\nUnited Kingdom\n19380.472986\n73.922583\n5.608780e+07\n\n\nOceania\nAustralia\n19980.595634\n74.662917\n1.464931e+07\n\n\nNew Zealand\n17262.622813\n73.989500\n3.100032e+06\n\n\n\n\n142 rows × 3 columns\n\n\n\n\n\n\n9.2.2 Practice exercise 1\nRead the table consisting of GDP per capita of countries from the webpage: https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita .\nTo only read the relevant table, read the tables that contain the word ‘Country’.\nEstimate the GDP per capita of each country as the average of the estimates of the three agencies - IMF, United Nations and World Bank.\nWe need to do a bit of data cleaning before we could directly use the groupby() function. Follow the steps below to answer this question:\n\nSet the first 2 columns containing country, and UN region as hierarchical row labels.\nApply the following function on all the columns to convert them to numeric: f = lambda x:pd.to_numeric(x,errors = 'coerce')\nNow use groupby() to find estimate the GDP per capita for each country.\n\nSolution:\n\ndfs = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita', match = 'Country')\ngdp_per_capita = dfs[0]\ngdp_per_capita_reindexed = gdp_per_capita.set_index([('Country/Territory','Country/Territory'),\n                                                     ('UN Region','UN Region')])\ngdp_per_capita_numeric=gdp_per_capita_reindexed.apply(lambda x:pd.to_numeric(x,errors = 'coerce'))\ngdp_per_capita_numeric.groupby(axis=1,level=1).mean().drop(columns='Year')\n\n\n\n9.2.3 agg()\nDirectly applying the aggregate methods of the GroupBy object such as mean, count, etc., lets us apply only one function at a time. Also, we may wish to apply an aggregate function of our own, which is not there in the set of methods of the GroupBy object, such as the range of values of a column.\nThe agg() function of a GroupBy object lets us aggregate data using:\n\nMultiple aggregation functions\nCustom aggregate functions (in addition to in-built functions like mean, std, count etc.)\n\nConsider the spotify dataset containing information about tracks and artists.\n\nspotify_data = pd.read_csv('./Datasets/spotify_data.csv')\nspotify_data.head(3)\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n0\n16996777\nrap\nJuice WRLD\n96\nAll Girls Are The Same\n0\n165820\n1\n2021\n0.673\n...\n0\n-7.226\n1\n0.306\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\n\n\n1\n16996777\nrap\nJuice WRLD\n96\nLucid Dreams\n0\n239836\n1\n2021\n0.511\n...\n6\n-7.230\n0\n0.200\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\n\n\n2\n16996777\nrap\nJuice WRLD\n96\nHear Me Calling\n0\n189977\n1\n2021\n0.699\n...\n7\n-3.997\n0\n0.106\n0.3080\n0.000036\n0.1210\n0.499\n88.933\n4\n\n\n\n\n3 rows × 21 columns\n\n\n\nSuppose, we wish to find the average popularity of tracks for each genre. We can do that by using the mean() method of a GroupBy object, as shown in the previous section. We will also sort the table by decreasing average popularity.\n\ngrouped=spotify_data.groupby('genres')\ngrouped['track_popularity'].mean().sort_values(ascending = False)\n\ngenres\nrap              51.162959\nhip hop          48.700665\nmetal            46.334539\nelectronic       43.253165\ncountry          41.132686\npop              37.783194\nlatin            37.563765\nrock             36.749623\nmiscellaneous    36.167401\npop & rock       35.619242\nhoerspiel        31.258670\nfolk             29.716767\njazz             20.349472\nName: track_popularity, dtype: float64\n\n\nLet us also find the standard deviation of the popularity of the tracks for each genre. We will also sort the table by decreasing standard deviation of popularity.\n\ngrouped['track_popularity'].std().sort_values(ascending = False)\n\ngenres\nrap              20.912240\ncountry          20.544338\npop              17.790385\nmiscellaneous    16.240129\nelectronic       16.075841\npop & rock       15.560975\nfolk             15.150717\njazz             15.087119\nlatin            14.492199\nrock             14.350580\nmetal            14.331154\nhip hop          12.824901\nhoerspiel         6.459370\nName: track_popularity, dtype: float64\n\n\nEven though rap is the most popular genre on an average, its popularity varies the most amongs listeners. So, it should probably be recommended only to rap listeners or the criteria to accept rap songs on Spotify should be more stringent.\n\n9.2.3.1 Multiple aggregate functions\nLet us use the agg() method of the GroupBy object to simultaneously find the mean and standard deviation of the track popularity for each genre.\nFor aggregating by multiple functions, we pass a list of strings to agg(), where the strings are the function names.\n\ngrouped['track_popularity'].agg(['mean','std']).sort_values(by = 'mean',ascending = False)\n\n\n\n\n\n\n\n\nmean\nstd\n\n\ngenres\n\n\n\n\n\n\nrap\n51.162959\n20.912240\n\n\nhip hop\n48.700665\n12.824901\n\n\nmetal\n46.334539\n14.331154\n\n\nelectronic\n43.253165\n16.075841\n\n\ncountry\n41.132686\n20.544338\n\n\npop\n37.783194\n17.790385\n\n\nlatin\n37.563765\n14.492199\n\n\nrock\n36.749623\n14.350580\n\n\nmiscellaneous\n36.167401\n16.240129\n\n\npop & rock\n35.619242\n15.560975\n\n\nhoerspiel\n31.258670\n6.459370\n\n\nfolk\n29.716767\n15.150717\n\n\njazz\n20.349472\n15.087119\n\n\n\n\n\n\n\nFrom the above table, we observe that people not just like hip-hop the second most on average, but they also like it more consistently than almost all other genres. We have also sorted the above table by decreasing average track popularity.\n\n\n9.2.3.2 Custom aggregate functions\nIn addition to the mean and standard deviation of the track popularirty of each genre, let us also include the \\(90^{th}\\) percentile of track popularity in the table above, and sort it by the same.\n\n#Defining a function that returns the 90th percentile value\ndef Ninety_pc(x):\n    return x.quantile(0.9)\n\n\ngrouped['track_popularity'].agg(['mean','std',Ninety_pc]).sort_values(by = 'Ninety_pc',ascending = False)\n\n\n\n\n\n\n\n\nmean\nstd\nNinety_pc\n\n\ngenres\n\n\n\n\n\n\n\nrap\n51.162959\n20.912240\n74\n\n\ncountry\n41.132686\n20.544338\n67\n\n\nhip hop\n48.700665\n12.824901\n64\n\n\nmetal\n46.334539\n14.331154\n63\n\n\nelectronic\n43.253165\n16.075841\n61\n\n\npop\n37.783194\n17.790385\n60\n\n\nmiscellaneous\n36.167401\n16.240129\n57\n\n\nlatin\n37.563765\n14.492199\n56\n\n\npop & rock\n35.619242\n15.560975\n56\n\n\nrock\n36.749623\n14.350580\n56\n\n\nfolk\n29.716767\n15.150717\n50\n\n\njazz\n20.349472\n15.087119\n43\n\n\nhoerspiel\n31.258670\n6.459370\n37\n\n\n\n\n\n\n\nFrom the above table, we observe that even though country songs are not as popular as hip-hop on an average, the top \\(10\\%\\) country tracks are more popular than the top \\(10\\%\\) hip hop tracks.\nFor aggregating by multiple functions & changing the column names resulting from those functions, we pass a list of tuples to agg(), where each tuple is of length two, and contains the new column name & the function to be applied.\n\n#Simultaneous renaming of columns while grouping\ngrouped['track_popularity'].agg([('Average','mean'),('Standard deviation','std'),('90th percentile',Ninety_pc)]).sort_values(by = '90th percentile',ascending = False)\n\n\n\n\n\n\n\n\nAverage\nStandard deviation\n90th percentile\n\n\ngenres\n\n\n\n\n\n\n\nrap\n51.162959\n20.912240\n74\n\n\ncountry\n41.132686\n20.544338\n67\n\n\nhip hop\n48.700665\n12.824901\n64\n\n\nmetal\n46.334539\n14.331154\n63\n\n\nelectronic\n43.253165\n16.075841\n61\n\n\npop\n37.783194\n17.790385\n60\n\n\nmiscellaneous\n36.167401\n16.240129\n57\n\n\nlatin\n37.563765\n14.492199\n56\n\n\npop & rock\n35.619242\n15.560975\n56\n\n\nrock\n36.749623\n14.350580\n56\n\n\nfolk\n29.716767\n15.150717\n50\n\n\njazz\n20.349472\n15.087119\n43\n\n\nhoerspiel\n31.258670\n6.459370\n37\n\n\n\n\n\n\n\nWe can put use a lambda function as well instead of separately defining the function Ninety_pc in the above code:\n\n#Simultaneous renaming of columns while grouping\ngrouped['track_popularity'].agg([('Average','mean'),('Standard deviation','std'),('90th percentile',lambda x:x.quantile(0.9))]).sort_values(by = '90th percentile',ascending = False)\n\n\n\n\n\n\n\n\nAverage\nStandard deviation\n90th percentile\n\n\ngenres\n\n\n\n\n\n\n\nrap\n51.162959\n20.912240\n74\n\n\ncountry\n41.132686\n20.544338\n67\n\n\nhip hop\n48.700665\n12.824901\n64\n\n\nmetal\n46.334539\n14.331154\n63\n\n\nelectronic\n43.253165\n16.075841\n61\n\n\npop\n37.783194\n17.790385\n60\n\n\nmiscellaneous\n36.167401\n16.240129\n57\n\n\nlatin\n37.563765\n14.492199\n56\n\n\npop & rock\n35.619242\n15.560975\n56\n\n\nrock\n36.749623\n14.350580\n56\n\n\nfolk\n29.716767\n15.150717\n50\n\n\njazz\n20.349472\n15.087119\n43\n\n\nhoerspiel\n31.258670\n6.459370\n37\n\n\n\n\n\n\n\n\n\n9.2.3.3 Grouping by multiple columns\nLet us find aggregate statistics when we group data by multiple columns. For that, let us create a new categorical column energy_lvl that has two levels - Low energy and High energy, depending on the energy of the track.\n\n#Creating a new categorical column 'energy_lvl'\nspotify_data['energy_lvl'] = pd.qcut(spotify_data.energy,2,labels=['Low energy', 'High energy'])\n\nNow, let us find the mean, standard deviation and 90th percentile value of the track popularity for each genre-energy level combination simultaneously.\n\n#Grouping the data with 'genres' and 'energy_lvl'\ngrouped=spotify_data.groupby(['genres','energy_lvl'])\n\n\n#Finding aggregate statistics of data grouped with multple columns\ngrouped['track_popularity'].agg(['mean','std',Ninety_pc])\n\n\n\n\n\n\n\n\n\nmean\nstd\nNinety_pc\n\n\ngenres\nenergy_lvl\n\n\n\n\n\n\n\ncountry\nLow energy\n34.982069\n19.877274\n64.0\n\n\nHigh energy\n49.859100\n18.196092\n69.0\n\n\nelectronic\nLow energy\n43.754789\n13.294925\n59.0\n\n\nHigh energy\n43.005671\n17.290356\n61.2\n\n\nfolk\nLow energy\n29.617831\n15.360910\n51.0\n\n\nHigh energy\n29.991957\n14.556622\n49.0\n\n\nhip hop\nLow energy\n50.283669\n12.423124\n66.0\n\n\nHigh energy\n48.012067\n12.936656\n63.0\n\n\nhoerspiel\nLow energy\n31.534779\n5.953968\n37.0\n\n\nHigh energy\n30.514032\n7.609088\n38.0\n\n\njazz\nLow energy\n19.421085\n14.599499\n41.0\n\n\nHigh energy\n25.715373\n16.662754\n50.0\n\n\nlatin\nLow energy\n34.308370\n15.126721\n53.0\n\n\nHigh energy\n39.992605\n13.504109\n58.0\n\n\nmetal\nLow energy\n38.612403\n16.000151\n60.4\n\n\nHigh energy\n46.985621\n13.993685\n64.0\n\n\nmiscellaneous\nLow energy\n34.157235\n16.516910\n56.0\n\n\nHigh energy\n39.394186\n15.241620\n59.0\n\n\npop\nLow energy\n34.722631\n18.340408\n59.0\n\n\nHigh energy\n40.597155\n16.784034\n61.0\n\n\npop & rock\nLow energy\n32.987221\n15.594202\n54.0\n\n\nHigh energy\n37.413357\n15.280915\n57.0\n\n\nrap\nLow energy\n57.177966\n16.491203\n75.0\n\n\nHigh energy\n48.225166\n22.182320\n74.0\n\n\nrock\nLow energy\n34.654871\n14.387263\n54.0\n\n\nHigh energy\n38.256199\n14.133726\n57.0\n\n\n\n\n\n\n\nFor most of the genres, there is not much difference between the average popularity of low energy and high energy tracks. However, in case of country tracks people seem to to prefer high energy tracks a lot more as compared to low energy tracks.\n\n\n9.2.3.4 Multiple aggregate functions on multiple columns\nLet us find the mean and standard deviation of track popularity and danceability for each genre and energy level.\n\nspotify_popularity_danceability = grouped[['track_popularity','danceability']].agg([('Average','mean'),('Standard deviation','std')])\nspotify_popularity_danceability\n\n\n\n\n\n\n\n\ntrack_popularity\ndanceability\n\n\n\nAverage\nStandard deviation\nAverage\nStandard deviation\n\n\ngenres\n\n\n\n\n\n\n\n\ncountry\n41.132686\n20.544338\n0.600392\n0.119116\n\n\nelectronic\n43.253165\n16.075841\n0.610729\n0.190133\n\n\nfolk\n29.716767\n15.150717\n0.525161\n0.150081\n\n\nhip hop\n48.700665\n12.824901\n0.719329\n0.128615\n\n\nhoerspiel\n31.258670\n6.459370\n0.661288\n0.078628\n\n\njazz\n20.349472\n15.087119\n0.518045\n0.149548\n\n\nlatin\n37.563765\n14.492199\n0.607981\n0.152661\n\n\nmetal\n46.334539\n14.331154\n0.419201\n0.125717\n\n\nmiscellaneous\n36.167401\n16.240129\n0.576601\n0.178777\n\n\npop\n37.783194\n17.790385\n0.588314\n0.153629\n\n\npop & rock\n35.619242\n15.560975\n0.545856\n0.143857\n\n\nrap\n51.162959\n20.912240\n0.723586\n0.133588\n\n\nrock\n36.749623\n14.350580\n0.520255\n0.154649\n\n\n\n\n\n\n\nWe get a couple of insights from the above table:\n\nHigh energy songs have higher danceability for most genres as expected. However, for hip-hop, country, rock and hoerspiel, even low-energy songs have comparable danceability.\nHip hop has the highest danceability as expected. However, high energy rap also has relatively high danceability.\n\n\n\n9.2.3.5 Distinct aggregate functions on multiple columns\nFor aggregating by multiple functions, we pass a list of strings to agg(), where the strings are the function names.\nFor aggregating by multiple functions & changing the column names resulting from those functions, we pass a list of tuples to agg(), where each tuple is of length two, and contains the new column name as the first object and the function to be applied as the second object of the tuple.\nFor aggregating by multiple functions such that a distinct set of functions is applied to each column, we pass a dictionary to agg(), where the keys are the column names on which the function is to be applied, and the values are the list of strings that are the function names, or a list of tuples if we also wish to name the aggregated columns.\nExample: For each genre and energy level, find the mean and standard deviation of the track popularity, and the minimum and maximum values of loudness.\n\n#Specifying arguments to the function as a dictionary if distinct functions are to be applied on distinct columns\ngrouped.agg({'track_popularity':[('Average','mean'),('Standard deviation','std')],'loudness':['min','max']})\n\n\n\n\n\n\n\n\n\ntrack_popularity\nloudness\n\n\n\n\nAverage\nStandard deviation\nmin\nmax\n\n\ngenres\nenergy_lvl\n\n\n\n\n\n\n\n\ncountry\nLow energy\n34.982069\n19.877274\n-23.163\n-4.145\n\n\nHigh energy\n49.859100\n18.196092\n-16.763\n-0.716\n\n\nelectronic\nLow energy\n43.754789\n13.294925\n-60.000\n-4.936\n\n\nHigh energy\n43.005671\n17.290356\n-19.756\n-0.533\n\n\nfolk\nLow energy\n29.617831\n15.360910\n-28.715\n-0.972\n\n\nHigh energy\n29.991957\n14.556622\n-16.383\n0.501\n\n\nhip hop\nLow energy\n50.283669\n12.423124\n-25.947\n-1.595\n\n\nHigh energy\n48.012067\n12.936656\n-18.273\n0.642\n\n\nhoerspiel\nLow energy\n31.534779\n5.953968\n-29.907\n-4.910\n\n\nHigh energy\n30.514032\n7.609088\n-22.046\n-2.130\n\n\njazz\nLow energy\n19.421085\n14.599499\n-60.000\n-2.962\n\n\nHigh energy\n25.715373\n16.662754\n-27.563\n-1.166\n\n\nlatin\nLow energy\n34.308370\n15.126721\n-31.897\n-2.331\n\n\nHigh energy\n39.992605\n13.504109\n-18.357\n-1.204\n\n\nmetal\nLow energy\n38.612403\n16.000151\n-32.032\n-4.311\n\n\nHigh energy\n46.985621\n13.993685\n-16.244\n-1.275\n\n\nmiscellaneous\nLow energy\n34.157235\n16.516910\n-44.410\n-1.409\n\n\nHigh energy\n39.394186\n15.241620\n-37.684\n1.634\n\n\npop\nLow energy\n34.722631\n18.340408\n-41.182\n-0.045\n\n\nHigh energy\n40.597155\n16.784034\n-27.575\n0.330\n\n\npop & rock\nLow energy\n32.987221\n15.594202\n-60.000\n-1.823\n\n\nHigh energy\n37.413357\n15.280915\n-22.234\n1.107\n\n\nrap\nLow energy\n57.177966\n16.491203\n-23.611\n-1.218\n\n\nHigh energy\n48.225166\n22.182320\n-15.012\n0.457\n\n\nrock\nLow energy\n34.654871\n14.387263\n-42.488\n-1.708\n\n\nHigh energy\n38.256199\n14.133726\n-25.291\n3.744\n\n\n\n\n\n\n\nFrom the above table, we observe that high energy songs are always louder than low energy songs. High energy Rock songs can be very loud."
  },
  {
    "objectID": "Data aggregation.html#apply-data-aggregation-filtering-transformation",
    "href": "Data aggregation.html#apply-data-aggregation-filtering-transformation",
    "title": "9  Data aggregation",
    "section": "9.3 apply(): Data aggregation, filtering & transformation",
    "text": "9.3 apply(): Data aggregation, filtering & transformation\nWith the apply() method of the GroupBy object, we can perform several operations on groups, other than data aggregation.\n\n9.3.1 Filtering data by group\nExample: Find the top 3 most popular tracks of each genre in the spotify dataset.\nWe’ll first define a function that sorts a dataset by decreasing track popularity and returns the top 3 rows. Then, we’ll apply this function on each group using the apply() method of the GroupBy object.\n\n#Defining the function that finds the top 3 most popular tracks from the dataset 'x'\ndef top_stats(x,col='track_popularity',n=3):\n    return x.sort_values(by=col,ascending = False)[0:n]\n\n\n#Defining the groups in the spotify data\ngrouped_spotify_data = spotify_data.groupby('genres')\n\nNow we’ll use the apply() method to apply the top_stats() function on each group of the object grouped_spotify_data.\n\n#Top 3 tracks of each genre\ntop3_tracks = grouped_spotify_data.apply(top_stats)\ntop3_tracks.head()\n\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\nenergy_lvl\n\n\ngenres\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\n4047\n4755356\ncountry\nLuke Combs\n85\nForever After All\n82\n232533\n0\n2020\n0.487\n...\n-5.195\n1\n0.0253\n0.1910\n0.000000\n0.0933\n0.456\n151.964\n4\nHigh energy\n\n\n2403\n1678738\ncountry\nMorgan Wallen\n88\nWasted On You\n81\n178520\n0\n2021\n0.505\n...\n-5.240\n0\n0.0318\n0.3730\n0.001070\n0.1260\n0.252\n196.000\n3\nHigh energy\n\n\n16244\n1678738\ncountry\nMorgan Wallen\n88\nWhiskey Glasses\n81\n234347\n0\n2018\n0.614\n...\n-4.580\n1\n0.0289\n0.3690\n0.000002\n0.1150\n0.707\n149.959\n4\nHigh energy\n\n\nelectronic\n83058\n7650304\nelectronic\nDaft Punk\n86\nOne More Time\n81\n320357\n0\n2001\n0.611\n...\n-8.618\n1\n0.1330\n0.0193\n0.000000\n0.3320\n0.476\n122.752\n4\nHigh energy\n\n\n13068\n28026432\nelectronic\nAlan Walker\n85\nFaded\n80\n212107\n0\n2018\n0.468\n...\n-5.085\n1\n0.0476\n0.0281\n0.000008\n0.1100\n0.159\n179.642\n4\nHigh energy\n\n\n\n\n5 rows × 22 columns\n\n\n\nThe top_stats() function is applied to each group, and the results are concatenated internally with the concat() function. The output therefore has a hierarchical index whose outer level indices are the group keys.\nWe can also use a lambda function instead of separately defining the function top_tracks():\n\n#Top 3 tracks of each genre - using lambda function\ntop3_tracks = grouped_spotify_data.apply(lambda x:x.sort_values(by = 'track_popularity',ascending=False)[0:3])\ntop3_tracks.head()\n\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\nenergy_lvl\n\n\ngenres\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\n4047\n4755356\ncountry\nLuke Combs\n85\nForever After All\n82\n232533\n0\n2020\n0.487\n...\n-5.195\n1\n0.0253\n0.1910\n0.000000\n0.0933\n0.456\n151.964\n4\nHigh energy\n\n\n2403\n1678738\ncountry\nMorgan Wallen\n88\nWasted On You\n81\n178520\n0\n2021\n0.505\n...\n-5.240\n0\n0.0318\n0.3730\n0.001070\n0.1260\n0.252\n196.000\n3\nHigh energy\n\n\n16244\n1678738\ncountry\nMorgan Wallen\n88\nWhiskey Glasses\n81\n234347\n0\n2018\n0.614\n...\n-4.580\n1\n0.0289\n0.3690\n0.000002\n0.1150\n0.707\n149.959\n4\nHigh energy\n\n\nelectronic\n83058\n7650304\nelectronic\nDaft Punk\n86\nOne More Time\n81\n320357\n0\n2001\n0.611\n...\n-8.618\n1\n0.1330\n0.0193\n0.000000\n0.3320\n0.476\n122.752\n4\nHigh energy\n\n\n13068\n28026432\nelectronic\nAlan Walker\n85\nFaded\n80\n212107\n0\n2018\n0.468\n...\n-5.085\n1\n0.0476\n0.0281\n0.000008\n0.1100\n0.159\n179.642\n4\nHigh energy\n\n\n\n\n5 rows × 22 columns\n\n\n\nWe can also pass arguments to the top_stats() function with the apply() function.\nExample: Find the most popular artist from each genre.\n\n#Applying the 'top_stats()' function to each group of the data (based on 'genre') to find the most popular artist \n#Dropping the inner row label as it is not informative\ngrouped_spotify_data.apply(top_stats,col='artist_popularity',n=1)['artist_name'].droplevel(axis=0, level = 1)\n\ngenres\ncountry               Morgan Wallen\nelectronic                Daft Punk\nfolk                       Bon Iver\nhip hop                      Eminem\nhoerspiel              Die drei ???\njazz             Earth, Wind & Fire\nlatin                        Maluma\nmetal                   Linkin Park\nmiscellaneous             Pop Smoke\npop                   Justin Bieber\npop & rock                 Maroon 5\nrap                       Bad Bunny\nrock                Imagine Dragons\nName: artist_name, dtype: object\n\n\n\n\n9.3.2 Practice exercise 2\nFilter the first 4 columns of the spotify dataset. Drop duplicate observartions in the resulting dataset using the Pandas DataFrame method drop_duplicates(). Find the top 3 most popular artists for each genre.\nSolution:\n\nspotify_data.iloc[:,0:4].drop_duplicates().groupby('genres').apply(lambda x:x.sort_values(by = 'artist_popularity', \n                                                                                          ascending = False)[0:3])\n\n\n\n9.3.3 Transforming data by group\nRecall method 3 for imputing missing values in Chapter 7. The method was to impute missing values based on correlated variables in data.\nIn the example shown for the method, values of GDP per capita for a few countries were missing. We imputed the missing value of GDP per capita for those countries as the average GDP per capita of the corresponding continent.\nWe will compare the approach we used with the approach using the groupby() & apply() methods.\nLet us read the datasets and the function that makes a visualization to compare the imputed values with the actual values.\n\n#Importing data with missing values\ngdp_missing_data = pd.read_csv('./Datasets/GDP_missing_data.csv')\n\n#Importing data with all values\ngdp_complete_data = pd.read_csv('./Datasets/GDP_complete_data.csv')\n\n\n#Index of rows with missing values for GDP per capita\nnull_ind_gdpPC = gdp_missing_data.index[gdp_missing_data.gdpPerCapita.isnull()]\n\n#Defining a function to plot the imputed values vs actual values \ndef plot_actual_vs_predicted():\n    fig, ax = plt.subplots(figsize=(8, 6))\n    plt.rc('xtick', labelsize=15) \n    plt.rc('ytick', labelsize=15) \n    x = gdp_complete_data.loc[null_ind_gdpPC,'gdpPerCapita']\n    y = gdp_imputed_data.loc[null_ind_gdpPC,'gdpPerCapita']\n    plt.scatter(x,y)\n    z=np.polyfit(x,y,1)\n    p=np.poly1d(z)\n    plt.plot(x,x,color='orange')\n    plt.xlabel('Actual GDP per capita',fontsize=20)\n    plt.ylabel('Imputed GDP per capita',fontsize=20)\n    ax.xaxis.set_major_formatter('${x:,.0f}')\n    ax.yaxis.set_major_formatter('${x:,.0f}')\n    rmse = np.sqrt(((x-y).pow(2)).mean())\n    print(\"RMSE=\",rmse)\n\nApproach 1: Using the approach we used in Section 7.1.5.3\n\n#Finding the mean GDP per capita of the continent\navg_gdpPerCapita = gdp_missing_data['gdpPerCapita'].groupby(gdp_missing_data['continent']).mean()\n\n#Creating a copy of missing data to impute missing values\ngdp_imputed_data = gdp_missing_data.copy()\n\n#Replacing missing GDP per capita with the mean GDP per capita for the corresponding continent\nfor cont in avg_gdpPerCapita.index:\n    gdp_imputed_data.loc[(gdp_imputed_data.continent==cont) & (gdp_imputed_data.gdpPerCapita.isnull()),\n                     'gdpPerCapita']=avg_gdpPerCapita[cont]\nplot_actual_vs_predicted()\n\nRMSE= 25473.20645170116\n\n\n\n\n\nApproach 2: Using the groupby() and apply() methods.\n\n#Creating a copy of missing data to impute missing values\ngdp_imputed_data = gdp_missing_data.copy()\n\n#Grouping data by continent\ngrouped = gdp_missing_data.groupby('continent')\n\n#Applying the lambda function on the 'gdpPerCapita' column of the groups\ngdp_imputed_data['gdpPerCapita'] = grouped['gdpPerCapita'].apply(lambda x: x.fillna(x.mean()))\n\nplot_actual_vs_predicted()\n\nRMSE= 25473.20645170116\n\n\n\n\n\nWith the apply() function, the missing value of gdpPerCapita for observations of each group are filled by the mean gdpPerCapita of that group. The code is not only more convenient to write, but also faster as compared to for loops. The for loop imputes the missing values of observations of one group at a time, while the imputation may happen in parallel for all groups with the apply() function.\n\n\n9.3.4 Sampling data by group\nThe groupby() and apply() method can be used to for stratified random sampling from a large dataset.\nThe spotify dataset has more than 200k observations. It may be expensive to operate with so many observations. Suppose, we wish to take a random sample of 650 observations to analyze spotify data, such that all genres are equally represented.\nBefore taking the random sample, let us find the number of tracks in each genre.\n\nspotify_data.genres.value_counts()\n\npop              70441\nrock             49785\npop & rock       43437\nmiscellaneous    35848\njazz             13363\nhoerspiel        12514\nhip hop           7373\nfolk              2821\nlatin             2125\nrap               1798\nmetal             1659\ncountry           1236\nelectronic         790\nName: genres, dtype: int64\n\n\nLet us take a random sample of 650 observations from the entire dataset.\n\nsample_spotify_data = spotify_data.sample(650)\n\nNow, let us see the number of track of each genre in our sample.\n\nsample_spotify_data.genres.value_counts()\n\npop              185\nrock             150\nmiscellaneous    102\npop & rock        98\njazz              37\nhoerspiel         25\nhip hop           22\nrap                7\nmetal              7\nlatin              5\ncountry            5\nfolk               5\nelectronic         2\nName: genres, dtype: int64\n\n\nSome of the genres have a very low representation in the data. To rectify this, we can take a random sample of 50 observations from each of the 13 genres. In other words, we can take a random sample from each of the genre-based groups.\n\nevenly_sampled_spotify_data = spotify_data.groupby('genres').apply(lambda x:x.sample(50))\nevenly_sampled_spotify_data.genres.value_counts()\n\nhoerspiel        50\nhip hop          50\npop & rock       50\nlatin            50\ncountry          50\nrap              50\nelectronic       50\nmetal            50\nfolk             50\npop              50\nmiscellaneous    50\nrock             50\njazz             50\nName: genres, dtype: int64\n\n\nThe above stratified random sample equally represents all the genres."
  },
  {
    "objectID": "Data aggregation.html#corr-correlation-by-group",
    "href": "Data aggregation.html#corr-correlation-by-group",
    "title": "9  Data aggregation",
    "section": "9.4 corr(): Correlation by group",
    "text": "9.4 corr(): Correlation by group\nThe corr() method of the GroupBy object returns the correlation between all pairs of columns within each group.\nExample: Find the correlation between danceability and track popularity for each genre-energy level combination.\n\nspotify_data.groupby(['genres','energy_lvl']).apply(lambda x:x['danceability'].corr(x['track_popularity']))\n\ngenres         energy_lvl \ncountry        Low energy    -0.171830\n               High energy   -0.154823\nelectronic     Low energy     0.378330\n               High energy    0.072343\nfolk           Low energy     0.187482\n               High energy    0.230419\nhip hop        Low energy     0.113421\n               High energy    0.027074\nhoerspiel      Low energy    -0.053908\n               High energy   -0.044211\njazz           Low energy     0.005733\n               High energy    0.332356\nlatin          Low energy    -0.083971\n               High energy    0.030276\nmetal          Low energy     0.127439\n               High energy    0.256165\nmiscellaneous  Low energy     0.163185\n               High energy    0.148818\npop            Low energy     0.208942\n               High energy    0.156764\npop & rock     Low energy     0.063127\n               High energy    0.060195\nrap            Low energy    -0.008394\n               High energy   -0.129873\nrock           Low energy     0.027876\n               High energy    0.065908\ndtype: float64\n\n\nThe popularity of low energy electronic music is the most correlated with its danceability."
  },
  {
    "objectID": "Data aggregation.html#pivot_table",
    "href": "Data aggregation.html#pivot_table",
    "title": "9  Data aggregation",
    "section": "9.5 pivot_table()",
    "text": "9.5 pivot_table()\nThe Pandas pivot_table() function is used to aggregate data groupwise where some of the group keys are along the rows and some along the columns. Note that pivot_table() is the same as pivot() except that pivot_table() aggregates the data as well in addition to re-arranging it.\nExample: Find the mean of track popularity for each genre-energy lvl combination such that each row corresponds to a genre, and the energy levels correspond to columns.\n\npd.pivot_table(data = spotify_data,values = 'track_popularity',index = 'genres', columns ='energy_lvl',aggfunc = 'mean',margins = True)\n\n\n\n\n\n\n\nenergy_lvl\nLow energy\nHigh energy\nAll\n\n\ngenres\n\n\n\n\n\n\n\ncountry\n34.982069\n49.859100\n41.132686\n\n\nelectronic\n43.754789\n43.005671\n43.253165\n\n\nfolk\n29.617831\n29.991957\n29.716767\n\n\nhip hop\n50.283669\n48.012067\n48.700665\n\n\nhoerspiel\n31.534779\n30.514032\n31.258670\n\n\njazz\n19.421085\n25.715373\n20.349472\n\n\nlatin\n34.308370\n39.992605\n37.563765\n\n\nmetal\n38.612403\n46.985621\n46.334539\n\n\nmiscellaneous\n34.157235\n39.394186\n36.167401\n\n\npop\n34.722631\n40.597155\n37.783194\n\n\npop & rock\n32.987221\n37.413357\n35.619242\n\n\nrap\n57.177966\n48.225166\n51.162959\n\n\nrock\n34.654871\n38.256199\n36.749623\n\n\nAll\n33.015545\n39.151701\n36.080772\n\n\n\n\n\n\n\nWe can use also use custom GroupBy aggregate functions with pivot_table().\nExample: Find the \\(90^{th}\\) percentile of track popularity for each genre-energy lvl combination such that each row corresponds to a genre, and the energy levels correspond to columns.\n\npd.pivot_table(data = spotify_data,values = 'track_popularity',index = 'genres', columns ='energy_lvl',aggfunc = lambda x:np.percentile(x,90))\n\n\n\n\n\n\n\nenergy_lvl\nLow energy\nHigh energy\n\n\ngenres\n\n\n\n\n\n\ncountry\n64.0\n69.0\n\n\nelectronic\n59.0\n61.2\n\n\nfolk\n51.0\n49.0\n\n\nhip hop\n66.0\n63.0\n\n\nhoerspiel\n37.0\n38.0\n\n\njazz\n41.0\n50.0\n\n\nlatin\n53.0\n58.0\n\n\nmetal\n60.4\n64.0\n\n\nmiscellaneous\n56.0\n59.0\n\n\npop\n59.0\n61.0\n\n\npop & rock\n54.0\n57.0\n\n\nrap\n75.0\n74.0\n\n\nrock\n54.0\n57.0"
  },
  {
    "objectID": "Data aggregation.html#crosstab",
    "href": "Data aggregation.html#crosstab",
    "title": "9  Data aggregation",
    "section": "9.6 crosstab()",
    "text": "9.6 crosstab()\nThe crosstab() method is a special case of a pivot table for computing group frequncies (or size of each group). We may often use it to check if the data is representative of all groups that are of interest to us.\nExample: Find the number of observations in each group, where each groups corresponds to a distinct genre-energy lvl combination\n\n#Cross tabulation of 'genres' and 'energy_lvl'\npd.crosstab(spotify_data.genres,spotify_data.energy_lvl,margins = True).sort_values(by = 'All',ascending = False)\n\n\n\n\n\n\n\nenergy_lvl\nLow energy\nHigh energy\nAll\n\n\ngenres\n\n\n\n\n\n\n\nAll\n121708\n121482\n243190\n\n\npop\n33742\n36699\n70441\n\n\nrock\n20827\n28958\n49785\n\n\npop & rock\n17607\n25830\n43437\n\n\nmiscellaneous\n22088\n13760\n35848\n\n\njazz\n11392\n1971\n13363\n\n\nhoerspiel\n9129\n3385\n12514\n\n\nhip hop\n2235\n5138\n7373\n\n\nfolk\n2075\n746\n2821\n\n\nlatin\n908\n1217\n2125\n\n\nrap\n590\n1208\n1798\n\n\nmetal\n129\n1530\n1659\n\n\ncountry\n725\n511\n1236\n\n\nelectronic\n261\n529\n790\n\n\n\n\n\n\n\nThe above table can be generated with the pivot_table() function using ‘count’ as the aggfunc argument, as shown below. However, the crosstab() function is more compact to code.\n\n#Generating cross-tabulation of 'genres' and 'energy_lvl' with 'pivot_table()'\npd.pivot_table(data = spotify_data,values = 'track_popularity',index = 'genres', columns ='energy_lvl',aggfunc = 'count',margins=True)\n\n\n\n\n\n\n\nenergy_lvl\nLow energy\nHigh energy\nAll\n\n\ngenres\n\n\n\n\n\n\n\ncountry\n725\n511\n1236\n\n\nelectronic\n261\n529\n790\n\n\nfolk\n2075\n746\n2821\n\n\nhip hop\n2235\n5138\n7373\n\n\nhoerspiel\n9129\n3385\n12514\n\n\njazz\n11392\n1971\n13363\n\n\nlatin\n908\n1217\n2125\n\n\nmetal\n129\n1530\n1659\n\n\nmiscellaneous\n22088\n13760\n35848\n\n\npop\n33742\n36699\n70441\n\n\npop & rock\n17607\n25830\n43437\n\n\nrap\n590\n1208\n1798\n\n\nrock\n20827\n28958\n49785\n\n\nAll\n121708\n121482\n243190\n\n\n\n\n\n\n\nExample: Find the percentage of observations in each group of the above table.\n\npd.crosstab(spotify_data.genres,spotify_data.energy_lvl,margins = True).sort_values(by = 'All',ascending = False)/spotify_data.shape[0]*100\n\n\n\n\n\n\n\nenergy_lvl\nLow energy\nHigh energy\nAll\n\n\ngenres\n\n\n\n\n\n\n\nAll\n50.046466\n49.953534\n100.000000\n\n\npop\n13.874748\n15.090670\n28.965418\n\n\nrock\n8.564086\n11.907562\n20.471648\n\n\npop & rock\n7.240018\n10.621325\n17.861343\n\n\nmiscellaneous\n9.082610\n5.658127\n14.740738\n\n\njazz\n4.684403\n0.810477\n5.494881\n\n\nhoerspiel\n3.753855\n1.391916\n5.145771\n\n\nhip hop\n0.919034\n2.112751\n3.031786\n\n\nfolk\n0.853242\n0.306756\n1.159998\n\n\nlatin\n0.373371\n0.500432\n0.873802\n\n\nrap\n0.242609\n0.496731\n0.739340\n\n\nmetal\n0.053045\n0.629138\n0.682183\n\n\ncountry\n0.298121\n0.210124\n0.508245\n\n\nelectronic\n0.107323\n0.217525\n0.324849\n\n\n\n\n\n\n\n\n9.6.1 Practice exercise 3\nWhat percentage of unique tracks are contributed by the top 5 artists of each genre?\nHint: Find the top 5 artists based on artist_popularity for each genre. Count the total number of unique tracks (track_name) contributed by these artists. Divide this number by the total number of unique tracks in the data. The nunique() function will be useful.\nSolution:\n\ntop5artists = spotify_data.iloc[:,0:4].drop_duplicates().groupby('genres').apply(lambda x:x.sort_values(by = 'artist_popularity', \n                            ascending = False)[0:5]).droplevel(axis=0,level=1)['artist_name']\ntop5artists_tracks = spotify_data.loc[spotify_data.artist_name.isin(top5artists),:]\ntop5artists_tracks.track_name.nunique()/spotify_data.track_name.nunique()"
  },
  {
    "objectID": "student_solutions.html#missing-value-imputation-based-on-correlated-variables-in-the-data",
    "href": "student_solutions.html#missing-value-imputation-based-on-correlated-variables-in-the-data",
    "title": "10  Alternative solutions by students",
    "section": "10.1 Missing value imputation based on correlated variables in the data",
    "text": "10.1 Missing value imputation based on correlated variables in the data\nThe code below refers to Section 7.1.5.3 of the book. Students have proposed some ways to avoid a for loop in the code, which will lead to parallel computations, thereby saving execution time.\n\n10.1.1 Original code (in the book)\n\nstart_time = tm.time()\ngdp_imputed_data = gdp_missing_values_data.copy()\nfor cont in avg_gdpPerCapita.index:\n    gdp_imputed_data.loc[(gdp_imputed_data.continent==cont) & (gdp_imputed_data.gdpPerCapita.isnull()),\n                     'gdpPerCapita']=avg_gdpPerCapita[cont]\nplot_actual_vs_predicted()\nprint(\"Time taken to execute code: \",np.round(tm.time()-start_time,3), \"seconds\")\n\nRMSE= 25473.20645170116\nTime taken to execute code:  0.032 seconds\n\n\n\n\n\nBelow are some more efficient ways to impute the missing values, as they avoid using the for loop.\n\n\n10.1.2 Alternative code 1:\nBy Victoria Shi\n\nstart_time = tm.time()\ngdp_imputed_data = gdp_missing_values_data.copy()\ngdp_imputed_data[\"gdpPerCapita\"] =  gdp_missing_values_data[['continent','gdpPerCapita']].groupby('continent').transform(lambda x: x.fillna(x.mean()))\nplot_actual_vs_predicted()\nprint(\"Time taken to execute code: \",np.round(tm.time()-start_time,3), \"seconds\")\n\nRMSE= 25473.20645170116\nTime taken to execute code:  0.024 seconds\n\n\n\n\n\n\n\n10.1.3 Alternative code 2:\nBy Elijah Nacar\n\nstart_time = tm.time()\ngdp_imputed_data = gdp_missing_values_data.copy()\ngdp_imputed_data.gdpPerCapita = gdp_imputed_data.apply(lambda x: avg_gdpPerCapita[x['continent']] if pd.isnull(x['gdpPerCapita']) else x['gdpPerCapita'],axis=1)\nplot_actual_vs_predicted()\nprint(\"Time taken to execute code: \",np.round(tm.time()-start_time,3), \"seconds\")\n\nRMSE= 25473.20645170116\nTime taken to execute code:  0.024 seconds\n\n\n\n\n\n\n\n10.1.4 Alternative code 3:\nBy Erica Zhou\n\nstart_time = tm.time()\ngdp_imputed_data = gdp_missing_values_data.copy()\n\ncond_list = [gdp_imputed_data['continent'] == 'Africa',gdp_imputed_data['continent'] == 'Asia',\n             gdp_imputed_data['continent'] == 'Europe',gdp_imputed_data['continent'] == 'North America',\n             gdp_imputed_data['continent'] == 'Oceania',gdp_imputed_data['continent'] == 'South America']\n\nchoice_list = list(avg_gdpPerCapita)\n\ngdp_imputed_data.gdpPerCapita = np.select(cond_list, choice_list, gdp_imputed_data.gdpPerCapita)\nplot_actual_vs_predicted()\nprint(\"Time taken to execute code: \",np.round(tm.time()-start_time,3), \"seconds\")\n\nRMSE= 25473.20645170116\nTime taken to execute code:  0.024 seconds"
  },
  {
    "objectID": "student_solutions.html#binning-with-equal-sized-bins",
    "href": "student_solutions.html#binning-with-equal-sized-bins",
    "title": "10  Alternative solutions by students",
    "section": "10.2 Binning with equal sized bins",
    "text": "10.2 Binning with equal sized bins\nThe code below refers to Section 7.2.2 of the book. A student has proposed a way to avoid the for loop in the code, which will lead to parallel computations, thereby saving execution time.\n\n10.2.1 Original code (in the book)\n\n#Bootstrapping to find 95% confidence intervals of Graduation Rate of US universities based on average expenditure per student\nstart_time = tm.time()\nfor expend_bin in college.Expend_bin.unique():\n    data_sub = college.loc[college.Expend_bin==expend_bin,:]\n    samples = np.random.choice(data_sub['Grad.Rate'], size=(10000,data_sub.shape[0]))\n    print(\"95% Confidence interval of Grad.Rate for \"+expend_bin+\" univeristies = [\"+str(np.round(np.percentile(samples.mean(axis=1),2.5),2))+\",\"+str(np.round(np.percentile(samples.mean(axis=1),97.5),2))+\"]\")\nprint(\"Time taken to execute code: \",np.round(tm.time()-start_time,3), \"seconds\")\n\n95% Confidence interval of Grad.Rate for Low expend univeristies = [55.31,59.35]\n95% Confidence interval of Grad.Rate for High expend univeristies = [71.03,74.9]\n95% Confidence interval of Grad.Rate for Med expend univeristies = [64.17,67.95]\nTime taken to execute code:  0.151 seconds\n\n\n\n\n10.2.2 Alternative code:\nBy Victoria Shi\n\nstart_time = tm.time()\ndef confidence_interval(df):\n    samples = np.random.choice(df['Grad.Rate'], size=(10000, df.shape[0]))\n    print(\"95% Confidence interval of Grad.Rate for \"+df[\"Expend_bin\"].iloc[0]+\" univeristies = [\"+str(np.round(np.percentile(samples.mean(axis=1),2.5),2))+\",\"+str(np.round(np.percentile(samples.mean(axis=1),97.5),2))+\"]\")\ncollege.groupby('Expend_bin').apply(confidence_interval)\nprint(\"Time taken to execute code: \",np.round(tm.time()-start_time,3), \"seconds\")\n\n95% Confidence interval of Grad.Rate for Low expend univeristies = [55.35,59.35]\n95% Confidence interval of Grad.Rate for Med expend univeristies = [64.16,67.95]\n95% Confidence interval of Grad.Rate for High expend univeristies = [71.05,74.96]\nTime taken to execute code:  0.139 seconds"
  },
  {
    "objectID": "Assignment A.html#instructions",
    "href": "Assignment A.html#instructions",
    "title": "Appendix A — Assignment A",
    "section": "Instructions",
    "text": "Instructions\n\nYou may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 8th October 2023 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (2 pts).\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)"
  },
  {
    "objectID": "Assignment A.html#list-comprehension",
    "href": "Assignment A.html#list-comprehension",
    "title": "Appendix A — Assignment A",
    "section": "A.1 List comprehension",
    "text": "A.1 List comprehension\nUSA’s GDP per capita from 1960 to 2021 is given by the tuple T in the code cell below. The values are arranged in ascending order of the year, i.e., the first value is for 1960, the second value is for 1961, and so on.\n\nT = (3007, 3067, 3244, 3375,3574, 3828, 4146, 4336, 4696, 5032,5234,5609,6094,6726,7226,7801,8592,9453,10565,11674,12575,13976,14434,15544,17121,18237,19071,20039,21417,22857,23889,24342,25419,26387,27695,28691,29968,31459,32854,34515,36330,37134,37998,39490,41725,44123,46302,48050,48570,47195,48651,50066,51784,53291,55124,56763,57867,59915,62805,65095,63028,69288)\n\n\nA.1.1 \nUse list comprehension to create a list of the gaps between consecutive entries in T, i.e, the increase in GDP per capita with respect to the previous year. The list with gaps should look like: [60, 177, …]. Let the name of this list be GDP_increase.\n(4 points)\n\n\nA.1.2 \nUse GDP_increase to find the maximum gap size, i.e, the maximum increase in GDP per capita.\n(1 point)\n\n\nA.1.3 \nUse list comprehension with GDP_increase to find the percentage of gaps that have size greater than $1000.\n(3 points)\n\n\nA.1.4 \nUse list comprehension with GDP_increase to print the list of years in which the GDP per capita increase was more than $2000.\nHint: The enumerate() function may help.\n(4 points)\n\n\nA.1.5 \nUse list comprehension to:\n\nCreate a list that consists of the difference between the maximum and minimum GDP per capita values for each of the 5 year-periods starting from 1976, i.e., for the periods 1976-1980, 1981-1985, 1986-1990, …, 2016-2020.\nFind the five year period in which the difference (between the maximum and minimum GDP per capita values) was the least.\n\n(4 + 2 points)"
  },
  {
    "objectID": "Assignment A.html#nested-list-comprehension",
    "href": "Assignment A.html#nested-list-comprehension",
    "title": "Appendix A — Assignment A",
    "section": "A.2 Nested list-comprehension",
    "text": "A.2 Nested list-comprehension\nBelow is the list consisting of the majors / minors of students of the course STAT303-1 Fall 2023. This data is a list of lists, where each sub-list (smaller list within the outer larger list) consists of the majors / minors of a student. Most of the students have majors / minors in one or more of these four areas:\n\nMath / Statistics / Computer Science\nHumanities / Communication\nSocial Sciences / Education\nPhysical Sciences / Natural Sciences / Engineering\n\nThere are some students having majors / minors in other areas as well.\nUse list comprehension for all the questions below.\n\nmajors_minors = majors_minors = [['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Humanities / Communications',  'Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Humanities / Communications',  'Social Sciences / Education',  'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Physical Sciences / Natural Sciences / Engineering'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Cognitive Science'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Music'], ['Social Sciences / Education'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Data Science'], ['Social Sciences / Education'], ['Math / Statistics / Computer Science', 'jazz'], ['Humanities / Communications', 'Social Sciences / Education'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Humanities / Communications',  'Social Sciences / Education',  'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Econ'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', ''], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Humanities / Communications',  'Social Sciences / Education',  'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Humanities / Communications', 'Social Sciences / Education'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Social Sciences / Education'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Humanities / Communications', 'Social Sciences / Education'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education'], ['Humanities / Communications'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education',  'Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Humanities / Communications', 'Social Sciences / Education'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Humanities / Communications',  'Social Sciences / Education',  'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Humanities / Communications'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education',  'Physical Sciences / Natural Sciences / Engineering'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science', 'Music'], ['Physical Sciences / Natural Sciences / Engineering'], ['Humanities / Communications'], ['Physical Sciences / Natural Sciences / Engineering'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Physical Sciences / Natural Sciences / Engineering'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science']]\n\n\nA.2.1 \nHow many students have major / minor in any three of the above mentioned four areas?\n(1 point)\n\n\nA.2.2 \nHow many students have Math / Statistics / Computer Science as an area of their major / minor?\nHint: Nested list comprehension\n(4 points)\n\n\nA.2.3 \nHow many students have Math / Statistics / Computer Science as the only area of their major / minor?\nHint: Nested list comprehension\n(5 points)\n\n\nA.2.4 \nHow many students have Math / Statistics / Computer Science and Social Sciences / Education as a couple of areas of their major / minor?\nHint: The in-built function all() may be useful.\n(6 points)"
  },
  {
    "objectID": "Assignment A.html#dictionary",
    "href": "Assignment A.html#dictionary",
    "title": "Appendix A — Assignment A",
    "section": "A.3 Dictionary",
    "text": "A.3 Dictionary\nThe code cell below defines an object having the nutrition information of drinks in starbucks.\n\nstarbucks_drinks_nutrition={'Cool Lime Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 45}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 11}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Strawberry Acai Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 80}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 18}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Very Berry Hibiscus Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 14}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Evolution Fresh™ Organic Ginger Limeade': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Iced Coffee': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Iced Espresso Classics - Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 21}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 65}], 'Iced Espresso Classics - Caffe Mocha': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 23}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 90}], 'Iced Espresso Classics - Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 21}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 65}], 'Shaken Sweet Tea': [{'Nutrition_type': 'Calories', 'value': 80}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Berry Blossom White': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Black Mango': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 38}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Black with Lemon': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Brambleberry': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Giant Peach': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Iced Passion': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Lemon Ginger': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 31}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Black Lemonade': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Iced Black Tea': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Iced Green Tea': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 31}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Plum Pomegranate': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Tazoberry': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 38}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled White Cranberry': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Teavana® Shaken Iced Black Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Black Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Teavana® Shaken Iced Green Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Green Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Passion Tango™ Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Passion Tango™ Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 24}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Teavana® Shaken Iced Peach Green Tea': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks Refreshers™ Raspberry Pomegranate': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 27}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks Refreshers™ Strawberry Lemonade': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 27}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Doubleshot Protein Dark Chocolate': [{'Nutrition_type': 'Calories', 'value': 210}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 33}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Starbucks® Doubleshot Protein Vanilla': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 120}], 'Starbucks® Iced Coffee Caramel': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 13}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Iced Coffee Light Sweetened': [{'Nutrition_type': 'Calories', 'value': 50}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 11}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Iced Coffee Unsweetened': [{'Nutrition_type': 'Calories', 'value': 10}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 2}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Blonde Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Clover® Brewed Coffee': [{'Nutrition_type': 'Calories', 'value': 10}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Decaf Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Featured Dark Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Nariño 70 Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Nariño 70 Cold Brew with Milk': [{'Nutrition_type': 'Calories', 'value': 0}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Nitro Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Nitro Cold Brew with Sweet Cream': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 5.0}, {'Nutrition_type': 'Carb', 'value': 5}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 20}], 'Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Vanilla Sweet Cream Cold Brew': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 14}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 25}], 'Hot Chocolate': [{'Nutrition_type': 'Calories', 'value': 320}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 47}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 14}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Starbucks® Signature Hot Chocolate': [{'Nutrition_type': 'Calories', 'value': 430}, {'Nutrition_type': 'Fat', 'value': 26.0}, {'Nutrition_type': 'Carb', 'value': 45}, {'Nutrition_type': 'Fiber', 'value': 5}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 13}, {'Nutrition_type': 'Sodium', 'value': 170}], 'Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 290}, {'Nutrition_type': 'Fat', 'value': 8.0}, {'Nutrition_type': 'Carb', 'value': 42}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 13}, {'Nutrition_type': 'Sodium', 'value': 140}], 'Cappuccino': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 12}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 8}, {'Nutrition_type': 'Sodium', 'value': 100}], 'Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 260}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 40}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 11}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 32}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Flat White': [{'Nutrition_type': 'Calories', 'value': 180}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 18}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Iced Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 4.5}, {'Nutrition_type': 'Carb', 'value': 13}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 8}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Iced Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 230}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 36}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 9}, {'Nutrition_type': 'Sodium', 'value': 90}], 'Iced Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Iced Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 7}, {'Nutrition_type': 'Sodium', 'value': 95}], 'Iced Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories', 'value': 260}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 11}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Iced Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 30}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 7}, {'Nutrition_type': 'Sodium', 'value': 100}], 'Iced White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 300}, {'Nutrition_type': 'Fat', 'value': 8.0}, {'Nutrition_type': 'Carb', 'value': 47}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 190}], 'Latte Macchiato': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Starbucks Doubleshot® on Ice Beverage': [{'Nutrition_type': 'Calories', 'value': 45}, {'Nutrition_type': 'Fat', 'value': 1.0}, {'Nutrition_type': 'Carb', 'value': 5}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 3}, {'Nutrition_type': 'Sodium', 'value': 40}], 'Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 150}], 'White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 360}, {'Nutrition_type': 'Fat', 'value': 11.0}, {'Nutrition_type': 'Carb', 'value': 53}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 14}, {'Nutrition_type': 'Sodium', 'value': 240}], 'Cinnamon Dolce Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 350}, {'Nutrition_type': 'Fat', 'value': 4.5}, {'Nutrition_type': 'Carb', 'value': 64}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 15}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Coffee Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 24}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 3}, {'Nutrition_type': 'Sodium', 'value': 200}], 'Mocha Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 280}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 60}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 4}, {'Nutrition_type': 'Sodium', 'value': 220}], 'Mocha Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.5}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 4}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Cinnamon Dolce Crème': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 135}], 'Vanilla Crème': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 135}], 'Chocolate Smoothie': [{'Nutrition_type': 'Calories', 'value': 320}, {'Nutrition_type': 'Fat', 'value': 5.0}, {'Nutrition_type': 'Carb', 'value': 53}, {'Nutrition_type': 'Fiber', 'value': 8}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 170}], 'Strawberry Smoothie': [{'Nutrition_type': 'Calories', 'value': 300}, {'Nutrition_type': 'Fat', 'value': 2.0}, {'Nutrition_type': 'Carb', 'value': 60}, {'Nutrition_type': 'Fiber', 'value': 7}, {'Nutrition_type': 'Protein', 'value': 16}, {'Nutrition_type': 'Sodium', 'value': 130}]}\n\n\nA.3.1 \nWhat is the nested data structure of the object starbucks_drinks_nutrition? An example of a nested data structure can be a list of dictionaries, where the dictionary values are a tuple of dictionaries?\n(1 point)\n\n\nA.3.2 \nUse dictionary-comprehension to print the name and carb content of all drinks that have a carb content of more than 50 units.\nHint: It will be a nested dictionary comprehension.\n(6 points)"
  },
  {
    "objectID": "Assignment A.html#ted-talks",
    "href": "Assignment A.html#ted-talks",
    "title": "Appendix A — Assignment A",
    "section": "A.4 Ted talks",
    "text": "A.4 Ted talks\n\nA.4.1 \nRead the data on ted talks from 2006 to 2017.\n(1 point)\n\n\nA.4.2 \nFind the number of talks in the dataset.\n(1 point)\n\n\nA.4.3 \nFind the headline, speaker and year_filmed of the talk with the highest number of views.\n(4 points)\n\n\nA.4.4 \nDo the majority of talks have less views than the average number of views for a talk? Justify your answer.\n(3 points)\nHint: Print summary statistics for questions (4) and (5).\n\n\nA.4.5 \nDo at least 25% of the talks have more views than the average number of views for a talk? Justify your answer.\n(3 points)\n\n\nA.4.6 \nThe last column of the dataset consists of votes obtained by the talk under different categories, such as Funny, Confusing, Fascinating, etc. For each category, create a new column in the dataset that contains the votes obtained by the tedtalk in that category. Print the first 5 rows of the updated dataset.\n(8 points)\n\n\nA.4.7 \nWith the data created in (a), find the headline of the talk that received the highest number of votes as Confusing.\n(4 points)\n\n\nA.4.8 \nWith the data created in (a), find the headline and the year of the talk that received the highest percentage of votes in the Fascinating category.\n\\[\\text{Percentage of } \\textit{Fascinating} \\text{ votes for a ted talk} = \\frac{Number \\ of \\  votes \\ in \\ the \\ category \\ `Fascinating`}{Total \\ votes \\ in \\ all  \\ categories}\\]\n(7 points)"
  },
  {
    "objectID": "Assignment A.html#university-rankings",
    "href": "Assignment A.html#university-rankings",
    "title": "Appendix A — Assignment A",
    "section": "A.5 University rankings",
    "text": "A.5 University rankings\n\nA.5.1 \nDownload the data set “univ.txt”. Read it with python.\n(1 point)\n\n\nA.5.2 \nFind summary statistics of the data. Based on the statistics, answer the next four questions.\n(1 point)\n\n\nA.5.3 \nHow many universities are there in the data set?\n(1 point)\n\n\nA.5.4 \nEstimate the maximum Tuition and fees among universities that are in the bottom 25% when ranked by total tuition and fees.\n(2 points)\n\n\nA.5.5 \nHow many universities share the ranking of 220? (If s universities share the same rank, say r, then the next lower rank is r+s, and all the ranks in between r and r+s are dropped)\n(4 points)\n\n\nA.5.6 \nCan you find the mean Tuition and fees for an undergrad student in the US from the summary statistics? Justify your answer.\n(3 points)\n\n\nA.5.7 \nFind the average Tuition and fees for an undergrad student in the US.\n(5 points)"
  },
  {
    "objectID": "Assignment A.html#file-formats",
    "href": "Assignment A.html#file-formats",
    "title": "Appendix A — Assignment A",
    "section": "A.6 File formats",
    "text": "A.6 File formats\nConsider the file formats - csv, JSON, txt. Mention one advantage and one disadvantage of each format over the other two formats.\n(2+2+2 = 6 points)"
  },
  {
    "objectID": "Assignment B.html#instructions",
    "href": "Assignment B.html#instructions",
    "title": "Appendix B — Assignment B",
    "section": "Instructions",
    "text": "Instructions\n\nYou may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 15th October 2023 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (2 pts).\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)"
  },
  {
    "objectID": "Assignment B.html#air-quality-sensors",
    "href": "Assignment B.html#air-quality-sensors",
    "title": "Appendix B — Assignment B",
    "section": "B.1 Air quality sensors",
    "text": "B.1 Air quality sensors\n(An application of broadcasting NumPy arrays)\nAir quality sensors are used to measure the amount of contaminants in air. This question will guide you in finding the location of installing 50 air quality sensors in the State of Colorado, such that they are as far away from each other as possible. The approach below is a greedy algorithm to find an approximate Maximin design.\nThe file colorado_coordinate_grid.txt contains the coordinate-pairs (latitude and longitude) of potential locations for installing an air quality sensor.\n\nB.1.1 Data\nRead the file with NumPy. How many coordinate-pairs are there in the file?\nNote that:\n\nA coordinate-pair means a latitude-longitude pair.\n‘Air quality sensor’ will be referred as ‘sensor’ in the questions below for brevity.\n\n(4 points)\n\n\nB.1.2 First sensor\nThe first sensor is to be installed closest to Denver (closest in terms of Euclidean distance). Find the coordinate-pair of the location where the first sensor will be installed. The coordinate-pair of Denver is: [39.7392\\(^{\\circ}\\) N, 104.9903\\(^{\\circ}\\) W]\nNote that the suffixes \\(^{\\circ}\\) N and \\(^{\\circ}\\) W are omitted in the file colorado_coordinate_grid.txt.\nHint: Broadcasting\n(4 points)\n\n\nB.1.3 Second sensor\nFind the coordinate-pair of the installation of the next sensor, such that it is as far as possible from the first sensor installed near Denver.\nHint: Broadcasting\n(4 points)\n\n\nB.1.4 First two sensors\nStack the coordinate-pairs of the first and second sensors vertically to obtain a 2 x 2 NumPy array. Name the array as air_sensor_coordinates.\nRun the code below to check if your results seem correct. The coordinate-pairs of the two air quality sensors will be marked as blue dots.\n(4 points)\n\n\nCode\nimport matplotlib.pyplot as plt\ndef sensor_viz():\n    img = plt.imread(\"colorado.jpg\")\n    fig, ax = plt.subplots(figsize=(10, 100),dpi=80)\n    fig.set_size_inches(10.5, 15)\n    ax.imshow(img,extent=[-109, -102, 37, 41])\n    plt.scatter(y = air_sensor_coordinates[:,0], x = -air_sensor_coordinates[:,1])\n    plt.xlim(-109.05,-101.95)\n    plt.ylim(36.95,41.05)\n    plt.xlabel(\"Longitude\")\n    plt.ylabel(\"Latitude\")\nsensor_viz()\n\n\n\n\nB.1.5 Third sensor\nNow you need to find the coordinate-pair for installing the third sensor such that it is far away from the two already-installed sensors. Proceed as follows:\n\nFind the minimum distance of each coordinate-pair in colorado_coordinate_grid.txt from the two already installed sensors. For example, if a coordinate-pair is at a distance of 5 units from the first sensor, and 10 units from the second sensor, then its minimum distance from the sensors will be \\(\\min(5,10) = 5\\) units.\nSelect the coordinate-pair (from colorado_coordinate_grid.txt) whose minimum distance from the two already installed sensors is the maximum.\nStack the coordinate-pair of the third air quality sensor vertically on the array air_sensor_coordinates.\n\nCall the function sensor_viz() to check if your results seem correct. The coordinate-pairs of the three air quality sensors will be marked as blue dots.\nHint:\nFor step (1) above:\n\nDefine a function which computes the distances of a coordinate-pair from all the coordinates of air_sensor_coordinates, and returns the minimum distance.\nApply the function on all the coordinate-pairs in colorado_coordinate_grid.txt using the NumPy function apply_along_axis().\n\n(20 points)\n\n\nB.1.6 All 50 sensors\nYou need to find 47 more coordinate-pairs to install air quality sensors well-spread across Colorado. We will generalize the steps in the previous question to proceed as follows:\n\nSuppose you have already found the coordinate-pairs for the installation of i sensors.\nFind the minimum distance of each coordinate in colorado_coordinate_grid.txt from the i already installed sensors. For example, if a coordinate-pair is at a distance of \\(d_1\\) from the first sensor, \\(d_2\\) from the second sensor,…, and \\(d_i\\) from the \\(i^{th}\\) sensor, then its minimum distance from the sensors will be \\(min(d_1, d_2, ..., d_i\\)).\nSelect the \\(i+1^{th}\\) coordinate-pair (from colorado_coordinate_grid.txt) as the one whose minimum distance from the \\(i\\) already installed sensors is the maximum.\n\nCall the function sensor_viz() to check if your results seem correct. You should see 50 blue dots well spread across Colorado.\n(10 points)"
  },
  {
    "objectID": "Assignment B.html#sales",
    "href": "Assignment B.html#sales",
    "title": "Appendix B — Assignment B",
    "section": "B.2 Sales",
    "text": "B.2 Sales\n(An application of matrix multiplication with NumPy arrays)\nWhen the monthly sales of a product are subject to seasonal fluctuations, a curve that approximates the sales formula might have the form:\n\\[y = a + b*x + c*\\sin\\bigg(2*\\pi*\\frac{x}{12}\\bigg),\\]\nwhere \\(x\\) is the time since the starting point in months and \\(y\\) is the monthly sales in USD (million). The term \\(a + b*x\\) gives the basic sales trend and the \\(\\sin\\) term reflects the seasonal changes in sales. Suppose the model parameters (i.e., \\(a\\), \\(b\\), and \\(c\\)) are estimated and put on the list below for the sales of a certain brand of sunscreen starting June 1, 2017.\n\n\nCode\nmodel_parameters = [2, 5, 18]\n\n\nThen, the total monthly sales in June 2017 will be calculated by plugging 1 as \\(x\\) into the equation.\nUsing matrix multiplication with NumPy, we wish to estimate the total sales between June 1 2017 and March 1, 2020. (So many models failed to predict sales after that - probably due to covid.)\nProceed as follows.\n\nB.2.1 Create first array\nCreate a numpy array where the first column is all \\(1\\)s, the second column is a range of numbers from 1 to the total number of months from June 1 2017 to March 1 2020 and the third column is \\(\\sin(2*\\pi*x/12)\\) values with \\(x\\) values as plugged-in in the second column.\n(10 points)\n\n\nB.2.2 Create second array\nCreate an array from the list model_parameters.\n(3 points)\n\n\nB.2.3 Multiply arrays\nUse matrix multiplication to get the monthly sales estimates for each month in the range: June 1 2017 and March 1, 2020.\n(8 points)\n\n\nB.2.4 Sum array elements\nFind the total sales between June 1 2017 and March 1, 2020.\n(3 points)"
  },
  {
    "objectID": "Assignment B.html#exercise-minutes",
    "href": "Assignment B.html#exercise-minutes",
    "title": "Appendix B — Assignment B",
    "section": "B.3 Exercise minutes",
    "text": "B.3 Exercise minutes\n(An application of parallel computation with NumPy)\nThis problem demonstrates the benefit of generating pseudo random number matrix with NumPy.\nThe list exercise_minutes below consists of exercise minutes per week of the students of STAT303-1 Fall 2022 class.\nWe wish to find the 95% confidence interval of mean exercise_minutes, using Bootstrapping.\nBootstrapping is a non-parametric method for obtaining confidence interval. The method is as follows.\n\nSuppose the list exercise_minutes has \\(N\\) values.\nRandomly sample \\(N\\) values with replacement from exercise_minutes\nFind the mean of the \\(N\\) values obtained in (b)\nRepeat steps (b) and (c) 10,000 times\nThe 95% Confidence interval is the range between the 2.5% and 97.5% percentile values of the 10,000 means obtained in (c)\n\n\n\nCode\nexercise_minutes=[240, 180, 60, 300, 0, 360, 60, 140, 60, 0, 150, 60, 0, 6, 60, 300, 90, 100, 250, 240, 300, 630, 420, 50, 0, 60, 240, 300, 180, 420, 90, 8, 180, 15, 8, 150, 180, 240, 60, 1200, 210, 360, 720, 240, 360, 240, 250, 180, 600, 120, 60, 200, 360, 120, 20, 250, 60, 420, 420, 150, 350, 180, 14, 60, 450, 180, 300, 1, 180, 7, 180, 300, 70, 40, 300, 60, 180, 225, 90, 300, 240, 200, 60, 200, 360, 3, 200, 300, 90, 60, 180, 120, 10, 0, 200, 700, 300, 300, 5, 60, 420, 300, 240, 200, 180, 180, 120, 300, 375, 60, 240, 180, 180, 90, 240, 180, 15, 300, 60, 120, 120, 240, 400, 200, 60, 480, 120, 300, 180, 250, 280, 7, 600, 240, 0, 420, 60, 2, 280, 300, 60, 0, 250, 180, 540, 30, 210, 2, 90, 120, 180, 240, 540, 400, 120, 150, 360, 180, 200, 180, 30, 60, 300, 80, 60, 210, 315, 360, 275, 200, 150, 180, 200, 150, 0, 1200, 240, 120, 300, 360, 180, 240, 630, 250, 240, 5, 30, 0, 300, 60, 90]\n\n\nAnswer the following questions.\n\nB.3.1 Sequential computation without NumPy\nWithout using NumPy, compute the:\n\nConfidence interval of mean exercise_minutes, and\nTime taken to execute the code\n\nHints:\n\nYou may use the library random.\nYou may use the library time for computing the time taken to execute the code.\n\n(12 points)\n\n\nB.3.2 Parallel computation with NumPy\nUsing NumPy, and without using loops, compute the:\n\nConfidence interval of mean exercise_minutes, and\nTime taken to execute the code\n\n(12 points)\n\n\nB.3.3 Time saving with NumPy\nReport the ratio of time taken to execute the code wihout NumPy to the time taken to execute the code with NumPy.\n(1 point)"
  },
  {
    "objectID": "Assignment C.html#instructions",
    "href": "Assignment C.html#instructions",
    "title": "Appendix C — Assignment C",
    "section": "Instructions",
    "text": "Instructions\n\nYou may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 22nd October 2023 at 11:59 pm. There is an optional bonus question worth 10 points. You can score a maximum of 110 (out of 100) points.\nYou are not allowed to use a for loop or any other kind of loop in this assignment."
  },
  {
    "objectID": "Assignment C.html#gdp-per-capita-social-indicators",
    "href": "Assignment C.html#gdp-per-capita-social-indicators",
    "title": "Appendix C — Assignment C",
    "section": "C.1 GDP per capita & social indicators",
    "text": "C.1 GDP per capita & social indicators\nRead the file social_indicator.txt with python. Set the first column as the index when reading the file. How many observations and variables are there in the data?\n(4 points)\n\nC.1.1 \nWhich variables have the strongest and weakest correlations with GDP per capita? Note that lifeFemale and lifeMale are the female and male life expectancies respectively.\nNote that only when the magnitude of the correlation is considered when judging a correlation as strong or weak.\n(4 points)\n\n\nC.1.2 \nDoes the male economic activity (in the column economicActivityMale) have a positive or negative correlation with GDP per capita? Did you expect the positive/negative correlation? If not, why do you think you are observing that correlation?\n(4 points)\n\n\nC.1.3 \nWhat is the rank of the US amongst all countries in terms of GDP per capita? Which countries lie immediately above, and immediately below the US in the ranking in terms of GDP per capita? The country having the highest GDP per capita ranks 1.\nNote that:\n\nThe US is mentioned as United States in the data.\nThe country with the highest GDP per capita will have rank 1, the country with the second highest GDP per capita will have rank 2, and so on.\n\nHint: rank()\n(4 points)\n\n\nC.1.4 \nWhich country or countries rank among the top 20 in terms of each of these social indicators - economicActivityFemale, economicActivityMale, gdpPerCapita, lifeFemale, lifeMale? For each of these social indicators, the country having the largest value ranks 1 for that indicator.\n(6 points)\nHint:\n\nUse rank(). Note that this method is different from the method given in the hint of the previous question. This method is of the DataFrame class, while the one in the previous question is of the Series class. This part of the hint is just for your understanding. You don’t need to write any code in this part.\nUsing rank(), get the DataFrame consisting of the ranks of countries on each of the relevant social indicators (one line of code).\nIn the DataFrame obtained in (2), filter the rows for which the maximum rank is less than or equal to 20 (one line of code).\n\n\n\nC.1.5 \nOn which social indicator among economicActivityFemale, economicActivityMale, gdpPerCapita, lifeFemale, lifeMale, illiteracyFemale, illiteracyMale, infantMortality, and totalfertilityrate does the US have its worst ranking, and what is the rank? Note that for illiteracyFemale, illiteracyMale, and infantMortality, the country having the lowest value will rank 1, in contrast to the other social indicators.\n(8 points)\n\n\nC.1.6 \nFind all the countries that have a lower GDP per capita than the US, despite having lower illiteracy rates (for both genders), higher economic activity (for both genders), higher life expectancy (for both genders), and lower infant mortality rate than the US?\n(6 points)"
  },
  {
    "objectID": "Assignment C.html#gdp-per-capita-vs-social-indicators",
    "href": "Assignment C.html#gdp-per-capita-vs-social-indicators",
    "title": "Appendix C — Assignment C",
    "section": "C.2 GDP per capita vs social indicators",
    "text": "C.2 GDP per capita vs social indicators\nWe’ll use the same data as in in the previous question. For the questions below, assume that all numeric columns, except GDP per capita, are social indicators.\n\nC.2.1 \nUse the column geographic_location to create a new column called continent. Merge the values of the geographic_location column appropriately to obtain 6 distinct values for the continent column – Asia, Africa, North America, South America, Europe and Oceania. Drop the column geographic_location. Print the first 5 observations of the updated DataFrame.\n(8 points)\nHint:\n\nUse value_counts() to see the values of geographic_location. The code if 'Asia' in 'something' will return True if ‘something’ contains the string ‘Asia’, for example, if ‘something’ is ‘North Asia’, the code with return True. This part of the hint is just for your understanding. You don’t need to write any code in this part.\nApply a lambda function on the Series geographic_location to replace a string that contains ‘Asia’ with ‘Asia’, replace a string that contains ‘Europe’ with ‘Europe’, and replace a string that contains ‘Africa’ with ‘Africa’. This will be a single line of code.\nRename the column georgaphic_location to continent.\n\n\n\nC.2.2 \nSort the column labels lexicographically. Drop the columns region and contraception. Print the first 5 observations of the updated DataFrame.\nHint: sort_index()\n(4 points)\n\n\nC.2.3 \nFind the percentage of the total countries in each continent.\nHint: One line of code with value_counts() and shape\n(4 points)\n\n\nC.2.4 \nWhich country has the highest GDP per capita? Let us call it country \\(G\\).\n(4 points)\n\n\nC.2.5 \nWe need to find the African country that is the closest to country \\(G\\) with regard to social indicators. Perform the following steps:\n\nC.2.5.1 \nStandardize each of the social indicators to a standard normal distribution so that all of them are on the same scale (remember to exclude GDP per capita from social indicators).\nHint:\n\nFor scaling a random variable to standard normal, subtract the mean from each value of the variable, and divide by its standard deviation.\nUse the apply method with a lambda function to scale all the social indicators to standard normal.\nThe above (1) and (2) together is a single line of code.\n\n(6 points)\n\n\nC.2.5.2 \nCompute the Manhattan distance between country \\(G\\) and each of the African countries, based on the scaled social indicators.\nHint:\n\nBroadcast a Series to a DataFrame\nThe Manhattan distance between two points \\((x_1, x_2, ..., x_p)\\) and \\((y_1, y_2, ..., y_p)\\) is \\(|x_1 - y_1| + |x_2 - y_2| + ... + |x_p-y_p|\\), where \\(|.|\\) stands for absolute value (for example, \\(|-2| = 2; |3| = 3\\)).\n\n(8 points)\n\n\nC.2.5.3 \nIdentify the African country, say country \\(A\\), with the least Manhattan distance to country \\(G\\).\n(8 points)\n\n\n\nC.2.6 \nFind the correlation between the Manhattan distance from country \\(G\\) and GDP per capita for African countries.\n(6 points)\n\n\nC.2.7 \nBased on the correlation coefficient in \\(2(f)\\), do you think African countries should try to emulate the social characteristics of country \\(G\\)? Justify your answer.\n(4 points)"
  },
  {
    "objectID": "Assignment C.html#medical-data",
    "href": "Assignment C.html#medical-data",
    "title": "Appendix C — Assignment C",
    "section": "C.3 Medical data",
    "text": "C.3 Medical data\nRead the data sets conditions.csv and patients.csv. Suppose we are interested in studying patients with prediabetes condition. Do not drop or compute any missing values. In condition.csv, the patient IDs are stored in column PATIENT, and the medical conditions are stored in column DESCRIPTION. In patient.csv, the patient IDs are stored in column Id.\n\nC.3.1 \nPrint the patient IDs of all the patients with prediabetes condition.\n(4 points)\n\n\nC.3.2 \nMake a subset of the data with only prediabetes patients. How many prediabetes patients are there?\n(4 points)\nHint: .isin()\n\n\nC.3.3 \nWhat proportion of the total HEALTHCARE_EXPENSES of all the patients correspond to the HEALTHCARE_EXPENSES of prediabetes patients.\n(4 points)"
  },
  {
    "objectID": "Assignment C.html#bonus-question",
    "href": "Assignment C.html#bonus-question",
    "title": "Appendix C — Assignment C",
    "section": "C.4 Bonus question",
    "text": "C.4 Bonus question\nThis is an optional question with no partial credit. You will get points only if your solution is completely correct. We advise you to attempt it only when you are done with the rest of the assignment.\n(10 points)\nRead the file STAT303-1 survey for data analysis.csv. In this question, we’ll work to clean this data a bit. As with every question, you are not allowed to use a for loop or any other loop.\nExecute the following code to read the data and clean the column names.\n\n\nCode\nsurvey_data = pd.read_csv('STAT303-1 survey for data analysis.csv')\nnew_col_names = ['parties_per_month', 'smoke', 'weed', 'introvert_extrovert', 'love_first_sight', 'learning_style', 'left_right_brained', 'personality_type', 'social_media', 'num_insta_followers', 'streaming_platforms', 'expected_marriage_age', 'expected_starting_salary', 'fav_sport', 'minutes_ex_per_week', 'sleep_hours_per_day', 'how_happy', 'farthest_distance_travelled', 'fav_number', 'fav_letter', 'internet_hours_per_day', 'only_child', 'birthdate_odd_even', 'birth_month', 'fav_season', 'living_location_on_campus', 'major', 'num_majors_minors', 'high_school_GPA', 'NU_GPA', 'age', 'height', 'height_father', 'height_mother', 'school_year', 'procrastinator', 'num_clubs', 'student_athlete', 'AP_stats', 'used_python_before', 'dominant_hand', 'childhood_in_US', 'gender', 'region_of_residence', 'political_affliation', 'cant_change_math_ability', 'can_change_math_ability', 'math_is_genetic', 'much_effort_is_lack_of_talent']\nsurvey_data.columns = list(survey_data.columns[0:2])+new_col_names\n\n\nCheck the datatype of the variables using the dtypes attribute of the Pandas DataFrame object. You will notice that only two variables are numeric. However, if you check the first few observations of the data with the function head() you will find the there are several more variables that seem to have numeric values.\nWrite a function that accepts a Pandas Series (or a column of a Pandas DataFrame object) as argument, and if the datatype of the Series is non-numeric, does the following:\n\nChecks if at least 10 values of the Series contain a digit in them.\nIf at least 10 values are found to contain a digit, then:\nA. Eliminate the characters ~, +, and , from all the values of the Series.\nB. Convert the Series to numeric (with coercion if needed).\nIf at least 10 values are NOT found to contain a digit, then:\nA. If the values of the Series are ‘Yes’ and ‘No’, then replace ‘Yes’ with 1 and ‘No’ with 0. The Series datatype must change to numeric as well.\nB. If the values of the Series are ‘Agree’ and ‘Disagree’, then replace ‘Agree’ with 1 and ‘Disagree’ with 0. The Series datatype must change to numeric as well.\n\nApply the function to each column of survey_data using the apply() method. Save the updated DataFrame as survey_data_clean. Then, execute the following code.\n\n\nCode\nsurvey_data_clean.describe().loc['mean',:]\n\n\nThe above code should print out the mean values of 28 numeric columns in the data survey_data_clean.\nThe variables that you should see as numeric in survey_data_clean are given in the list numeric_columns below (this is just to check your work):\n\n\nCode\nnumeric_columns = ['parties_per_month', 'love_first_sight', 'num_insta_followers',\n       'expected_marriage_age', 'expected_starting_salary',\n       'minutes_ex_per_week', 'sleep_hours_per_day',\n       'farthest_distance_travelled', 'fav_number', 'internet_hours_per_day',\n       'only_child', 'num_majors_minors', 'high_school_GPA', 'NU_GPA', 'age',\n       'height', 'height_father', 'height_mother', 'procrastinator',\n       'num_clubs', 'student_athlete', 'AP_stats', 'used_python_before',\n       'childhood_in_US', 'cant_change_math_ability',\n       'can_change_math_ability', 'math_is_genetic',\n       'much_effort_is_lack_of_talent']\n\n\nNote that your function must be general, i.e., it must work for any other dataset as well. This means, you cannot hard code a column name, or anything specific to survey_data in the function."
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix D — Datasets & Templates",
    "section": "",
    "text": "Datasets used in the book, and assignment / project templates can be found here"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Rubin, Donald B. 1976. “Inference and Missing Data.”\nBiometrika 63 (3): 581–92."
  }
]